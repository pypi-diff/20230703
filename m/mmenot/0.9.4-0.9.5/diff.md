# Comparing `tmp/mmenot-0.9.4-py3-none-any.whl.zip` & `tmp/mmenot-0.9.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,24 +1,24 @@
-Zip file size: 28888 bytes, number of entries: 22
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-06 10:51 __init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-May-17 05:36 mmenot/__init__.py
--rw-r--r--  2.0 unx     2984 b- defN 23-Jun-06 11:15 mmenot/exporter.py
--rw-r--r--  2.0 unx     4722 b- defN 23-Jun-23 08:26 mmenot/hyp_searcher.py
--rw-r--r--  2.0 unx     5567 b- defN 23-Jun-06 11:15 mmenot/labels.py
--rw-r--r--  2.0 unx     1454 b- defN 23-Jun-02 17:43 mmenot/patches.py
--rw-r--r--  2.0 unx     5732 b- defN 23-Jun-05 06:33 mmenot/pruner.py
--rw-r--r--  2.0 unx      689 b- defN 23-Jun-05 06:33 mmenot/runner.py
--rw-r--r--  2.0 unx     3081 b- defN 23-Jun-06 11:15 mmenot/tester.py
--rw-r--r--  2.0 unx     5044 b- defN 23-Jun-06 11:15 mmenot/trainer.py
--rw-r--r--  2.0 unx      120 b- defN 23-Jun-02 17:48 mmenot/loops/__init__.py
--rw-r--r--  2.0 unx     2700 b- defN 23-Jun-06 11:36 mmenot/loops/epoch_optuna.py
--rw-r--r--  2.0 unx     6075 b- defN 23-Jun-06 11:15 mmenot/loops/epoch_pruning.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-06 11:20 mmenot/utils/__init__.py
--rw-r--r--  2.0 unx    11727 b- defN 23-Jun-06 11:15 mmenot/utils/common.py
--rw-r--r--  2.0 unx     2185 b- defN 23-Jun-23 08:26 mmenot/utils/hpo.py
--rw-rw-rw-  2.0 unx    11338 b- defN 23-Jun-23 08:37 mmenot-0.9.4.dist-info/LICENSE
--rw-r--r--  2.0 unx    14279 b- defN 23-Jun-23 08:37 mmenot-0.9.4.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-23 08:37 mmenot-0.9.4.dist-info/WHEEL
--rw-r--r--  2.0 unx      179 b- defN 23-Jun-23 08:37 mmenot-0.9.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       16 b- defN 23-Jun-23 08:37 mmenot-0.9.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1693 b- defN 23-Jun-23 08:37 mmenot-0.9.4.dist-info/RECORD
-22 files, 79677 bytes uncompressed, 26182 bytes compressed:  67.1%
+Zip file size: 29270 bytes, number of entries: 22
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 07:09 __init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 07:09 mmenot/__init__.py
+-rw-r--r--  2.0 unx     2984 b- defN 23-Jul-03 07:04 mmenot/exporter.py
+-rw-r--r--  2.0 unx     4722 b- defN 23-Jul-03 07:04 mmenot/hyp_searcher.py
+-rw-r--r--  2.0 unx     5567 b- defN 23-Jul-03 07:04 mmenot/labels.py
+-rw-r--r--  2.0 unx     1454 b- defN 23-Jul-03 07:04 mmenot/patches.py
+-rw-r--r--  2.0 unx     5732 b- defN 23-Jul-03 07:04 mmenot/pruner.py
+-rw-r--r--  2.0 unx      689 b- defN 23-Jul-03 07:04 mmenot/runner.py
+-rw-r--r--  2.0 unx     3081 b- defN 23-Jul-03 07:04 mmenot/tester.py
+-rw-r--r--  2.0 unx     6048 b- defN 23-Jul-03 07:04 mmenot/trainer.py
+-rw-r--r--  2.0 unx      120 b- defN 23-Jul-03 07:04 mmenot/loops/__init__.py
+-rw-r--r--  2.0 unx     2700 b- defN 23-Jul-03 07:04 mmenot/loops/epoch_optuna.py
+-rw-r--r--  2.0 unx     6075 b- defN 23-Jul-03 07:04 mmenot/loops/epoch_pruning.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 07:09 mmenot/utils/__init__.py
+-rw-r--r--  2.0 unx    12560 b- defN 23-Jul-03 07:04 mmenot/utils/common.py
+-rw-r--r--  2.0 unx     2185 b- defN 23-Jul-03 07:04 mmenot/utils/hpo.py
+-rw-rw-rw-  2.0 unx    11338 b- defN 23-Jul-03 07:09 mmenot-0.9.5.dist-info/LICENSE
+-rw-r--r--  2.0 unx    14312 b- defN 23-Jul-03 07:09 mmenot-0.9.5.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-03 07:09 mmenot-0.9.5.dist-info/WHEEL
+-rw-r--r--  2.0 unx      179 b- defN 23-Jul-03 07:09 mmenot-0.9.5.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       16 b- defN 23-Jul-03 07:09 mmenot-0.9.5.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     1693 b- defN 23-Jul-03 07:09 mmenot-0.9.5.dist-info/RECORD
+22 files, 81547 bytes uncompressed, 26564 bytes compressed:  67.4%
```

## zipnote {}

```diff
@@ -42,26 +42,26 @@
 
 Filename: mmenot/utils/common.py
 Comment: 
 
 Filename: mmenot/utils/hpo.py
 Comment: 
 
-Filename: mmenot-0.9.4.dist-info/LICENSE
+Filename: mmenot-0.9.5.dist-info/LICENSE
 Comment: 
 
-Filename: mmenot-0.9.4.dist-info/METADATA
+Filename: mmenot-0.9.5.dist-info/METADATA
 Comment: 
 
-Filename: mmenot-0.9.4.dist-info/WHEEL
+Filename: mmenot-0.9.5.dist-info/WHEEL
 Comment: 
 
-Filename: mmenot-0.9.4.dist-info/entry_points.txt
+Filename: mmenot-0.9.5.dist-info/entry_points.txt
 Comment: 
 
-Filename: mmenot-0.9.4.dist-info/top_level.txt
+Filename: mmenot-0.9.5.dist-info/top_level.txt
 Comment: 
 
-Filename: mmenot-0.9.4.dist-info/RECORD
+Filename: mmenot-0.9.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## mmenot/trainer.py

```diff
@@ -5,25 +5,27 @@
 
 from mmengine.config import Config
 from mmengine.config import DictAction
 from mmengine.logging import print_log
 from mmengine.registry import RUNNERS
 from mmengine.runner import Runner
 
+from mmenot.utils.common import build_model_from_cfg
 from mmenot.utils.common import from_cfg
 from mmenot.utils.common import runner_save_checkpoint
 
 Runner.from_cfg = classmethod(from_cfg)  # type: ignore
 Runner.save_checkpoint = runner_save_checkpoint
 
 
 def parse_args():  # pylint: disable=missing-function-docstring
     parser = argparse.ArgumentParser(description='Tune a model')
     parser.add_argument('config', help='train config file path')
     parser.add_argument('checkpoint', help='checkpoint file')
+    parser.add_argument('--distill', action='store_true', default=False, help='enable distillation mode')
     parser.add_argument('--work-dir', help='the dir to save logs and models')
     parser.add_argument('--amp', action='store_true', default=False, help='enable automatic-mixed-precision training')
     parser.add_argument('--auto-scale-lr', action='store_true', help='enable automatically scaling LR.')
     parser.add_argument(
         '--resume',
         nargs='?',
         type=str,
@@ -54,14 +56,22 @@
 
     return args
 
 
 def train():  # pylint: disable=too-many-branches, missing-function-docstring
     args = parse_args()
 
+    if args.distill:
+        # pylint: disable=import-outside-toplevel
+        from mmengine.registry import MODELS
+        from mmrazor.utils import register_all_modules
+
+        register_all_modules(init_default_scope=False)  # mmrazor
+        MODELS.build_func = build_model_from_cfg
+
     try:
         # pylint: disable=import-outside-toplevel
         from mmdet.utils import setup_cache_size_limit_of_dynamo
 
         setup_cache_size_limit_of_dynamo()
     except ImportError:
         pass
@@ -77,25 +87,36 @@
         # update configs according to CLI args if args.work_dir is not None
         cfg.work_dir = args.work_dir
     elif cfg.get('work_dir', None) is None:
         # use config filename as default work_dir if cfg.work_dir is None
         cfg.work_dir = osp.join('./work_dirs', osp.splitext(osp.basename(args.config))[0])
 
     # enable automatic-mixed-precision training
-    if args.amp is True:
-        optim_wrapper = cfg.optim_wrapper.type
-        if optim_wrapper == 'AmpOptimWrapper':
-            print_log('AMP training is already enabled in your config.', logger='current', level=logging.WARNING)
-        else:
-            assert optim_wrapper == 'OptimWrapper', (
-                '`--amp` is only supported when the optimizer wrapper type is '
-                f'`OptimWrapper` but got {optim_wrapper}.'
-            )
-            cfg.optim_wrapper.type = 'AmpOptimWrapper'
-            cfg.optim_wrapper.loss_scale = 'dynamic'
+    if args.amp:
+        if getattr(cfg.optim_wrapper, 'type', None):
+            optim_wrapper = cfg.optim_wrapper.type
+            if optim_wrapper == 'AmpOptimWrapper':
+                print_log(
+                    msg='AMP training is already enabled in your config.',
+                    logger='current',
+                    level=logging.WARNING,
+                )
+            else:
+                assert optim_wrapper == 'OptimWrapper', (
+                    '`--amp` is only supported when the optimizer wrapper type is '
+                    f'`OptimWrapper` but got {optim_wrapper}.'
+                )
+                cfg.optim_wrapper.type = 'AmpOptimWrapper'
+                cfg.optim_wrapper.loss_scale = 'dynamic'
+
+        # from mmrazor
+        if getattr(cfg.optim_wrapper, 'constructor', None):
+            if cfg.optim_wrapper.architecture.type == 'OptimWrapper':
+                cfg.optim_wrapper.architecture.type = 'AmpOptimWrapper'
+                cfg.optim_wrapper.architecture.loss_scale = 'dynamic'
 
     # enable automatically scaling LR
     if args.auto_scale_lr:
         if 'auto_scale_lr' in cfg and 'enable' in cfg.auto_scale_lr and 'base_batch_size' in cfg.auto_scale_lr:
             cfg.auto_scale_lr.enable = True
         else:
             raise RuntimeError(
@@ -109,15 +130,19 @@
     if args.resume == 'auto':
         cfg.resume = True
         cfg.load_from = None
     elif args.resume is not None:
         cfg.resume = True
         cfg.load_from = args.resume
 
-    cfg['model'] = args.checkpoint  # replace model config by checkpoint path
+    # replace model config by checkpoint path
+    if args.distill:
+        cfg['model']['architecture'] = args.checkpoint
+    else:
+        cfg['model'] = args.checkpoint
 
     # build the runner from config
     if 'runner_type' not in cfg:
         # build the default runner
         runner = Runner.from_cfg(cfg)
     else:
         # build customized runner from the registry
```

## mmenot/utils/common.py

```diff
@@ -25,14 +25,16 @@
 from mmengine.dist import master_only
 from mmengine.fileio import FileClient
 from mmengine.fileio import join_path
 from mmengine.logging.logger import MMLogger
 from mmengine.model import is_model_wrapper
 from mmengine.model import revert_sync_batchnorm
 from mmengine.optim import OptimWrapper
+from mmengine.registry import Registry
+from mmengine.registry.build_functions import build_from_cfg
 from mmengine.runner import Runner
 from mmengine.runner.checkpoint import get_state_dict
 from mmengine.runner.checkpoint import save_checkpoint
 from mmengine.runner.checkpoint import weights_to_cpu
 from mmengine.utils import get_git_hash
 from onnx import ModelProto
 from onnx import TensorProto
@@ -139,14 +141,31 @@
         randomness=cfg.get('randomness', {'seed': None}),
         experiment_name=cfg.get('experiment_name'),
         cfg=cfg,
     )
     return runner
 
 
+def build_model_from_cfg(
+    cfg: Union[dict, ConfigDict, Config],
+    registry: Registry,
+    default_args: Optional[Union[dict, 'ConfigDict', 'Config']] = None,
+) -> nn.Module:
+    """Patched mmengine.registry.build_functions.build_model_from_cfg."""
+    from mmengine.model import Sequential  # pylint: disable=import-outside-toplevel
+
+    if 'architecture' in cfg:
+        cfg['architecture'] = torch.load(cfg['architecture'], map_location='cpu')
+
+    if isinstance(cfg, list):
+        modules = [build_from_cfg(_cfg, registry, default_args) for _cfg in cfg]
+        return Sequential(*modules)
+    return build_from_cfg(cfg, registry, default_args)
+
+
 @master_only
 def runner_save_checkpoint(  # pylint: disable=too-many-branches
     self,
     out_dir: str,
     filename: str,
     file_client_args: Optional[dict] = None,
     save_optimizer: bool = True,
@@ -235,14 +254,18 @@
             for scheduler in self.param_schedulers:  # type: ignore
                 state_dict = scheduler.state_dict()  # type: ignore
                 checkpoint['param_schedulers'].append(state_dict)
 
     self.call_hook('before_save_checkpoint', checkpoint=checkpoint)
 
     save_checkpoint(checkpoint, filepath)
+
+    if hasattr(model, 'architecture'):
+        model = model.architecture
+
     save_checkpoint(model, checkpoint_filepath)
 
 
 def load_model_from_checkpoint(
     self,
     model_checkpoint: Optional[str] = None,
     cfg_options: Optional[Dict] = None,
```

## Comparing `mmenot-0.9.4.dist-info/LICENSE` & `mmenot-0.9.5.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `mmenot-0.9.4.dist-info/METADATA` & `mmenot-0.9.5.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: mmenot
-Version: 0.9.4
+Version: 0.9.5
 Summary: ENOT Framework integration package for OpenMMLab codebase
 Author-email: ENOT LLC <enot@enot.ai>
 License: Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
            TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
@@ -216,14 +216,15 @@
 Classifier: Programming Language :: Python :: 3 :: Only
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: mmengine (>=0.7.2)
 Requires-Dist: mmcv (>=2.0.0)
 Requires-Dist: mmdeploy (>=1.0.0)
+Requires-Dist: mmrazor (>=1.0.0)
 Requires-Dist: onnx
 Requires-Dist: numpy
 Requires-Dist: enot-latency-server
 Requires-Dist: enot-autodl
 Requires-Dist: optuna
 Provides-Extra: dev
 Requires-Dist: black ; extra == 'dev'
```

## Comparing `mmenot-0.9.4.dist-info/RECORD` & `mmenot-0.9.5.dist-info/RECORD`

 * *Files 14% similar despite different names*

```diff
@@ -3,20 +3,20 @@
 mmenot/exporter.py,sha256=jpIA-IadAUG6dGtROQvPzbg4OvWrN_54PRI5frWQVXY,2984
 mmenot/hyp_searcher.py,sha256=kidAVMYNeRPxn7CZI-YQI0lWBqSR8g9Z8lmvao34ZJY,4722
 mmenot/labels.py,sha256=hrUDRLunoRj0e8zvWd_4C-cxmDOS-U5Cqvzz2YGBiJU,5567
 mmenot/patches.py,sha256=eAAA_cTW8L2jEhCp3_QGMKGSJia9kugy5C26lAGeZvQ,1454
 mmenot/pruner.py,sha256=aFTaD5z6xVAWrv4GvrbbYaidJeBHa2ihA9FT9BdnW_0,5732
 mmenot/runner.py,sha256=mV2Me-1EVkVWrMBFvnZyHZHc3D03q6CKHX6n0DLClNk,689
 mmenot/tester.py,sha256=STRaECaGWo88YglT6JPz3SmpqeUKjNwKI55lKIKXYg0,3081
-mmenot/trainer.py,sha256=LmQOD2D-YMjEtWS03omqffsYA33onFCBwWqttMj7NAI,5044
+mmenot/trainer.py,sha256=77aaojxZ80DzYIua5BUdueN9ky9tqNEM_cXMjdocZ0A,6048
 mmenot/loops/__init__.py,sha256=OoUG7Z9n3SvZFV_R9sLk-2B5C_INFDh7ecJP20MqZTM,120
 mmenot/loops/epoch_optuna.py,sha256=NShq6HfgDUE6FxI1M-49jiYdX_FdW8UPwO5NCa4bCiQ,2700
 mmenot/loops/epoch_pruning.py,sha256=q60fqumaQPuQf4d_mu-DcXkp-VYrE-R206dajuHGj8U,6075
 mmenot/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-mmenot/utils/common.py,sha256=DNEhes3E4vF0JdvoP7xL5mQ9t-Fsm00pnk-7JcaINVA,11727
+mmenot/utils/common.py,sha256=cGkyZ0mX9SUTOtUosCatQ6bOlLRse0-pZf8zvv378gY,12560
 mmenot/utils/hpo.py,sha256=IbC-5sM-jC7efRq3GHVASKZnTCKWLVf0qh-DU9hgQT0,2185
-mmenot-0.9.4.dist-info/LICENSE,sha256=iWLEJUustLjDYQcmBAwQmj0-kh3XqQi2kHpvDIc8suA,11338
-mmenot-0.9.4.dist-info/METADATA,sha256=Dv3Hr2osLdYfHIzEgVyF7AdQ6rbslfFFljnAP5R2iNc,14279
-mmenot-0.9.4.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-mmenot-0.9.4.dist-info/entry_points.txt,sha256=m8G4sSO_-5Cm3QTYwDE5MgovucW9CQ5NV-cpw6qbVqQ,179
-mmenot-0.9.4.dist-info/top_level.txt,sha256=hvzXbej5hbn5YET1ne-y6ZJc6cSeCAMmha4HDiFCA5Q,16
-mmenot-0.9.4.dist-info/RECORD,,
+mmenot-0.9.5.dist-info/LICENSE,sha256=iWLEJUustLjDYQcmBAwQmj0-kh3XqQi2kHpvDIc8suA,11338
+mmenot-0.9.5.dist-info/METADATA,sha256=nqJWa5gsOdBSuX4UIqZ-pLGs-3B00wa-84VhYHpDfDk,14312
+mmenot-0.9.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+mmenot-0.9.5.dist-info/entry_points.txt,sha256=m8G4sSO_-5Cm3QTYwDE5MgovucW9CQ5NV-cpw6qbVqQ,179
+mmenot-0.9.5.dist-info/top_level.txt,sha256=hvzXbej5hbn5YET1ne-y6ZJc6cSeCAMmha4HDiFCA5Q,16
+mmenot-0.9.5.dist-info/RECORD,,
```

