# Comparing `tmp/HELIOS_RL_Test-0.4.19-py3-none-any.whl.zip` & `tmp/HELIOS_RL_Test-0.4.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,8 +1,8 @@
-Zip file size: 64747 bytes, number of entries: 36
+Zip file size: 61689 bytes, number of entries: 36
 -rw-r--r--  2.0 unx      533 b- defN 23-May-17 13:39 helios_rl/__init__.py
 -rw-r--r--  2.0 unx    19502 b- defN 23-May-17 13:39 helios_rl/analysis.py
 -rw-r--r--  2.0 unx     3039 b- defN 23-May-17 13:39 helios_rl/config.py
 -rw-r--r--  2.0 unx     2391 b- defN 23-May-17 13:39 helios_rl/config_local.py
 -rw-r--r--  2.0 unx      715 b- defN 23-May-17 13:39 helios_rl/adapters/__init__.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-17 13:39 helios_rl/agents/__init__.py
 -rw-r--r--  2.0 unx      536 b- defN 23-May-17 13:39 helios_rl/agents/agent_abstract.py
@@ -11,28 +11,28 @@
 -rw-r--r--  2.0 unx     7770 b- defN 23-May-17 13:39 helios_rl/agents/table_q_agent.py
 -rw-r--r--  2.0 unx     1498 b- defN 23-May-17 13:39 helios_rl/encoders/__init__.py
 -rw-r--r--  2.0 unx      804 b- defN 23-May-17 13:39 helios_rl/encoders/encoder_abstract.py
 -rw-r--r--  2.0 unx     1674 b- defN 23-May-17 13:39 helios_rl/encoders/observable_objects_encoded.py
 -rw-r--r--  2.0 unx     1424 b- defN 23-May-17 13:39 helios_rl/encoders/poss_actions_encoded.py
 -rw-r--r--  2.0 unx     1562 b- defN 23-May-17 13:39 helios_rl/encoders/poss_state_encoded.py
 -rw-r--r--  2.0 unx     1647 b- defN 23-May-17 13:39 helios_rl/encoders/prior_actions_encoded.py
--rw-r--r--  2.0 unx     1989 b- defN 23-May-21 17:58 helios_rl/encoders/sentence_transformer_MiniLM_L6v2.py
+-rw-r--r--  2.0 unx     1887 b- defN 23-May-17 13:39 helios_rl/encoders/sentence_transformer_MiniLM_L6v2.py
 -rw-r--r--  2.0 unx     6105 b- defN 23-May-17 13:39 helios_rl/environment_setup/helios_info.py
 -rw-r--r--  2.0 unx     1645 b- defN 23-May-17 13:39 helios_rl/environment_setup/imports.py
 -rw-r--r--  2.0 unx     4842 b- defN 23-May-17 13:39 helios_rl/environment_setup/results_table.py
 -rw-r--r--  2.0 unx     3228 b- defN 23-May-17 13:39 helios_rl/evaluation/combined_variance_visual.py
 -rw-r--r--  2.0 unx     6246 b- defN 23-May-17 13:39 helios_rl/evaluation/convergence_measure.py
 -rw-r--r--  2.0 unx      358 b- defN 23-May-17 13:39 helios_rl/evaluation/tabular_output.py
 -rw-r--r--  2.0 unx    20983 b- defN 23-May-17 13:39 helios_rl/evaluation/visual_output.py
 -rw-r--r--  2.0 unx        0 b- defN 23-May-17 13:39 helios_rl/experiments/experience_sampling.py
 -rw-r--r--  2.0 unx     1238 b- defN 23-May-17 13:39 helios_rl/experiments/helios_instr_input.py
--rw-r--r--  2.0 unx    30150 b- defN 23-May-19 16:23 helios_rl/experiments/helios_instruction_following.py
--rw-r--r--  2.0 unx    27534 b- defN 23-May-21 18:32 helios_rl/experiments/helios_instruction_search.py
--rw-r--r--  2.0 unx    15554 b- defN 23-May-19 10:14 helios_rl/experiments/standard.py
+-rw-r--r--  2.0 unx    30161 b- defN 23-May-17 15:53 helios_rl/experiments/helios_instruction_following.py
+-rw-r--r--  2.0 unx    25931 b- defN 23-May-17 14:39 helios_rl/experiments/helios_instruction_search.py
+-rw-r--r--  2.0 unx    15540 b- defN 23-May-17 13:39 helios_rl/experiments/standard.py
 -rw-r--r--  2.0 unx    15901 b- defN 23-May-17 13:39 helios_rl/experiments/supervised_instruction_following.py
 -rw-r--r--  2.0 unx    10752 b- defN 23-May-17 13:39 helios_rl/experiments/unsupervised_instruction_following.py
--rw-r--r--  2.0 unx    35821 b- defN 23-May-21 18:32 HELIOS_RL_Test-0.4.19.dist-info/LICENSE
--rw-r--r--  2.0 unx      471 b- defN 23-May-21 18:32 HELIOS_RL_Test-0.4.19.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-May-21 18:32 HELIOS_RL_Test-0.4.19.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 23-May-21 18:32 HELIOS_RL_Test-0.4.19.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     3389 b- defN 23-May-21 18:32 HELIOS_RL_Test-0.4.19.dist-info/RECORD
-36 files, 238261 bytes uncompressed, 59189 bytes compressed:  75.2%
+-rw-r--r--  2.0 unx    27030 b- defN 23-May-17 15:53 HELIOS_RL_Test-0.4.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx      470 b- defN 23-May-17 15:53 HELIOS_RL_Test-0.4.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-17 15:53 HELIOS_RL_Test-0.4.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 23-May-17 15:53 HELIOS_RL_Test-0.4.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     3384 b- defN 23-May-17 15:53 HELIOS_RL_Test-0.4.2.dist-info/RECORD
+36 files, 227756 bytes uncompressed, 56141 bytes compressed:  75.4%
```

## zipnote {}

```diff
@@ -87,23 +87,23 @@
 
 Filename: helios_rl/experiments/supervised_instruction_following.py
 Comment: 
 
 Filename: helios_rl/experiments/unsupervised_instruction_following.py
 Comment: 
 
-Filename: HELIOS_RL_Test-0.4.19.dist-info/LICENSE
+Filename: HELIOS_RL_Test-0.4.2.dist-info/LICENSE
 Comment: 
 
-Filename: HELIOS_RL_Test-0.4.19.dist-info/METADATA
+Filename: HELIOS_RL_Test-0.4.2.dist-info/METADATA
 Comment: 
 
-Filename: HELIOS_RL_Test-0.4.19.dist-info/WHEEL
+Filename: HELIOS_RL_Test-0.4.2.dist-info/WHEEL
 Comment: 
 
-Filename: HELIOS_RL_Test-0.4.19.dist-info/top_level.txt
+Filename: HELIOS_RL_Test-0.4.2.dist-info/top_level.txt
 Comment: 
 
-Filename: HELIOS_RL_Test-0.4.19.dist-info/RECORD
+Filename: HELIOS_RL_Test-0.4.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## helios_rl/encoders/sentence_transformer_MiniLM_L6v2.py

```diff
@@ -21,19 +21,18 @@
         self.sentence_model: SentenceTransformer = SentenceTransformer('all-MiniLM-L6-v2', device=self.device)
         
 
     def encode(self, state: List[str], legal_actions:list = None, episode_action_history:list = None, 
                indexed: bool = False) -> Tensor:
         if (len(state) == 0):
             state = [""]
-        
-        # I think typing is overriding the input type anyway -> need to ensure sentences are split up
-        if type(state) == type(''):
-            state = state.split('.')
-                
+            
+        if type(state) != type(list('')):
+            state = list(state)
+            
         to_encode = [sent for sent in state if sent not in LanguageEncoder._cached_enc]
         if (to_encode):
             encoded = self.sentence_model.encode(to_encode, convert_to_tensor=True)
             LanguageEncoder._cached_enc.update({to_encode[i]: encoded[i] for i in range(len(to_encode))})
         
         LanguageEncoder._cached_freq.update(state)
         LanguageEncoder._cached_freq.subtract(LanguageEncoder._cached_freq.keys())
```

## helios_rl/experiments/helios_instruction_following.py

```diff
@@ -69,16 +69,16 @@
                 self.testing_path = predicted_path
         else:
             self.instruction_path = instruction_path
             self.testing_path = False
         self.known_instructions = list(self.instruction_path.keys())
         self.known_instructions_dict = {}
         for instr in self.known_instructions:
-            start = instr.split("---")[0]
-            end = instr.split("---")[1]
+            start = instr.split("-")[0]
+            end = instr.split("-")[1]
 
             for n, agent_type in enumerate(self.setup_info['agent_select']):
                 adapter = self.setup_info["adapter_select"][n]
                 if (agent_type+'_'+adapter) not in self.known_instructions_dict:
                         self.known_instructions_dict[(agent_type+'_'+adapter)] = {}
 
                 if (agent_type+'_'+adapter) in self.instruction_path[instr]:
@@ -166,15 +166,15 @@
                     live_env = self.env(train_setup_info)
                     # ----------------
                     #  Start obs reset between seeds but fixed for repeat
                     if training_repeat > 1:
                         live_env.start_obs = env_start
 
                     env_start = live_env.start_obs
-                    goal = str(env_start).split(".")[0] + "---" + "GOAL"
+                    goal = str(env_start).split(".")[0] + "-" + "GOAL"
                     print("Long-term Goal: ", goal)
                     if goal in seed_recall:
                         setup_num = seed_recall[goal]
                     else:
                         seed_recall[goal] = 1
                     # Load results table from previous seeds to continue output graphs
                     if goal in seed_results_connection:
@@ -210,15 +210,15 @@
                             while True:
                                 if start in self.known_instructions_dict[(agent_type+'_'+adapter)]:
                                     i+=1
                                     max_count = 0
                                     for end in self.known_instructions_dict[(agent_type+'_'+adapter)][start]:
                                         if self.known_instructions_dict[(agent_type+'_'+adapter)][start][end] > max_count:
                                             max_count = self.known_instructions_dict[(agent_type+'_'+adapter)][start][end]
-                                            instr = start + "---" + end
+                                            instr = start + "-" + end
                                             print("Sub-instr: ", instr)
                                     # ---
                                     # Override trained agent with known instruction agent 
                                     if instr in sample_batch_agent_store:
                                         live_env.agent = sample_batch_agent_store[instr]
                                     elif instr in self.trained_agents[str(agent_type) + '_' + str(adapter)]:
                                         live_env.agent = self.trained_agents[str(agent_type) + '_' + str(adapter)][instr].clone()
@@ -235,15 +235,15 @@
                                     training_results = live_env.episode_loop()
                                     sample_batch_agent_store[instr] = live_env.agent.clone()
                                     # Override episode numbers and store for next batch
                                     training_results['episode'] = training_results.index
                                     sample_batch_results_store[instr] = live_env.results.copy()
                                     # ---
                                     # Store instruction results
-                                    instr_save_dir = agent_save_dir+'/'+str(i)+"-"+instr.replace(" ","").replace("/","_")
+                                    instr_save_dir = agent_save_dir+'/'+str(i)+"-"+instr.replace(" ","")
                                     if not os.path.exists(instr_save_dir):
                                         os.mkdir(instr_save_dir)
 
                                     # Produce training report with Analysis.py
                                     Return = self.analysis.train_report(training_results, instr_save_dir, self.show_figures)
                                     if instr not in temp_agent_store:
                                         temp_agent_store[instr] = {}
@@ -308,39 +308,39 @@
                         while True:
                             i+=1
                             max_count = 0
                             if start in self.known_instructions_dict[(agent_type+'_'+adapter)]:
                                 for end in self.known_instructions_dict[(agent_type+'_'+adapter)][start]:
                                     if self.known_instructions_dict[(agent_type+'_'+adapter)][start][end] > max_count:
                                         max_count = self.known_instructions_dict[(agent_type+'_'+adapter)][start][end]
-                                        instr = start + "---" + end
+                                        instr = start + "-" + end
                                         print("Sub-instr: ", instr)
                                         
-                                # Instructions use fewer epispdes, lower bound to 10
+                                # Instructions use fewer epispdes, lower bound to 50
                                 number_instr_episodes = int(number_training_episodes*self.instruction_episode_ratio)
-                                if number_instr_episodes<10:
-                                    number_instr_episodes=10
+                                if number_instr_episodes<50:
+                                    number_instr_episodes=50
                                 total_instr_episodes+=number_instr_episodes
                                 live_env.num_train_episodes = number_instr_episodes
                                 # ---
                                 # Override trained agent with known instruction agent
                                 if instr in self.trained_agents[str(agent_type) + '_' + str(adapter)]:
                                     live_env.agent = self.trained_agents[str(agent_type) + '_' + str(adapter)][instr].clone()
                                 # TODO: ADOPT AGENT OF MOST SIMILAR POLICY
                                 # ---
                                 sub_goal = self.instruction_path[instr][agent_type+'_'+adapter]['sub_goal']
                                 live_env.sub_goal = sub_goal
                                 live_env.agent.exploration_parameter_reset()
                                 if type(instr_results)==type(pd.DataFrame()):
-                                    live_env.results.load(instr_results)
+                                    live_env.results.load(                                                                                    )
                                 training_results = live_env.episode_loop()
                                 training_results['episode'] = training_results.index
                                 # ---
                                 # Store instruction results
-                                instr_save_dir = agent_save_dir+'/'+str(i)+"-"+instr.replace(" ","").replace("/","_")
+                                instr_save_dir = agent_save_dir+'/'+str(i)+"-"+instr.replace(" ","")
                                 if not os.path.exists(instr_save_dir):
                                     os.mkdir(instr_save_dir)
 
                                 # Produce training report with Analysis.py
                                 Return = self.analysis.train_report(training_results, instr_save_dir, self.show_figures)
                                 if instr not in temp_agent_store:
                                     temp_agent_store[instr] = {}
@@ -359,16 +359,16 @@
                                         # Reset exploration parameter between seeds so to not get 'trapped'
                                         live_env.agent.exploration_parameter_reset()
                                 break
                         # train for entire path 
                         #live_env.start_obs = env_start
                         # Number of episodes used reduced by those used for instructions (lower bounded)
                         if (number_training_episodes-total_instr_episodes)<int(number_training_episodes*self.instruction_episode_ratio):
-                            if int(number_training_episodes*self.instruction_episode_ratio) < 10:
-                                live_env.num_train_epispdes = 10
+                            if int(number_training_episodes*self.instruction_episode_ratio) < 50:
+                                live_env.num_train_epispdes = 50
                             else:
                                 live_env.num_train_epispdes = int(number_training_episodes*self.instruction_episode_ratio)
                         else:
                             live_env.num_train_episodes = number_training_episodes - total_instr_episodes
                         # Remove sub-goal
                         live_env.sub_goal = None
                         print("Goal: ", goal)
@@ -444,27 +444,27 @@
             agent_adapter = test_setup_info['agent_type'] + "_" + test_setup_info['adapter_select']
 
             for testing_repeat in range(0, test_setup_info['number_test_repeats']):  
                 # Re-init env for testing
                 env = self.env(test_setup_info)
                 # ---
                 start_obs = env.start_obs
-                goal = str(start_obs).split(".")[0] + "---" + "GOAL"
+                goal = str(start_obs).split(".")[0] + "-" + "GOAL"
                 print("Flat agent Goal: ", goal)
                 # Override with trained agent if goal seen previously
                 if goal in self.trained_agents[test_setup_info['agent_type']+ '_' +test_setup_info['adapter_select']]:
                     print("Trained agent available for testing.")
                     env.agent = self.trained_agents[test_setup_info['agent_type']+'_'+test_setup_info['adapter_select']][goal]
                 else:
                     print("NO agent available for testing position.")
                 env.agent.epsilon = 0 # Remove random actions
                 # ---
                 # Testing generally is the agents replaying on the testing ENV
                 testing_results = env.episode_loop() 
-                test_save_dir = (self.save_dir+'/'+agent_adapter+'__testing_results_'+str(goal).split("/")[0]+"_"+str(testing_repeat) )
+                test_save_dir = (self.save_dir+'/'+agent_adapter+'__testing_results_'+str(goal)+"_"+str(testing_repeat) )
                 if not os.path.exists(test_save_dir):
                     os.mkdir(test_save_dir)
                 # Produce training report with Analysis.py
                 Return = self.analysis.test_report(testing_results, test_save_dir, self.show_figures)
 
         # Path is the experiment save dir + the final instruction
         if number_training_repeats>1:
```

## helios_rl/experiments/helios_instruction_search.py

```diff
@@ -87,15 +87,14 @@
         self.feedback_results: dict = {}
         self.feedback_repeats: int = feedback_repeats
         # Instruction input
         self.helios_input = HeliosInput()
 
 
     def search(self, action_cap:int=5, re_search_override:bool=False, simulated_instr_goal:any=None):
-        device = "cuda" if torch.cuda.is_available() else "cpu" 
         # Trigger re-search
         if re_search_override:
             self.observed_states:dict = {}
         
         #instruction_vector = torch.rand(observed_states[list(observed_states.keys())[0]].size()) # NEED TO FIND A METHOD TO VECTORIZE OBSERVED STATES AND INSTRUCTIONS TO COMPARE
         # - TODO: Only use first given agent to do exploration - repeating search for multiple agents doesn't make sense
         for n, agent_type in enumerate(self.setup_info['agent_select']): 
@@ -143,21 +142,21 @@
                 feedback_count = 0
                 for repeat in range(0,self.feedback_repeats): # Arbitrary repeat to further reinforce matching state to instr
                     print("===")
                     print("REINFORCEMENT Repeated search num ", repeat+1)
                     for i,instr in enumerate(instructions):
                         
                         if i == 0:
-                            instruction = str(sample_env.start_obs).split(".")[0] + "---" + instr
+                            instruction = str(sample_env.start_obs).split(".")[0] + "-" + instr
                         else:
-                            instruction = instructions[i-1] + "---" + instr
+                            instruction = instructions[i-1] + "-" + instr
                         
                         instr_description = instr_descriptions[i]
                         
-                        if type(instr_description) == type(''):
+                        if type(instr_description) != type(List[str]):
                             instr_description = instr_description.split('.')
                             instr_description = list(filter(None, instr_description))
                         # Create tensor vector of description
                         instruction_vector = self.enc.encode(instr_description)
                         # EXPLORE TO FIND LOCATION OF SUB-GOAL
                         sub_goal = None
                         # ---------------------------
@@ -169,22 +168,22 @@
                                 feedback_layer = self.instruction_results[instruction][agent_type+'_'+adapter]['feedback_layer']
                                 if (self.instruction_results[instruction][agent_type+'_'+adapter]['sim_score']>=self.sim_threshold):
                                     sub_goal = self.instruction_results[instruction][agent_type+'_'+adapter]['sub_goal'][0]
                                     sub_goal_list = self.instruction_results[instruction][agent_type+'_'+adapter]['sub_goal']
                                     sim = self.instruction_results[instruction][agent_type+'_'+adapter]['sim_score']
                             else:
                                 self.instruction_results[instruction][agent_type+'_'+adapter] = {}
-                                feedback_layer = torch.zeros(instruction_vector.size()).to(device)
+                                feedback_layer = torch.zeros(instruction_vector.size())
                             self.instruction_results[instruction][agent_type+'_'+adapter]['count'] = self.instruction_results[instruction][agent_type+'_'+adapter]['count']+1
                         else:
                             self.instruction_results[instruction] = {}    
                             self.instruction_results[instruction][agent_type+'_'+adapter] = {} 
                             self.instruction_results[instruction][agent_type+'_'+adapter]['count'] = 1
                             self.instruction_results[instruction][agent_type+'_'+adapter]['action_cap'] = action_cap
-                            feedback_layer = torch.zeros(instruction_vector.size()).to(device)
+                            feedback_layer = torch.zeros(instruction_vector.size())
                             self.instruction_results[instruction][agent_type+'_'+adapter]['feedback_layer'] = feedback_layer
                         # ---------------------------
                         search_count=0
                         while not sub_goal:
                             sim_delta = 1 # Parameter that stops updates when change to feedback is small
                             # If no description -> no sub-goal (i.e. envs terminal goal position)
                             if not instr_description: 
@@ -222,49 +221,43 @@
                                     t_state = self.enc.encode(str_state)
                                     # ---
                                     total_sim = 0
                                     for idx,instr_sentence in enumerate(instruction_vector):
                                         feedback_layer_sent = feedback_layer[idx]
                                         for state_sentence in t_state:
                                             total_sim+=self.cos(torch.add(state_sentence, feedback_layer_sent), instr_sentence)
-                                    
-                                    # Tensor will only have one dimension if only one sentence
-                                    instruction_vector_dim = len(instruction_vector) if len(instruction_vector.size())>1 else 1
-                                    t_state_dim = len(t_state) if len(t_state.size())>1 else 1
-                                    sim = total_sim.item()/(len(instruction_vector_dim)*len(t_state_dim))
+                                    sim = total_sim.item()/(len(instruction_vector)*len(t_state))
                                     if sim > max_sim:
                                         max_sim  = sim
                                         sub_goal_max = obs_state
                                         sub_goal_max_t = t_state
                                     if sim >= self.sim_threshold:
                                         sub_goal = obs_state # Sub-Goal code
                                         sub_goal_list.append(sub_goal)
                 
-                                # OR if none above threshold matching max sim
+                                # TODO: OR if none above threshold within (1-threshold%) of max sim
                                 if max_sim < self.sim_threshold:
                                     sub_goal = sub_goal_max
-                                    sub_goal_t = sub_goal_max_t                                    
-                                    # Find all states that have same sim as max
-                                    for obs_state in self.observed_states:
-                                        str_state = self.observed_states[obs_state]
-                                        #str_state_stacked = ' '.join(str_state)
-                                        t_state = self.enc.encode(str_state)
-                                        # ---
-                                        total_sim = 0
-                                        # Average sim across each sentence in instruction vs state
-                                        for i,instr_sentence in enumerate(instruction_vector):
-                                            feedback_layer_sent = feedback_layer[i]
-                                            for state_sentence in t_state:
-                                                total_sim+=self.cos(torch.add(state_sentence, feedback_layer_sent), instr_sentence) 
-                                        # Tensor will only have one dimension if only one sentence
-                                        instruction_vector_dim = len(instruction_vector) if len(instruction_vector.size())>1 else 1
-                                        t_state_dim = len(t_state) if len(t_state.size())>1 else 1
-                                        sim = total_sim.item()/(len(instruction_vector_dim)*len(t_state_dim))
-                                        if sim >= (max_sim):
-                                            sub_goal_list.append(obs_state)
+                                    sub_goal_list.append(sub_goal)
+                                    sub_goal_t = sub_goal_max_t
+                                    # for obs_state in self.observed_states:
+                                    #     str_state = self.observed_states[obs_state]
+                                    #     #str_state_stacked = ' '.join(str_state)
+                                    #     t_state = self.enc.encode(str_state)
+                                    #     # ---
+                                    #     total_sim = 0
+                                    #     # Average sim across each sentence in instruction vs state
+                                    #     for i,instr_sentence in enumerate(instruction_vector):
+                                    #         feedback_layer_sent = feedback_layer[i]
+                                    #         for state_sentence in t_state:
+                                    #             total_sim+=self.cos(torch.add(state_sentence, feedback_layer_sent), instr_sentence) 
+                                    #     sim = total_sim.item()/(len(instruction_vector)*len(t_state))
+                                    #     if sim >= max_sim*(self.sim_threshold):
+                                    #         sub_goal = obs_state # Temp Sub-Goal as most known similar
+                                    #         sub_goal_list.append(sub_goal)
 
                                 if max_sim < self.sim_threshold:
                                     print("Minimum sim for observed states to match instruction not found, using best match instead. Best match sim value = ", max_sim )
                             # If adapter is poor to match to instruction vector none of them observed states match
                             if (max_sim<-1)|(sub_goal is None):#|(max_sim>1):
                                 print("All observed states result in similarity outside bounds (i.e. strongly opposite vectors to instruction). Re-starting Search.")
                                 print("Max sim found = ", max_sim)
@@ -273,25 +266,21 @@
                             elif (sim_delta<0.05)|(search_count>=5):
                                 print("Change in sim less than or equal to delta cap or search cap reached, assume goal-state not observed. Re-starting Search.")
                                 if simulated_instr_goal:
                                     print("-- Known sub_goal position: ", simulated_instr_goal)
                                 print("-- Best match: ", sub_goal_max)
                                 sub_goal = None
                                 self.observed_states = {}
-                                search_count = 0
                             else:
                                 if not simulated_instr_goal:
                                     print(" ")
                                     print("===========")
                                     print("Instruction: ", instr)
                                     print(sub_goal)
                                     print("Best match state for instruction \n ______ \n Adapted form: \n\t - ", self.observed_states[sub_goal], " \n Engine observation: \n\t - " , sub_goal)
-                                    print("Full list of matching states...",)
-                                    for s_g in sub_goal_list:
-                                        print(self.observed_states[s_g], ": ", s_g)
                                     feedback = input("-- Does this match the expectation instruction outcome? (Y/N)")
                                     feedback_count+=1 
                                 else:
                                     if type(simulated_instr_goal[0]) != type(sub_goal_max):
                                         print("- ERROR: Typing of simulated sub-goal check does not match typing of state from environment, please correct this.")
                                         print(type(simulated_instr_goal[0])," - ", type(sub_goal_max))
                                         #print("Observed States Examples: ", self.observed_states(list(self.observed_states.keys())[0:5]))
@@ -315,36 +304,30 @@
                                             feedback_layer[idx] = torch.add(feedback_layer_sent, self.feedback_increment*(torch.sub(instr_sentence, sentence))) 
                                     total_sim = 0
                                     # Average sim across each sentence in instruction vs state
                                     for idx,instr_sentence in enumerate(instruction_vector):
                                         feedback_layer_sent = feedback_layer[idx]
                                         for state_sentence in sub_goal_t:
                                             total_sim+=self.cos(torch.add(state_sentence, feedback_layer_sent), instr_sentence)
-                                    # Tensor will only have one dimension if only one sentence
-                                    instruction_vector_dim = len(instruction_vector) if len(instruction_vector.size())>1 else 1
-                                    t_state_dim = len(t_state) if len(t_state.size())>1 else 1
-                                    sim = total_sim.item()/(len(instruction_vector_dim)*len(t_state_dim))
+                                    sim = total_sim.item()/(len(instruction_vector)*len(t_state))
                                     sim_delta = sim-max_sim
                                     print("--- Change in sim results with POSITIVE reinforcement of correct state match =", sim_delta)
                                     print("--- New Sim Value = ",  sim)
                                 else:
                                     for idx,instr_sentence in enumerate(instruction_vector):
                                         feedback_layer_sent = feedback_layer[idx]
                                         for sentence in sub_goal_t:
                                             feedback_layer[idx] = torch.sub(feedback_layer_sent, self.feedback_increment*(torch.sub(instr_sentence, sentence)))
                                     total_sim = 0
                                     # Average sim across each sentence in instruction vs state
                                     for idx,instr_sentence in enumerate(instruction_vector):
                                         feedback_layer_sent = feedback_layer[idx]
                                         for state_sentence in sub_goal_t:
                                             total_sim+=self.cos(torch.add(state_sentence, feedback_layer_sent), instr_sentence)
-                                    # Tensor will only have one dimension if only one sentence
-                                    instruction_vector_dim = len(instruction_vector) if len(instruction_vector.size())>1 else 1
-                                    t_state_dim = len(t_state) if len(t_state.size())>1 else 1
-                                    sim = total_sim.item()/(len(instruction_vector_dim)*len(t_state_dim))
+                                    sim = total_sim.item()/(len(instruction_vector)*len(t_state))
                                     sim_delta = sim-max_sim
                                     print("--- Change in sim results with NEGATIVE reinforcement for NO MATCH =", sim_delta)
                                     sub_goal = None
                         if (agent_type+'_'+adapter) not in self.instruction_results[instruction]:
                             self.instruction_results[instruction][agent_type+'_'+adapter] = {}   
                         self.instruction_results[instruction][agent_type+'_'+adapter]['sub_goal'] = sub_goal_list
                         self.instruction_results[instruction][agent_type+'_'+adapter]['sim_score'] = sim
```

## helios_rl/experiments/standard.py

```diff
@@ -247,14 +247,14 @@
                     env.agent = self.trained_agents[test_setup_info['agent_type']+'_'+test_setup_info['adapter_select']][goal]
                 else:
                     print("NO agent available for testing position.")
                 env.agent.epsilon = 0 # Remove random actions
                 # ---
                 # TODO: Testing generally is the agents replaying on the testing ENV
                 testing_results = env.episode_loop() 
-                test_save_dir = (self.save_dir+'/'+agent_adapter+'__testing_results_'+str(goal).split("/")[0]+"_"+str(testing_repeat) ) # test_setup_info['train_save_dir']+'/testing_results_'+str(testing_repeat)
+                test_save_dir = (self.save_dir+'/'+agent_adapter+'__testing_results_'+str(goal)+"_"+str(testing_repeat) ) # test_setup_info['train_save_dir']+'/testing_results_'+str(testing_repeat)
                 if not os.path.exists(test_save_dir):
                     os.mkdir(test_save_dir)
                 # Produce training report with Analysis.py
                 Return = self.analysis.test_report(testing_results, test_save_dir, self.show_figures)
         if number_training_repeats>1:
             self.analysis.testing_variance_report(self.save_dir, self.show_figures)
```

## Comparing `HELIOS_RL_Test-0.4.19.dist-info/RECORD` & `HELIOS_RL_Test-0.4.2.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -10,27 +10,27 @@
 helios_rl/agents/table_q_agent.py,sha256=edtUOLG2ZHHEuZ7cgpbZSRF05d3q51L7wEs0_HZ2Ga8,7770
 helios_rl/encoders/__init__.py,sha256=YqjQoNpFtAXCfZA1StW18OAlAdOUg1RnrCQfkezTiUo,1498
 helios_rl/encoders/encoder_abstract.py,sha256=RVImGRYisxDCMWIQIM_FWFKD41Uuo260F1eQRYpcJak,804
 helios_rl/encoders/observable_objects_encoded.py,sha256=yRUUehek2hGh7ADEXdCaDa52QuPybP3H-xSYVNYaKTw,1674
 helios_rl/encoders/poss_actions_encoded.py,sha256=JM6a9NMurxQOFFIGgPqMNQRct3h2PH1W0pc2L0I0duI,1424
 helios_rl/encoders/poss_state_encoded.py,sha256=45GISdpMWflKfVG7_hUPLCo8-jNfHucZ8ubjc4Ng1XM,1562
 helios_rl/encoders/prior_actions_encoded.py,sha256=uj-umpmgNju6Tr8q-C4o5UV73szj4nfxiuqy27ok7Ro,1647
-helios_rl/encoders/sentence_transformer_MiniLM_L6v2.py,sha256=NQxrF-x_PH0fZzSnHh0_QVDIb62aD4VWP8RLZ-tbwec,1989
+helios_rl/encoders/sentence_transformer_MiniLM_L6v2.py,sha256=Rzg6qaVHnZ5wfFpIiuGxmSn1-ZD2J_h1TxIhXbaObws,1887
 helios_rl/environment_setup/helios_info.py,sha256=BEOf2NgGAdc6iKfIxKF67ikdjIjGXe0U4UzK7WGD2_s,6105
 helios_rl/environment_setup/imports.py,sha256=D9XA8EBWIf_kGjOL6I3IkCrVrnWFuDvX0DnM6kr3qLE,1645
 helios_rl/environment_setup/results_table.py,sha256=Dg7qeGEWTp6p0SXQOba_s6Y676Hw6whtHl3SyJPX6hA,4842
 helios_rl/evaluation/combined_variance_visual.py,sha256=3qACD4ZiC3JYp8GW7oQJ0ECliTJdrnJytd8dABdjpEI,3228
 helios_rl/evaluation/convergence_measure.py,sha256=6h3BBoiL59n-5wFbPjXI3cNR4MjBmNeOFoNUqzacT0Y,6246
 helios_rl/evaluation/tabular_output.py,sha256=wYmd6SL8wJ7o5cH749nh0o1S3U-2Qt8gooSJc8W4zLA,358
 helios_rl/evaluation/visual_output.py,sha256=OUx0KISD0kpg90-gmHRrnsMQP_g2Cn3b7EfqKbfVStI,20983
 helios_rl/experiments/experience_sampling.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 helios_rl/experiments/helios_instr_input.py,sha256=hQR0u94MUpAtxlaYko1Bvc4C5FW9ek8tU-zmBf_IcWI,1238
-helios_rl/experiments/helios_instruction_following.py,sha256=aiQ23kQEkHhHeSRBd5yCL4fx72GB5AaF7Rcbg6_dpRc,30150
-helios_rl/experiments/helios_instruction_search.py,sha256=VBj25QSQaqbUyl719jHWJKNCB65QxMTdDX96mNZY8Lg,27534
-helios_rl/experiments/standard.py,sha256=rvi4E6ndjpqTrcdt6e-9luY-W-OSQ8eLleGHvdlIf28,15554
+helios_rl/experiments/helios_instruction_following.py,sha256=KB4ztRSBAKvK3zL4OXv4Oxt-uGcxIYGAmVDqrUfNDiw,30161
+helios_rl/experiments/helios_instruction_search.py,sha256=mdk2ucmFLzMx3ovB7Ug5bUqwZKKpNXO67wF57cADqlc,25931
+helios_rl/experiments/standard.py,sha256=D3OLwfVPReAV7V8PcYL0Bxt5DyDlunHxcok5OspqyHA,15540
 helios_rl/experiments/supervised_instruction_following.py,sha256=rDJLiy3Qx5lv-HgkmZGIP2hHV0NXjGPHb9aV8SJbIcU,15901
 helios_rl/experiments/unsupervised_instruction_following.py,sha256=PfripIUDII7ZC8RuHVd3zd5oeJmjg2WlMfpD4hTvwvQ,10752
-HELIOS_RL_Test-0.4.19.dist-info/LICENSE,sha256=gcuuhKKc5-dwvyvHsXjlC9oM6N5gZ6umYbC8ewW1Yvg,35821
-HELIOS_RL_Test-0.4.19.dist-info/METADATA,sha256=5x4osEoK7ML_K0cmvTfkopdGJ0R3Bq2xtaWyhRouqjY,471
-HELIOS_RL_Test-0.4.19.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-HELIOS_RL_Test-0.4.19.dist-info/top_level.txt,sha256=abYTq8tqgFzlsa2ct4SQYkd0MYAAFRB0gAvEvcE4Q9I,10
-HELIOS_RL_Test-0.4.19.dist-info/RECORD,,
+HELIOS_RL_Test-0.4.2.dist-info/LICENSE,sha256=f_4ZVFh8d9-6HPjrmy6nQ2cfpuY_nnovJYEZ1C4U7v4,27030
+HELIOS_RL_Test-0.4.2.dist-info/METADATA,sha256=7kWX1ZJjH86XY8LdlGx4nrdjNjXcLE6qz8h1uJX516k,470
+HELIOS_RL_Test-0.4.2.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+HELIOS_RL_Test-0.4.2.dist-info/top_level.txt,sha256=abYTq8tqgFzlsa2ct4SQYkd0MYAAFRB0gAvEvcE4Q9I,10
+HELIOS_RL_Test-0.4.2.dist-info/RECORD,,
```

