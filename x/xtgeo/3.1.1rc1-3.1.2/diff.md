# Comparing `tmp/xtgeo-3.1.1rc1-cp39-cp39-win_amd64.whl.zip` & `tmp/xtgeo-3.1.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,110 +1,123 @@
-Zip file size: 577009 bytes, number of entries: 108
--rw-rw-rw-  2.0 fat     5955 b- defN 23-Jun-30 10:48 xtgeo/__init__.py
--rw-rw-rw-  2.0 fat      167 b- defN 23-Jun-30 10:48 xtgeo/_theversion.py
--rw-rw-rw-  2.0 fat      385 b- defN 23-Jun-30 10:48 xtgeo/common/__init__.py
--rw-rw-rw-  2.0 fat     9518 b- defN 23-Jun-30 10:48 xtgeo/common/calc.py
--rw-rw-rw-  2.0 fat      557 b- defN 23-Jun-30 10:48 xtgeo/common/constants.py
--rw-rw-rw-  2.0 fat     1110 b- defN 23-Jun-30 10:48 xtgeo/common/exceptions.py
--rw-rw-rw-  2.0 fat    24110 b- defN 23-Jun-30 10:48 xtgeo/common/sys.py
--rw-rw-rw-  2.0 fat    17736 b- defN 23-Jun-30 10:48 xtgeo/common/xtgeo_dialog.py
--rw-rw-rw-  2.0 fat      153 b- defN 23-Jun-30 10:48 xtgeo/cube/__init__.py
--rw-rw-rw-  2.0 fat     6790 b- defN 23-Jun-30 10:48 xtgeo/cube/_cube_export.py
--rw-rw-rw-  2.0 fat    20060 b- defN 23-Jun-30 10:48 xtgeo/cube/_cube_import.py
--rw-rw-rw-  2.0 fat     5108 b- defN 23-Jun-30 10:48 xtgeo/cube/_cube_roxapi.py
--rw-rw-rw-  2.0 fat     7731 b- defN 23-Jun-30 10:48 xtgeo/cube/_cube_utils.py
--rw-rw-rw-  2.0 fat    36529 b- defN 23-Jun-30 10:48 xtgeo/cube/cube1.py
--rw-rw-rw-  2.0 fat        2 b- defN 23-Jun-30 10:48 xtgeo/cxtgeo/__init__.py
--rw-rw-rw-  2.0 fat   605696 b- defN 23-Jun-30 10:48 xtgeo/cxtgeo/_cxtgeo.cp39-win_amd64.pyd
--rw-rw-rw-  2.0 fat      356 b- defN 23-Jun-30 10:48 xtgeo/grid3d/__init__.py
--rw-rw-rw-  2.0 fat    26874 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_ecl_grid.py
--rw-rw-rw-  2.0 fat     4310 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_ecl_inte_head.py
--rw-rw-rw-  2.0 fat     2733 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_ecl_logi_head.py
--rw-rw-rw-  2.0 fat     1987 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_ecl_output_file.py
--rw-rw-rw-  2.0 fat    34754 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_egrid.py
--rw-rw-rw-  2.0 fat    22108 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_find_gridprop_in_eclrun.py
--rw-rw-rw-  2.0 fat     8496 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grdecl_format.py
--rw-rw-rw-  2.0 fat    13075 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grdecl_grid.py
--rw-rw-rw-  2.0 fat     1227 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid3d.py
--rw-rw-rw-  2.0 fat     5127 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid3d_fence.py
--rw-rw-rw-  2.0 fat     5859 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid3d_utils.py
--rw-rw-rw-  2.0 fat    38736 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_etc1.py
--rw-rw-rw-  2.0 fat     6502 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_export.py
--rw-rw-rw-  2.0 fat     1444 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_hybrid.py
--rw-rw-rw-  2.0 fat     1926 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_import.py
--rw-rw-rw-  2.0 fat     3191 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_import_ecl.py
--rw-rw-rw-  2.0 fat     3550 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_import_roff.py
--rw-rw-rw-  2.0 fat     8955 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_import_xtgcpgeom.py
--rw-rw-rw-  2.0 fat     3611 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_refine.py
--rw-rw-rw-  2.0 fat    15576 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_roxapi.py
--rw-rw-rw-  2.0 fat     5298 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_grid_wellzone.py
--rw-rw-rw-  2.0 fat     3957 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_export.py
--rw-rw-rw-  2.0 fat     4848 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_import_eclrun.py
--rw-rw-rw-  2.0 fat     3518 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_import_grdecl.py
--rw-rw-rw-  2.0 fat     1566 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_import_roff.py
--rw-rw-rw-  2.0 fat     3139 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_import_xtgcpprop.py
--rw-rw-rw-  2.0 fat     4877 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_lowlevel.py
--rw-rw-rw-  2.0 fat     4939 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_op1.py
--rw-rw-rw-  2.0 fat     6742 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_roxapi.py
--rw-rw-rw-  2.0 fat     3369 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprop_value_init.py
--rw-rw-rw-  2.0 fat    10671 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprops_import_eclrun.py
--rw-rw-rw-  2.0 fat     2009 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_gridprops_import_roff.py
--rw-rw-rw-  2.0 fat    16965 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_roff_grid.py
--rw-rw-rw-  2.0 fat    10778 b- defN 23-Jun-30 10:48 xtgeo/grid3d/_roff_parameter.py
--rw-rw-rw-  2.0 fat    95006 b- defN 23-Jun-30 10:48 xtgeo/grid3d/grid.py
--rw-rw-rw-  2.0 fat    29435 b- defN 23-Jun-30 10:48 xtgeo/grid3d/grid_properties.py
--rw-rw-rw-  2.0 fat    50918 b- defN 23-Jun-30 10:48 xtgeo/grid3d/grid_property.py
--rw-rw-rw-  2.0 fat      191 b- defN 23-Jun-30 10:48 xtgeo/metadata/__init__.py
--rw-rw-rw-  2.0 fat    13133 b- defN 23-Jun-30 10:48 xtgeo/metadata/metadata.py
--rw-rw-rw-  2.0 fat      277 b- defN 23-Jun-30 10:48 xtgeo/plot/__init__.py
--rw-rw-rw-  2.0 fat    12395 b- defN 23-Jun-30 10:48 xtgeo/plot/_colortables.py
--rw-rw-rw-  2.0 fat     8556 b- defN 23-Jun-30 10:48 xtgeo/plot/baseplot.py
--rw-rw-rw-  2.0 fat     6968 b- defN 23-Jun-30 10:48 xtgeo/plot/grid3d_slice.py
--rw-rw-rw-  2.0 fat    38946 b- defN 23-Jun-30 10:48 xtgeo/plot/xsection.py
--rw-rw-rw-  2.0 fat     7926 b- defN 23-Jun-30 10:48 xtgeo/plot/xtmap.py
--rw-rw-rw-  2.0 fat      121 b- defN 23-Jun-30 10:48 xtgeo/roxutils/__init__.py
--rw-rw-rw-  2.0 fat     4185 b- defN 23-Jun-30 10:48 xtgeo/roxutils/_roxutils_etc.py
--rw-rw-rw-  2.0 fat     7297 b- defN 23-Jun-30 10:48 xtgeo/roxutils/roxutils.py
--rw-rw-rw-  2.0 fat      132 b- defN 23-Jun-30 10:48 xtgeo/surface/__init__.py
--rw-rw-rw-  2.0 fat      856 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_boundary.py
--rw-rw-rw-  2.0 fat     5288 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_cube.py
--rw-rw-rw-  2.0 fat    10289 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_cube_window.py
--rw-rw-rw-  2.0 fat     6673 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_cube_window_v2.py
--rw-rw-rw-  2.0 fat    12876 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_export.py
--rw-rw-rw-  2.0 fat     5137 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_grid3d.py
--rw-rw-rw-  2.0 fat    10368 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_gridding.py
--rw-rw-rw-  2.0 fat    15699 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_import.py
--rw-rw-rw-  2.0 fat     3610 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_lowlevel.py
--rw-rw-rw-  2.0 fat    14313 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_oper.py
--rw-rw-rw-  2.0 fat     8174 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_roxapi.py
--rw-rw-rw-  2.0 fat     1408 b- defN 23-Jun-30 10:48 xtgeo/surface/_regsurf_utils.py
--rw-rw-rw-  2.0 fat     1337 b- defN 23-Jun-30 10:48 xtgeo/surface/_surfs_import.py
--rw-rw-rw-  2.0 fat     4354 b- defN 23-Jun-30 10:48 xtgeo/surface/_zmap_parser.py
--rw-rw-rw-  2.0 fat   115479 b- defN 23-Jun-30 10:48 xtgeo/surface/regular_surface.py
--rw-rw-rw-  2.0 fat     7842 b- defN 23-Jun-30 10:48 xtgeo/surface/surfaces.py
--rw-rw-rw-  2.0 fat      208 b- defN 23-Jun-30 10:48 xtgeo/well/__init__.py
--rw-rw-rw-  2.0 fat     6760 b- defN 23-Jun-30 10:48 xtgeo/well/_blockedwell_roxapi.py
--rw-rw-rw-  2.0 fat     2029 b- defN 23-Jun-30 10:48 xtgeo/well/_blockedwells_roxapi.py
--rw-rw-rw-  2.0 fat    10201 b- defN 23-Jun-30 10:48 xtgeo/well/_well_io.py
--rw-rw-rw-  2.0 fat    18723 b- defN 23-Jun-30 10:48 xtgeo/well/_well_oper.py
--rw-rw-rw-  2.0 fat     9086 b- defN 23-Jun-30 10:48 xtgeo/well/_well_roxapi.py
--rw-rw-rw-  2.0 fat    14345 b- defN 23-Jun-30 10:48 xtgeo/well/_wellmarkers.py
--rw-rw-rw-  2.0 fat     4983 b- defN 23-Jun-30 10:48 xtgeo/well/_wells_utils.py
--rw-rw-rw-  2.0 fat     7787 b- defN 23-Jun-30 10:48 xtgeo/well/blocked_well.py
--rw-rw-rw-  2.0 fat     5980 b- defN 23-Jun-30 10:48 xtgeo/well/blocked_wells.py
--rw-rw-rw-  2.0 fat    59650 b- defN 23-Jun-30 10:48 xtgeo/well/well1.py
--rw-rw-rw-  2.0 fat     9592 b- defN 23-Jun-30 10:48 xtgeo/well/wells.py
--rw-rw-rw-  2.0 fat      207 b- defN 23-Jun-30 10:48 xtgeo/xyz/__init__.py
--rw-rw-rw-  2.0 fat     9561 b- defN 23-Jun-30 10:48 xtgeo/xyz/_polygons_oper.py
--rw-rw-rw-  2.0 fat    13727 b- defN 23-Jun-30 10:48 xtgeo/xyz/_xyz.py
--rw-rw-rw-  2.0 fat    15033 b- defN 23-Jun-30 10:48 xtgeo/xyz/_xyz_io.py
--rw-rw-rw-  2.0 fat     1172 b- defN 23-Jun-30 10:48 xtgeo/xyz/_xyz_lowlevel.py
--rw-rw-rw-  2.0 fat    15881 b- defN 23-Jun-30 10:48 xtgeo/xyz/_xyz_oper.py
--rw-rw-rw-  2.0 fat    19050 b- defN 23-Jun-30 10:48 xtgeo/xyz/_xyz_roxapi.py
--rw-rw-rw-  2.0 fat    33599 b- defN 23-Jun-30 10:48 xtgeo/xyz/points.py
--rw-rw-rw-  2.0 fat    29860 b- defN 23-Jun-30 10:48 xtgeo/xyz/polygons.py
--rw-rw-rw-  2.0 fat     7817 b- defN 23-Jun-30 10:48 xtgeo-3.1.1rc1.dist-info/LICENSE.md
--rw-rw-rw-  2.0 fat     5636 b- defN 23-Jun-30 10:48 xtgeo-3.1.1rc1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       94 b- defN 23-Jun-30 10:48 xtgeo-3.1.1rc1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        6 b- defN 23-Jun-30 10:48 xtgeo-3.1.1rc1.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     9161 b- defN 23-Jun-30 10:48 xtgeo-3.1.1rc1.dist-info/RECORD
-108 files, 1844017 bytes uncompressed, 562697 bytes compressed:  69.5%
+Zip file size: 584419 bytes, number of entries: 121
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo-3.1.2.dist-info/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo.libs/
+-rw-r--r--  2.0 unx     5497 b- defN 23-Jul-03 16:16 xtgeo-3.1.2.dist-info/METADATA
+-rw-r--r--  2.0 unx     7652 b- defN 23-Jul-03 16:16 xtgeo-3.1.2.dist-info/LICENSE.md
+-rw-r--r--  2.0 unx        6 b- defN 23-Jul-03 16:16 xtgeo-3.1.2.dist-info/top_level.txt
+-rw-r--r--  2.0 unx      142 b- defN 23-Jul-03 16:16 xtgeo-3.1.2.dist-info/WHEEL
+-rw-rw-r--  2.0 unx     9265 b- defN 23-Jul-03 16:16 xtgeo-3.1.2.dist-info/RECORD
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/xyz/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/cube/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/common/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/roxutils/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/plot/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/cxtgeo/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/metadata/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/surface/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/well/
+drwxr-xr-x  2.0 unx        0 b- stor 23-Jul-03 16:16 xtgeo/grid3d/
+-rw-r--r--  2.0 unx      160 b- defN 23-Jul-03 16:16 xtgeo/_theversion.py
+-rw-r--r--  2.0 unx     5746 b- defN 23-Jul-03 16:16 xtgeo/__init__.py
+-rw-r--r--  2.0 unx    28996 b- defN 23-Jul-03 16:16 xtgeo/xyz/polygons.py
+-rw-r--r--  2.0 unx     9279 b- defN 23-Jul-03 16:16 xtgeo/xyz/_polygons_oper.py
+-rw-r--r--  2.0 unx    13334 b- defN 23-Jul-03 16:16 xtgeo/xyz/_xyz.py
+-rw-r--r--  2.0 unx    15379 b- defN 23-Jul-03 16:16 xtgeo/xyz/_xyz_oper.py
+-rw-r--r--  2.0 unx      199 b- defN 23-Jul-03 16:16 xtgeo/xyz/__init__.py
+-rw-r--r--  2.0 unx    14552 b- defN 23-Jul-03 16:16 xtgeo/xyz/_xyz_io.py
+-rw-r--r--  2.0 unx    18493 b- defN 23-Jul-03 16:16 xtgeo/xyz/_xyz_roxapi.py
+-rw-r--r--  2.0 unx     1125 b- defN 23-Jul-03 16:16 xtgeo/xyz/_xyz_lowlevel.py
+-rw-r--r--  2.0 unx    32611 b- defN 23-Jul-03 16:16 xtgeo/xyz/points.py
+-rw-r--r--  2.0 unx    35461 b- defN 23-Jul-03 16:16 xtgeo/cube/cube1.py
+-rw-r--r--  2.0 unx      149 b- defN 23-Jul-03 16:16 xtgeo/cube/__init__.py
+-rw-r--r--  2.0 unx     4921 b- defN 23-Jul-03 16:16 xtgeo/cube/_cube_roxapi.py
+-rw-r--r--  2.0 unx     7442 b- defN 23-Jul-03 16:16 xtgeo/cube/_cube_utils.py
+-rw-r--r--  2.0 unx    19440 b- defN 23-Jul-03 16:16 xtgeo/cube/_cube_import.py
+-rw-r--r--  2.0 unx     6571 b- defN 23-Jul-03 16:16 xtgeo/cube/_cube_export.py
+-rw-r--r--  2.0 unx      534 b- defN 23-Jul-03 16:16 xtgeo/common/constants.py
+-rw-r--r--  2.0 unx     1071 b- defN 23-Jul-03 16:16 xtgeo/common/exceptions.py
+-rw-r--r--  2.0 unx      372 b- defN 23-Jul-03 16:16 xtgeo/common/__init__.py
+-rw-r--r--  2.0 unx     9157 b- defN 23-Jul-03 16:16 xtgeo/common/calc.py
+-rw-r--r--  2.0 unx    17129 b- defN 23-Jul-03 16:16 xtgeo/common/xtgeo_dialog.py
+-rw-r--r--  2.0 unx    23421 b- defN 23-Jul-03 16:16 xtgeo/common/sys.py
+-rw-r--r--  2.0 unx     4052 b- defN 23-Jul-03 16:16 xtgeo/roxutils/_roxutils_etc.py
+-rw-r--r--  2.0 unx      115 b- defN 23-Jul-03 16:16 xtgeo/roxutils/__init__.py
+-rw-r--r--  2.0 unx     7058 b- defN 23-Jul-03 16:16 xtgeo/roxutils/roxutils.py
+-rw-r--r--  2.0 unx     8300 b- defN 23-Jul-03 16:16 xtgeo/plot/baseplot.py
+-rw-r--r--  2.0 unx      267 b- defN 23-Jul-03 16:16 xtgeo/plot/__init__.py
+-rw-r--r--  2.0 unx     6761 b- defN 23-Jul-03 16:16 xtgeo/plot/grid3d_slice.py
+-rw-r--r--  2.0 unx    37742 b- defN 23-Jul-03 16:16 xtgeo/plot/xsection.py
+-rw-r--r--  2.0 unx    12040 b- defN 23-Jul-03 16:16 xtgeo/plot/_colortables.py
+-rw-r--r--  2.0 unx     7676 b- defN 23-Jul-03 16:16 xtgeo/plot/xtmap.py
+-rwxr-xr-x  2.0 unx   648840 b- defN 23-Jul-03 16:16 xtgeo/cxtgeo/_cxtgeo.cpython-39-x86_64-linux-gnu.so
+-rw-r--r--  2.0 unx        1 b- defN 23-Jul-03 16:16 xtgeo/cxtgeo/__init__.py
+-rw-r--r--  2.0 unx    12689 b- defN 23-Jul-03 16:16 xtgeo/metadata/metadata.py
+-rw-r--r--  2.0 unx      185 b- defN 23-Jul-03 16:16 xtgeo/metadata/__init__.py
+-rw-r--r--  2.0 unx    13772 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_oper.py
+-rw-r--r--  2.0 unx    15166 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_import.py
+-rw-r--r--  2.0 unx     4212 b- defN 23-Jul-03 16:16 xtgeo/surface/_zmap_parser.py
+-rw-r--r--  2.0 unx     1353 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_utils.py
+-rw-r--r--  2.0 unx      126 b- defN 23-Jul-03 16:16 xtgeo/surface/__init__.py
+-rw-r--r--  2.0 unx    10025 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_gridding.py
+-rw-r--r--  2.0 unx     9910 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_cube_window.py
+-rw-r--r--  2.0 unx     6366 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_cube_window_v2.py
+-rw-r--r--  2.0 unx     3488 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_lowlevel.py
+-rw-r--r--  2.0 unx     1290 b- defN 23-Jul-03 16:16 xtgeo/surface/_surfs_import.py
+-rw-r--r--  2.0 unx     7609 b- defN 23-Jul-03 16:16 xtgeo/surface/surfaces.py
+-rw-r--r--  2.0 unx   112381 b- defN 23-Jul-03 16:16 xtgeo/surface/regular_surface.py
+-rw-r--r--  2.0 unx    12391 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_export.py
+-rw-r--r--  2.0 unx     5067 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_cube.py
+-rw-r--r--  2.0 unx     7940 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_roxapi.py
+-rw-r--r--  2.0 unx      828 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_boundary.py
+-rw-r--r--  2.0 unx     4950 b- defN 23-Jul-03 16:16 xtgeo/surface/_regsurf_grid3d.py
+-rw-r--r--  2.0 unx    18145 b- defN 23-Jul-03 16:16 xtgeo/well/_well_oper.py
+-rw-r--r--  2.0 unx     4836 b- defN 23-Jul-03 16:16 xtgeo/well/_wells_utils.py
+-rw-r--r--  2.0 unx    57921 b- defN 23-Jul-03 16:16 xtgeo/well/well1.py
+-rw-r--r--  2.0 unx     8792 b- defN 23-Jul-03 16:16 xtgeo/well/_well_roxapi.py
+-rw-r--r--  2.0 unx     7549 b- defN 23-Jul-03 16:16 xtgeo/well/blocked_well.py
+-rw-r--r--  2.0 unx      198 b- defN 23-Jul-03 16:16 xtgeo/well/__init__.py
+-rw-r--r--  2.0 unx     5792 b- defN 23-Jul-03 16:16 xtgeo/well/blocked_wells.py
+-rw-r--r--  2.0 unx    13868 b- defN 23-Jul-03 16:16 xtgeo/well/_wellmarkers.py
+-rw-r--r--  2.0 unx     9871 b- defN 23-Jul-03 16:16 xtgeo/well/_well_io.py
+-rw-r--r--  2.0 unx     9294 b- defN 23-Jul-03 16:16 xtgeo/well/wells.py
+-rw-r--r--  2.0 unx     1960 b- defN 23-Jul-03 16:16 xtgeo/well/_blockedwells_roxapi.py
+-rw-r--r--  2.0 unx     6555 b- defN 23-Jul-03 16:16 xtgeo/well/_blockedwell_roxapi.py
+-rw-r--r--  2.0 unx     4779 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_op1.py
+-rw-r--r--  2.0 unx     1909 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_ecl_output_file.py
+-rw-r--r--  2.0 unx     1182 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid3d.py
+-rw-r--r--  2.0 unx     1944 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprops_import_roff.py
+-rw-r--r--  2.0 unx     4717 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_lowlevel.py
+-rw-r--r--  2.0 unx     3412 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_import_grdecl.py
+-rw-r--r--  2.0 unx    33747 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_egrid.py
+-rw-r--r--  2.0 unx     3096 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_import_ecl.py
+-rw-r--r--  2.0 unx    26086 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_ecl_grid.py
+-rw-r--r--  2.0 unx    21476 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_find_gridprop_in_eclrun.py
+-rw-r--r--  2.0 unx     2663 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_ecl_logi_head.py
+-rw-r--r--  2.0 unx     4969 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid3d_fence.py
+-rw-r--r--  2.0 unx      341 b- defN 23-Jul-03 16:16 xtgeo/grid3d/__init__.py
+-rw-r--r--  2.0 unx     3491 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_refine.py
+-rw-r--r--  2.0 unx     3032 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_import_xtgcpprop.py
+-rw-r--r--  2.0 unx     1514 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_import_roff.py
+-rw-r--r--  2.0 unx     3433 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_import_roff.py
+-rw-r--r--  2.0 unx    10489 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_roff_parameter.py
+-rw-r--r--  2.0 unx     5650 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid3d_utils.py
+-rw-r--r--  2.0 unx    16521 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_roff_grid.py
+-rw-r--r--  2.0 unx     1869 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_import.py
+-rw-r--r--  2.0 unx     1383 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_hybrid.py
+-rw-r--r--  2.0 unx     3818 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_export.py
+-rw-r--r--  2.0 unx     8239 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grdecl_format.py
+-rw-r--r--  2.0 unx    28624 b- defN 23-Jul-03 16:16 xtgeo/grid3d/grid_properties.py
+-rw-r--r--  2.0 unx     5144 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_wellzone.py
+-rw-r--r--  2.0 unx     6515 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_roxapi.py
+-rw-r--r--  2.0 unx     6299 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_export.py
+-rw-r--r--  2.0 unx    92430 b- defN 23-Jul-03 16:16 xtgeo/grid3d/grid.py
+-rw-r--r--  2.0 unx     4704 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_import_eclrun.py
+-rw-r--r--  2.0 unx     3268 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprop_value_init.py
+-rw-r--r--  2.0 unx    12695 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grdecl_grid.py
+-rw-r--r--  2.0 unx    37296 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_etc1.py
+-rw-r--r--  2.0 unx     8665 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_import_xtgcpgeom.py
+-rw-r--r--  2.0 unx    49493 b- defN 23-Jul-03 16:16 xtgeo/grid3d/grid_property.py
+-rw-r--r--  2.0 unx    10365 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_gridprops_import_eclrun.py
+-rw-r--r--  2.0 unx     4170 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_ecl_inte_head.py
+-rw-r--r--  2.0 unx    15062 b- defN 23-Jul-03 16:16 xtgeo/grid3d/_grid_roxapi.py
+121 files, 1849975 bytes uncompressed, 568795 bytes compressed:  69.3%
```

## zipnote {}

```diff
@@ -1,325 +1,364 @@
-Filename: xtgeo/__init__.py
+Filename: xtgeo-3.1.2.dist-info/
 Comment: 
 
-Filename: xtgeo/_theversion.py
+Filename: xtgeo/
 Comment: 
 
-Filename: xtgeo/common/__init__.py
+Filename: xtgeo.libs/
 Comment: 
 
-Filename: xtgeo/common/calc.py
+Filename: xtgeo-3.1.2.dist-info/METADATA
 Comment: 
 
-Filename: xtgeo/common/constants.py
+Filename: xtgeo-3.1.2.dist-info/LICENSE.md
 Comment: 
 
-Filename: xtgeo/common/exceptions.py
+Filename: xtgeo-3.1.2.dist-info/top_level.txt
 Comment: 
 
-Filename: xtgeo/common/sys.py
+Filename: xtgeo-3.1.2.dist-info/WHEEL
 Comment: 
 
-Filename: xtgeo/common/xtgeo_dialog.py
+Filename: xtgeo-3.1.2.dist-info/RECORD
 Comment: 
 
-Filename: xtgeo/cube/__init__.py
+Filename: xtgeo/xyz/
 Comment: 
 
-Filename: xtgeo/cube/_cube_export.py
+Filename: xtgeo/cube/
 Comment: 
 
-Filename: xtgeo/cube/_cube_import.py
+Filename: xtgeo/common/
 Comment: 
 
-Filename: xtgeo/cube/_cube_roxapi.py
+Filename: xtgeo/roxutils/
 Comment: 
 
-Filename: xtgeo/cube/_cube_utils.py
+Filename: xtgeo/plot/
 Comment: 
 
-Filename: xtgeo/cube/cube1.py
+Filename: xtgeo/cxtgeo/
 Comment: 
 
-Filename: xtgeo/cxtgeo/__init__.py
+Filename: xtgeo/metadata/
 Comment: 
 
-Filename: xtgeo/cxtgeo/_cxtgeo.cp39-win_amd64.pyd
+Filename: xtgeo/surface/
 Comment: 
 
-Filename: xtgeo/grid3d/__init__.py
+Filename: xtgeo/well/
 Comment: 
 
-Filename: xtgeo/grid3d/_ecl_grid.py
+Filename: xtgeo/grid3d/
 Comment: 
 
-Filename: xtgeo/grid3d/_ecl_inte_head.py
-Comment: 
-
-Filename: xtgeo/grid3d/_ecl_logi_head.py
+Filename: xtgeo/_theversion.py
 Comment: 
 
-Filename: xtgeo/grid3d/_ecl_output_file.py
+Filename: xtgeo/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/_egrid.py
+Filename: xtgeo/xyz/polygons.py
 Comment: 
 
-Filename: xtgeo/grid3d/_find_gridprop_in_eclrun.py
+Filename: xtgeo/xyz/_polygons_oper.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grdecl_format.py
+Filename: xtgeo/xyz/_xyz.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grdecl_grid.py
+Filename: xtgeo/xyz/_xyz_oper.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid3d.py
+Filename: xtgeo/xyz/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid3d_fence.py
+Filename: xtgeo/xyz/_xyz_io.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid3d_utils.py
+Filename: xtgeo/xyz/_xyz_roxapi.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_etc1.py
+Filename: xtgeo/xyz/_xyz_lowlevel.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_export.py
+Filename: xtgeo/xyz/points.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_hybrid.py
+Filename: xtgeo/cube/cube1.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_import.py
+Filename: xtgeo/cube/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_import_ecl.py
+Filename: xtgeo/cube/_cube_roxapi.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_import_roff.py
+Filename: xtgeo/cube/_cube_utils.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_import_xtgcpgeom.py
+Filename: xtgeo/cube/_cube_import.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_refine.py
+Filename: xtgeo/cube/_cube_export.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_roxapi.py
+Filename: xtgeo/common/constants.py
 Comment: 
 
-Filename: xtgeo/grid3d/_grid_wellzone.py
+Filename: xtgeo/common/exceptions.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_export.py
+Filename: xtgeo/common/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_import_eclrun.py
+Filename: xtgeo/common/calc.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_import_grdecl.py
+Filename: xtgeo/common/xtgeo_dialog.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_import_roff.py
+Filename: xtgeo/common/sys.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_import_xtgcpprop.py
+Filename: xtgeo/roxutils/_roxutils_etc.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_lowlevel.py
+Filename: xtgeo/roxutils/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_op1.py
+Filename: xtgeo/roxutils/roxutils.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_roxapi.py
+Filename: xtgeo/plot/baseplot.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprop_value_init.py
+Filename: xtgeo/plot/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprops_import_eclrun.py
+Filename: xtgeo/plot/grid3d_slice.py
 Comment: 
 
-Filename: xtgeo/grid3d/_gridprops_import_roff.py
+Filename: xtgeo/plot/xsection.py
 Comment: 
 
-Filename: xtgeo/grid3d/_roff_grid.py
+Filename: xtgeo/plot/_colortables.py
 Comment: 
 
-Filename: xtgeo/grid3d/_roff_parameter.py
+Filename: xtgeo/plot/xtmap.py
 Comment: 
 
-Filename: xtgeo/grid3d/grid.py
+Filename: xtgeo/cxtgeo/_cxtgeo.cpython-39-x86_64-linux-gnu.so
 Comment: 
 
-Filename: xtgeo/grid3d/grid_properties.py
+Filename: xtgeo/cxtgeo/__init__.py
 Comment: 
 
-Filename: xtgeo/grid3d/grid_property.py
+Filename: xtgeo/metadata/metadata.py
 Comment: 
 
 Filename: xtgeo/metadata/__init__.py
 Comment: 
 
-Filename: xtgeo/metadata/metadata.py
+Filename: xtgeo/surface/_regsurf_oper.py
 Comment: 
 
-Filename: xtgeo/plot/__init__.py
+Filename: xtgeo/surface/_regsurf_import.py
 Comment: 
 
-Filename: xtgeo/plot/_colortables.py
+Filename: xtgeo/surface/_zmap_parser.py
 Comment: 
 
-Filename: xtgeo/plot/baseplot.py
+Filename: xtgeo/surface/_regsurf_utils.py
 Comment: 
 
-Filename: xtgeo/plot/grid3d_slice.py
+Filename: xtgeo/surface/__init__.py
 Comment: 
 
-Filename: xtgeo/plot/xsection.py
+Filename: xtgeo/surface/_regsurf_gridding.py
 Comment: 
 
-Filename: xtgeo/plot/xtmap.py
+Filename: xtgeo/surface/_regsurf_cube_window.py
 Comment: 
 
-Filename: xtgeo/roxutils/__init__.py
+Filename: xtgeo/surface/_regsurf_cube_window_v2.py
 Comment: 
 
-Filename: xtgeo/roxutils/_roxutils_etc.py
+Filename: xtgeo/surface/_regsurf_lowlevel.py
 Comment: 
 
-Filename: xtgeo/roxutils/roxutils.py
+Filename: xtgeo/surface/_surfs_import.py
 Comment: 
 
-Filename: xtgeo/surface/__init__.py
+Filename: xtgeo/surface/surfaces.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_boundary.py
+Filename: xtgeo/surface/regular_surface.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_cube.py
+Filename: xtgeo/surface/_regsurf_export.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_cube_window.py
+Filename: xtgeo/surface/_regsurf_cube.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_cube_window_v2.py
+Filename: xtgeo/surface/_regsurf_roxapi.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_export.py
+Filename: xtgeo/surface/_regsurf_boundary.py
 Comment: 
 
 Filename: xtgeo/surface/_regsurf_grid3d.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_gridding.py
+Filename: xtgeo/well/_well_oper.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_import.py
+Filename: xtgeo/well/_wells_utils.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_lowlevel.py
+Filename: xtgeo/well/well1.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_oper.py
+Filename: xtgeo/well/_well_roxapi.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_roxapi.py
+Filename: xtgeo/well/blocked_well.py
 Comment: 
 
-Filename: xtgeo/surface/_regsurf_utils.py
+Filename: xtgeo/well/__init__.py
 Comment: 
 
-Filename: xtgeo/surface/_surfs_import.py
+Filename: xtgeo/well/blocked_wells.py
 Comment: 
 
-Filename: xtgeo/surface/_zmap_parser.py
+Filename: xtgeo/well/_wellmarkers.py
 Comment: 
 
-Filename: xtgeo/surface/regular_surface.py
+Filename: xtgeo/well/_well_io.py
 Comment: 
 
-Filename: xtgeo/surface/surfaces.py
+Filename: xtgeo/well/wells.py
 Comment: 
 
-Filename: xtgeo/well/__init__.py
+Filename: xtgeo/well/_blockedwells_roxapi.py
 Comment: 
 
 Filename: xtgeo/well/_blockedwell_roxapi.py
 Comment: 
 
-Filename: xtgeo/well/_blockedwells_roxapi.py
+Filename: xtgeo/grid3d/_gridprop_op1.py
 Comment: 
 
-Filename: xtgeo/well/_well_io.py
+Filename: xtgeo/grid3d/_ecl_output_file.py
 Comment: 
 
-Filename: xtgeo/well/_well_oper.py
+Filename: xtgeo/grid3d/_grid3d.py
 Comment: 
 
-Filename: xtgeo/well/_well_roxapi.py
+Filename: xtgeo/grid3d/_gridprops_import_roff.py
 Comment: 
 
-Filename: xtgeo/well/_wellmarkers.py
+Filename: xtgeo/grid3d/_gridprop_lowlevel.py
 Comment: 
 
-Filename: xtgeo/well/_wells_utils.py
+Filename: xtgeo/grid3d/_gridprop_import_grdecl.py
 Comment: 
 
-Filename: xtgeo/well/blocked_well.py
+Filename: xtgeo/grid3d/_egrid.py
 Comment: 
 
-Filename: xtgeo/well/blocked_wells.py
+Filename: xtgeo/grid3d/_grid_import_ecl.py
 Comment: 
 
-Filename: xtgeo/well/well1.py
+Filename: xtgeo/grid3d/_ecl_grid.py
 Comment: 
 
-Filename: xtgeo/well/wells.py
+Filename: xtgeo/grid3d/_find_gridprop_in_eclrun.py
 Comment: 
 
-Filename: xtgeo/xyz/__init__.py
+Filename: xtgeo/grid3d/_ecl_logi_head.py
 Comment: 
 
-Filename: xtgeo/xyz/_polygons_oper.py
+Filename: xtgeo/grid3d/_grid3d_fence.py
 Comment: 
 
-Filename: xtgeo/xyz/_xyz.py
+Filename: xtgeo/grid3d/__init__.py
 Comment: 
 
-Filename: xtgeo/xyz/_xyz_io.py
+Filename: xtgeo/grid3d/_grid_refine.py
 Comment: 
 
-Filename: xtgeo/xyz/_xyz_lowlevel.py
+Filename: xtgeo/grid3d/_gridprop_import_xtgcpprop.py
 Comment: 
 
-Filename: xtgeo/xyz/_xyz_oper.py
+Filename: xtgeo/grid3d/_gridprop_import_roff.py
 Comment: 
 
-Filename: xtgeo/xyz/_xyz_roxapi.py
+Filename: xtgeo/grid3d/_grid_import_roff.py
 Comment: 
 
-Filename: xtgeo/xyz/points.py
+Filename: xtgeo/grid3d/_roff_parameter.py
 Comment: 
 
-Filename: xtgeo/xyz/polygons.py
+Filename: xtgeo/grid3d/_grid3d_utils.py
 Comment: 
 
-Filename: xtgeo-3.1.1rc1.dist-info/LICENSE.md
+Filename: xtgeo/grid3d/_roff_grid.py
 Comment: 
 
-Filename: xtgeo-3.1.1rc1.dist-info/METADATA
+Filename: xtgeo/grid3d/_grid_import.py
 Comment: 
 
-Filename: xtgeo-3.1.1rc1.dist-info/WHEEL
+Filename: xtgeo/grid3d/_grid_hybrid.py
 Comment: 
 
-Filename: xtgeo-3.1.1rc1.dist-info/top_level.txt
+Filename: xtgeo/grid3d/_gridprop_export.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grdecl_format.py
+Comment: 
+
+Filename: xtgeo/grid3d/grid_properties.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grid_wellzone.py
+Comment: 
+
+Filename: xtgeo/grid3d/_gridprop_roxapi.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grid_export.py
 Comment: 
 
-Filename: xtgeo-3.1.1rc1.dist-info/RECORD
+Filename: xtgeo/grid3d/grid.py
+Comment: 
+
+Filename: xtgeo/grid3d/_gridprop_import_eclrun.py
+Comment: 
+
+Filename: xtgeo/grid3d/_gridprop_value_init.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grdecl_grid.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grid_etc1.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grid_import_xtgcpgeom.py
+Comment: 
+
+Filename: xtgeo/grid3d/grid_property.py
+Comment: 
+
+Filename: xtgeo/grid3d/_gridprops_import_eclrun.py
+Comment: 
+
+Filename: xtgeo/grid3d/_ecl_inte_head.py
+Comment: 
+
+Filename: xtgeo/grid3d/_grid_roxapi.py
 Comment: 
 
 Zip file comment:
```

## filetype from file(1)

```diff
@@ -1 +1 @@
-Zip archive data, at least v2.0 to extract, compression method=deflate
+Zip archive data, at least v2.0 to extract, compression method=store
```

## xtgeo/__init__.py

 * *Ordering differences only*

```diff
@@ -1,209 +1,209 @@
-# -*- coding: utf-8 -*-
-# flake8: noqa
-# pylint: skip-file
-# type: ignore
-
-"""The XTGeo Python library."""
-
-
-import os
-import timeit
-import warnings
-
-
-try:
-    from ._theversion import version
-
-    __version__ = version
-except ImportError:
-    __version__ = "0.0.0"
-
-
-def _timer(*args):
-    time1 = timeit.default_timer()
-
-    if args:
-        return time1 - args[0]
-
-    return time1
-
-
-TIME0 = _timer()
-
-DEBUG = 19
-
-if os.environ.get("XTG_DEBUG_DEV") is None:
-    DEBUG = 0
-
-
-def _xprint(msg):
-    difftime = _timer(TIME0)
-
-    if DEBUG:
-        print(f"({difftime:4.3f})  {msg}")
-
-
-_xprint("XTGEO __init__ ...")
-
-ROXAR = True
-try:
-    import roxar
-except Exception:
-    ROXAR = False
-
-
-# to avoid problems in batch runs when no DISPLAY is set:
-_xprint("Import matplotlib etc...")
-if not ROXAR:
-    import matplotlib as mplib
-
-    display = os.environ.get("DISPLAY", "")
-    host1 = os.environ.get("HOSTNAME", "")
-    host2 = os.environ.get("HOST", "")
-    dhost = host1 + host2 + display
-
-    ertbool = "LSB_JOBID" in os.environ
-
-    if display == "" or "grid" in dhost or "lgc" in dhost or ertbool:
-        _xprint("")
-        _xprint("=" * 79)
-
-        _xprint(
-            "XTGeo info: No display found or a batch (e.g. ERT) server. "
-            "Using non-interactive Agg backend for matplotlib"
-        )
-        mplib.use("Agg")
-        _xprint("=" * 79)
-
-#
-# Order matters!
-#
-_xprint("Import matplotlib etc...DONE")
-
-from xtgeo.common.constants import UNDEF
-from xtgeo.common.constants import UNDEF_LIMIT
-from xtgeo.common.constants import UNDEF_INT
-from xtgeo.common.constants import UNDEF_INT_LIMIT
-
-from xtgeo.common.exceptions import DateNotFoundError
-from xtgeo.common.exceptions import KeywordNotFoundError
-from xtgeo.common.exceptions import KeywordFoundNoDateError
-from xtgeo.common.exceptions import WellNotFoundError
-from xtgeo.common.exceptions import GridNotFoundError
-from xtgeo.common.exceptions import BlockedWellsNotFoundError
-from xtgeo.cxtgeo._cxtgeo import XTGeoCLibError
-
-from xtgeo.common.xtgeo_dialog import XTGeoDialog
-from xtgeo.common.sys import _XTGeoFile
-
-_xprint("Import common... done")
-
-_xprint("Import various XTGeo modules...")
-
-from xtgeo.roxutils import roxutils
-from xtgeo.roxutils.roxutils import RoxUtils
-
-from xtgeo.well import well1
-from xtgeo.well import wells
-from xtgeo.well import blocked_well
-from xtgeo.well import blocked_wells
-
-from xtgeo.well.well1 import Well
-from xtgeo.well.wells import Wells
-from xtgeo.well.blocked_well import BlockedWell
-from xtgeo.well.blocked_wells import BlockedWells
-
-_xprint("Import various XTGeo modules... wells...")
-
-from xtgeo.surface import regular_surface
-from xtgeo.surface.regular_surface import RegularSurface
-from xtgeo.surface.surfaces import Surfaces
-
-_xprint("Import various XTGeo modules... surface...")
-
-from xtgeo.cube import cube1
-from xtgeo.cube.cube1 import Cube
-
-_xprint("Import various XTGeo modules... cube...")
-
-from xtgeo.grid3d import grid
-from xtgeo.grid3d import grid_property
-from xtgeo.grid3d import grid_properties
-
-from xtgeo.grid3d import Units
-from xtgeo.grid3d import GridRelative
-from xtgeo.grid3d.grid import Grid
-from xtgeo.grid3d.grid_property import GridProperty
-from xtgeo.grid3d.grid_properties import GridProperties
-from xtgeo.grid3d.grid_properties import gridproperties_dataframe
-
-_xprint("Import various XTGeo modules... 3D grids...")
-
-from xtgeo.xyz import points
-from xtgeo.xyz import polygons
-
-from xtgeo.xyz.points import Points
-from xtgeo.xyz.polygons import Polygons
-
-from xtgeo.metadata.metadata import MetaDataRegularSurface
-from xtgeo.metadata.metadata import MetaDataRegularCube
-from xtgeo.metadata.metadata import MetaDataCPGeometry
-from xtgeo.metadata.metadata import MetaDataCPProperty
-from xtgeo.metadata.metadata import MetaDataWell
-
-
-_xprint("Import various XTGeo modules... xyz...")
-
-if not ROXAR:
-    from xtgeo.plot import baseplot
-    from xtgeo.plot import xsection
-    from xtgeo.plot import xtmap
-    from xtgeo.plot import grid3d_slice
-
-_xprint("Import various XTGeo modules... plots...")
-
-_xprint("Import various XTGeo modules...DONE")
-
-# some function wrappers to initiate objects from imports
-_xprint("Import various XTGeo wrappers...")
-from xtgeo.surface.regular_surface import surface_from_file
-from xtgeo.surface.regular_surface import surface_from_roxar
-from xtgeo.surface.regular_surface import surface_from_cube
-from xtgeo.surface.regular_surface import surface_from_grid3d
-
-from xtgeo.grid3d.grid import grid_from_file
-from xtgeo.grid3d.grid import grid_from_roxar
-from xtgeo.grid3d.grid import create_box_grid
-
-from xtgeo.grid3d.grid_property import gridproperty_from_file
-from xtgeo.grid3d.grid_property import gridproperty_from_roxar
-
-from xtgeo.grid3d.grid_properties import gridproperties_from_file
-
-from xtgeo.cube.cube1 import cube_from_file
-from xtgeo.cube.cube1 import cube_from_roxar
-
-from xtgeo.well.well1 import well_from_file
-from xtgeo.well.well1 import well_from_roxar
-
-from xtgeo.well.wells import wells_from_files
-
-from xtgeo.well.blocked_well import blockedwell_from_file
-from xtgeo.well.blocked_well import blockedwell_from_roxar
-
-from xtgeo.well.blocked_wells import blockedwells_from_roxar
-from xtgeo.well.blocked_wells import blockedwells_from_files
-
-from xtgeo.xyz.polygons import polygons_from_file
-from xtgeo.xyz.polygons import polygons_from_roxar
-from xtgeo.xyz.polygons import polygons_from_wells
-
-from xtgeo.xyz.points import points_from_file
-from xtgeo.xyz.points import points_from_roxar
-from xtgeo.xyz.points import points_from_surface
-from xtgeo.xyz.points import points_from_wells
-from xtgeo.xyz.points import points_from_wells_dfrac
-
-warnings.filterwarnings("default", category=DeprecationWarning, module="xtgeo")
-
-_xprint("XTGEO __init__ done")
+# -*- coding: utf-8 -*-
+# flake8: noqa
+# pylint: skip-file
+# type: ignore
+
+"""The XTGeo Python library."""
+
+
+import os
+import timeit
+import warnings
+
+
+try:
+    from ._theversion import version
+
+    __version__ = version
+except ImportError:
+    __version__ = "0.0.0"
+
+
+def _timer(*args):
+    time1 = timeit.default_timer()
+
+    if args:
+        return time1 - args[0]
+
+    return time1
+
+
+TIME0 = _timer()
+
+DEBUG = 19
+
+if os.environ.get("XTG_DEBUG_DEV") is None:
+    DEBUG = 0
+
+
+def _xprint(msg):
+    difftime = _timer(TIME0)
+
+    if DEBUG:
+        print(f"({difftime:4.3f})  {msg}")
+
+
+_xprint("XTGEO __init__ ...")
+
+ROXAR = True
+try:
+    import roxar
+except Exception:
+    ROXAR = False
+
+
+# to avoid problems in batch runs when no DISPLAY is set:
+_xprint("Import matplotlib etc...")
+if not ROXAR:
+    import matplotlib as mplib
+
+    display = os.environ.get("DISPLAY", "")
+    host1 = os.environ.get("HOSTNAME", "")
+    host2 = os.environ.get("HOST", "")
+    dhost = host1 + host2 + display
+
+    ertbool = "LSB_JOBID" in os.environ
+
+    if display == "" or "grid" in dhost or "lgc" in dhost or ertbool:
+        _xprint("")
+        _xprint("=" * 79)
+
+        _xprint(
+            "XTGeo info: No display found or a batch (e.g. ERT) server. "
+            "Using non-interactive Agg backend for matplotlib"
+        )
+        mplib.use("Agg")
+        _xprint("=" * 79)
+
+#
+# Order matters!
+#
+_xprint("Import matplotlib etc...DONE")
+
+from xtgeo.common.constants import UNDEF
+from xtgeo.common.constants import UNDEF_LIMIT
+from xtgeo.common.constants import UNDEF_INT
+from xtgeo.common.constants import UNDEF_INT_LIMIT
+
+from xtgeo.common.exceptions import DateNotFoundError
+from xtgeo.common.exceptions import KeywordNotFoundError
+from xtgeo.common.exceptions import KeywordFoundNoDateError
+from xtgeo.common.exceptions import WellNotFoundError
+from xtgeo.common.exceptions import GridNotFoundError
+from xtgeo.common.exceptions import BlockedWellsNotFoundError
+from xtgeo.cxtgeo._cxtgeo import XTGeoCLibError
+
+from xtgeo.common.xtgeo_dialog import XTGeoDialog
+from xtgeo.common.sys import _XTGeoFile
+
+_xprint("Import common... done")
+
+_xprint("Import various XTGeo modules...")
+
+from xtgeo.roxutils import roxutils
+from xtgeo.roxutils.roxutils import RoxUtils
+
+from xtgeo.well import well1
+from xtgeo.well import wells
+from xtgeo.well import blocked_well
+from xtgeo.well import blocked_wells
+
+from xtgeo.well.well1 import Well
+from xtgeo.well.wells import Wells
+from xtgeo.well.blocked_well import BlockedWell
+from xtgeo.well.blocked_wells import BlockedWells
+
+_xprint("Import various XTGeo modules... wells...")
+
+from xtgeo.surface import regular_surface
+from xtgeo.surface.regular_surface import RegularSurface
+from xtgeo.surface.surfaces import Surfaces
+
+_xprint("Import various XTGeo modules... surface...")
+
+from xtgeo.cube import cube1
+from xtgeo.cube.cube1 import Cube
+
+_xprint("Import various XTGeo modules... cube...")
+
+from xtgeo.grid3d import grid
+from xtgeo.grid3d import grid_property
+from xtgeo.grid3d import grid_properties
+
+from xtgeo.grid3d import Units
+from xtgeo.grid3d import GridRelative
+from xtgeo.grid3d.grid import Grid
+from xtgeo.grid3d.grid_property import GridProperty
+from xtgeo.grid3d.grid_properties import GridProperties
+from xtgeo.grid3d.grid_properties import gridproperties_dataframe
+
+_xprint("Import various XTGeo modules... 3D grids...")
+
+from xtgeo.xyz import points
+from xtgeo.xyz import polygons
+
+from xtgeo.xyz.points import Points
+from xtgeo.xyz.polygons import Polygons
+
+from xtgeo.metadata.metadata import MetaDataRegularSurface
+from xtgeo.metadata.metadata import MetaDataRegularCube
+from xtgeo.metadata.metadata import MetaDataCPGeometry
+from xtgeo.metadata.metadata import MetaDataCPProperty
+from xtgeo.metadata.metadata import MetaDataWell
+
+
+_xprint("Import various XTGeo modules... xyz...")
+
+if not ROXAR:
+    from xtgeo.plot import baseplot
+    from xtgeo.plot import xsection
+    from xtgeo.plot import xtmap
+    from xtgeo.plot import grid3d_slice
+
+_xprint("Import various XTGeo modules... plots...")
+
+_xprint("Import various XTGeo modules...DONE")
+
+# some function wrappers to initiate objects from imports
+_xprint("Import various XTGeo wrappers...")
+from xtgeo.surface.regular_surface import surface_from_file
+from xtgeo.surface.regular_surface import surface_from_roxar
+from xtgeo.surface.regular_surface import surface_from_cube
+from xtgeo.surface.regular_surface import surface_from_grid3d
+
+from xtgeo.grid3d.grid import grid_from_file
+from xtgeo.grid3d.grid import grid_from_roxar
+from xtgeo.grid3d.grid import create_box_grid
+
+from xtgeo.grid3d.grid_property import gridproperty_from_file
+from xtgeo.grid3d.grid_property import gridproperty_from_roxar
+
+from xtgeo.grid3d.grid_properties import gridproperties_from_file
+
+from xtgeo.cube.cube1 import cube_from_file
+from xtgeo.cube.cube1 import cube_from_roxar
+
+from xtgeo.well.well1 import well_from_file
+from xtgeo.well.well1 import well_from_roxar
+
+from xtgeo.well.wells import wells_from_files
+
+from xtgeo.well.blocked_well import blockedwell_from_file
+from xtgeo.well.blocked_well import blockedwell_from_roxar
+
+from xtgeo.well.blocked_wells import blockedwells_from_roxar
+from xtgeo.well.blocked_wells import blockedwells_from_files
+
+from xtgeo.xyz.polygons import polygons_from_file
+from xtgeo.xyz.polygons import polygons_from_roxar
+from xtgeo.xyz.polygons import polygons_from_wells
+
+from xtgeo.xyz.points import points_from_file
+from xtgeo.xyz.points import points_from_roxar
+from xtgeo.xyz.points import points_from_surface
+from xtgeo.xyz.points import points_from_wells
+from xtgeo.xyz.points import points_from_wells_dfrac
+
+warnings.filterwarnings("default", category=DeprecationWarning, module="xtgeo")
+
+_xprint("XTGEO __init__ done")
```

## xtgeo/_theversion.py

```diff
@@ -1,4 +1,4 @@
-# file generated by setuptools_scm
-# don't change, don't track in version control
-__version__ = version = '3.1.1rc1'
-__version_tuple__ = version_tuple = (3, 1, 1)
+# file generated by setuptools_scm
+# don't change, don't track in version control
+__version__ = version = '3.1.2'
+__version_tuple__ = version_tuple = (3, 1, 2)
```

## xtgeo/common/__init__.py

 * *Ordering differences only*

```diff
@@ -1,13 +1,13 @@
-# -*- coding: utf-8 -*-
-"""The XTGeo common package"""
-
-
-# flake8: noqa
-from xtgeo.common.xtgeo_dialog import XTGeoDialog
-from xtgeo.common.xtgeo_dialog import XTGDescription
-from xtgeo.common.xtgeo_dialog import XTGShowProgress
-
-from xtgeo.common.sys import _XTGeoFile
-from xtgeo.common.sys import inherit_docstring
-
-from xtgeo.common.exceptions import WellNotFoundError
+# -*- coding: utf-8 -*-
+"""The XTGeo common package"""
+
+
+# flake8: noqa
+from xtgeo.common.xtgeo_dialog import XTGeoDialog
+from xtgeo.common.xtgeo_dialog import XTGDescription
+from xtgeo.common.xtgeo_dialog import XTGShowProgress
+
+from xtgeo.common.sys import _XTGeoFile
+from xtgeo.common.sys import inherit_docstring
+
+from xtgeo.common.exceptions import WellNotFoundError
```

## xtgeo/common/calc.py

 * *Ordering differences only*

```diff
@@ -1,361 +1,361 @@
-"""Some common XTGEO calculation routines."""
-from typing import List, Tuple
-
-import numpy as np
-
-from xtgeo import XTGeoCLibError
-from xtgeo.common import XTGeoDialog
-from xtgeo.cxtgeo import _cxtgeo
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def ib_to_ijk(ib, nx, ny, nz, ibbase=0, forder=True):
-    """Convert a 1D index (starting from ibbase) to cell indices I J K.
-
-    The default is F-order, but ``forder=False`` gives C order
-
-    Returns I J K as a tuple.
-    """
-
-    logger.info("IB to IJK")
-
-    if forder:
-        iv, jv, kv = _cxtgeo.x_ib2ijk(ib, nx, ny, nz, ibbase)
-    else:
-        iv, jv, kv = _cxtgeo.x_ic2ijk(ib, nx, ny, nz, ibbase)
-
-    return (iv, jv, kv)
-
-
-def ijk_to_ib(i, j, k, nx, ny, nz, ibbase=0, forder=True):
-    """
-    Convert a cell indices I J K to 1D index (starting from ibbase).
-
-    Both Fortran order and C order (Fortran order is default).
-    """
-
-    if forder:
-        # fortran order
-        ib = _cxtgeo.x_ijk2ib(i, j, k, nx, ny, nz, ibbase)
-    else:
-        # c order
-        ib = _cxtgeo.x_ijk2ic(i, j, k, nx, ny, nz, ibbase)
-    if ib < 0:
-        raise IndexError(f"Negative index: {ib}")
-    if ibbase == 0 and ib > nx * ny * nz - 1:
-        xtg.warn("Something is wrong with IJK conversion")
-        xtg.warn(f"I J K, NX, NY, NZ IB: {i} {j} {k} {nx} {ny} {nz} {ib}")
-
-    return ib
-
-
-def xyori_from_ij(
-    iind: int,
-    jind: int,
-    xcor: float,
-    ycor: float,
-    xinc: float,
-    yinc: float,
-    ncol: int,
-    nrow: int,
-    yflip: int,
-    rotation: float,
-) -> Tuple[float, float]:
-    """Get xori and yori given X Y, geometrics and indices for regular maps/cubes.
-
-    Args:
-        iind: I index (zero based)
-        jind: J index (zero based)
-        xcor: X coordinate
-        ycor: Y coordinate
-        xinc: X increment (in non-rotated space)
-        yinc: Y increment (in non-rotated space)
-        ncol: Number of columns
-        nrow: Number of rows
-        yflip: YFLIP (handedness) indicator, 1 og -1
-        rotation: Rotation in degrees, anticlock from X axis
-
-    """
-
-    if iind >= ncol or iind < 0 or jind >= nrow or jind < 0:
-        raise ValueError(
-            f"Indices out of range, offending indices are I, J = ({iind}, {jind}) "
-            f"and valid ranges are (0.. {ncol - 1}, 0.. {nrow - 1})",
-        )
-
-    # the C library and indices with base 1; hence ned to add 1
-    ier, xori, yori = _cxtgeo.surf_xyori_from_ij(
-        iind + 1,
-        jind + 1,
-        xcor,
-        ycor,
-        xinc,
-        yinc,
-        ncol,
-        nrow,
-        yflip,
-        rotation,
-        0,
-    )
-    if ier != 0:
-        raise RuntimeError(f"Error code {ier} from _cxtgeo.surf_xyori_from_ij")
-
-    return xori, yori
-
-
-def vectorinfo2(x1, x2, y1, y2, option=1):
-    """
-    Get length and angles from 2 points in space (2D plane).
-
-    Option = 1 gives normal school angle (counterclock from X), while 0 gives azimuth:
-    positive direction clockwise from North.
-    """
-
-    llen, rad, deg = _cxtgeo.x_vector_info2(x1, x2, y1, y2, option)
-
-    return llen, rad, deg
-
-
-def diffangle(angle1, angle2, option=1):
-    """
-    Find the minimim difference between two angles, option=1 means degress,
-    otherwise radians. The routine think clockwise for differences.
-
-    Examples::
-
-        res = diffangle(30, 40)  # res shall be -10
-        res = diffangle(360, 170)  # res shall be -170
-    """
-
-    return _cxtgeo.x_diff_angle(angle1, angle2, option)
-
-
-def averageangle(anglelist):
-    """
-    Find the average of a list of angles, in degress
-    """
-    return _cxtgeo.x_avg_angles(anglelist)
-
-
-def find_flip(xv, yv):
-    """Find the XY flip status by computing the cross products.
-
-    If flip is 1, then the system is right handed in school algebra
-    but left-handed in reservoir models (where typically
-    X is East, Y is North, assuming Z downwards).
-
-    Args:
-        xv (tuple): First vector (x1, y1, z1)
-        yv (tuple): Second vector (x2, y2, z2)
-
-    Return:
-        Flip flag (1 of -1)
-
-    .. versionchanged:: 2.1 Reverse the number returned, skip zv
-    """
-    flip = 0
-
-    xv = np.array(xv)
-    yv = np.array(yv)
-
-    xycross = np.cross(xv, yv)
-
-    logger.debug("Cross product XY is %s", xycross)
-
-    if xycross[2] < 0:
-        flip = -1
-    else:
-        flip = 1
-
-    return flip
-
-
-def angle2azimuth(inangle, mode="degrees"):
-    """Return the Azimuth angle given input normal angle.
-
-    Normal angle means counterclock rotation from X (East) axis, while
-    azimuth is clockwise rotation from Y (North)
-
-    Args:
-        inangle (float): Input angle in normal manner ("school")
-        mode (str): "degrees" (default) or "radians"
-
-    Return:
-        Azimuth angle (in degrees or radian)
-    """
-    nmode1 = 0
-    nmode2 = 2
-    if mode == "radians":
-        nmode1 += 1
-        nmode2 += 1
-
-    return _cxtgeo.x_rotation_conv(inangle, nmode1, nmode2, 0)
-
-
-def azimuth2angle(inangle, mode="degrees"):
-    """Return the "school" angle given input azimuth angle.
-
-    Normal "school" angle means counterclock rotation from X (East) axis, while
-    azimuth is clockwise rotation from Y (North)
-
-    Args:
-        inangle (float): Input angle in azimuth manner
-        mode (str): "degrees" (default) or "radians"
-
-    Return:
-        Angle (in degrees or radians)
-    """
-    nmode1 = 2
-    nmode2 = 0
-    if mode == "radians":
-        nmode1 += 1
-        nmode2 += 1
-
-    return _cxtgeo.x_rotation_conv(inangle, nmode1, nmode2, 0)
-
-
-def tetrehedron_volume(vertices):
-    """Compute volume of an irregular tetrahedron
-
-    Input is an array of lenght 12 element, and is "list-like" meaning that
-    both lists and contiguous numpy arrays are accepted
-
-    Args:
-        vertices (list-like): Vertices as e.g. numpy array [[x1, y1, z1], [x2, y2, ...]
-
-    Returns:
-        Volume
-    """
-
-    vertices = np.array(vertices, dtype=np.float64)
-
-    return _cxtgeo.x_tetrahedron_volume(vertices)
-
-
-def point_in_tetrahedron(x0, y0, z0, vertices):
-    """Check if point P0 is inside a tetrahedron.
-
-    Args:
-        x0 (double): X xoord of point P0
-        y0 (double): Y xoord of point P0
-        z0 (double): Z xoord of point P0
-        vertices (list-like): Vertices as e.g. numpy array [[x1, y1, z1], [x2, y2, ...]
-
-    Returns:
-        True of inside or on edge, False else
-    """
-
-    vertices = np.array(vertices, dtype=np.float64)
-
-    status = _cxtgeo.x_point_in_tetrahedron(x0, y0, z0, vertices)
-    if status == 1:
-        raise XTGeoCLibError("Error in x_point_in_tetrahedron")
-    if status == 100:
-        return True
-
-    return False
-
-
-def point_in_hexahedron(x0, y0, z0, vertices, _algorithm=1):
-    """Check if point P0 is inside a tetrahedron.
-
-    Vertices my be in order of what 3D cells normally have
-
-    ::
-
-       3        4     7        8      Note in C code, cell corners may be starting
-       |--------|     |--------|      with 0 index, not 1 as shown here
-       |  top   |     |        |
-       |        |     |        |
-       |--------|     |--------|
-       1        2     5        6
-
-
-    Args:
-        x0 (double): X xoord of point P0
-        y0 (double): Y xoord of point P0
-        z0 (double): Z xoord of point P0
-        vertices (list-like): Vertices as e.g. numpy array [[x1, y1, z1], [x2, y2, ...]
-        _algorithm (int): Method for calculation (experimental, default may change)
-
-    Returns:
-        True of inside or on edge, False else
-    """
-
-    vertices = np.array(vertices, dtype=np.float64)
-
-    status = _cxtgeo.x_point_in_hexahedron(x0, y0, z0, vertices, _algorithm)
-
-    if status >= 1:
-        return True
-
-    return False
-
-
-def vectorpair_angle3d(p0, p1, p2, degrees=True, birdview=False):
-    """Find angle in 3D for two vectors
-
-    ::
-
-             / p1
-            /
-           / ) a
-          /---------------- p2
-        p0
-
-    Args:
-        p0 (list like): Common point P0 (x y z)
-        p1 (list like): Point P1 (x y z)
-        p2 (list like): Point P2 (x y z)
-        degrees (bool): The get result in degrees if True, radians if False
-        birdview (bool): If True, find angles projected in Z (bird perspective)
-
-    Returns:
-        Angle. If some problem, e.g. one vector is too short, None is returned
-
-    Raises:
-        ValueError: Errors in input dimensions, all points must have 3 values
-    """
-
-    p0 = np.array(p0)
-    p1 = np.array(p1)
-    p2 = np.array(p2)
-
-    if p0.size != p1.size or p0.size != p2.size or p0.size != 3:
-        raise ValueError("Errors in input dimensions, all points must have 3 values")
-
-    degs = 1
-    if not degrees:
-        degs = 0
-
-    bird = 0
-    if birdview:
-        bird = 1
-
-    angle = _cxtgeo.x_vectorpair_angle3d(p0, p1, p2, degs, bird)
-
-    if angle == -999:
-        return None
-
-    return angle
-
-
-def _swap_axes(
-    rotation: float,
-    yflip: int,
-    **values: List[List],
-):
-    swapped_values = {}
-    for name, val in values.items():
-        swapped_values[name] = np.ascontiguousarray(np.swapaxes(val, 0, 1))
-
-    rotation = rotation + yflip * 90
-    if rotation < 0:
-        rotation += 360
-    if rotation >= 360:
-        rotation -= 360
-
-    return rotation, yflip * -1, swapped_values
+"""Some common XTGEO calculation routines."""
+from typing import List, Tuple
+
+import numpy as np
+
+from xtgeo import XTGeoCLibError
+from xtgeo.common import XTGeoDialog
+from xtgeo.cxtgeo import _cxtgeo
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def ib_to_ijk(ib, nx, ny, nz, ibbase=0, forder=True):
+    """Convert a 1D index (starting from ibbase) to cell indices I J K.
+
+    The default is F-order, but ``forder=False`` gives C order
+
+    Returns I J K as a tuple.
+    """
+
+    logger.info("IB to IJK")
+
+    if forder:
+        iv, jv, kv = _cxtgeo.x_ib2ijk(ib, nx, ny, nz, ibbase)
+    else:
+        iv, jv, kv = _cxtgeo.x_ic2ijk(ib, nx, ny, nz, ibbase)
+
+    return (iv, jv, kv)
+
+
+def ijk_to_ib(i, j, k, nx, ny, nz, ibbase=0, forder=True):
+    """
+    Convert a cell indices I J K to 1D index (starting from ibbase).
+
+    Both Fortran order and C order (Fortran order is default).
+    """
+
+    if forder:
+        # fortran order
+        ib = _cxtgeo.x_ijk2ib(i, j, k, nx, ny, nz, ibbase)
+    else:
+        # c order
+        ib = _cxtgeo.x_ijk2ic(i, j, k, nx, ny, nz, ibbase)
+    if ib < 0:
+        raise IndexError(f"Negative index: {ib}")
+    if ibbase == 0 and ib > nx * ny * nz - 1:
+        xtg.warn("Something is wrong with IJK conversion")
+        xtg.warn(f"I J K, NX, NY, NZ IB: {i} {j} {k} {nx} {ny} {nz} {ib}")
+
+    return ib
+
+
+def xyori_from_ij(
+    iind: int,
+    jind: int,
+    xcor: float,
+    ycor: float,
+    xinc: float,
+    yinc: float,
+    ncol: int,
+    nrow: int,
+    yflip: int,
+    rotation: float,
+) -> Tuple[float, float]:
+    """Get xori and yori given X Y, geometrics and indices for regular maps/cubes.
+
+    Args:
+        iind: I index (zero based)
+        jind: J index (zero based)
+        xcor: X coordinate
+        ycor: Y coordinate
+        xinc: X increment (in non-rotated space)
+        yinc: Y increment (in non-rotated space)
+        ncol: Number of columns
+        nrow: Number of rows
+        yflip: YFLIP (handedness) indicator, 1 og -1
+        rotation: Rotation in degrees, anticlock from X axis
+
+    """
+
+    if iind >= ncol or iind < 0 or jind >= nrow or jind < 0:
+        raise ValueError(
+            f"Indices out of range, offending indices are I, J = ({iind}, {jind}) "
+            f"and valid ranges are (0.. {ncol - 1}, 0.. {nrow - 1})",
+        )
+
+    # the C library and indices with base 1; hence ned to add 1
+    ier, xori, yori = _cxtgeo.surf_xyori_from_ij(
+        iind + 1,
+        jind + 1,
+        xcor,
+        ycor,
+        xinc,
+        yinc,
+        ncol,
+        nrow,
+        yflip,
+        rotation,
+        0,
+    )
+    if ier != 0:
+        raise RuntimeError(f"Error code {ier} from _cxtgeo.surf_xyori_from_ij")
+
+    return xori, yori
+
+
+def vectorinfo2(x1, x2, y1, y2, option=1):
+    """
+    Get length and angles from 2 points in space (2D plane).
+
+    Option = 1 gives normal school angle (counterclock from X), while 0 gives azimuth:
+    positive direction clockwise from North.
+    """
+
+    llen, rad, deg = _cxtgeo.x_vector_info2(x1, x2, y1, y2, option)
+
+    return llen, rad, deg
+
+
+def diffangle(angle1, angle2, option=1):
+    """
+    Find the minimim difference between two angles, option=1 means degress,
+    otherwise radians. The routine think clockwise for differences.
+
+    Examples::
+
+        res = diffangle(30, 40)  # res shall be -10
+        res = diffangle(360, 170)  # res shall be -170
+    """
+
+    return _cxtgeo.x_diff_angle(angle1, angle2, option)
+
+
+def averageangle(anglelist):
+    """
+    Find the average of a list of angles, in degress
+    """
+    return _cxtgeo.x_avg_angles(anglelist)
+
+
+def find_flip(xv, yv):
+    """Find the XY flip status by computing the cross products.
+
+    If flip is 1, then the system is right handed in school algebra
+    but left-handed in reservoir models (where typically
+    X is East, Y is North, assuming Z downwards).
+
+    Args:
+        xv (tuple): First vector (x1, y1, z1)
+        yv (tuple): Second vector (x2, y2, z2)
+
+    Return:
+        Flip flag (1 of -1)
+
+    .. versionchanged:: 2.1 Reverse the number returned, skip zv
+    """
+    flip = 0
+
+    xv = np.array(xv)
+    yv = np.array(yv)
+
+    xycross = np.cross(xv, yv)
+
+    logger.debug("Cross product XY is %s", xycross)
+
+    if xycross[2] < 0:
+        flip = -1
+    else:
+        flip = 1
+
+    return flip
+
+
+def angle2azimuth(inangle, mode="degrees"):
+    """Return the Azimuth angle given input normal angle.
+
+    Normal angle means counterclock rotation from X (East) axis, while
+    azimuth is clockwise rotation from Y (North)
+
+    Args:
+        inangle (float): Input angle in normal manner ("school")
+        mode (str): "degrees" (default) or "radians"
+
+    Return:
+        Azimuth angle (in degrees or radian)
+    """
+    nmode1 = 0
+    nmode2 = 2
+    if mode == "radians":
+        nmode1 += 1
+        nmode2 += 1
+
+    return _cxtgeo.x_rotation_conv(inangle, nmode1, nmode2, 0)
+
+
+def azimuth2angle(inangle, mode="degrees"):
+    """Return the "school" angle given input azimuth angle.
+
+    Normal "school" angle means counterclock rotation from X (East) axis, while
+    azimuth is clockwise rotation from Y (North)
+
+    Args:
+        inangle (float): Input angle in azimuth manner
+        mode (str): "degrees" (default) or "radians"
+
+    Return:
+        Angle (in degrees or radians)
+    """
+    nmode1 = 2
+    nmode2 = 0
+    if mode == "radians":
+        nmode1 += 1
+        nmode2 += 1
+
+    return _cxtgeo.x_rotation_conv(inangle, nmode1, nmode2, 0)
+
+
+def tetrehedron_volume(vertices):
+    """Compute volume of an irregular tetrahedron
+
+    Input is an array of lenght 12 element, and is "list-like" meaning that
+    both lists and contiguous numpy arrays are accepted
+
+    Args:
+        vertices (list-like): Vertices as e.g. numpy array [[x1, y1, z1], [x2, y2, ...]
+
+    Returns:
+        Volume
+    """
+
+    vertices = np.array(vertices, dtype=np.float64)
+
+    return _cxtgeo.x_tetrahedron_volume(vertices)
+
+
+def point_in_tetrahedron(x0, y0, z0, vertices):
+    """Check if point P0 is inside a tetrahedron.
+
+    Args:
+        x0 (double): X xoord of point P0
+        y0 (double): Y xoord of point P0
+        z0 (double): Z xoord of point P0
+        vertices (list-like): Vertices as e.g. numpy array [[x1, y1, z1], [x2, y2, ...]
+
+    Returns:
+        True of inside or on edge, False else
+    """
+
+    vertices = np.array(vertices, dtype=np.float64)
+
+    status = _cxtgeo.x_point_in_tetrahedron(x0, y0, z0, vertices)
+    if status == 1:
+        raise XTGeoCLibError("Error in x_point_in_tetrahedron")
+    if status == 100:
+        return True
+
+    return False
+
+
+def point_in_hexahedron(x0, y0, z0, vertices, _algorithm=1):
+    """Check if point P0 is inside a tetrahedron.
+
+    Vertices my be in order of what 3D cells normally have
+
+    ::
+
+       3        4     7        8      Note in C code, cell corners may be starting
+       |--------|     |--------|      with 0 index, not 1 as shown here
+       |  top   |     |        |
+       |        |     |        |
+       |--------|     |--------|
+       1        2     5        6
+
+
+    Args:
+        x0 (double): X xoord of point P0
+        y0 (double): Y xoord of point P0
+        z0 (double): Z xoord of point P0
+        vertices (list-like): Vertices as e.g. numpy array [[x1, y1, z1], [x2, y2, ...]
+        _algorithm (int): Method for calculation (experimental, default may change)
+
+    Returns:
+        True of inside or on edge, False else
+    """
+
+    vertices = np.array(vertices, dtype=np.float64)
+
+    status = _cxtgeo.x_point_in_hexahedron(x0, y0, z0, vertices, _algorithm)
+
+    if status >= 1:
+        return True
+
+    return False
+
+
+def vectorpair_angle3d(p0, p1, p2, degrees=True, birdview=False):
+    """Find angle in 3D for two vectors
+
+    ::
+
+             / p1
+            /
+           / ) a
+          /---------------- p2
+        p0
+
+    Args:
+        p0 (list like): Common point P0 (x y z)
+        p1 (list like): Point P1 (x y z)
+        p2 (list like): Point P2 (x y z)
+        degrees (bool): The get result in degrees if True, radians if False
+        birdview (bool): If True, find angles projected in Z (bird perspective)
+
+    Returns:
+        Angle. If some problem, e.g. one vector is too short, None is returned
+
+    Raises:
+        ValueError: Errors in input dimensions, all points must have 3 values
+    """
+
+    p0 = np.array(p0)
+    p1 = np.array(p1)
+    p2 = np.array(p2)
+
+    if p0.size != p1.size or p0.size != p2.size or p0.size != 3:
+        raise ValueError("Errors in input dimensions, all points must have 3 values")
+
+    degs = 1
+    if not degrees:
+        degs = 0
+
+    bird = 0
+    if birdview:
+        bird = 1
+
+    angle = _cxtgeo.x_vectorpair_angle3d(p0, p1, p2, degs, bird)
+
+    if angle == -999:
+        return None
+
+    return angle
+
+
+def _swap_axes(
+    rotation: float,
+    yflip: int,
+    **values: List[List],
+):
+    swapped_values = {}
+    for name, val in values.items():
+        swapped_values[name] = np.ascontiguousarray(np.swapaxes(val, 0, 1))
+
+    rotation = rotation + yflip * 90
+    if rotation < 0:
+        rotation += 360
+    if rotation >= 360:
+        rotation -= 360
+
+    return rotation, yflip * -1, swapped_values
```

## xtgeo/common/constants.py

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-# -*- coding: utf-8 -*-
-"""Module for basic XTGeo constants"""
-
-# align with cxtgeo libxtg.h!
-import xtgeo.cxtgeo._cxtgeo as cx
-
-M_PI = 3.14159265358979323846
-PI = M_PI
-PIHALF = 1.57079632679489661923
-
-UNDEF = 10e32
-UNDEF_LIMIT = 9.9e32
-UNDEF_INT = 2000000000
-UNDEF_INT_LIMIT = 1999999999
-
-VERYLARGEPOSITIVE = 10e30
-VERYLARGENEGATIVE = -10e30
-
-UNDEF_MAP_IRAPB = 1e30
-UNDEF_MAP_IRAPA = 9999900.0000
-
-MAXKEYWORDS = cx.MAXKEYWORDS  # maximum keywords for ECL and ROFF scanning
-MAXDATES = cx.MAXDATES  # maximum keywords for ECL scanning
+# -*- coding: utf-8 -*-
+"""Module for basic XTGeo constants"""
+
+# align with cxtgeo libxtg.h!
+import xtgeo.cxtgeo._cxtgeo as cx
+
+M_PI = 3.14159265358979323846
+PI = M_PI
+PIHALF = 1.57079632679489661923
+
+UNDEF = 10e32
+UNDEF_LIMIT = 9.9e32
+UNDEF_INT = 2000000000
+UNDEF_INT_LIMIT = 1999999999
+
+VERYLARGEPOSITIVE = 10e30
+VERYLARGENEGATIVE = -10e30
+
+UNDEF_MAP_IRAPB = 1e30
+UNDEF_MAP_IRAPA = 9999900.0000
+
+MAXKEYWORDS = cx.MAXKEYWORDS  # maximum keywords for ECL and ROFF scanning
+MAXDATES = cx.MAXDATES  # maximum keywords for ECL scanning
```

## xtgeo/common/exceptions.py

 * *Ordering differences only*

```diff
@@ -1,39 +1,39 @@
-# -*- coding: utf-8 -*-
-"""Module for XTGeo defined Exceptions.
-
-This module implements a number of Python exceptions you can raise from
-within your views to trigger a special Exception. Alternatively, they can
-be catched by the standard ValueError which is the base class.
-
-These exceptions will be present on top xtgeo level as e.g.::
-
-  try:
-      a_function
-  except xtgeo.WellNotFoundError:
-      an_action
-
-"""
-
-
-class DateNotFoundError(ValueError):
-    """Invalid date in restart import (date not found) (ValueError)"""
-
-
-class KeywordFoundNoDateError(ValueError):
-    """Keyword found in restart, but not at the given date (ValueError)"""
-
-
-class KeywordNotFoundError(ValueError):
-    """Keyword not found in input (restart, init, roff) (ValueError)"""
-
-
-class WellNotFoundError(ValueError):
-    """Well is not found in the request (ValueError)"""
-
-
-class GridNotFoundError(ValueError):
-    """3D grid is not found in the request (ValueError)"""
-
-
-class BlockedWellsNotFoundError(ValueError):
-    """Blocked Wells icon is not found in the request (ValueError)"""
+# -*- coding: utf-8 -*-
+"""Module for XTGeo defined Exceptions.
+
+This module implements a number of Python exceptions you can raise from
+within your views to trigger a special Exception. Alternatively, they can
+be catched by the standard ValueError which is the base class.
+
+These exceptions will be present on top xtgeo level as e.g.::
+
+  try:
+      a_function
+  except xtgeo.WellNotFoundError:
+      an_action
+
+"""
+
+
+class DateNotFoundError(ValueError):
+    """Invalid date in restart import (date not found) (ValueError)"""
+
+
+class KeywordFoundNoDateError(ValueError):
+    """Keyword found in restart, but not at the given date (ValueError)"""
+
+
+class KeywordNotFoundError(ValueError):
+    """Keyword not found in input (restart, init, roff) (ValueError)"""
+
+
+class WellNotFoundError(ValueError):
+    """Well is not found in the request (ValueError)"""
+
+
+class GridNotFoundError(ValueError):
+    """3D grid is not found in the request (ValueError)"""
+
+
+class BlockedWellsNotFoundError(ValueError):
+    """Blocked Wells icon is not found in the request (ValueError)"""
```

## xtgeo/common/sys.py

 * *Ordering differences only*

```diff
@@ -1,689 +1,689 @@
-# -*- coding: utf-8 -*-
-"""Module for basic XTGeo interaction with OS/system files and folders."""
-
-import hashlib
-import io
-import os
-import pathlib
-import re
-import struct
-import uuid
-from os.path import join
-from platform import system as plfsys
-from tempfile import mkstemp
-from types import BuiltinFunctionType
-from typing import Optional
-
-import h5py
-import numpy as np
-
-from xtgeo.cxtgeo import _cxtgeo
-
-from .xtgeo_dialog import XTGeoDialog
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-SUPPORTED_FORMATS = {
-    "rmswell": ["rmswell", "rmsw", "w", "bw"],
-    "roff_binary": ["roff_binary", "roff", "roff_bin", "roff-bin", "roffbin", "roff.*"],
-    "roff_ascii": ["roff_ascii", "roff_asc", "roff-asc", "roffasc", "asc"],
-    "egrid": ["egrid"],
-    "fegrid": ["fegrid"],
-    "init": ["init"],
-    "finit": ["finit"],
-    "unrst": ["unrst"],
-    "funrst": ["funrst"],
-    "grdecl": ["grdecl"],
-    "bgrdecl": ["bgrdecl"],
-    "irap_binary": ["irap_binary", "irap_bin", "rms_binary", "irapbin", "gri"],
-    "irap_ascii": ["irap_ascii", "irap_asc", "rms_ascii", "irapasc", "fgr"],
-    "hdf": ["hdf", "hdf5", "h5"],
-    "segy": ["segy", "sgy", "segy.*"],
-    "storm": ["storm"],
-    "zmap_ascii": ["zmap", "zmap+", "zmap_ascii", "zmap-ascii", "zmap-asc", "zmap.*"],
-    "ijxyz": ["ijxyz"],
-    "petromod": ["pmd", "petromod"],
-    "xtg": ["xtg", "xtgeo", "xtgf", "xtg.*"],
-    "xyz": ["xyz", "poi", "pol"],
-    "rms_attr": ["rms_attr", "rms_attrs", "rmsattr.*"],
-}
-
-VALID_FILE_ALIASES = ["$fmu-v1", "$md5sum", "$random"]
-
-
-def npfromfile(fname, dtype=np.float32, count=1, offset=0, mmap=False):
-    """Wrapper round np.fromfile to be compatible with older np versions."""
-    try:
-        if mmap:
-            vals = np.memmap(
-                fname, dtype=dtype, shape=(count,), mode="r", offset=offset
-            )
-        else:
-            vals = np.fromfile(fname, dtype=dtype, count=count, offset=offset)
-    except TypeError as err:
-        # offset keyword requires numpy >= 1.17, need this for backward compat.:
-        if "'offset' is an invalid" in str(err):
-            with open(fname, "rb") as buffer:
-                buffer.seek(offset)
-                vals = np.fromfile(buffer, dtype=dtype, count=count)
-        else:
-            raise
-    return vals
-
-
-def check_folder(fname, raiseerror=None):
-    """General function to check folder."""
-    _nn = _XTGeoFile(fname)
-    status = _nn.check_folder(raiseerror=raiseerror)
-    del _nn
-    return status
-
-
-def generic_hash(gid, hashmethod="md5"):
-    """Return a unique hash ID for current instance.
-
-    This hash can e.g. be used to compare two instances for equality.
-
-    Args:
-        gid (str): Any string as signature, e.g. cumulative attributes of an instance.
-        hashmethod (str or function): Supported methods are "md5", "sha256", "blake2b"
-            or use a full function signature e.g. hashlib.sha128.
-
-    Returns:
-        Hash signature.
-
-    Raises:
-        KeyError: String in hashmethod has an invalid option
-
-    .. versionadded:: 2.14
-    """
-    validmethods = {
-        "md5": hashlib.md5,
-        "sha256": hashlib.sha256,
-        "blake2b": hashlib.blake2b,
-    }
-
-    mhash = None
-    if isinstance(hashmethod, str):
-        mhash = validmethods[hashmethod]()
-    elif isinstance(hashmethod, BuiltinFunctionType):
-        mhash = hashmethod()
-
-    mhash.update(gid.encode())
-    return mhash.hexdigest()
-
-
-class _XTGeoFile(object):
-    """A private class for file/stream handling in/out of XTGeo and CXTGeo.
-
-    Interesting attributes:
-
-    xfile = _XTGeoFile(..some Path or  str or BytesIO ...)
-
-    xfile.name: The absolute path to the file (str)
-    xfile.file: The pathlib.Path instance
-    xfile.memstream: Is True if memory stream
-
-    xfile.exist(): Returns True (provided mode 'r') if file exists, always True for 'w'
-    xfile.check_file(...): As above but may raise an Excpetion
-    xfile.check_folder(...): For folder; may raise an Excpetion
-    xfile.splitext(): return file's stem and extension
-    xfile.get_cfhandle(): Get C SWIG file handle
-    xfile.cfclose(): Close current C SWIG filehandle
-
-
-    """
-
-    def __init__(self, fobj, mode="rb", obj=None):
-        self._file = None  # Path instance or BytesIO memory stream
-        self._tmpfile = None
-        self._delete_after = False  # delete file (e.g. tmp) afterwards
-        self._mode = mode
-        self._memstream = False
-
-        self._cfhandle = 0
-        self._cfhandlecount = 0
-
-        # for internal usage in tests; mimick window/mac with no fmemopen in C
-        self._fake_nofmem = False
-
-        logger.debug("Init ran for _XTGeoFile")
-
-        # The self._file must be a Pathlib or a BytesIO instance
-        if isinstance(fobj, pathlib.Path):
-            self._file = fobj
-        elif isinstance(fobj, str):
-            self._file = pathlib.Path(fobj)
-        elif isinstance(fobj, io.BytesIO):
-            self._file = fobj
-            self._memstream = True
-        elif isinstance(fobj, io.StringIO):
-            self._file = fobj
-            self._memstream = True
-        elif isinstance(fobj, _XTGeoFile):
-            raise RuntimeError("Reinstancing object, not allowed", self.__class__)
-        else:
-            raise RuntimeError(
-                f"Illegal input, cannot continue ({self.__class__}) "
-                f"{fobj}: {type(fobj)}"
-            )
-
-        if obj and not self._memstream:
-            self.resolve_alias(obj)
-
-        logger.info("Ran init of %s, ID is %s", __name__, id(self))
-
-    @property
-    def memstream(self):
-        """Read only: Get True if file object is a memory stream (BytesIO)."""
-        return self._memstream
-
-    @property
-    def file(self):
-        """Read only: Get Path object (if input was file) or BytesIO object."""
-        return self._file
-
-    @property
-    def name(self):
-        """The absolute path name of a file."""
-        logger.info("Get absolute name of file...")
-
-        if self._memstream:
-            return self._file
-
-        try:
-            logger.debug("Try resolve...")
-            fname = str(self._file.resolve())
-        except OSError:
-            try:
-                logger.debug("Try resolve parent, then file...")
-                fname = os.path.abspath(
-                    join(str(self._file.parent.resolve()), str(self._file.name))
-                )
-            except OSError:
-                # means that also folder is invalid
-                logger.debug("Last attempt of name resolving...")
-                fname = os.path.abspath(str(self._file))
-        return fname
-
-    def resolve_alias(self, obj):
-        """Change a file path name alias to autogenerated name, based on rules.
-
-        Only the file stem name will be updated, not the file name extension. Any
-        parent folders and file suffix/extension will be returned as is.
-
-        Aliases supported so far are '$md5sum' '$random' '$fmu-v1'
-
-        Args:
-            obj (XTGeo instance): Instance of e.g. RegularSurface()
-
-        Example::
-            >>> import xtgeo
-            >>> surf = xtgeo.surface_from_file(surface_dir + "/topreek_rota.gri")
-            >>> xx = _XTGeoFile("/tmp/$md5sum.gri", "rb", surf)
-            >>> print(xx.file)
-            /tmp/c144fe19742adac8187b97e7976ac68c.gri
-
-        .. versionadded:: 2.14
-        """
-        fileobj = self._file
-
-        parent = fileobj.parent
-        stem = fileobj.stem
-        suffix = fileobj.suffix
-
-        if "$" in stem and stem not in VALID_FILE_ALIASES:
-            raise ValueError(
-                "A '$' is present in file name but this is not a valid alias"
-            )
-
-        newname = stem
-        if stem == "$md5sum":
-            newname = obj.generate_hash()
-        elif stem == "$random":
-            newname = uuid.uuid4().hex  # random name
-        elif stem == "$fmu-v1":
-            # will make name such as topvalysar--avg_porosity based on metadata
-            short = obj.metadata.opt.shortname.lower().replace(" ", "_")
-            desc = obj.metadata.opt.description.lower().replace(" ", "_")
-            date = obj.metadata.opt.datetime
-            newname = short + "--" + desc
-            if date:
-                newname += "--" + date
-        else:
-            # return without modifications of self._file to avoid with_suffix() issues
-            # if the file name stem itself contains multiple '.'
-            return
-
-        self._file = (parent / newname).with_suffix(suffix)
-
-    def exists(self):  # was: file_exists
-        """Check if 'r' file, memory stream or folder exists, and returns True of OK."""
-        if "r" in self._mode:
-            if isinstance(self._file, io.BytesIO):
-                return True
-
-            if self._file.exists():
-                return True
-
-            return False
-
-        return True
-
-    def check_file(self, raiseerror=None, raisetext=None):
-        """Check if a file exists, and raises an OSError if not.
-
-        This is only meaningful for 'r' files.
-
-        Args:
-            raiseerror (Exception): Type of Exception, default is None, which means
-                no Exception, just return False or True
-            raisetext (str): Which message to display if raiseerror, None gives a
-                default message.
-
-        Return:
-            status: True, if file exists and is readable, False if not.
-        """
-        logger.info("Checking file...")
-
-        if self.memstream:
-            return True
-
-        if raisetext is None:
-            raisetext = f"File {self.name} does not exist or cannot be accessed"
-
-        if "r" in self._mode:
-            if not self._file.is_file() or not self.exists():
-                if raiseerror is not None:
-                    raise raiseerror(raisetext)
-
-                return False
-
-        return True
-
-    def check_folder(self, raiseerror=None, raisetext=None):
-        """Check if folder given in xfile exists and is writeable.
-
-        The file itself may not exist (yet), only the folder is checked
-
-        Args:
-            raiseerror (exception): If none, then return True or False, else raise the
-                given Exception if False
-            raisetext (str): Text to raise.
-
-        Return:
-            status: True, if folder exists and is writable, False if not.
-
-        Raises:
-            ValueError: If the file is a memstream
-
-        """
-        if self.memstream:
-            raise ValueError("Tried to check folder status of a in-memory file")
-
-        logger.info("Checking folder...")
-
-        status = True
-        folder = self._file.parent
-        if raisetext is None:
-            raisetext = f"Folder {folder.name} does not exist or cannot be accessed"
-
-        if not folder.exists():
-            if raiseerror:
-                raise raiseerror(raisetext)
-
-            status = False
-
-        return status
-
-        # # Here are issues here on Windows/Mac in particular
-
-        # status = True
-
-        # if os.path.isdir(self._file):
-        #     folder = self._file
-        # else:
-        #     folder = os.path.dirname(self._file)
-        #     if folder == "":
-        #         folder = "."
-
-        # if not os.path.exists(folder):
-        #     if raiseerror:
-        #         raise raiseerror("Folder does not exist: <{}>".format(folder))
-
-        #     status = False
-
-        # if os.path.exists(folder) and not os.access(folder, os.W_OK):
-        #     if raiseerror:
-        #         raise raiseerror(
-        #             "Folder does exist but is not writable: <{}>".format(folder)
-        #         )
-
-        #     status = False
-
-        # return status
-
-    def splitext(self, lower=False):
-        """Return file stem and suffix, always lowercase if lower is True."""
-        logger.info("Run splitext to get stem and suffix...")
-
-        stem = self._file.stem
-        suffix = self._file.suffix
-        suffix = suffix.replace(".", "")
-
-        if lower:
-            stem = stem.lower()
-            suffix = suffix.lower()
-
-        return stem, suffix
-
-    def get_cfhandle(self):  # was get_handle
-        """Get SWIG C file handle for CXTgeo.
-
-        This is tied to cfclose() which closes the file.
-
-        if _cfhandle already exists, then _cfhandlecount is increased with 1
-
-        """
-        # differ on Linux and other OS as Linux can use fmemopen() in C
-        islinux = True
-        fobj = None
-        if plfsys() != "Linux":
-            islinux = False
-
-        if self._cfhandle and "Swig Object of type 'FILE" in str(self._cfhandle):
-            self._cfhandlecount += 1
-            logger.info("Get SWIG C fhandle no %s", self._cfhandlecount)
-            return self._cfhandle
-
-        if isinstance(self._file, io.BytesIO) and self._mode == "rb" and islinux:
-            fobj = self._file.getvalue()  # bytes type in Python3, str in Python2
-
-            # note that the typemap in swig computes the length for the buf/fobj!
-            self._memstream = True
-
-        elif isinstance(self._file, io.BytesIO) and self._mode == "wb" and islinux:
-            fobj = bytes()
-            self._memstream = True
-
-        elif (
-            isinstance(self._file, io.BytesIO)
-            and self._mode == "rb"
-            and not islinux  # Windows or Darwin
-        ):
-            # windows/mac miss fmemopen; write buffer to a tmp instead as workaround
-            fds, self._tmpfile = mkstemp(prefix="tmpxtgeoio")
-            os.close(fds)
-            with open(self._tmpfile, "wb") as newfile:
-                newfile.write(self._file.getvalue())
-
-        else:
-            fobj = self.name
-
-        if self._memstream:
-            if islinux:
-                cfhandle = _cxtgeo.xtg_fopen_bytestream(fobj, self._mode)
-            else:
-                cfhandle = _cxtgeo.xtg_fopen(self._tmpfile, self._mode)
-
-        else:
-            try:
-                cfhandle = _cxtgeo.xtg_fopen(fobj, self._mode)
-            except TypeError as err:
-                raise IOError(f"Cannot open file: {fobj}") from err
-
-        self._cfhandle = cfhandle
-        self._cfhandlecount = 1
-
-        logger.info("Get initial SWIG C fhandle no %s", self._cfhandlecount)
-        return self._cfhandle
-
-    def cfclose(self, strict=True):
-        """Close SWIG C file handle by keeping track of _cfhandlecount.
-
-        Return True if cfhandle is really closed.
-        """
-        logger.info("Request for closing SWIG fhandle no: %s", self._cfhandlecount)
-
-        if self._cfhandle is None or self._cfhandlecount == 0:
-            if strict:
-                raise RuntimeError("Ask to close a nonexisting C file handle")
-
-            self._cfhandle = None
-            self._cfhandlecount = 0
-            return True
-
-        if self._cfhandlecount > 1 or self._cfhandlecount == 0:
-            self._cfhandlecount -= 1
-            logger.info(
-                "Remaining SWIG cfhandles: %s, do not close...", self._cfhandlecount
-            )
-            return False
-
-        if self._memstream and self._cfhandle and "w" in self._mode:
-            # this assures that the file pointer is in the end of the current filehandle
-            npos = _cxtgeo.xtg_ftell(self._cfhandle)
-            buf = bytes(npos)
-            ier = _cxtgeo.xtg_get_fbuffer(self._cfhandle, buf)
-            if ier == 0:
-                self._file.write(buf)  # write to bytesIO instance
-                _cxtgeo.xtg_fflush(self._cfhandle)
-            else:
-                raise RuntimeError("Could not write stream for unknown reasons")
-
-        ier = _cxtgeo.xtg_fclose(self._cfhandle)
-        if ier != 0:
-            raise RuntimeError(f"Could not close C file, code {ier}")
-
-        logger.info("File is now closed for C io: %s", self.name)
-
-        if self._tmpfile:
-            try:
-                os.remove(self._tmpfile)
-            except Exception as ex:  # pylint: disable=W0703
-                logger.error("Could not remove tempfile for some reason: %s", ex)
-
-        self._cfhandle = None
-        self._cfhandlecount = 0
-        logger.info("Remaining SWIG cfhandles: %s, return is True", self._cfhandlecount)
-
-        return True
-
-    def detect_fformat(
-        self, details: Optional[bool] = False, suffixonly: Optional[bool] = False
-    ):
-        """Try to deduce format from looking at file signature.
-
-        The file signature may be the initial part of the binary file/stream but if
-        that fails, the file extension is used.
-
-        Args:
-            details: If True, more info is added to the return string (useful for some
-                formats) e.g. "hdf RegularSurface xtgeo"
-            suffixonly: If True, look at file suffix only.
-
-        Returns:
-            A string with format spesification, e.g. "hdf".
-        """
-
-        if not suffixonly:
-            fformat = self._detect_fformat_by_contents(details)
-            if fformat is not None:
-                return fformat
-
-        # if looking at contents failed, look at extension
-        fmt = self._detect_format_by_extension()
-        return self._validate_format(fmt)
-
-    def _detect_fformat_by_contents(self, details: Optional[bool] = False):
-        # Try the read the N first bytes
-        maxbuf = 100
-
-        if self.memstream:
-            self.file.seek(0)
-            buf = self.file.read(maxbuf)
-            self.file.seek(0)
-        else:
-            if not self.exists():
-                raise ValueError(f"File {self.name} does not exist")
-            with open(self.file, "rb") as fhandle:
-                buf = fhandle.read(maxbuf)
-
-        if not isinstance(buf, bytes):
-            return None
-
-        # HDF format, different variants
-        if len(buf) >= 4:
-            _, hdf = struct.unpack("b 3s", buf[:4])
-            if hdf == b"HDF":
-                logger.info("Signature is hdf")
-
-                main = self._validate_format("hdf")
-                fmt = ""
-                provider = ""
-                if details:
-                    with h5py.File(self.file, "r") as hstream:
-                        for xtgtype in ["RegularSurface", "Well", "CornerPointGrid"]:
-                            if xtgtype in hstream.keys():
-                                fmt = xtgtype
-                                grp = hstream.require_group(xtgtype)
-                                try:
-                                    provider = grp.attrs["provider"]
-                                except KeyError:
-                                    provider = "unknown"
-                                break
-
-                    return f"{main} {fmt} {provider}"
-                else:
-                    return main
-
-        # Irap binary regular surface format
-        if len(buf) >= 8:
-            fortranblock, gricode = struct.unpack(">ii", buf[:8])
-            if fortranblock == 32 and gricode == -996:
-                logger.info("Signature is irap binary")
-                return self._validate_format("irap_binary")
-
-        # Petromod binary regular surface
-        if b"Content=Map" in buf and b"DataUnitDistance" in buf:
-            logger.info("Signature is petromod")
-            return self._validate_format("petromod")
-
-        # Eclipse binary 3D EGRID, look at FILEHEAD:
-        #  'FILEHEAD'         100 'INTE'
-        #   3        2016           0           0           0           0
-        #  (ver)    (release)      (reserved)   (backw)    (gtype)      (dualporo)
-
-        if len(buf) >= 24:
-            fort1, name, num, _, fort2 = struct.unpack("> i 8s i 4s i", buf[:24])
-            if fort1 == 16 and name == b"FILEHEAD" and num == 100 and fort2 == 16:
-                # Eclipse corner point EGRID
-                logger.info("Signature is egrid")
-                return self._validate_format("egrid")
-            # Eclipse binary 3D UNRST, look for SEQNUM:
-            #  'SEQNUM'         1 'INTE'
-            if fort1 == 16 and name == b"SEQNUM  " and num == 1 and fort2 == 16:
-                # Eclipse UNRST
-                logger.info("Signature is unrst")
-                return self._validate_format("unrst")
-            # Eclipse binary 3D INIT, look for INTEHEAD:
-            #  'INTEHEAD'         411 'INTE'
-            if fort1 == 16 and name == b"INTEHEAD" and num > 400 and fort2 == 16:
-                # Eclipse INIT
-                logger.info("Signature is init")
-
-                return self._validate_format("init")
-
-        if len(buf) >= 9:
-            name, _ = struct.unpack("8s b", buf[:9])
-            # ROFF binary 3D
-            if name == b"roff-bin":
-                logger.info("Signature is roff_binary")
-                return self._validate_format("roff_binary")
-            # ROFF ascii 3D
-            if name == b"roff-asc":
-                logger.info("Signature is roff_ascii")
-                return self._validate_format("roff_ascii")
-
-        # RMS well format (ascii)
-        # 1.0
-        # Unknown
-        # WELL12 90941.63200000004 5506367.711 23.0
-        # ...
-        # The signature here is one float in first line with values 1.0; one string
-        # in second line; and 3 or 4 items in the next (sometimes RKB is missing)
-        try:
-            xbuf = buf.decode().split("\n")
-        except UnicodeDecodeError:
-            return None
-
-        if (
-            len(xbuf) >= 3
-            and xbuf[0] == "1.0"
-            and len(xbuf[1]) >= 1
-            and len(xbuf[2]) >= 10
-        ):
-            logger.info("Signature is rmswell")
-            return self._validate_format("rmswell")
-
-        return None
-
-    def _detect_format_by_extension(self):
-        """Detect format by extension."""
-        if self.memstream:
-            return "unknown"
-
-        suffix = self.file.suffix[1:].lower()
-
-        for fmt, variants in SUPPORTED_FORMATS.items():
-            if suffix in variants:
-                logger.info("Extension hints: %s", fmt)
-                return fmt
-
-        # if none of these above are accepted, check regular expression
-        # (intentional to complete all variant in loop above first before trying re())
-        for fmt, variants in SUPPORTED_FORMATS.items():
-            for var in variants:
-                if "*" in var:
-                    pattern = re.compile(var)
-                    if pattern.match(suffix):
-                        logger.info("Extension by regexp hints %s", fmt)
-                        return fmt
-
-        return "unknown"
-
-    @staticmethod
-    def _validate_format(fmt):
-        """Validate format."""
-        if fmt in SUPPORTED_FORMATS.keys() or fmt == "unknown":
-            return fmt
-        else:
-            raise RuntimeError(f"Invalid format: {fmt}")
-
-    @staticmethod
-    def generic_format_by_proposal(propose):
-        """Get generic format by proposal."""
-        for fmt, variants in SUPPORTED_FORMATS.items():
-            if propose in variants:
-                return fmt
-
-        # if none of these above are accepted, check regular expression
-        for fmt, variants in SUPPORTED_FORMATS.items():
-            for var in variants:
-                if "*" in var:
-                    pattern = re.compile(var)
-                    if pattern.match(propose):
-                        return fmt
-
-        raise ValueError(f"Non-supportred file extension: {propose}")
-
-
-def inherit_docstring(inherit_from):
-    def decorator_set_docstring(func):
-        if func.__doc__ is None and inherit_from.__doc__ is not None:
-            func.__doc__ = inherit_from.__doc__
-        return func
-
-    return decorator_set_docstring
+# -*- coding: utf-8 -*-
+"""Module for basic XTGeo interaction with OS/system files and folders."""
+
+import hashlib
+import io
+import os
+import pathlib
+import re
+import struct
+import uuid
+from os.path import join
+from platform import system as plfsys
+from tempfile import mkstemp
+from types import BuiltinFunctionType
+from typing import Optional
+
+import h5py
+import numpy as np
+
+from xtgeo.cxtgeo import _cxtgeo
+
+from .xtgeo_dialog import XTGeoDialog
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+SUPPORTED_FORMATS = {
+    "rmswell": ["rmswell", "rmsw", "w", "bw"],
+    "roff_binary": ["roff_binary", "roff", "roff_bin", "roff-bin", "roffbin", "roff.*"],
+    "roff_ascii": ["roff_ascii", "roff_asc", "roff-asc", "roffasc", "asc"],
+    "egrid": ["egrid"],
+    "fegrid": ["fegrid"],
+    "init": ["init"],
+    "finit": ["finit"],
+    "unrst": ["unrst"],
+    "funrst": ["funrst"],
+    "grdecl": ["grdecl"],
+    "bgrdecl": ["bgrdecl"],
+    "irap_binary": ["irap_binary", "irap_bin", "rms_binary", "irapbin", "gri"],
+    "irap_ascii": ["irap_ascii", "irap_asc", "rms_ascii", "irapasc", "fgr"],
+    "hdf": ["hdf", "hdf5", "h5"],
+    "segy": ["segy", "sgy", "segy.*"],
+    "storm": ["storm"],
+    "zmap_ascii": ["zmap", "zmap+", "zmap_ascii", "zmap-ascii", "zmap-asc", "zmap.*"],
+    "ijxyz": ["ijxyz"],
+    "petromod": ["pmd", "petromod"],
+    "xtg": ["xtg", "xtgeo", "xtgf", "xtg.*"],
+    "xyz": ["xyz", "poi", "pol"],
+    "rms_attr": ["rms_attr", "rms_attrs", "rmsattr.*"],
+}
+
+VALID_FILE_ALIASES = ["$fmu-v1", "$md5sum", "$random"]
+
+
+def npfromfile(fname, dtype=np.float32, count=1, offset=0, mmap=False):
+    """Wrapper round np.fromfile to be compatible with older np versions."""
+    try:
+        if mmap:
+            vals = np.memmap(
+                fname, dtype=dtype, shape=(count,), mode="r", offset=offset
+            )
+        else:
+            vals = np.fromfile(fname, dtype=dtype, count=count, offset=offset)
+    except TypeError as err:
+        # offset keyword requires numpy >= 1.17, need this for backward compat.:
+        if "'offset' is an invalid" in str(err):
+            with open(fname, "rb") as buffer:
+                buffer.seek(offset)
+                vals = np.fromfile(buffer, dtype=dtype, count=count)
+        else:
+            raise
+    return vals
+
+
+def check_folder(fname, raiseerror=None):
+    """General function to check folder."""
+    _nn = _XTGeoFile(fname)
+    status = _nn.check_folder(raiseerror=raiseerror)
+    del _nn
+    return status
+
+
+def generic_hash(gid, hashmethod="md5"):
+    """Return a unique hash ID for current instance.
+
+    This hash can e.g. be used to compare two instances for equality.
+
+    Args:
+        gid (str): Any string as signature, e.g. cumulative attributes of an instance.
+        hashmethod (str or function): Supported methods are "md5", "sha256", "blake2b"
+            or use a full function signature e.g. hashlib.sha128.
+
+    Returns:
+        Hash signature.
+
+    Raises:
+        KeyError: String in hashmethod has an invalid option
+
+    .. versionadded:: 2.14
+    """
+    validmethods = {
+        "md5": hashlib.md5,
+        "sha256": hashlib.sha256,
+        "blake2b": hashlib.blake2b,
+    }
+
+    mhash = None
+    if isinstance(hashmethod, str):
+        mhash = validmethods[hashmethod]()
+    elif isinstance(hashmethod, BuiltinFunctionType):
+        mhash = hashmethod()
+
+    mhash.update(gid.encode())
+    return mhash.hexdigest()
+
+
+class _XTGeoFile(object):
+    """A private class for file/stream handling in/out of XTGeo and CXTGeo.
+
+    Interesting attributes:
+
+    xfile = _XTGeoFile(..some Path or  str or BytesIO ...)
+
+    xfile.name: The absolute path to the file (str)
+    xfile.file: The pathlib.Path instance
+    xfile.memstream: Is True if memory stream
+
+    xfile.exist(): Returns True (provided mode 'r') if file exists, always True for 'w'
+    xfile.check_file(...): As above but may raise an Excpetion
+    xfile.check_folder(...): For folder; may raise an Excpetion
+    xfile.splitext(): return file's stem and extension
+    xfile.get_cfhandle(): Get C SWIG file handle
+    xfile.cfclose(): Close current C SWIG filehandle
+
+
+    """
+
+    def __init__(self, fobj, mode="rb", obj=None):
+        self._file = None  # Path instance or BytesIO memory stream
+        self._tmpfile = None
+        self._delete_after = False  # delete file (e.g. tmp) afterwards
+        self._mode = mode
+        self._memstream = False
+
+        self._cfhandle = 0
+        self._cfhandlecount = 0
+
+        # for internal usage in tests; mimick window/mac with no fmemopen in C
+        self._fake_nofmem = False
+
+        logger.debug("Init ran for _XTGeoFile")
+
+        # The self._file must be a Pathlib or a BytesIO instance
+        if isinstance(fobj, pathlib.Path):
+            self._file = fobj
+        elif isinstance(fobj, str):
+            self._file = pathlib.Path(fobj)
+        elif isinstance(fobj, io.BytesIO):
+            self._file = fobj
+            self._memstream = True
+        elif isinstance(fobj, io.StringIO):
+            self._file = fobj
+            self._memstream = True
+        elif isinstance(fobj, _XTGeoFile):
+            raise RuntimeError("Reinstancing object, not allowed", self.__class__)
+        else:
+            raise RuntimeError(
+                f"Illegal input, cannot continue ({self.__class__}) "
+                f"{fobj}: {type(fobj)}"
+            )
+
+        if obj and not self._memstream:
+            self.resolve_alias(obj)
+
+        logger.info("Ran init of %s, ID is %s", __name__, id(self))
+
+    @property
+    def memstream(self):
+        """Read only: Get True if file object is a memory stream (BytesIO)."""
+        return self._memstream
+
+    @property
+    def file(self):
+        """Read only: Get Path object (if input was file) or BytesIO object."""
+        return self._file
+
+    @property
+    def name(self):
+        """The absolute path name of a file."""
+        logger.info("Get absolute name of file...")
+
+        if self._memstream:
+            return self._file
+
+        try:
+            logger.debug("Try resolve...")
+            fname = str(self._file.resolve())
+        except OSError:
+            try:
+                logger.debug("Try resolve parent, then file...")
+                fname = os.path.abspath(
+                    join(str(self._file.parent.resolve()), str(self._file.name))
+                )
+            except OSError:
+                # means that also folder is invalid
+                logger.debug("Last attempt of name resolving...")
+                fname = os.path.abspath(str(self._file))
+        return fname
+
+    def resolve_alias(self, obj):
+        """Change a file path name alias to autogenerated name, based on rules.
+
+        Only the file stem name will be updated, not the file name extension. Any
+        parent folders and file suffix/extension will be returned as is.
+
+        Aliases supported so far are '$md5sum' '$random' '$fmu-v1'
+
+        Args:
+            obj (XTGeo instance): Instance of e.g. RegularSurface()
+
+        Example::
+            >>> import xtgeo
+            >>> surf = xtgeo.surface_from_file(surface_dir + "/topreek_rota.gri")
+            >>> xx = _XTGeoFile("/tmp/$md5sum.gri", "rb", surf)
+            >>> print(xx.file)
+            /tmp/c144fe19742adac8187b97e7976ac68c.gri
+
+        .. versionadded:: 2.14
+        """
+        fileobj = self._file
+
+        parent = fileobj.parent
+        stem = fileobj.stem
+        suffix = fileobj.suffix
+
+        if "$" in stem and stem not in VALID_FILE_ALIASES:
+            raise ValueError(
+                "A '$' is present in file name but this is not a valid alias"
+            )
+
+        newname = stem
+        if stem == "$md5sum":
+            newname = obj.generate_hash()
+        elif stem == "$random":
+            newname = uuid.uuid4().hex  # random name
+        elif stem == "$fmu-v1":
+            # will make name such as topvalysar--avg_porosity based on metadata
+            short = obj.metadata.opt.shortname.lower().replace(" ", "_")
+            desc = obj.metadata.opt.description.lower().replace(" ", "_")
+            date = obj.metadata.opt.datetime
+            newname = short + "--" + desc
+            if date:
+                newname += "--" + date
+        else:
+            # return without modifications of self._file to avoid with_suffix() issues
+            # if the file name stem itself contains multiple '.'
+            return
+
+        self._file = (parent / newname).with_suffix(suffix)
+
+    def exists(self):  # was: file_exists
+        """Check if 'r' file, memory stream or folder exists, and returns True of OK."""
+        if "r" in self._mode:
+            if isinstance(self._file, io.BytesIO):
+                return True
+
+            if self._file.exists():
+                return True
+
+            return False
+
+        return True
+
+    def check_file(self, raiseerror=None, raisetext=None):
+        """Check if a file exists, and raises an OSError if not.
+
+        This is only meaningful for 'r' files.
+
+        Args:
+            raiseerror (Exception): Type of Exception, default is None, which means
+                no Exception, just return False or True
+            raisetext (str): Which message to display if raiseerror, None gives a
+                default message.
+
+        Return:
+            status: True, if file exists and is readable, False if not.
+        """
+        logger.info("Checking file...")
+
+        if self.memstream:
+            return True
+
+        if raisetext is None:
+            raisetext = f"File {self.name} does not exist or cannot be accessed"
+
+        if "r" in self._mode:
+            if not self._file.is_file() or not self.exists():
+                if raiseerror is not None:
+                    raise raiseerror(raisetext)
+
+                return False
+
+        return True
+
+    def check_folder(self, raiseerror=None, raisetext=None):
+        """Check if folder given in xfile exists and is writeable.
+
+        The file itself may not exist (yet), only the folder is checked
+
+        Args:
+            raiseerror (exception): If none, then return True or False, else raise the
+                given Exception if False
+            raisetext (str): Text to raise.
+
+        Return:
+            status: True, if folder exists and is writable, False if not.
+
+        Raises:
+            ValueError: If the file is a memstream
+
+        """
+        if self.memstream:
+            raise ValueError("Tried to check folder status of a in-memory file")
+
+        logger.info("Checking folder...")
+
+        status = True
+        folder = self._file.parent
+        if raisetext is None:
+            raisetext = f"Folder {folder.name} does not exist or cannot be accessed"
+
+        if not folder.exists():
+            if raiseerror:
+                raise raiseerror(raisetext)
+
+            status = False
+
+        return status
+
+        # # Here are issues here on Windows/Mac in particular
+
+        # status = True
+
+        # if os.path.isdir(self._file):
+        #     folder = self._file
+        # else:
+        #     folder = os.path.dirname(self._file)
+        #     if folder == "":
+        #         folder = "."
+
+        # if not os.path.exists(folder):
+        #     if raiseerror:
+        #         raise raiseerror("Folder does not exist: <{}>".format(folder))
+
+        #     status = False
+
+        # if os.path.exists(folder) and not os.access(folder, os.W_OK):
+        #     if raiseerror:
+        #         raise raiseerror(
+        #             "Folder does exist but is not writable: <{}>".format(folder)
+        #         )
+
+        #     status = False
+
+        # return status
+
+    def splitext(self, lower=False):
+        """Return file stem and suffix, always lowercase if lower is True."""
+        logger.info("Run splitext to get stem and suffix...")
+
+        stem = self._file.stem
+        suffix = self._file.suffix
+        suffix = suffix.replace(".", "")
+
+        if lower:
+            stem = stem.lower()
+            suffix = suffix.lower()
+
+        return stem, suffix
+
+    def get_cfhandle(self):  # was get_handle
+        """Get SWIG C file handle for CXTgeo.
+
+        This is tied to cfclose() which closes the file.
+
+        if _cfhandle already exists, then _cfhandlecount is increased with 1
+
+        """
+        # differ on Linux and other OS as Linux can use fmemopen() in C
+        islinux = True
+        fobj = None
+        if plfsys() != "Linux":
+            islinux = False
+
+        if self._cfhandle and "Swig Object of type 'FILE" in str(self._cfhandle):
+            self._cfhandlecount += 1
+            logger.info("Get SWIG C fhandle no %s", self._cfhandlecount)
+            return self._cfhandle
+
+        if isinstance(self._file, io.BytesIO) and self._mode == "rb" and islinux:
+            fobj = self._file.getvalue()  # bytes type in Python3, str in Python2
+
+            # note that the typemap in swig computes the length for the buf/fobj!
+            self._memstream = True
+
+        elif isinstance(self._file, io.BytesIO) and self._mode == "wb" and islinux:
+            fobj = bytes()
+            self._memstream = True
+
+        elif (
+            isinstance(self._file, io.BytesIO)
+            and self._mode == "rb"
+            and not islinux  # Windows or Darwin
+        ):
+            # windows/mac miss fmemopen; write buffer to a tmp instead as workaround
+            fds, self._tmpfile = mkstemp(prefix="tmpxtgeoio")
+            os.close(fds)
+            with open(self._tmpfile, "wb") as newfile:
+                newfile.write(self._file.getvalue())
+
+        else:
+            fobj = self.name
+
+        if self._memstream:
+            if islinux:
+                cfhandle = _cxtgeo.xtg_fopen_bytestream(fobj, self._mode)
+            else:
+                cfhandle = _cxtgeo.xtg_fopen(self._tmpfile, self._mode)
+
+        else:
+            try:
+                cfhandle = _cxtgeo.xtg_fopen(fobj, self._mode)
+            except TypeError as err:
+                raise IOError(f"Cannot open file: {fobj}") from err
+
+        self._cfhandle = cfhandle
+        self._cfhandlecount = 1
+
+        logger.info("Get initial SWIG C fhandle no %s", self._cfhandlecount)
+        return self._cfhandle
+
+    def cfclose(self, strict=True):
+        """Close SWIG C file handle by keeping track of _cfhandlecount.
+
+        Return True if cfhandle is really closed.
+        """
+        logger.info("Request for closing SWIG fhandle no: %s", self._cfhandlecount)
+
+        if self._cfhandle is None or self._cfhandlecount == 0:
+            if strict:
+                raise RuntimeError("Ask to close a nonexisting C file handle")
+
+            self._cfhandle = None
+            self._cfhandlecount = 0
+            return True
+
+        if self._cfhandlecount > 1 or self._cfhandlecount == 0:
+            self._cfhandlecount -= 1
+            logger.info(
+                "Remaining SWIG cfhandles: %s, do not close...", self._cfhandlecount
+            )
+            return False
+
+        if self._memstream and self._cfhandle and "w" in self._mode:
+            # this assures that the file pointer is in the end of the current filehandle
+            npos = _cxtgeo.xtg_ftell(self._cfhandle)
+            buf = bytes(npos)
+            ier = _cxtgeo.xtg_get_fbuffer(self._cfhandle, buf)
+            if ier == 0:
+                self._file.write(buf)  # write to bytesIO instance
+                _cxtgeo.xtg_fflush(self._cfhandle)
+            else:
+                raise RuntimeError("Could not write stream for unknown reasons")
+
+        ier = _cxtgeo.xtg_fclose(self._cfhandle)
+        if ier != 0:
+            raise RuntimeError(f"Could not close C file, code {ier}")
+
+        logger.info("File is now closed for C io: %s", self.name)
+
+        if self._tmpfile:
+            try:
+                os.remove(self._tmpfile)
+            except Exception as ex:  # pylint: disable=W0703
+                logger.error("Could not remove tempfile for some reason: %s", ex)
+
+        self._cfhandle = None
+        self._cfhandlecount = 0
+        logger.info("Remaining SWIG cfhandles: %s, return is True", self._cfhandlecount)
+
+        return True
+
+    def detect_fformat(
+        self, details: Optional[bool] = False, suffixonly: Optional[bool] = False
+    ):
+        """Try to deduce format from looking at file signature.
+
+        The file signature may be the initial part of the binary file/stream but if
+        that fails, the file extension is used.
+
+        Args:
+            details: If True, more info is added to the return string (useful for some
+                formats) e.g. "hdf RegularSurface xtgeo"
+            suffixonly: If True, look at file suffix only.
+
+        Returns:
+            A string with format spesification, e.g. "hdf".
+        """
+
+        if not suffixonly:
+            fformat = self._detect_fformat_by_contents(details)
+            if fformat is not None:
+                return fformat
+
+        # if looking at contents failed, look at extension
+        fmt = self._detect_format_by_extension()
+        return self._validate_format(fmt)
+
+    def _detect_fformat_by_contents(self, details: Optional[bool] = False):
+        # Try the read the N first bytes
+        maxbuf = 100
+
+        if self.memstream:
+            self.file.seek(0)
+            buf = self.file.read(maxbuf)
+            self.file.seek(0)
+        else:
+            if not self.exists():
+                raise ValueError(f"File {self.name} does not exist")
+            with open(self.file, "rb") as fhandle:
+                buf = fhandle.read(maxbuf)
+
+        if not isinstance(buf, bytes):
+            return None
+
+        # HDF format, different variants
+        if len(buf) >= 4:
+            _, hdf = struct.unpack("b 3s", buf[:4])
+            if hdf == b"HDF":
+                logger.info("Signature is hdf")
+
+                main = self._validate_format("hdf")
+                fmt = ""
+                provider = ""
+                if details:
+                    with h5py.File(self.file, "r") as hstream:
+                        for xtgtype in ["RegularSurface", "Well", "CornerPointGrid"]:
+                            if xtgtype in hstream.keys():
+                                fmt = xtgtype
+                                grp = hstream.require_group(xtgtype)
+                                try:
+                                    provider = grp.attrs["provider"]
+                                except KeyError:
+                                    provider = "unknown"
+                                break
+
+                    return f"{main} {fmt} {provider}"
+                else:
+                    return main
+
+        # Irap binary regular surface format
+        if len(buf) >= 8:
+            fortranblock, gricode = struct.unpack(">ii", buf[:8])
+            if fortranblock == 32 and gricode == -996:
+                logger.info("Signature is irap binary")
+                return self._validate_format("irap_binary")
+
+        # Petromod binary regular surface
+        if b"Content=Map" in buf and b"DataUnitDistance" in buf:
+            logger.info("Signature is petromod")
+            return self._validate_format("petromod")
+
+        # Eclipse binary 3D EGRID, look at FILEHEAD:
+        #  'FILEHEAD'         100 'INTE'
+        #   3        2016           0           0           0           0
+        #  (ver)    (release)      (reserved)   (backw)    (gtype)      (dualporo)
+
+        if len(buf) >= 24:
+            fort1, name, num, _, fort2 = struct.unpack("> i 8s i 4s i", buf[:24])
+            if fort1 == 16 and name == b"FILEHEAD" and num == 100 and fort2 == 16:
+                # Eclipse corner point EGRID
+                logger.info("Signature is egrid")
+                return self._validate_format("egrid")
+            # Eclipse binary 3D UNRST, look for SEQNUM:
+            #  'SEQNUM'         1 'INTE'
+            if fort1 == 16 and name == b"SEQNUM  " and num == 1 and fort2 == 16:
+                # Eclipse UNRST
+                logger.info("Signature is unrst")
+                return self._validate_format("unrst")
+            # Eclipse binary 3D INIT, look for INTEHEAD:
+            #  'INTEHEAD'         411 'INTE'
+            if fort1 == 16 and name == b"INTEHEAD" and num > 400 and fort2 == 16:
+                # Eclipse INIT
+                logger.info("Signature is init")
+
+                return self._validate_format("init")
+
+        if len(buf) >= 9:
+            name, _ = struct.unpack("8s b", buf[:9])
+            # ROFF binary 3D
+            if name == b"roff-bin":
+                logger.info("Signature is roff_binary")
+                return self._validate_format("roff_binary")
+            # ROFF ascii 3D
+            if name == b"roff-asc":
+                logger.info("Signature is roff_ascii")
+                return self._validate_format("roff_ascii")
+
+        # RMS well format (ascii)
+        # 1.0
+        # Unknown
+        # WELL12 90941.63200000004 5506367.711 23.0
+        # ...
+        # The signature here is one float in first line with values 1.0; one string
+        # in second line; and 3 or 4 items in the next (sometimes RKB is missing)
+        try:
+            xbuf = buf.decode().split("\n")
+        except UnicodeDecodeError:
+            return None
+
+        if (
+            len(xbuf) >= 3
+            and xbuf[0] == "1.0"
+            and len(xbuf[1]) >= 1
+            and len(xbuf[2]) >= 10
+        ):
+            logger.info("Signature is rmswell")
+            return self._validate_format("rmswell")
+
+        return None
+
+    def _detect_format_by_extension(self):
+        """Detect format by extension."""
+        if self.memstream:
+            return "unknown"
+
+        suffix = self.file.suffix[1:].lower()
+
+        for fmt, variants in SUPPORTED_FORMATS.items():
+            if suffix in variants:
+                logger.info("Extension hints: %s", fmt)
+                return fmt
+
+        # if none of these above are accepted, check regular expression
+        # (intentional to complete all variant in loop above first before trying re())
+        for fmt, variants in SUPPORTED_FORMATS.items():
+            for var in variants:
+                if "*" in var:
+                    pattern = re.compile(var)
+                    if pattern.match(suffix):
+                        logger.info("Extension by regexp hints %s", fmt)
+                        return fmt
+
+        return "unknown"
+
+    @staticmethod
+    def _validate_format(fmt):
+        """Validate format."""
+        if fmt in SUPPORTED_FORMATS.keys() or fmt == "unknown":
+            return fmt
+        else:
+            raise RuntimeError(f"Invalid format: {fmt}")
+
+    @staticmethod
+    def generic_format_by_proposal(propose):
+        """Get generic format by proposal."""
+        for fmt, variants in SUPPORTED_FORMATS.items():
+            if propose in variants:
+                return fmt
+
+        # if none of these above are accepted, check regular expression
+        for fmt, variants in SUPPORTED_FORMATS.items():
+            for var in variants:
+                if "*" in var:
+                    pattern = re.compile(var)
+                    if pattern.match(propose):
+                        return fmt
+
+        raise ValueError(f"Non-supportred file extension: {propose}")
+
+
+def inherit_docstring(inherit_from):
+    def decorator_set_docstring(func):
+        if func.__doc__ is None and inherit_from.__doc__ is not None:
+            func.__doc__ = inherit_from.__doc__
+        return func
+
+    return decorator_set_docstring
```

## xtgeo/common/xtgeo_dialog.py

 * *Ordering differences only*

```diff
@@ -1,607 +1,607 @@
-# -*- coding: utf-8 -*-
-"""
-Module for basic XTGeo dialog, basic interaction with user,
-including logging for debugging.
-
-Logging is enabled by setting a environment variable::
-
-  export XTG_LOGGING_LEVEL=INFO   # if bash; will set logging to INFO level
-  setenv XTG_LOGGING_LEVEL INFO   # if tcsh; will set logging to INFO level
-
-Other levels are DEBUG and CRITICAL. CRITICAL is default (cf. Pythons logging)
-
-Usage of logging in scripts::
-
-  import xtgeo
-  xtg = xtgeo.common.XTGeoDialog()
-  logger = xtg.basiclogger(__name__)
-  logger.info('This is logging of %s', something)
-
-Other than logging, there is also a template for user interaction, which shall
-be used in client scripts::
-
-  xtg.echo('This is a message')
-  xtg.warn('This is a warning')
-  xtg.error('This is an error, will continue')
-  xtg.critical('This is a big error, will exit')
-
-In addition there are other classes:
-
-* XTGShowProgress()
-
-* XTGDescription()
-
-"""
-
-
-import getpass
-import inspect
-import logging
-import os
-import pathlib
-import platform
-import re
-import sys
-import timeit
-import warnings
-from datetime import datetime as dtime
-
-import xtgeo
-
-DEBUG = 0
-MLS = 10000000.0
-
-
-HEADER = "\033[1;96m"
-OKBLUE = "\033[94m"
-OKGREEN = "\033[92m"
-WARN = "\033[93;43m"
-ERROR = "\033[93;41m"
-CRITICAL = "\033[1;91m"
-ENDC = "\033[0m"
-BOLD = "\033[1m"
-UNDERLINE = "\033[4m"
-
-
-def _printdebug(*args):
-    """local unction to print debugging while initializing logging"""
-
-    if DEBUG:
-        print("XTG DEBUG:", *args)
-
-
-class XTGShowProgress(object):
-    """Class for showing progress of a computation to the terminal.
-
-    Example::
-
-        # assuming 30 steps in calculation
-        theprogress = XTGShowProgress(30, info='Compute stuff')
-        for i in range(30):
-            do_slow_computation()
-            theprogress.flush(i)
-        theprogress.finished()
-    """
-
-    def __init__(self, maxiter, info="", leadtext="", skip=1, show=True):
-        self._max = maxiter
-        self._info = info
-        self._show = show
-        self._leadtext = leadtext
-        self._skip = skip
-        self._next = 0
-
-    def flush(self, step):
-        if not self._show:
-            return
-        progress = int(float(step) / float(self._max) * 100.0)
-        if progress >= self._next:
-            print(f"{self._leadtext}{progress}% {self._info}")
-            self._next += self._skip
-
-    def finished(self):
-        if not self._show:
-            return
-        print(f"{self._leadtext}{100}% {self._info}")
-
-
-class XTGDescription(object):
-    """Class for making desciptions of object instances"""
-
-    def __init__(self):
-        self._txt = []
-
-    def title(self, atitle):
-        fmt = "=" * 99
-        self._txt.append(fmt)
-        fmt = f"{atitle}"
-        self._txt.append(fmt)
-        fmt = "=" * 99
-        self._txt.append(fmt)
-
-    def txt(self, *atxt):
-        atxt = list(atxt)
-        fmt = self._smartfmt(atxt)
-        self._txt.append(fmt)
-
-    def flush(self):
-        fmt = "=" * 99
-        self._txt.append(fmt)
-
-        for line in self._txt:
-            print(line)
-
-    def astext(self):
-        thetext = ""
-        fmt = "=" * 99
-        self._txt.append(fmt)
-
-        for line in self._txt:
-            thetext += line + "\n"
-
-        return thetext[:-1]  # skip last \n
-
-    @staticmethod
-    def _smartfmt(atxt):
-        # pylint: disable=consider-using-f-string  # f-string does not work with starred
-        alen = len(atxt)
-        atxt.insert(1, "=>")
-        if alen == 1:
-            fmt = "{:40s}".format(*atxt)
-        elif alen == 2:
-            fmt = "{:40s} {:>2s} {}".format(*atxt)
-        elif alen == 3:
-            fmt = "{:40s} {:>2s} {}  {}".format(*atxt)
-        elif alen == 4:
-            fmt = "{:40s} {:>2s} {}  {}  {}".format(*atxt)
-        elif alen == 5:
-            fmt = "{:40s} {:>2s} {}  {}  {}  {}".format(*atxt)
-        elif alen == 6:
-            fmt = "{:40s} {:>2s} {}  {}  {}  {}  {}".format(*atxt)
-        elif alen == 7:
-            fmt = "{:40s} {:>2s} {}  {}  {}  {}  {}  {}".format(*atxt)
-        else:
-            fmt = "{:40s} {:>2s} {}  {}  {}  {}  {}  {}  {}".format(*atxt)
-        return fmt
-
-
-class _TimeFilter(logging.Filter):  # pylint: disable=too-few-public-methods
-    """handling difftimes in logging..."""
-
-    # cf https://stackoverflow.com/questions/31521859/
-    # \python-logging-module-time-since-last-log
-
-    def filter(self, record):
-        # pylint: disable=access-member-before-definition
-        # pylint: disable=attribute-defined-outside-init
-        try:
-            last = self.last
-        except AttributeError:
-            last = record.relativeCreated
-
-        dlt = dtime.fromtimestamp(
-            record.relativeCreated / 1000.0
-        ) - dtime.fromtimestamp(last / 1000.0)
-
-        record.relative = f"{dlt.seconds + dlt.microseconds / MLS:7.3f}"
-
-        self.last = record.relativeCreated
-        return True
-
-
-class _Formatter(logging.Formatter):
-    """Override record.pathname to truncate strings"""
-
-    # https://stackoverflow.com/questions/14429724/
-    # python-logging-how-do-i-truncate-the-pathname-to-just-the-last-few-characters
-    def format(self, record):
-        filename = "unset_filename"
-
-        if "pathname" in record.__dict__.keys():
-            # truncate the pathname
-            filename = record.pathname
-            if len(filename) > 40:
-                filename = re.sub(r".*src/", "", filename)
-            record.pathname = filename
-
-        return super().format(record)
-
-
-class XTGeoDialog(object):  # pylint: disable=too-many-public-methods
-    """System for handling dialogs and messages in XTGeo.
-
-    This module cooperates with Python logging module.
-
-    """
-
-    def __init__(self):
-        """Initializing XTGeoDialog."""
-        self._callclass = None
-        self._caller = None
-        self._rootlogger = logging.getLogger()
-        self._lformat = None
-        self._lformatlevel = 1
-        self._logginglevel = "CRITICAL"
-        self._logginglevel_fromenv = None
-        self._loggingname = ""
-        self._test_env = True
-        self._testpath = "../xtgeo-testdata"
-        self._showrtwarnings = True
-
-        # a string, for Python logging:
-        self._logginglevel_fromenv = os.environ.get("XTG_LOGGING_LEVEL", None)
-
-        # a number, for format, 1 is simple, 2 is more info etc
-        loggingformat = os.environ.get("XTG_LOGGING_FORMAT")
-
-        _printdebug("Logging format is", loggingformat)
-
-        if self._logginglevel_fromenv:
-            self.logginglevel = self._logginglevel_fromenv
-
-        if loggingformat is not None:
-            self._lformatlevel = int(loggingformat)
-
-        if "XTG_TESTPATH" in os.environ:
-            self._testpath = os.environ.get("XTG_TESTPATH")
-
-    @property
-    def testpathobj(self):
-        """Return testpath as pathlib.Path object."""
-        return pathlib.Path(self._testpath)
-
-    @property
-    def testpath(self):
-        """Return or setting up testpath."""
-        return self._testpath
-
-    @testpath.setter
-    def testpath(self, newtestpath):
-        if not os.path.isdir(newtestpath):
-            raise RuntimeError(f"Proposed test path is not valid: {newtestpath}")
-
-        self._testpath = newtestpath
-
-    @property
-    def logginglevel(self):
-        """Set or return a logging level property, e.g. logging.CRITICAL"""
-
-        return self._logginglevel
-
-    @logginglevel.setter
-    def logginglevel(self, level):
-        # pylint: disable=pointless-statement
-
-        validlevels = ("INFO", "WARNING", "DEBUG", "CRITICAL")
-        if level in validlevels:
-            self._logginglevel = level
-        else:
-            raise ValueError(f"Invalid level given, must be in {validlevels}")
-
-    @property
-    def numericallogginglevel(self):
-        """Return a numerical logging level (read only)"""
-        llo = logging.CRITICAL
-        if self._logginglevel == "INFO":
-            llo = logging.INFO
-        elif self._logginglevel == "WARNING":
-            llo = logging.WARNING
-        elif self._logginglevel == "DEBUG":
-            llo = logging.DEBUG
-
-        return llo
-
-    @property
-    def loggingformatlevel(self):
-        return self._lformatlevel
-
-    @property
-    def loggingformat(self):
-        """Returns the format string to be used in logging"""
-
-        _printdebug("Logging format is", self._lformatlevel)
-
-        if self._lformatlevel <= 1:
-            fmt = logging.Formatter(fmt="%(levelname)8s: (%(relative)ss) \t%(message)s")
-
-        elif self._lformatlevel == 2:
-            fmt = _Formatter(
-                fmt="%(levelname)8s (%(relative)ss) %(pathname)44s "
-                "[%(funcName)40s()] %(lineno)4d >> \t%(message)s"
-            )
-
-        else:
-            fmt = logging.Formatter(
-                fmt="%(asctime)s Line: %(lineno)4d %(name)44s "
-                "(Delta=%(relative)ss) "
-                "[%(funcName)40s()]"
-                "%(levelname)8s:"
-                "\t%(message)s"
-            )
-
-        log = self._rootlogger
-        _tmp1 = [hndl.addFilter(_TimeFilter()) for hndl in log.handlers]
-        _tmp2 = [hndl.setFormatter(fmt) for hndl in log.handlers]
-
-        _printdebug("TMP1:", _tmp1)
-        _printdebug("TMP2:", _tmp2)
-
-        self._lformat = fmt._fmt  # private attribute in Formatter()
-        return self._lformat
-
-    @staticmethod
-    def get_xtgeo_info(variant="clibinfo"):
-        """Prints a banner for a XTGeo app to STDOUT.
-
-        Args:
-            variant (str): Variant of info
-
-        Returns:
-            info (str): A string with XTGeo system info
-
-        """
-
-        if variant == "clibinfo":
-            return (
-                f"XTGeo version {xtgeo.__version__} (Python "
-                f"{platform.python_version()} on {platform.system()})"
-            )
-
-        return "Invalid"
-
-    @staticmethod
-    def print_xtgeo_header(appname, appversion, info=None):
-        """Prints a banner for a XTGeo app to STDOUT.
-
-        Args:
-            appname (str): Name of application.
-            appversion (str): Version of application on form '3.2.1'
-            info (str, optional): More info, e.g. if beta release
-
-        Example::
-
-            xtg.print_xtgeo_header('myapp', '0.2.1', info='Beta release!')
-        """
-
-        cur_version = "Python " + str(sys.version_info[0]) + "."
-        cur_version += str(sys.version_info[1]) + "." + str(sys.version_info[2])
-
-        app = appname + ", version " + str(appversion)
-        if info:
-            app = app + " (" + info + ")"
-        print("")
-        print(HEADER)
-        print("#" * 79)
-        print(f"#{app.center(77)}#")
-        print("#" * 79)
-        nowtime = dtime.now().strftime("%Y-%m-%d %H:%M:%S")
-        ver = "Using XTGeo version " + xtgeo.__version__
-        cur_version += f" @ {nowtime} on {platform.node()} by {getpass.getuser()}"
-        print(f"#{ver.center(77)}#")
-        print(f"#{cur_version.center(77)}#")
-        print("#" * 79)
-        print(ENDC)
-        print("")
-
-    def basiclogger(self, name, logginglevel=None, loggingformat=None, info=False):
-        """Initiate the logger by some default settings."""
-
-        if logginglevel is not None and self._logginglevel_fromenv is None:
-            self.logginglevel = logginglevel
-
-        if loggingformat is not None and isinstance(loggingformat, int):
-            self._lformatlevel = loggingformat
-
-        logging.basicConfig(stream=sys.stdout)
-        fmt = self.loggingformat
-        self._loggingname = name
-        if info:
-            print(
-                f"Logginglevel is {self.logginglevel}, formatlevel is "
-                f"{self._lformatlevel}, and format is {fmt}"
-            )
-        self._rootlogger.setLevel(self.numericallogginglevel)
-
-        logging.captureWarnings(True)
-
-        return logging.getLogger(self._loggingname)
-
-    @staticmethod
-    def functionlogger(name):
-        """Get the logger for functions (not top level)."""
-
-        logger = logging.getLogger(name)
-        logger.addHandler(logging.NullHandler())
-        return logger
-
-    def testsetup(self):
-        """Basic setup for XTGeo testing (private; only relevant for tests)"""
-
-        tstpath = os.environ.get("XTG_TESTPATH", "../xtgeo-testdata")
-        if not os.path.isdir(tstpath):
-            raise RuntimeError(f"Test path is not valid: {tstpath}")
-
-        self._test_env = True
-        self._testpath = tstpath
-
-        return True
-
-    @staticmethod
-    def timer(*args):
-        """Without args; return the time, with a time as arg return the
-        difference.
-        """
-        time1 = timeit.default_timer()
-
-        if args:
-            return time1 - args[0]
-
-        return time1
-
-    def show_runtimewarnings(self, flag=True):
-        """Show warnings issued by xtg.warn, if flag is True."""
-        self._showrtwarnings = flag
-
-    def insane(self, string):
-        level = 4
-        idx = 0
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    def trace(self, string):
-        level = 3
-        idx = 0
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    def debug(self, string):
-        level = 2
-        idx = 0
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    def speak(self, string):
-        level = 1
-        idx = 1
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    info = speak
-
-    def say(self, string):
-        level = -5
-        idx = 3
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    def warn(self, string):
-        """Show warnings at Runtime (pure user info/warns)."""
-        level = 0
-        idx = 6
-
-        if self._showrtwarnings:
-            caller = sys._getframe(1).f_code.co_name
-            frame = inspect.stack()[1][0]
-            self.get_callerinfo(caller, frame)
-
-            self._output(idx, level, string)
-
-    warning = warn
-
-    @staticmethod
-    def warndeprecated(string):
-        """Show Deprecation warnings using Python warnings"""
-
-        warnings.simplefilter("default", DeprecationWarning)
-        warnings.warn(string, DeprecationWarning, stacklevel=2)
-
-    @staticmethod
-    def warnuser(string):
-        """Show User warnings, using Python warnings"""
-
-        warnings.simplefilter("default", UserWarning)
-        warnings.warn(string, UserWarning, stacklevel=2)
-
-    def error(self, string):
-        level = -8
-        idx = 8
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    def critical(self, string):
-        level = -9
-        idx = 9
-
-        caller = sys._getframe(1).f_code.co_name
-        frame = inspect.stack()[1][0]
-        self.get_callerinfo(caller, frame)
-
-        self._output(idx, level, string)
-
-    def get_callerinfo(self, caller, frame):
-        the_class = self._get_class_from_frame(frame)
-
-        # just keep the last class element
-        x = str(the_class)
-        x = x.split(".")
-        the_class = x[-1]
-
-        self._caller = caller
-        self._callclass = the_class
-
-        return (self._caller, self._callclass)
-
-    # =============================================================================
-    # Private routines
-    # =============================================================================
-
-    @staticmethod
-    def _get_class_from_frame(fr):
-        # pylint: disable=deprecated-method
-        args, _, _, value_dict = inspect.getargvalues(fr)
-
-        # we check the first parameter for the frame function is
-        # named 'self'
-        if args and args[0] == "self":
-            instance = value_dict.get("self", None)
-            if instance:
-                # return its class
-                return getattr(instance, "__class__", None)
-        # return None otherwise
-        return None
-
-    def _output(self, idx, level, string):
-        prefix = ""
-        endfix = ""
-
-        if idx == 0:
-            prefix = "++"
-        elif idx == 1:
-            prefix = "**"
-        elif idx == 3:
-            prefix = ">>"
-        elif idx == 6:
-            prefix = WARN + "##"
-            endfix = ENDC
-        elif idx == 8:
-            prefix = ERROR + "!#"
-            endfix = ENDC
-        elif idx == 9:
-            prefix = CRITICAL + "!!"
-            endfix = ENDC
-
-        ulevel = str(level)
-        if level == -5:
-            ulevel = "M"
-        if level == -8:
-            ulevel = "E"
-        if level == -9:
-            ulevel = "W"
-        print(
-            f"{prefix} <{ulevel}> [{self._callclass:23s}-> "
-            f"{self._caller:>33s}] {string}{endfix}"
-        )
+# -*- coding: utf-8 -*-
+"""
+Module for basic XTGeo dialog, basic interaction with user,
+including logging for debugging.
+
+Logging is enabled by setting a environment variable::
+
+  export XTG_LOGGING_LEVEL=INFO   # if bash; will set logging to INFO level
+  setenv XTG_LOGGING_LEVEL INFO   # if tcsh; will set logging to INFO level
+
+Other levels are DEBUG and CRITICAL. CRITICAL is default (cf. Pythons logging)
+
+Usage of logging in scripts::
+
+  import xtgeo
+  xtg = xtgeo.common.XTGeoDialog()
+  logger = xtg.basiclogger(__name__)
+  logger.info('This is logging of %s', something)
+
+Other than logging, there is also a template for user interaction, which shall
+be used in client scripts::
+
+  xtg.echo('This is a message')
+  xtg.warn('This is a warning')
+  xtg.error('This is an error, will continue')
+  xtg.critical('This is a big error, will exit')
+
+In addition there are other classes:
+
+* XTGShowProgress()
+
+* XTGDescription()
+
+"""
+
+
+import getpass
+import inspect
+import logging
+import os
+import pathlib
+import platform
+import re
+import sys
+import timeit
+import warnings
+from datetime import datetime as dtime
+
+import xtgeo
+
+DEBUG = 0
+MLS = 10000000.0
+
+
+HEADER = "\033[1;96m"
+OKBLUE = "\033[94m"
+OKGREEN = "\033[92m"
+WARN = "\033[93;43m"
+ERROR = "\033[93;41m"
+CRITICAL = "\033[1;91m"
+ENDC = "\033[0m"
+BOLD = "\033[1m"
+UNDERLINE = "\033[4m"
+
+
+def _printdebug(*args):
+    """local unction to print debugging while initializing logging"""
+
+    if DEBUG:
+        print("XTG DEBUG:", *args)
+
+
+class XTGShowProgress(object):
+    """Class for showing progress of a computation to the terminal.
+
+    Example::
+
+        # assuming 30 steps in calculation
+        theprogress = XTGShowProgress(30, info='Compute stuff')
+        for i in range(30):
+            do_slow_computation()
+            theprogress.flush(i)
+        theprogress.finished()
+    """
+
+    def __init__(self, maxiter, info="", leadtext="", skip=1, show=True):
+        self._max = maxiter
+        self._info = info
+        self._show = show
+        self._leadtext = leadtext
+        self._skip = skip
+        self._next = 0
+
+    def flush(self, step):
+        if not self._show:
+            return
+        progress = int(float(step) / float(self._max) * 100.0)
+        if progress >= self._next:
+            print(f"{self._leadtext}{progress}% {self._info}")
+            self._next += self._skip
+
+    def finished(self):
+        if not self._show:
+            return
+        print(f"{self._leadtext}{100}% {self._info}")
+
+
+class XTGDescription(object):
+    """Class for making desciptions of object instances"""
+
+    def __init__(self):
+        self._txt = []
+
+    def title(self, atitle):
+        fmt = "=" * 99
+        self._txt.append(fmt)
+        fmt = f"{atitle}"
+        self._txt.append(fmt)
+        fmt = "=" * 99
+        self._txt.append(fmt)
+
+    def txt(self, *atxt):
+        atxt = list(atxt)
+        fmt = self._smartfmt(atxt)
+        self._txt.append(fmt)
+
+    def flush(self):
+        fmt = "=" * 99
+        self._txt.append(fmt)
+
+        for line in self._txt:
+            print(line)
+
+    def astext(self):
+        thetext = ""
+        fmt = "=" * 99
+        self._txt.append(fmt)
+
+        for line in self._txt:
+            thetext += line + "\n"
+
+        return thetext[:-1]  # skip last \n
+
+    @staticmethod
+    def _smartfmt(atxt):
+        # pylint: disable=consider-using-f-string  # f-string does not work with starred
+        alen = len(atxt)
+        atxt.insert(1, "=>")
+        if alen == 1:
+            fmt = "{:40s}".format(*atxt)
+        elif alen == 2:
+            fmt = "{:40s} {:>2s} {}".format(*atxt)
+        elif alen == 3:
+            fmt = "{:40s} {:>2s} {}  {}".format(*atxt)
+        elif alen == 4:
+            fmt = "{:40s} {:>2s} {}  {}  {}".format(*atxt)
+        elif alen == 5:
+            fmt = "{:40s} {:>2s} {}  {}  {}  {}".format(*atxt)
+        elif alen == 6:
+            fmt = "{:40s} {:>2s} {}  {}  {}  {}  {}".format(*atxt)
+        elif alen == 7:
+            fmt = "{:40s} {:>2s} {}  {}  {}  {}  {}  {}".format(*atxt)
+        else:
+            fmt = "{:40s} {:>2s} {}  {}  {}  {}  {}  {}  {}".format(*atxt)
+        return fmt
+
+
+class _TimeFilter(logging.Filter):  # pylint: disable=too-few-public-methods
+    """handling difftimes in logging..."""
+
+    # cf https://stackoverflow.com/questions/31521859/
+    # \python-logging-module-time-since-last-log
+
+    def filter(self, record):
+        # pylint: disable=access-member-before-definition
+        # pylint: disable=attribute-defined-outside-init
+        try:
+            last = self.last
+        except AttributeError:
+            last = record.relativeCreated
+
+        dlt = dtime.fromtimestamp(
+            record.relativeCreated / 1000.0
+        ) - dtime.fromtimestamp(last / 1000.0)
+
+        record.relative = f"{dlt.seconds + dlt.microseconds / MLS:7.3f}"
+
+        self.last = record.relativeCreated
+        return True
+
+
+class _Formatter(logging.Formatter):
+    """Override record.pathname to truncate strings"""
+
+    # https://stackoverflow.com/questions/14429724/
+    # python-logging-how-do-i-truncate-the-pathname-to-just-the-last-few-characters
+    def format(self, record):
+        filename = "unset_filename"
+
+        if "pathname" in record.__dict__.keys():
+            # truncate the pathname
+            filename = record.pathname
+            if len(filename) > 40:
+                filename = re.sub(r".*src/", "", filename)
+            record.pathname = filename
+
+        return super().format(record)
+
+
+class XTGeoDialog(object):  # pylint: disable=too-many-public-methods
+    """System for handling dialogs and messages in XTGeo.
+
+    This module cooperates with Python logging module.
+
+    """
+
+    def __init__(self):
+        """Initializing XTGeoDialog."""
+        self._callclass = None
+        self._caller = None
+        self._rootlogger = logging.getLogger()
+        self._lformat = None
+        self._lformatlevel = 1
+        self._logginglevel = "CRITICAL"
+        self._logginglevel_fromenv = None
+        self._loggingname = ""
+        self._test_env = True
+        self._testpath = "../xtgeo-testdata"
+        self._showrtwarnings = True
+
+        # a string, for Python logging:
+        self._logginglevel_fromenv = os.environ.get("XTG_LOGGING_LEVEL", None)
+
+        # a number, for format, 1 is simple, 2 is more info etc
+        loggingformat = os.environ.get("XTG_LOGGING_FORMAT")
+
+        _printdebug("Logging format is", loggingformat)
+
+        if self._logginglevel_fromenv:
+            self.logginglevel = self._logginglevel_fromenv
+
+        if loggingformat is not None:
+            self._lformatlevel = int(loggingformat)
+
+        if "XTG_TESTPATH" in os.environ:
+            self._testpath = os.environ.get("XTG_TESTPATH")
+
+    @property
+    def testpathobj(self):
+        """Return testpath as pathlib.Path object."""
+        return pathlib.Path(self._testpath)
+
+    @property
+    def testpath(self):
+        """Return or setting up testpath."""
+        return self._testpath
+
+    @testpath.setter
+    def testpath(self, newtestpath):
+        if not os.path.isdir(newtestpath):
+            raise RuntimeError(f"Proposed test path is not valid: {newtestpath}")
+
+        self._testpath = newtestpath
+
+    @property
+    def logginglevel(self):
+        """Set or return a logging level property, e.g. logging.CRITICAL"""
+
+        return self._logginglevel
+
+    @logginglevel.setter
+    def logginglevel(self, level):
+        # pylint: disable=pointless-statement
+
+        validlevels = ("INFO", "WARNING", "DEBUG", "CRITICAL")
+        if level in validlevels:
+            self._logginglevel = level
+        else:
+            raise ValueError(f"Invalid level given, must be in {validlevels}")
+
+    @property
+    def numericallogginglevel(self):
+        """Return a numerical logging level (read only)"""
+        llo = logging.CRITICAL
+        if self._logginglevel == "INFO":
+            llo = logging.INFO
+        elif self._logginglevel == "WARNING":
+            llo = logging.WARNING
+        elif self._logginglevel == "DEBUG":
+            llo = logging.DEBUG
+
+        return llo
+
+    @property
+    def loggingformatlevel(self):
+        return self._lformatlevel
+
+    @property
+    def loggingformat(self):
+        """Returns the format string to be used in logging"""
+
+        _printdebug("Logging format is", self._lformatlevel)
+
+        if self._lformatlevel <= 1:
+            fmt = logging.Formatter(fmt="%(levelname)8s: (%(relative)ss) \t%(message)s")
+
+        elif self._lformatlevel == 2:
+            fmt = _Formatter(
+                fmt="%(levelname)8s (%(relative)ss) %(pathname)44s "
+                "[%(funcName)40s()] %(lineno)4d >> \t%(message)s"
+            )
+
+        else:
+            fmt = logging.Formatter(
+                fmt="%(asctime)s Line: %(lineno)4d %(name)44s "
+                "(Delta=%(relative)ss) "
+                "[%(funcName)40s()]"
+                "%(levelname)8s:"
+                "\t%(message)s"
+            )
+
+        log = self._rootlogger
+        _tmp1 = [hndl.addFilter(_TimeFilter()) for hndl in log.handlers]
+        _tmp2 = [hndl.setFormatter(fmt) for hndl in log.handlers]
+
+        _printdebug("TMP1:", _tmp1)
+        _printdebug("TMP2:", _tmp2)
+
+        self._lformat = fmt._fmt  # private attribute in Formatter()
+        return self._lformat
+
+    @staticmethod
+    def get_xtgeo_info(variant="clibinfo"):
+        """Prints a banner for a XTGeo app to STDOUT.
+
+        Args:
+            variant (str): Variant of info
+
+        Returns:
+            info (str): A string with XTGeo system info
+
+        """
+
+        if variant == "clibinfo":
+            return (
+                f"XTGeo version {xtgeo.__version__} (Python "
+                f"{platform.python_version()} on {platform.system()})"
+            )
+
+        return "Invalid"
+
+    @staticmethod
+    def print_xtgeo_header(appname, appversion, info=None):
+        """Prints a banner for a XTGeo app to STDOUT.
+
+        Args:
+            appname (str): Name of application.
+            appversion (str): Version of application on form '3.2.1'
+            info (str, optional): More info, e.g. if beta release
+
+        Example::
+
+            xtg.print_xtgeo_header('myapp', '0.2.1', info='Beta release!')
+        """
+
+        cur_version = "Python " + str(sys.version_info[0]) + "."
+        cur_version += str(sys.version_info[1]) + "." + str(sys.version_info[2])
+
+        app = appname + ", version " + str(appversion)
+        if info:
+            app = app + " (" + info + ")"
+        print("")
+        print(HEADER)
+        print("#" * 79)
+        print(f"#{app.center(77)}#")
+        print("#" * 79)
+        nowtime = dtime.now().strftime("%Y-%m-%d %H:%M:%S")
+        ver = "Using XTGeo version " + xtgeo.__version__
+        cur_version += f" @ {nowtime} on {platform.node()} by {getpass.getuser()}"
+        print(f"#{ver.center(77)}#")
+        print(f"#{cur_version.center(77)}#")
+        print("#" * 79)
+        print(ENDC)
+        print("")
+
+    def basiclogger(self, name, logginglevel=None, loggingformat=None, info=False):
+        """Initiate the logger by some default settings."""
+
+        if logginglevel is not None and self._logginglevel_fromenv is None:
+            self.logginglevel = logginglevel
+
+        if loggingformat is not None and isinstance(loggingformat, int):
+            self._lformatlevel = loggingformat
+
+        logging.basicConfig(stream=sys.stdout)
+        fmt = self.loggingformat
+        self._loggingname = name
+        if info:
+            print(
+                f"Logginglevel is {self.logginglevel}, formatlevel is "
+                f"{self._lformatlevel}, and format is {fmt}"
+            )
+        self._rootlogger.setLevel(self.numericallogginglevel)
+
+        logging.captureWarnings(True)
+
+        return logging.getLogger(self._loggingname)
+
+    @staticmethod
+    def functionlogger(name):
+        """Get the logger for functions (not top level)."""
+
+        logger = logging.getLogger(name)
+        logger.addHandler(logging.NullHandler())
+        return logger
+
+    def testsetup(self):
+        """Basic setup for XTGeo testing (private; only relevant for tests)"""
+
+        tstpath = os.environ.get("XTG_TESTPATH", "../xtgeo-testdata")
+        if not os.path.isdir(tstpath):
+            raise RuntimeError(f"Test path is not valid: {tstpath}")
+
+        self._test_env = True
+        self._testpath = tstpath
+
+        return True
+
+    @staticmethod
+    def timer(*args):
+        """Without args; return the time, with a time as arg return the
+        difference.
+        """
+        time1 = timeit.default_timer()
+
+        if args:
+            return time1 - args[0]
+
+        return time1
+
+    def show_runtimewarnings(self, flag=True):
+        """Show warnings issued by xtg.warn, if flag is True."""
+        self._showrtwarnings = flag
+
+    def insane(self, string):
+        level = 4
+        idx = 0
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    def trace(self, string):
+        level = 3
+        idx = 0
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    def debug(self, string):
+        level = 2
+        idx = 0
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    def speak(self, string):
+        level = 1
+        idx = 1
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    info = speak
+
+    def say(self, string):
+        level = -5
+        idx = 3
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    def warn(self, string):
+        """Show warnings at Runtime (pure user info/warns)."""
+        level = 0
+        idx = 6
+
+        if self._showrtwarnings:
+            caller = sys._getframe(1).f_code.co_name
+            frame = inspect.stack()[1][0]
+            self.get_callerinfo(caller, frame)
+
+            self._output(idx, level, string)
+
+    warning = warn
+
+    @staticmethod
+    def warndeprecated(string):
+        """Show Deprecation warnings using Python warnings"""
+
+        warnings.simplefilter("default", DeprecationWarning)
+        warnings.warn(string, DeprecationWarning, stacklevel=2)
+
+    @staticmethod
+    def warnuser(string):
+        """Show User warnings, using Python warnings"""
+
+        warnings.simplefilter("default", UserWarning)
+        warnings.warn(string, UserWarning, stacklevel=2)
+
+    def error(self, string):
+        level = -8
+        idx = 8
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    def critical(self, string):
+        level = -9
+        idx = 9
+
+        caller = sys._getframe(1).f_code.co_name
+        frame = inspect.stack()[1][0]
+        self.get_callerinfo(caller, frame)
+
+        self._output(idx, level, string)
+
+    def get_callerinfo(self, caller, frame):
+        the_class = self._get_class_from_frame(frame)
+
+        # just keep the last class element
+        x = str(the_class)
+        x = x.split(".")
+        the_class = x[-1]
+
+        self._caller = caller
+        self._callclass = the_class
+
+        return (self._caller, self._callclass)
+
+    # =============================================================================
+    # Private routines
+    # =============================================================================
+
+    @staticmethod
+    def _get_class_from_frame(fr):
+        # pylint: disable=deprecated-method
+        args, _, _, value_dict = inspect.getargvalues(fr)
+
+        # we check the first parameter for the frame function is
+        # named 'self'
+        if args and args[0] == "self":
+            instance = value_dict.get("self", None)
+            if instance:
+                # return its class
+                return getattr(instance, "__class__", None)
+        # return None otherwise
+        return None
+
+    def _output(self, idx, level, string):
+        prefix = ""
+        endfix = ""
+
+        if idx == 0:
+            prefix = "++"
+        elif idx == 1:
+            prefix = "**"
+        elif idx == 3:
+            prefix = ">>"
+        elif idx == 6:
+            prefix = WARN + "##"
+            endfix = ENDC
+        elif idx == 8:
+            prefix = ERROR + "!#"
+            endfix = ENDC
+        elif idx == 9:
+            prefix = CRITICAL + "!!"
+            endfix = ENDC
+
+        ulevel = str(level)
+        if level == -5:
+            ulevel = "M"
+        if level == -8:
+            ulevel = "E"
+        if level == -9:
+            ulevel = "W"
+        print(
+            f"{prefix} <{ulevel}> [{self._callclass:23s}-> "
+            f"{self._caller:>33s}] {string}{endfix}"
+        )
```

## xtgeo/cube/__init__.py

 * *Ordering differences only*

```diff
@@ -1,4 +1,4 @@
-# -*- coding: utf-8 -*-
-"""The XTGeo cube package."""
-
-from xtgeo.cube.cube1 import Cube  # type: ignore # noqa # pylint: disable=undefined-variable
+# -*- coding: utf-8 -*-
+"""The XTGeo cube package."""
+
+from xtgeo.cube.cube1 import Cube  # type: ignore # noqa # pylint: disable=undefined-variable
```

## xtgeo/cube/_cube_export.py

 * *Ordering differences only*

```diff
@@ -1,219 +1,219 @@
-"""Export Cube data via SegyIO library or XTGeo CLIB."""
-import shutil
-import struct
-import json
-import numpy as np
-
-import segyio
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo import XTGeoCLibError
-
-xtg = XTGeoDialog()
-
-
-logger = xtg.functionlogger(__name__)
-
-
-def export_segy(self, sfile, template=None, pristine=False, engine="xtgeo"):
-    """Export on SEGY using segyio library.
-
-    Args:
-        self (:class:`xtgeo.cube.Cube`): The instance
-        sfile (str): File name to export to.
-        template (str): Use an existing file a template.
-        pristine (bool): Make SEGY from scrtach if True; otherwise use an
-            existing SEGY file.
-        engine (str): Use 'xtgeo' or (later?) 'segyio'
-    """
-    if not isinstance(self, xtgeo.cube.Cube):
-        raise ValueError("first argument is not a Cube instance")
-
-    if engine == "segyio":
-        _export_segy_segyio(self, sfile, template=template, pristine=pristine)
-    else:
-        _export_segy_xtgeo(self, sfile)
-
-
-def _export_segy_segyio(self, sfile, template=None, pristine=False):
-    """Export on SEGY using segyio library.
-
-    Args:
-        self (:class:`xtgeo.cube.Cube`): The instance
-        sfile (str): File name to export to.
-        template (str): Use an existing file a template.
-        pristine (bool): Make SEGY from scrtach if True; otherwise use an
-            existing SEGY file.
-    """
-    logger.debug("Export segy format using segyio...")
-
-    if template is None and self._segyfile is None:
-        raise NotImplementedError("Error, template=None is not yet made!")
-
-    # There is an existing _segyfile attribute, in this case the current SEGY
-    # headers etc are applied for the new data. Requires that shapes etc are
-    # equal.
-    if template is None and self._segyfile is not None:
-        template = self._segyfile
-
-    cvalues = self.values
-
-    if template is not None and not pristine:
-        try:
-            shutil.copyfile(self._segyfile, sfile)
-        except Exception as errormsg:
-            xtg.warn(f"Error message: {errormsg}")
-            raise
-
-        logger.debug("Input segy file copied ...")
-
-        with segyio.open(sfile, "r+") as segyfile:
-            logger.debug("Output segy file is now open...")
-
-            if segyfile.sorting == 1:
-                logger.info("xline sorting")
-                for xll, xline in enumerate(segyfile.xlines):
-                    segyfile.xline[xline] = cvalues[xll]  # broadcasting
-            else:
-                logger.info("iline sorting")
-                ixv, jyv, kzv = cvalues.shape
-                for ill, iline in enumerate(segyfile.ilines):
-                    if ixv != jyv != kzv or ixv != kzv != jyv:
-                        segyfile.iline[iline] = cvalues[ill]  # broadcasting
-                    else:
-                        # safer but a bit slower than broadcasting
-                        segyfile.iline[iline] = cvalues[ill, :, :]
-
-    else:
-        # NOT FINISHED!
-        logger.debug("Input segy file from scratch ...")
-
-        # sintv = int(self.zinc * 1000)
-        spec = segyio.spec()
-
-        spec.sorting = 2
-        spec.format = 1
-
-        spec.samples = np.arange(self.nlay)
-        spec.ilines = np.arange(self.ncol)
-        spec.xlines = np.arange(self.nrow)
-
-        with segyio.create(sfile, spec) as fseg:
-            # write the line itself to the file and the inline number
-            # in all this line's headers
-            for ill, ilno in enumerate(spec.ilines):
-                fseg.iline[ilno] = cvalues[ill]
-                # f.header.iline[ilno] = {
-                #     segyio.TraceField.INLINE_3D: ilno,
-                #     segyio.TraceField.offset: 0,
-                #     segyio.TraceField.TRACE_SAMPLE_INTERVAL: sintv
-                # }
-
-            # # then do the same for xlines
-            # for xlno in spec.xlines:
-            #     f.header.xline[xlno] = {
-            #         segyio.TraceField.CROSSLINE_3D: xlno,
-            #         segyio.TraceField.TRACE_SAMPLE_INTERVAL: sintv
-            #     }
-
-
-def _export_segy_xtgeo(self, sfile):
-    """Export SEGY via XTGeo internal C routine."""
-
-    values1d = self.values.reshape(-1)
-
-    ilinesp = _cxtgeo.new_intarray(len(self._ilines))
-    xlinesp = _cxtgeo.new_intarray(len(self._xlines))
-    tracidp = _cxtgeo.new_intarray(self.ncol * self.nrow)
-
-    ilns = self._ilines.astype(np.int32)
-    xlns = self._xlines.astype(np.int32)
-    trid = self._traceidcodes.flatten().astype(np.int32)
-
-    _cxtgeo.swig_numpy_to_carr_i1d(ilns, ilinesp)
-    _cxtgeo.swig_numpy_to_carr_i1d(xlns, xlinesp)
-    _cxtgeo.swig_numpy_to_carr_i1d(trid, tracidp)
-
-    status = _cxtgeo.cube_export_segy(
-        sfile,
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        values1d,
-        self.xori,
-        self.xinc,
-        self.yori,
-        self.yinc,
-        self.zori,
-        self.zinc,
-        self.rotation,
-        self.yflip,
-        1,
-        ilinesp,
-        xlinesp,
-        tracidp,
-        0,
-    )
-
-    if status != 0:
-        raise XTGeoCLibError("Error when exporting to SEGY (xtgeo engine)")
-
-    _cxtgeo.delete_intarray(ilinesp)
-    _cxtgeo.delete_intarray(xlinesp)
-
-
-def export_rmsreg(self, sfile):
-    """Export on RMS regular format."""
-
-    logger.debug("Export to RMS regular format...")
-    values1d = self.values.reshape(-1)
-
-    status = _cxtgeo.cube_export_rmsregular(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self.xori,
-        self.yori,
-        self.zori,
-        self.xinc,
-        self.yinc * self.yflip,
-        self.zinc,
-        self.rotation,
-        self.yflip,
-        values1d,
-        sfile,
-    )
-
-    if status != 0:
-        raise RuntimeError("Error when exporting to RMS regular")
-
-
-def export_xtgregcube(self, mfile):
-    """Export to experimental xtgregcube format, python version."""
-    logger.info("Export as xtgregcube...")
-    self.metadata.required = self
-
-    prevalues = (1, 1201, 4, self.ncol, self.nrow, self.nlay)
-    mystruct = struct.Struct("= i i i q q q")
-    pre = mystruct.pack(*prevalues)
-
-    meta = self.metadata.get_metadata()
-
-    jmeta = json.dumps(meta).encode()
-
-    with open(mfile, "wb") as fout:
-        fout.write(pre)
-
-    with open(mfile, "ab") as fout:
-        # TODO. Treat dead traces as undef
-        self.values.tofile(fout)
-
-    with open(mfile, "ab") as fout:
-        fout.write("\nXTGMETA.v01\n".encode())
-
-    with open(mfile, "ab") as fout:
-        fout.write(jmeta)
-
-    logger.info("Export as xtgregcube... done")
+"""Export Cube data via SegyIO library or XTGeo CLIB."""
+import shutil
+import struct
+import json
+import numpy as np
+
+import segyio
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo import XTGeoCLibError
+
+xtg = XTGeoDialog()
+
+
+logger = xtg.functionlogger(__name__)
+
+
+def export_segy(self, sfile, template=None, pristine=False, engine="xtgeo"):
+    """Export on SEGY using segyio library.
+
+    Args:
+        self (:class:`xtgeo.cube.Cube`): The instance
+        sfile (str): File name to export to.
+        template (str): Use an existing file a template.
+        pristine (bool): Make SEGY from scrtach if True; otherwise use an
+            existing SEGY file.
+        engine (str): Use 'xtgeo' or (later?) 'segyio'
+    """
+    if not isinstance(self, xtgeo.cube.Cube):
+        raise ValueError("first argument is not a Cube instance")
+
+    if engine == "segyio":
+        _export_segy_segyio(self, sfile, template=template, pristine=pristine)
+    else:
+        _export_segy_xtgeo(self, sfile)
+
+
+def _export_segy_segyio(self, sfile, template=None, pristine=False):
+    """Export on SEGY using segyio library.
+
+    Args:
+        self (:class:`xtgeo.cube.Cube`): The instance
+        sfile (str): File name to export to.
+        template (str): Use an existing file a template.
+        pristine (bool): Make SEGY from scrtach if True; otherwise use an
+            existing SEGY file.
+    """
+    logger.debug("Export segy format using segyio...")
+
+    if template is None and self._segyfile is None:
+        raise NotImplementedError("Error, template=None is not yet made!")
+
+    # There is an existing _segyfile attribute, in this case the current SEGY
+    # headers etc are applied for the new data. Requires that shapes etc are
+    # equal.
+    if template is None and self._segyfile is not None:
+        template = self._segyfile
+
+    cvalues = self.values
+
+    if template is not None and not pristine:
+        try:
+            shutil.copyfile(self._segyfile, sfile)
+        except Exception as errormsg:
+            xtg.warn(f"Error message: {errormsg}")
+            raise
+
+        logger.debug("Input segy file copied ...")
+
+        with segyio.open(sfile, "r+") as segyfile:
+            logger.debug("Output segy file is now open...")
+
+            if segyfile.sorting == 1:
+                logger.info("xline sorting")
+                for xll, xline in enumerate(segyfile.xlines):
+                    segyfile.xline[xline] = cvalues[xll]  # broadcasting
+            else:
+                logger.info("iline sorting")
+                ixv, jyv, kzv = cvalues.shape
+                for ill, iline in enumerate(segyfile.ilines):
+                    if ixv != jyv != kzv or ixv != kzv != jyv:
+                        segyfile.iline[iline] = cvalues[ill]  # broadcasting
+                    else:
+                        # safer but a bit slower than broadcasting
+                        segyfile.iline[iline] = cvalues[ill, :, :]
+
+    else:
+        # NOT FINISHED!
+        logger.debug("Input segy file from scratch ...")
+
+        # sintv = int(self.zinc * 1000)
+        spec = segyio.spec()
+
+        spec.sorting = 2
+        spec.format = 1
+
+        spec.samples = np.arange(self.nlay)
+        spec.ilines = np.arange(self.ncol)
+        spec.xlines = np.arange(self.nrow)
+
+        with segyio.create(sfile, spec) as fseg:
+            # write the line itself to the file and the inline number
+            # in all this line's headers
+            for ill, ilno in enumerate(spec.ilines):
+                fseg.iline[ilno] = cvalues[ill]
+                # f.header.iline[ilno] = {
+                #     segyio.TraceField.INLINE_3D: ilno,
+                #     segyio.TraceField.offset: 0,
+                #     segyio.TraceField.TRACE_SAMPLE_INTERVAL: sintv
+                # }
+
+            # # then do the same for xlines
+            # for xlno in spec.xlines:
+            #     f.header.xline[xlno] = {
+            #         segyio.TraceField.CROSSLINE_3D: xlno,
+            #         segyio.TraceField.TRACE_SAMPLE_INTERVAL: sintv
+            #     }
+
+
+def _export_segy_xtgeo(self, sfile):
+    """Export SEGY via XTGeo internal C routine."""
+
+    values1d = self.values.reshape(-1)
+
+    ilinesp = _cxtgeo.new_intarray(len(self._ilines))
+    xlinesp = _cxtgeo.new_intarray(len(self._xlines))
+    tracidp = _cxtgeo.new_intarray(self.ncol * self.nrow)
+
+    ilns = self._ilines.astype(np.int32)
+    xlns = self._xlines.astype(np.int32)
+    trid = self._traceidcodes.flatten().astype(np.int32)
+
+    _cxtgeo.swig_numpy_to_carr_i1d(ilns, ilinesp)
+    _cxtgeo.swig_numpy_to_carr_i1d(xlns, xlinesp)
+    _cxtgeo.swig_numpy_to_carr_i1d(trid, tracidp)
+
+    status = _cxtgeo.cube_export_segy(
+        sfile,
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        values1d,
+        self.xori,
+        self.xinc,
+        self.yori,
+        self.yinc,
+        self.zori,
+        self.zinc,
+        self.rotation,
+        self.yflip,
+        1,
+        ilinesp,
+        xlinesp,
+        tracidp,
+        0,
+    )
+
+    if status != 0:
+        raise XTGeoCLibError("Error when exporting to SEGY (xtgeo engine)")
+
+    _cxtgeo.delete_intarray(ilinesp)
+    _cxtgeo.delete_intarray(xlinesp)
+
+
+def export_rmsreg(self, sfile):
+    """Export on RMS regular format."""
+
+    logger.debug("Export to RMS regular format...")
+    values1d = self.values.reshape(-1)
+
+    status = _cxtgeo.cube_export_rmsregular(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self.xori,
+        self.yori,
+        self.zori,
+        self.xinc,
+        self.yinc * self.yflip,
+        self.zinc,
+        self.rotation,
+        self.yflip,
+        values1d,
+        sfile,
+    )
+
+    if status != 0:
+        raise RuntimeError("Error when exporting to RMS regular")
+
+
+def export_xtgregcube(self, mfile):
+    """Export to experimental xtgregcube format, python version."""
+    logger.info("Export as xtgregcube...")
+    self.metadata.required = self
+
+    prevalues = (1, 1201, 4, self.ncol, self.nrow, self.nlay)
+    mystruct = struct.Struct("= i i i q q q")
+    pre = mystruct.pack(*prevalues)
+
+    meta = self.metadata.get_metadata()
+
+    jmeta = json.dumps(meta).encode()
+
+    with open(mfile, "wb") as fout:
+        fout.write(pre)
+
+    with open(mfile, "ab") as fout:
+        # TODO. Treat dead traces as undef
+        self.values.tofile(fout)
+
+    with open(mfile, "ab") as fout:
+        fout.write("\nXTGMETA.v01\n".encode())
+
+    with open(mfile, "ab") as fout:
+        fout.write(jmeta)
+
+    logger.info("Export as xtgregcube... done")
```

## xtgeo/cube/_cube_import.py

 * *Ordering differences only*

```diff
@@ -1,620 +1,620 @@
-"""Import Cube data via SegyIO library or XTGeo CLIB.
-
-Data model in XTGeo illustrated by example: ncol=3, nrow=4 (non-rotated):
-
-                    3              7                    ^ "J, ~NORTH" direction
-  xline=1022       +--------------+--------------+11    |     (if unrotated)
-                   |              |              |      |
-                   |              |              |      |
-                   |              |              |
-                   |2             |6             |10
-  xline=1020       +--------------+--------------+      Rotation is school angle of
-                   |              |              |      "I east" vs X axis
-                   |              |              |
-                   |              |              |
-                   |1             |5             |9
-  xline=1018       +--------------+--------------+
-                   |              |              |
-                   |              |              |
-                   |              |              |
-                   |0             |4             |8
-  xline=1016       +--------------+--------------+      ------> "I, ~EAST" direction
-
-                iline=4400     iline=4401     iline=4402
-
-Indices is fastest along "J" (C order), and xlines and ilines spacing may vary (2 for
-xline, 1 for iline in this example) but shall be constant per axis
-
-"""
-import json
-from collections import OrderedDict, defaultdict
-from copy import deepcopy
-from struct import unpack
-from typing import Dict, List, Tuple
-from warnings import warn
-
-import numpy as np
-import segyio
-from segyio import TraceField as TF
-
-import xtgeo
-import xtgeo.common.calc as xcalc
-import xtgeo.common.sys as xsys
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def import_segy(sfile: xtgeo._XTGeoFile) -> Dict:
-    """Import SEGY via the SegyIO library.
-
-    Args:
-        sfile: File object for SEGY file
-    """
-    sfile = sfile.file
-
-    attributes = dict()
-
-    try:
-        # cube with all traces present
-        with segyio.open(sfile, "r") as segyfile:
-            attributes = _import_segy_all_traces(segyfile)
-    except ValueError as verr:
-        if any([word in str(verr) for word in ["Invalid dimensions", "inconsistent"]]):
-            # cube with missing traces is now handled but his is complex, hence users
-            # shall be warned. With more experience, this warning can be removed.
-            warn(
-                "Missing or inconsistent traces in SEGY detected, xtgeo will try "
-                "to import by infilling, but please check result carefully!",
-                UserWarning,
-            )
-
-            with segyio.open(sfile, "r", ignore_geometry=True) as segyfile:
-                attributes = _import_segy_incomplete_traces(segyfile)
-        else:
-            raise
-    except Exception as anyerror:  # catch the rest
-        raise IOError(f"Cannot parse SEGY file: {str(anyerror)}") from anyerror
-
-    if not attributes:
-        raise ValueError("Could not get attributes for segy file")
-
-    attributes["segyfile"] = sfile
-    return attributes
-
-
-def _import_segy_all_traces(segyfile: segyio.segy.SegyFile) -> Dict:
-    """Import a a full cube SEGY via the SegyIO library to xtgeo format spec.
-
-    Here, the segyio.tools.cube function can be applied
-
-    Args:
-        segyfile: Filehandle from segyio
-    """
-    segyfile.mmap()
-
-    values = _process_cube_values(segyio.tools.cube(segyfile))
-
-    ilines = segyfile.ilines
-    xlines = segyfile.xlines
-
-    ncol, nrow, nlay = values.shape
-
-    # get additional but required geometries for xtgeo; xori, yori, xinc, ..., rotation
-    attrs = _segy_all_traces_attributes(segyfile, ncol, nrow, nlay)
-
-    attrs["ilines"] = ilines
-    attrs["xlines"] = xlines
-    attrs["values"] = values
-
-    return attrs
-
-
-def _process_cube_values(values: np.ndarray) -> np.ndarray:
-    """Helper function to validate/check values."""
-    if values.dtype != np.float32:
-        xtg.warnuser(f"Values are converted from {values.dtype} to float32")
-        values = values.astype(np.float32)
-    if np.any(np.isnan(values)):
-        raise ValueError(
-            f"The input values: {values} contains NaN values which is currently "
-            "not handled!"
-        )
-
-    return values
-
-
-def _segy_all_traces_attributes(
-    segyfile: segyio.segy.SegyFile, ncol, nrow, nlay
-) -> Dict:
-    """Get the geometrical values xtgeo needs for a cube definition."""
-    trcode = segyio.TraceField.TraceIdentificationCode
-    traceidcodes = segyfile.attributes(trcode)[:].reshape(ncol, nrow)
-
-    # need positions in corners for making vectors to compute geometries
-    c1v = xcalc.ijk_to_ib(1, 1, 1, ncol, nrow, 1, forder=False)
-    c2v = xcalc.ijk_to_ib(ncol, 1, 1, ncol, nrow, 1, forder=False)
-    c3v = xcalc.ijk_to_ib(1, nrow, 1, ncol, nrow, 1, forder=False)
-
-    xori, yori, zori, zinc = _get_coordinate(segyfile, c1v)
-    point_x1, point_y1, _, _ = _get_coordinate(segyfile, c2v)
-    point_x2, point_y2, _, _ = _get_coordinate(segyfile, c3v)
-
-    slen1, _, rotation = xcalc.vectorinfo2(xori, point_x1, yori, point_y1)
-    xinc = slen1 / (ncol - 1)
-
-    slen2, _, _ = xcalc.vectorinfo2(xori, point_x2, yori, point_y2)
-    yinc = slen2 / (nrow - 1)
-
-    # find YFLIP by cross products
-    yflip = xcalc.find_flip(
-        (point_x1 - xori, point_y1 - yori, 0), (point_x2 - xori, point_y2 - yori, 0)
-    )
-
-    return {
-        "ncol": ncol,
-        "nrow": nrow,
-        "nlay": nlay,
-        "xori": xori,
-        "xinc": xinc,
-        "yori": yori,
-        "yinc": yinc,
-        "zori": zori,
-        "zinc": zinc,
-        "rotation": rotation,
-        "yflip": yflip,
-        "traceidcodes": traceidcodes,
-    }
-
-
-def _import_segy_incomplete_traces(segyfile: segyio.segy.SegyFile) -> Dict:
-    """Import a a cube SEGY with incomplete traces via the SegyIO library.
-
-    Note that the undefined value will be xtgeo.UNDEF (large number)!
-
-    It is also logical to treat missing traces as dead traces, i.e. the should
-    get value 2 in traceidcodes
-
-    Args:
-        segyfile: Filehandle from segyio
-    """
-    segyfile.mmap()
-    # get data (which will need padding later for missing traces)
-    data = segyfile.trace.raw[:]
-
-    trcode = segyio.TraceField.TraceIdentificationCode
-    traceidcodes_input = segyfile.attributes(trcode)[:]
-
-    ilines_case = np.array([h[TF.INLINE_3D] for h in segyfile.header])
-    xlines_case = np.array([h[TF.CROSSLINE_3D] for h in segyfile.header])
-
-    # detect minimum inline and xline spacing (e.g.sampling could be every second)
-    idiff = np.diff(ilines_case)
-    xdiff = np.diff(xlines_case)
-    ispacing = int(np.abs(idiff[idiff != 0]).min())
-    xspacing = int(np.abs(xdiff[xdiff != 0]).min())
-
-    ncol = int(abs(ilines_case.min() - ilines_case.max()) / ispacing) + 1
-    nrow = int(abs(xlines_case.min() - xlines_case.max()) / xspacing) + 1
-    nlay = data.shape[1]
-
-    values = np.full((ncol, nrow, nlay), xtgeo.UNDEF, dtype=np.float32)
-    traceidcodes = np.full((ncol, nrow), 2, dtype=np.int64)
-
-    ilines_shifted = (ilines_case / ispacing).astype(np.int64)
-    ilines_shifted -= ilines_shifted.min()
-    xlines_shifted = (xlines_case / xspacing).astype(np.int64)
-    xlines_shifted -= xlines_shifted.min()
-
-    values[ilines_shifted, xlines_shifted, :] = data
-    values = _process_cube_values(values)
-    traceidcodes[ilines_shifted, xlines_shifted] = traceidcodes_input
-
-    # generate new ilines, xlines vector with unique values
-    ilines = np.array(
-        range(ilines_case.min(), ilines_case.max() + ispacing, ispacing), dtype=np.int32
-    )
-    xlines = np.array(
-        range(xlines_case.min(), xlines_case.max() + xspacing, xspacing), dtype=np.int32
-    )
-
-    attrs = _geometry_incomplete_traces(
-        segyfile,
-        ncol,
-        nrow,
-        ilines,
-        xlines,
-        ilines_case,
-        xlines_case,
-        ispacing,
-        xspacing,
-    )
-
-    attrs["ncol"] = ncol
-    attrs["nrow"] = nrow
-    attrs["nlay"] = nlay
-    attrs["ilines"] = ilines
-    attrs["xlines"] = xlines
-    attrs["values"] = values
-    attrs["traceidcodes"] = traceidcodes
-    return attrs
-
-
-def _inverse_anyline_map(anylines: List[int]) -> Dict:
-    """Small helper function to get e.g. inline 2345: [0, 1, 2, .., 70].
-
-    I.e. to get a mapping between inline number and a list of possible indices
-
-    """
-    anyll = defaultdict(list)
-    for ind, key in enumerate(anylines):
-        anyll[key].append(ind)
-
-    return anyll
-
-
-def _find_long_line(anyll: dict, nany: int) -> List:
-    """Helper function; get index of vectors indices to be used for calculations.
-
-    Look for a "sufficiently" long inline/xline, as long distance between points
-    increases accuracy.
-
-    """
-    minimumlen = int(nany * 0.8)  # get at least 80% length if possible
-    maxlenfound = -1
-
-    keepresult = []
-    for indices in anyll.values():
-        result = [indices[0], indices[-1]]
-        indlen = len(indices)
-        if indlen > maxlenfound:
-            maxlenfound = indlen
-            keepresult = deepcopy(result)
-
-        if indlen >= minimumlen:
-            break
-
-    if not keepresult or abs(keepresult[1] - keepresult[0]) == 0:
-        raise RuntimeError("Not able to get inline or xline vector for geometry")
-    return keepresult
-
-
-def _geometry_incomplete_traces(
-    segyfile: segyio.segy.SegyFile,
-    ncol: int,
-    nrow: int,
-    ilines: List[int],
-    xlines: List[int],
-    ilines_case: List[int],
-    xlines_case: List[int],
-    ispacing: int,
-    xspacing: int,
-) -> List:
-    """Compute xtgeo attributes (mostly geometries) for incomplete trace cube."""
-    attrs = dict()
-
-    ill = _inverse_anyline_map(ilines_case)
-    xll = _inverse_anyline_map(xlines_case)
-
-    # need both partial and full reverse lookup of indices vs (iline, xxline)
-    # for computing cube origin later
-    index_case = {
-        ind: (h[TF.INLINE_3D], h[TF.CROSSLINE_3D])
-        for ind, h in enumerate(segyfile.header)
-    }
-
-    reverseindex_full = {
-        (il, xl): (ind, xnd)
-        for ind, il in enumerate(ilines)
-        for xnd, xl in enumerate(xlines)
-    }
-
-    jnd1, jnd2 = _find_long_line(ill, ncol)  # 2 indices along constant iline, aka JY
-    ind1, ind2 = _find_long_line(xll, nrow)  # 2 indices along constant xline, aka IX
-
-    il1x, il1y, zori, zinc = _get_coordinate(segyfile, ind1)
-    il2x, il2y, _, _ = _get_coordinate(segyfile, ind2)
-
-    jl1x, jl1y, _, _ = _get_coordinate(segyfile, jnd1)
-    jl2x, jl2y, _, _ = _get_coordinate(segyfile, jnd2)
-
-    xslen, _, rot1 = xcalc.vectorinfo2(il1x, il2x, il1y, il2y)
-    xinc = ispacing * xslen / (abs(ilines_case[ind1] - ilines_case[ind2]))
-    yslen, _, _ = xcalc.vectorinfo2(jl1x, jl2x, jl1y, jl2y)
-    yinc = xspacing * yslen / (abs(xlines_case[jnd1] - xlines_case[jnd2]))
-
-    yflip = xcalc.find_flip(
-        (il2x - il1x, il2y - il1y, 0), (jl2x - jl1x, jl2y - jl1y, 0)
-    )
-
-    # need to compute xori and yori from 'case' I J indices with known x y and
-    # (iline, xline); use ind1 with assosiated coordinates il1x il1y
-    i_use, j_use = reverseindex_full[index_case[ind1]]
-    xori, yori = xcalc.xyori_from_ij(
-        i_use, j_use, il1x, il1y, xinc, yinc, ncol, nrow, yflip, rot1
-    )
-
-    attrs["xori"] = xori
-    attrs["yori"] = yori
-    attrs["zori"] = zori
-    attrs["xinc"] = xinc
-    attrs["yinc"] = yinc
-    attrs["zinc"] = zinc
-    attrs["yflip"] = yflip
-    attrs["rotation"] = rot1
-
-    return attrs
-
-
-def _get_coordinate(
-    segyfile: segyio.segy.SegyFile, segyindex: int
-) -> Tuple[float, float, float, float]:
-    """Helper function to get coordinates given a index."""
-    origin = segyfile.header[segyindex][
-        segyio.su.cdpx,
-        segyio.su.cdpy,
-        segyio.su.scalco,
-        segyio.su.delrt,
-        segyio.su.dt,
-        segyio.su.iline,
-        segyio.su.xline,
-    ]
-
-    point_x = origin[segyio.su.cdpx]
-    point_y = origin[segyio.su.cdpy]
-    scaler = origin[segyio.su.scalco]
-    if scaler < 0:
-        point_x = -1 * float(point_x) / scaler
-        point_y = -1 * float(point_y) / scaler
-    else:
-        point_x = point_x * scaler
-        point_y = point_y * scaler
-
-    zori = origin[segyio.su.delrt]
-    zinc = origin[segyio.su.dt] / 1000.0
-
-    return point_x, point_y, zori, zinc
-
-
-def _scan_segy_header(sfile, outfile):
-    ptr_gn_bitsheader = _cxtgeo.new_intpointer()
-    ptr_gn_formatcode = _cxtgeo.new_intpointer()
-    ptr_gf_segyformat = _cxtgeo.new_floatpointer()
-    ptr_gn_samplespertrace = _cxtgeo.new_intpointer()
-    ptr_gn_measuresystem = _cxtgeo.new_intpointer()
-
-    _cxtgeo.cube_scan_segy_hdr(
-        sfile,
-        ptr_gn_bitsheader,
-        ptr_gn_formatcode,
-        ptr_gf_segyformat,
-        ptr_gn_samplespertrace,
-        ptr_gn_measuresystem,
-        1,
-        outfile,
-    )
-    gn_bitsheader = _cxtgeo.intpointer_value(ptr_gn_bitsheader)
-    logger.info("Scan SEGY header ... %s bytes ... DONE", gn_bitsheader)
-
-
-def _scan_segy_trace(sfile, outfile):
-    ptr_gn_bitsheader = _cxtgeo.new_intpointer()
-    ptr_gn_formatcode = _cxtgeo.new_intpointer()
-    ptr_gf_segyformat = _cxtgeo.new_floatpointer()
-    ptr_gn_samplespertrace = _cxtgeo.new_intpointer()
-    ptr_gn_measuresystem = _cxtgeo.new_intpointer()
-
-    _cxtgeo.cube_scan_segy_hdr(
-        sfile,
-        ptr_gn_bitsheader,
-        ptr_gn_formatcode,
-        ptr_gf_segyformat,
-        ptr_gn_samplespertrace,
-        ptr_gn_measuresystem,
-        0,
-        outfile,
-    )
-
-    gn_bitsheader = _cxtgeo.intpointer_value(ptr_gn_bitsheader)
-    gn_formatcode = _cxtgeo.intpointer_value(ptr_gn_formatcode)
-    gf_segyformat = _cxtgeo.floatpointer_value(ptr_gf_segyformat)
-    gn_samplespertrace = _cxtgeo.intpointer_value(ptr_gn_samplespertrace)
-
-    logger.info("Scan SEGY header ... %s bytes ... DONE", gn_bitsheader)
-    ptr_ncol = _cxtgeo.new_intpointer()
-    ptr_nrow = _cxtgeo.new_intpointer()
-    ptr_nlay = _cxtgeo.new_intpointer()
-    ptr_xori = _cxtgeo.new_doublepointer()
-    ptr_yori = _cxtgeo.new_doublepointer()
-    ptr_zori = _cxtgeo.new_doublepointer()
-    ptr_xinc = _cxtgeo.new_doublepointer()
-    ptr_yinc = _cxtgeo.new_doublepointer()
-    ptr_zinc = _cxtgeo.new_doublepointer()
-    ptr_rotation = _cxtgeo.new_doublepointer()
-    ptr_minval = _cxtgeo.new_doublepointer()
-    ptr_maxval = _cxtgeo.new_doublepointer()
-    ptr_dummy = _cxtgeo.new_floatpointer()
-    ptr_yflip = _cxtgeo.new_intpointer()
-    ptr_zflip = _cxtgeo.new_intpointer()
-
-    logger.debug("Scan via C wrapper...")
-    _cxtgeo.cube_import_segy(
-        sfile,
-        # input
-        gn_bitsheader,
-        gn_formatcode,
-        gf_segyformat,
-        gn_samplespertrace,
-        # result (as pointers)
-        ptr_ncol,
-        ptr_nrow,
-        ptr_nlay,
-        ptr_dummy,
-        ptr_xori,
-        ptr_xinc,
-        ptr_yori,
-        ptr_yinc,
-        ptr_zori,
-        ptr_zinc,
-        ptr_rotation,
-        ptr_yflip,
-        ptr_zflip,
-        ptr_minval,
-        ptr_maxval,
-        # options
-        1,
-        1,
-        outfile,
-    )
-
-    logger.debug("Scan via C wrapper... done")
-
-
-def import_stormcube(
-    sfile: xtgeo._XTGeoFile,
-) -> Dict:
-    """Import on StormCube format."""
-    # The ASCII header has all the metadata on the form:
-    # ---------------------------------------------------------------------
-    # storm_petro_binary       // always
-    #
-    # 0 ModelFile -999 // zonenumber, source_of_file,  undef_value
-    #
-    # UNKNOWN // name_of_parameter?
-    #
-    # 452638.45298827 6262.499 6780706.6462283 10762.4999 1800 2500 0 0
-    # 700 -0.80039470880765
-    #
-    # 501 861 140
-    # ---------------------------------------------------------------------
-    # The rest is float32 binary data, I (column fastest), then J, then K
-    # a total of ncol * nrow * nlay
-
-    # Scan the header with Python; then use CLIB for the binary data
-    sfile = str(sfile.file)
-    with open(sfile, "rb") as stf:
-        iline = 0
-
-        ncol = nrow = nlay = nlines = 1
-        xori = yori = zori = xinc = yinc = rotation = rot = 0.999
-        xlen = ylen = zlen = 0.999
-
-        for line in range(10):
-            xline = stf.readline()
-            if not xline.strip():
-                continue
-
-            iline += 1
-            if iline == 1:
-                pass
-            elif iline == 2:
-                _, _, _ = xline.strip().split()
-            elif iline == 3:
-                pass
-            elif iline == 4:
-                (xori, xlen, yori, ylen, zori, _, _, _) = xline.strip().split()
-            elif iline == 5:
-                zlen, rot = xline.strip().split()
-            elif iline == 6:
-                ncol, nrow, nlay = xline.strip().split()
-                nlines = line + 2
-                break
-
-    ncol = int(ncol)
-    nrow = int(nrow)
-    nlay = int(nlay)
-    nrcl = ncol * nrow * nlay
-
-    xori = float(xori)
-    yori = float(yori)
-    zori = float(zori)
-
-    rotation = float(rot)
-    if rotation < 0:
-        rotation += 360
-
-    xinc = float(xlen) / ncol
-    yinc = float(ylen) / nrow
-    zinc = float(zlen) / nlay
-
-    yflip = 1
-
-    if yinc < 0:
-        yflip = -1
-        yinc = yinc * yflip  # not sure if this will ever happen
-
-    ier, values = _cxtgeo.cube_import_storm(ncol, nrow, nlay, sfile, nlines, nrcl, 0)
-
-    if ier != 0:
-        raise RuntimeError(f"Something went wrong in {__name__}, code is {ier}")
-
-    return {
-        "ncol": ncol,
-        "nrow": nrow,
-        "nlay": nlay,
-        "xori": xori,
-        "xinc": xinc,
-        "yori": yori,
-        "yinc": yinc,
-        "zori": zori,
-        "zinc": zinc,
-        "rotation": rotation,
-        "values": values.reshape((ncol, nrow, nlay)),
-        "yflip": yflip,
-    }
-
-
-def import_xtgregcube(mfile, values=True):
-    """Using pure python for experimental cube import, xtgregsurf format."""
-    logger.info("Importing cube on xtgregcube format...")
-
-    offset = 36
-    with open(mfile.file, "rb") as fhandle:
-        buf = fhandle.read(offset)
-
-    # unpack header
-    swap, magic, nfloat, ncol, nrow, nlay = unpack("= i i i q q q", buf)
-
-    if swap != 1 or magic != 1201:
-        raise ValueError("Invalid file format (wrong swap id or magic number).")
-
-    dtype = np.float32 if nfloat == 4 else np.float64
-
-    vals = None
-    narr = ncol * nrow * nlay
-
-    if values:
-        vals = xsys.npfromfile(mfile.file, dtype=dtype, count=narr, offset=offset)
-
-    # read metadata which will be at position offet + nfloat*narr +13
-    pos = offset + nfloat * narr + 13
-
-    with open(mfile.file, "rb") as fhandle:
-        fhandle.seek(pos)
-        jmeta = fhandle.read().decode()
-
-    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-    req = meta["_required_"]
-
-    reqattrs = xtgeo.MetaDataRegularCube.REQUIRED
-
-    results = {myattr: req[myattr] for myattr in reqattrs}
-
-    # For backwards compatability, xtgeo outputs files with the undef field set
-    # although we do not support initializing with any other value.
-    # As xtgeo-format is only written/read by xtgeo as far as we know, this should
-    # be unproblematic for now.
-    if results.pop("undef", None) != xtgeo.UNDEF:
-        raise ValueError(
-            f"File {mfile.file} has non-standard undef, not supported by xtgeo"
-        )
-
-    # TODO: dead traces and traceidcodes
-    if values:
-        results["values"] = vals.reshape(
-            results["ncol"], results["nrow"], results["nlay"]
-        )
-
-    return results
+"""Import Cube data via SegyIO library or XTGeo CLIB.
+
+Data model in XTGeo illustrated by example: ncol=3, nrow=4 (non-rotated):
+
+                    3              7                    ^ "J, ~NORTH" direction
+  xline=1022       +--------------+--------------+11    |     (if unrotated)
+                   |              |              |      |
+                   |              |              |      |
+                   |              |              |
+                   |2             |6             |10
+  xline=1020       +--------------+--------------+      Rotation is school angle of
+                   |              |              |      "I east" vs X axis
+                   |              |              |
+                   |              |              |
+                   |1             |5             |9
+  xline=1018       +--------------+--------------+
+                   |              |              |
+                   |              |              |
+                   |              |              |
+                   |0             |4             |8
+  xline=1016       +--------------+--------------+      ------> "I, ~EAST" direction
+
+                iline=4400     iline=4401     iline=4402
+
+Indices is fastest along "J" (C order), and xlines and ilines spacing may vary (2 for
+xline, 1 for iline in this example) but shall be constant per axis
+
+"""
+import json
+from collections import OrderedDict, defaultdict
+from copy import deepcopy
+from struct import unpack
+from typing import Dict, List, Tuple
+from warnings import warn
+
+import numpy as np
+import segyio
+from segyio import TraceField as TF
+
+import xtgeo
+import xtgeo.common.calc as xcalc
+import xtgeo.common.sys as xsys
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def import_segy(sfile: xtgeo._XTGeoFile) -> Dict:
+    """Import SEGY via the SegyIO library.
+
+    Args:
+        sfile: File object for SEGY file
+    """
+    sfile = sfile.file
+
+    attributes = dict()
+
+    try:
+        # cube with all traces present
+        with segyio.open(sfile, "r") as segyfile:
+            attributes = _import_segy_all_traces(segyfile)
+    except ValueError as verr:
+        if any([word in str(verr) for word in ["Invalid dimensions", "inconsistent"]]):
+            # cube with missing traces is now handled but his is complex, hence users
+            # shall be warned. With more experience, this warning can be removed.
+            warn(
+                "Missing or inconsistent traces in SEGY detected, xtgeo will try "
+                "to import by infilling, but please check result carefully!",
+                UserWarning,
+            )
+
+            with segyio.open(sfile, "r", ignore_geometry=True) as segyfile:
+                attributes = _import_segy_incomplete_traces(segyfile)
+        else:
+            raise
+    except Exception as anyerror:  # catch the rest
+        raise IOError(f"Cannot parse SEGY file: {str(anyerror)}") from anyerror
+
+    if not attributes:
+        raise ValueError("Could not get attributes for segy file")
+
+    attributes["segyfile"] = sfile
+    return attributes
+
+
+def _import_segy_all_traces(segyfile: segyio.segy.SegyFile) -> Dict:
+    """Import a a full cube SEGY via the SegyIO library to xtgeo format spec.
+
+    Here, the segyio.tools.cube function can be applied
+
+    Args:
+        segyfile: Filehandle from segyio
+    """
+    segyfile.mmap()
+
+    values = _process_cube_values(segyio.tools.cube(segyfile))
+
+    ilines = segyfile.ilines
+    xlines = segyfile.xlines
+
+    ncol, nrow, nlay = values.shape
+
+    # get additional but required geometries for xtgeo; xori, yori, xinc, ..., rotation
+    attrs = _segy_all_traces_attributes(segyfile, ncol, nrow, nlay)
+
+    attrs["ilines"] = ilines
+    attrs["xlines"] = xlines
+    attrs["values"] = values
+
+    return attrs
+
+
+def _process_cube_values(values: np.ndarray) -> np.ndarray:
+    """Helper function to validate/check values."""
+    if values.dtype != np.float32:
+        xtg.warnuser(f"Values are converted from {values.dtype} to float32")
+        values = values.astype(np.float32)
+    if np.any(np.isnan(values)):
+        raise ValueError(
+            f"The input values: {values} contains NaN values which is currently "
+            "not handled!"
+        )
+
+    return values
+
+
+def _segy_all_traces_attributes(
+    segyfile: segyio.segy.SegyFile, ncol, nrow, nlay
+) -> Dict:
+    """Get the geometrical values xtgeo needs for a cube definition."""
+    trcode = segyio.TraceField.TraceIdentificationCode
+    traceidcodes = segyfile.attributes(trcode)[:].reshape(ncol, nrow)
+
+    # need positions in corners for making vectors to compute geometries
+    c1v = xcalc.ijk_to_ib(1, 1, 1, ncol, nrow, 1, forder=False)
+    c2v = xcalc.ijk_to_ib(ncol, 1, 1, ncol, nrow, 1, forder=False)
+    c3v = xcalc.ijk_to_ib(1, nrow, 1, ncol, nrow, 1, forder=False)
+
+    xori, yori, zori, zinc = _get_coordinate(segyfile, c1v)
+    point_x1, point_y1, _, _ = _get_coordinate(segyfile, c2v)
+    point_x2, point_y2, _, _ = _get_coordinate(segyfile, c3v)
+
+    slen1, _, rotation = xcalc.vectorinfo2(xori, point_x1, yori, point_y1)
+    xinc = slen1 / (ncol - 1)
+
+    slen2, _, _ = xcalc.vectorinfo2(xori, point_x2, yori, point_y2)
+    yinc = slen2 / (nrow - 1)
+
+    # find YFLIP by cross products
+    yflip = xcalc.find_flip(
+        (point_x1 - xori, point_y1 - yori, 0), (point_x2 - xori, point_y2 - yori, 0)
+    )
+
+    return {
+        "ncol": ncol,
+        "nrow": nrow,
+        "nlay": nlay,
+        "xori": xori,
+        "xinc": xinc,
+        "yori": yori,
+        "yinc": yinc,
+        "zori": zori,
+        "zinc": zinc,
+        "rotation": rotation,
+        "yflip": yflip,
+        "traceidcodes": traceidcodes,
+    }
+
+
+def _import_segy_incomplete_traces(segyfile: segyio.segy.SegyFile) -> Dict:
+    """Import a a cube SEGY with incomplete traces via the SegyIO library.
+
+    Note that the undefined value will be xtgeo.UNDEF (large number)!
+
+    It is also logical to treat missing traces as dead traces, i.e. the should
+    get value 2 in traceidcodes
+
+    Args:
+        segyfile: Filehandle from segyio
+    """
+    segyfile.mmap()
+    # get data (which will need padding later for missing traces)
+    data = segyfile.trace.raw[:]
+
+    trcode = segyio.TraceField.TraceIdentificationCode
+    traceidcodes_input = segyfile.attributes(trcode)[:]
+
+    ilines_case = np.array([h[TF.INLINE_3D] for h in segyfile.header])
+    xlines_case = np.array([h[TF.CROSSLINE_3D] for h in segyfile.header])
+
+    # detect minimum inline and xline spacing (e.g.sampling could be every second)
+    idiff = np.diff(ilines_case)
+    xdiff = np.diff(xlines_case)
+    ispacing = int(np.abs(idiff[idiff != 0]).min())
+    xspacing = int(np.abs(xdiff[xdiff != 0]).min())
+
+    ncol = int(abs(ilines_case.min() - ilines_case.max()) / ispacing) + 1
+    nrow = int(abs(xlines_case.min() - xlines_case.max()) / xspacing) + 1
+    nlay = data.shape[1]
+
+    values = np.full((ncol, nrow, nlay), xtgeo.UNDEF, dtype=np.float32)
+    traceidcodes = np.full((ncol, nrow), 2, dtype=np.int64)
+
+    ilines_shifted = (ilines_case / ispacing).astype(np.int64)
+    ilines_shifted -= ilines_shifted.min()
+    xlines_shifted = (xlines_case / xspacing).astype(np.int64)
+    xlines_shifted -= xlines_shifted.min()
+
+    values[ilines_shifted, xlines_shifted, :] = data
+    values = _process_cube_values(values)
+    traceidcodes[ilines_shifted, xlines_shifted] = traceidcodes_input
+
+    # generate new ilines, xlines vector with unique values
+    ilines = np.array(
+        range(ilines_case.min(), ilines_case.max() + ispacing, ispacing), dtype=np.int32
+    )
+    xlines = np.array(
+        range(xlines_case.min(), xlines_case.max() + xspacing, xspacing), dtype=np.int32
+    )
+
+    attrs = _geometry_incomplete_traces(
+        segyfile,
+        ncol,
+        nrow,
+        ilines,
+        xlines,
+        ilines_case,
+        xlines_case,
+        ispacing,
+        xspacing,
+    )
+
+    attrs["ncol"] = ncol
+    attrs["nrow"] = nrow
+    attrs["nlay"] = nlay
+    attrs["ilines"] = ilines
+    attrs["xlines"] = xlines
+    attrs["values"] = values
+    attrs["traceidcodes"] = traceidcodes
+    return attrs
+
+
+def _inverse_anyline_map(anylines: List[int]) -> Dict:
+    """Small helper function to get e.g. inline 2345: [0, 1, 2, .., 70].
+
+    I.e. to get a mapping between inline number and a list of possible indices
+
+    """
+    anyll = defaultdict(list)
+    for ind, key in enumerate(anylines):
+        anyll[key].append(ind)
+
+    return anyll
+
+
+def _find_long_line(anyll: dict, nany: int) -> List:
+    """Helper function; get index of vectors indices to be used for calculations.
+
+    Look for a "sufficiently" long inline/xline, as long distance between points
+    increases accuracy.
+
+    """
+    minimumlen = int(nany * 0.8)  # get at least 80% length if possible
+    maxlenfound = -1
+
+    keepresult = []
+    for indices in anyll.values():
+        result = [indices[0], indices[-1]]
+        indlen = len(indices)
+        if indlen > maxlenfound:
+            maxlenfound = indlen
+            keepresult = deepcopy(result)
+
+        if indlen >= minimumlen:
+            break
+
+    if not keepresult or abs(keepresult[1] - keepresult[0]) == 0:
+        raise RuntimeError("Not able to get inline or xline vector for geometry")
+    return keepresult
+
+
+def _geometry_incomplete_traces(
+    segyfile: segyio.segy.SegyFile,
+    ncol: int,
+    nrow: int,
+    ilines: List[int],
+    xlines: List[int],
+    ilines_case: List[int],
+    xlines_case: List[int],
+    ispacing: int,
+    xspacing: int,
+) -> List:
+    """Compute xtgeo attributes (mostly geometries) for incomplete trace cube."""
+    attrs = dict()
+
+    ill = _inverse_anyline_map(ilines_case)
+    xll = _inverse_anyline_map(xlines_case)
+
+    # need both partial and full reverse lookup of indices vs (iline, xxline)
+    # for computing cube origin later
+    index_case = {
+        ind: (h[TF.INLINE_3D], h[TF.CROSSLINE_3D])
+        for ind, h in enumerate(segyfile.header)
+    }
+
+    reverseindex_full = {
+        (il, xl): (ind, xnd)
+        for ind, il in enumerate(ilines)
+        for xnd, xl in enumerate(xlines)
+    }
+
+    jnd1, jnd2 = _find_long_line(ill, ncol)  # 2 indices along constant iline, aka JY
+    ind1, ind2 = _find_long_line(xll, nrow)  # 2 indices along constant xline, aka IX
+
+    il1x, il1y, zori, zinc = _get_coordinate(segyfile, ind1)
+    il2x, il2y, _, _ = _get_coordinate(segyfile, ind2)
+
+    jl1x, jl1y, _, _ = _get_coordinate(segyfile, jnd1)
+    jl2x, jl2y, _, _ = _get_coordinate(segyfile, jnd2)
+
+    xslen, _, rot1 = xcalc.vectorinfo2(il1x, il2x, il1y, il2y)
+    xinc = ispacing * xslen / (abs(ilines_case[ind1] - ilines_case[ind2]))
+    yslen, _, _ = xcalc.vectorinfo2(jl1x, jl2x, jl1y, jl2y)
+    yinc = xspacing * yslen / (abs(xlines_case[jnd1] - xlines_case[jnd2]))
+
+    yflip = xcalc.find_flip(
+        (il2x - il1x, il2y - il1y, 0), (jl2x - jl1x, jl2y - jl1y, 0)
+    )
+
+    # need to compute xori and yori from 'case' I J indices with known x y and
+    # (iline, xline); use ind1 with assosiated coordinates il1x il1y
+    i_use, j_use = reverseindex_full[index_case[ind1]]
+    xori, yori = xcalc.xyori_from_ij(
+        i_use, j_use, il1x, il1y, xinc, yinc, ncol, nrow, yflip, rot1
+    )
+
+    attrs["xori"] = xori
+    attrs["yori"] = yori
+    attrs["zori"] = zori
+    attrs["xinc"] = xinc
+    attrs["yinc"] = yinc
+    attrs["zinc"] = zinc
+    attrs["yflip"] = yflip
+    attrs["rotation"] = rot1
+
+    return attrs
+
+
+def _get_coordinate(
+    segyfile: segyio.segy.SegyFile, segyindex: int
+) -> Tuple[float, float, float, float]:
+    """Helper function to get coordinates given a index."""
+    origin = segyfile.header[segyindex][
+        segyio.su.cdpx,
+        segyio.su.cdpy,
+        segyio.su.scalco,
+        segyio.su.delrt,
+        segyio.su.dt,
+        segyio.su.iline,
+        segyio.su.xline,
+    ]
+
+    point_x = origin[segyio.su.cdpx]
+    point_y = origin[segyio.su.cdpy]
+    scaler = origin[segyio.su.scalco]
+    if scaler < 0:
+        point_x = -1 * float(point_x) / scaler
+        point_y = -1 * float(point_y) / scaler
+    else:
+        point_x = point_x * scaler
+        point_y = point_y * scaler
+
+    zori = origin[segyio.su.delrt]
+    zinc = origin[segyio.su.dt] / 1000.0
+
+    return point_x, point_y, zori, zinc
+
+
+def _scan_segy_header(sfile, outfile):
+    ptr_gn_bitsheader = _cxtgeo.new_intpointer()
+    ptr_gn_formatcode = _cxtgeo.new_intpointer()
+    ptr_gf_segyformat = _cxtgeo.new_floatpointer()
+    ptr_gn_samplespertrace = _cxtgeo.new_intpointer()
+    ptr_gn_measuresystem = _cxtgeo.new_intpointer()
+
+    _cxtgeo.cube_scan_segy_hdr(
+        sfile,
+        ptr_gn_bitsheader,
+        ptr_gn_formatcode,
+        ptr_gf_segyformat,
+        ptr_gn_samplespertrace,
+        ptr_gn_measuresystem,
+        1,
+        outfile,
+    )
+    gn_bitsheader = _cxtgeo.intpointer_value(ptr_gn_bitsheader)
+    logger.info("Scan SEGY header ... %s bytes ... DONE", gn_bitsheader)
+
+
+def _scan_segy_trace(sfile, outfile):
+    ptr_gn_bitsheader = _cxtgeo.new_intpointer()
+    ptr_gn_formatcode = _cxtgeo.new_intpointer()
+    ptr_gf_segyformat = _cxtgeo.new_floatpointer()
+    ptr_gn_samplespertrace = _cxtgeo.new_intpointer()
+    ptr_gn_measuresystem = _cxtgeo.new_intpointer()
+
+    _cxtgeo.cube_scan_segy_hdr(
+        sfile,
+        ptr_gn_bitsheader,
+        ptr_gn_formatcode,
+        ptr_gf_segyformat,
+        ptr_gn_samplespertrace,
+        ptr_gn_measuresystem,
+        0,
+        outfile,
+    )
+
+    gn_bitsheader = _cxtgeo.intpointer_value(ptr_gn_bitsheader)
+    gn_formatcode = _cxtgeo.intpointer_value(ptr_gn_formatcode)
+    gf_segyformat = _cxtgeo.floatpointer_value(ptr_gf_segyformat)
+    gn_samplespertrace = _cxtgeo.intpointer_value(ptr_gn_samplespertrace)
+
+    logger.info("Scan SEGY header ... %s bytes ... DONE", gn_bitsheader)
+    ptr_ncol = _cxtgeo.new_intpointer()
+    ptr_nrow = _cxtgeo.new_intpointer()
+    ptr_nlay = _cxtgeo.new_intpointer()
+    ptr_xori = _cxtgeo.new_doublepointer()
+    ptr_yori = _cxtgeo.new_doublepointer()
+    ptr_zori = _cxtgeo.new_doublepointer()
+    ptr_xinc = _cxtgeo.new_doublepointer()
+    ptr_yinc = _cxtgeo.new_doublepointer()
+    ptr_zinc = _cxtgeo.new_doublepointer()
+    ptr_rotation = _cxtgeo.new_doublepointer()
+    ptr_minval = _cxtgeo.new_doublepointer()
+    ptr_maxval = _cxtgeo.new_doublepointer()
+    ptr_dummy = _cxtgeo.new_floatpointer()
+    ptr_yflip = _cxtgeo.new_intpointer()
+    ptr_zflip = _cxtgeo.new_intpointer()
+
+    logger.debug("Scan via C wrapper...")
+    _cxtgeo.cube_import_segy(
+        sfile,
+        # input
+        gn_bitsheader,
+        gn_formatcode,
+        gf_segyformat,
+        gn_samplespertrace,
+        # result (as pointers)
+        ptr_ncol,
+        ptr_nrow,
+        ptr_nlay,
+        ptr_dummy,
+        ptr_xori,
+        ptr_xinc,
+        ptr_yori,
+        ptr_yinc,
+        ptr_zori,
+        ptr_zinc,
+        ptr_rotation,
+        ptr_yflip,
+        ptr_zflip,
+        ptr_minval,
+        ptr_maxval,
+        # options
+        1,
+        1,
+        outfile,
+    )
+
+    logger.debug("Scan via C wrapper... done")
+
+
+def import_stormcube(
+    sfile: xtgeo._XTGeoFile,
+) -> Dict:
+    """Import on StormCube format."""
+    # The ASCII header has all the metadata on the form:
+    # ---------------------------------------------------------------------
+    # storm_petro_binary       // always
+    #
+    # 0 ModelFile -999 // zonenumber, source_of_file,  undef_value
+    #
+    # UNKNOWN // name_of_parameter?
+    #
+    # 452638.45298827 6262.499 6780706.6462283 10762.4999 1800 2500 0 0
+    # 700 -0.80039470880765
+    #
+    # 501 861 140
+    # ---------------------------------------------------------------------
+    # The rest is float32 binary data, I (column fastest), then J, then K
+    # a total of ncol * nrow * nlay
+
+    # Scan the header with Python; then use CLIB for the binary data
+    sfile = str(sfile.file)
+    with open(sfile, "rb") as stf:
+        iline = 0
+
+        ncol = nrow = nlay = nlines = 1
+        xori = yori = zori = xinc = yinc = rotation = rot = 0.999
+        xlen = ylen = zlen = 0.999
+
+        for line in range(10):
+            xline = stf.readline()
+            if not xline.strip():
+                continue
+
+            iline += 1
+            if iline == 1:
+                pass
+            elif iline == 2:
+                _, _, _ = xline.strip().split()
+            elif iline == 3:
+                pass
+            elif iline == 4:
+                (xori, xlen, yori, ylen, zori, _, _, _) = xline.strip().split()
+            elif iline == 5:
+                zlen, rot = xline.strip().split()
+            elif iline == 6:
+                ncol, nrow, nlay = xline.strip().split()
+                nlines = line + 2
+                break
+
+    ncol = int(ncol)
+    nrow = int(nrow)
+    nlay = int(nlay)
+    nrcl = ncol * nrow * nlay
+
+    xori = float(xori)
+    yori = float(yori)
+    zori = float(zori)
+
+    rotation = float(rot)
+    if rotation < 0:
+        rotation += 360
+
+    xinc = float(xlen) / ncol
+    yinc = float(ylen) / nrow
+    zinc = float(zlen) / nlay
+
+    yflip = 1
+
+    if yinc < 0:
+        yflip = -1
+        yinc = yinc * yflip  # not sure if this will ever happen
+
+    ier, values = _cxtgeo.cube_import_storm(ncol, nrow, nlay, sfile, nlines, nrcl, 0)
+
+    if ier != 0:
+        raise RuntimeError(f"Something went wrong in {__name__}, code is {ier}")
+
+    return {
+        "ncol": ncol,
+        "nrow": nrow,
+        "nlay": nlay,
+        "xori": xori,
+        "xinc": xinc,
+        "yori": yori,
+        "yinc": yinc,
+        "zori": zori,
+        "zinc": zinc,
+        "rotation": rotation,
+        "values": values.reshape((ncol, nrow, nlay)),
+        "yflip": yflip,
+    }
+
+
+def import_xtgregcube(mfile, values=True):
+    """Using pure python for experimental cube import, xtgregsurf format."""
+    logger.info("Importing cube on xtgregcube format...")
+
+    offset = 36
+    with open(mfile.file, "rb") as fhandle:
+        buf = fhandle.read(offset)
+
+    # unpack header
+    swap, magic, nfloat, ncol, nrow, nlay = unpack("= i i i q q q", buf)
+
+    if swap != 1 or magic != 1201:
+        raise ValueError("Invalid file format (wrong swap id or magic number).")
+
+    dtype = np.float32 if nfloat == 4 else np.float64
+
+    vals = None
+    narr = ncol * nrow * nlay
+
+    if values:
+        vals = xsys.npfromfile(mfile.file, dtype=dtype, count=narr, offset=offset)
+
+    # read metadata which will be at position offet + nfloat*narr +13
+    pos = offset + nfloat * narr + 13
+
+    with open(mfile.file, "rb") as fhandle:
+        fhandle.seek(pos)
+        jmeta = fhandle.read().decode()
+
+    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+    req = meta["_required_"]
+
+    reqattrs = xtgeo.MetaDataRegularCube.REQUIRED
+
+    results = {myattr: req[myattr] for myattr in reqattrs}
+
+    # For backwards compatability, xtgeo outputs files with the undef field set
+    # although we do not support initializing with any other value.
+    # As xtgeo-format is only written/read by xtgeo as far as we know, this should
+    # be unproblematic for now.
+    if results.pop("undef", None) != xtgeo.UNDEF:
+        raise ValueError(
+            f"File {mfile.file} has non-standard undef, not supported by xtgeo"
+        )
+
+    # TODO: dead traces and traceidcodes
+    if values:
+        results["values"] = vals.reshape(
+            results["ncol"], results["nrow"], results["nlay"]
+        )
+
+    return results
```

## xtgeo/cube/_cube_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,187 +1,187 @@
-# coding: utf-8
-"""Roxar API functions for XTGeo Cube
-
-Note on rotation:
-
-xtgeo uses rotation of "columns" whick is xline direction counterclockwise
-measured from X axis.
-
-roxarapi uses rotation of inline direction (rows) relative to Y axis.
-api < 1.4: counterclockwise "rotation"
-api >= 1.4 clockwise "orientation"
-
-Seems like self._rotation == roxar.orientation * -1 anyway @ reverse engineering/testing
-
-"""
-
-import numpy as np
-
-from xtgeo import RoxUtils
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_cube_roxapi(self, project, name, folder=None):  # pragma: no cover
-    """Import (transfer) a Cube via ROXAR API container to XTGeo.
-
-    .. versionadded:: 2.1
-    """
-    rox = RoxUtils(project, readonly=True)
-
-    proj = rox.project
-
-    _roxapi_import_cube(self, rox, proj, name, folder)
-
-
-def _roxapi_import_cube(self, rox, proj, name, folder):  # pragma: no cover
-    """Short summary.
-
-    Args:
-        proj (object): RMS magic project.
-        name (str): Name of cube.
-        folder (str): Cube folder in RMS.
-
-    """
-    # note that name must be in brackets
-    path = [name]
-    if folder is not None:
-        fld = folder.split("/")
-        path = fld + path
-
-    if path not in proj.seismic.data.keys():
-        raise ValueError(f"Path {path} is not within RMS Seismic Cube container")
-    try:
-        rcube = proj.seismic.data[path]
-        _roxapi_cube_to_xtgeo(self, rox, rcube)
-    except KeyError as emsg:
-        logger.error(emsg)
-        raise
-
-
-def _roxapi_cube_to_xtgeo(self, rox, rcube):  # pragma: no cover
-    """Tranforming cube from ROXAPI to XTGeo object."""
-    logger.info("Cube from roxapi to xtgeo...")
-
-    # roxrotation is cube rotation clockwise from azimuth but not consistent
-    roxrotation = rcube.orientation
-
-    roxhandedness = str(rcube.handedness)
-
-    self._xori, self._yori = rcube.origin
-    self._zori = rcube.first_z
-    self._zinc = rcube.sample_rate
-    self._ncol, self._nrow, self._nlay = rcube.dimensions
-    self._xinc, self._yinc = rcube.increment
-
-    self._rotation = roxrotation * -1
-
-    if self._rotation < 0:
-        self._rotation += 360
-    elif self._rotation > 360:
-        self._rotation -= 360
-
-    self._yflip = 1
-    if roxhandedness == "right":
-        self._yflip = -1
-
-    ilstart = rcube.get_inline(0)
-    xlstart = rcube.get_crossline(0)
-    ilincr, xlincr = rcube.inline_crossline_increment
-
-    self._ilines = np.array(
-        range(ilstart, self._ncol + ilstart, ilincr), dtype=np.int32
-    )
-    self._xlines = np.array(
-        range(xlstart, self._nrow + xlstart, xlincr), dtype=np.int32
-    )
-
-    # roxar API does not store traceid codes, assume 1
-    self._traceidcodes = np.ones((self._ncol, self._nrow), dtype=np.int32)
-
-    if rcube.is_empty:
-        xtg.warn("Cube has no data; assume 0")
-    else:
-        self.values = rcube.get_values()
-
-
-def export_cube_roxapi(
-    self, project, name, folder=None, domain="time", compression=("wavelet", 5)
-):  # pragma: no cover
-    """Export (store) a Seismic cube to RMS via ROXAR API spec."""
-    rox = RoxUtils(project, readonly=False)
-
-    logger.debug("TODO: compression %s", compression)
-
-    _roxapi_export_cube(
-        self,
-        rox.project,
-        rox,
-        name,
-        folder=folder,
-        domain=domain,
-        compression=compression,
-    )
-
-    if rox._roxexternal:
-        rox.project.save()
-
-    rox.safe_close()
-
-
-def _roxapi_export_cube(
-    self, proj, rox, name, folder=None, domain="time", compression=("wavelet", 5)
-):  # type: ignore # pragma: no cover
-    roxar = None
-    try:
-        import roxar  # type: ignore
-    except ImportError:
-        pass
-
-    logger.info(
-        "There are issues with compression %s, hence it is ignored", compression
-    )
-
-    path = []
-    if folder is not None:
-        fld = folder.split("/")
-        path = fld + path
-
-    rcube = proj.seismic.data.create_cube(name, path=path)
-
-    # populate
-    origin = (float(self.xori), float(self.yori))
-    first_z = self.zori
-    increment = (self.xinc, self.yinc)
-    sample_rate = self.zinc
-    rotation = self.rotation
-    vertical_domain = roxar.VerticalDomain.time
-    if domain == "depth":
-        vertical_domain = roxar.VerticalDomain.depth
-
-    values = self.values.copy()  # copy() needed?
-
-    handedness = roxar.Direction.left
-    if self.yflip == -1:
-        handedness = roxar.Direction.right
-
-    # inline xline vector
-    ilstart = self.ilines[0]
-    xlstart = self.xlines[0]
-    ilincr = self.ilines[1] - self.ilines[0]
-    xlincr = self.xlines[1] - self.xlines[0]
-
-    rcube.set_seismic(
-        values,
-        origin,
-        increment,
-        first_z,
-        sample_rate,
-        rotation * -1,
-        vertical_domain=vertical_domain,
-        handedness=handedness,
-        inline_crossline_start=(ilstart, xlstart),
-        inline_crossline_increment=(ilincr, xlincr),
-    )
+# coding: utf-8
+"""Roxar API functions for XTGeo Cube
+
+Note on rotation:
+
+xtgeo uses rotation of "columns" whick is xline direction counterclockwise
+measured from X axis.
+
+roxarapi uses rotation of inline direction (rows) relative to Y axis.
+api < 1.4: counterclockwise "rotation"
+api >= 1.4 clockwise "orientation"
+
+Seems like self._rotation == roxar.orientation * -1 anyway @ reverse engineering/testing
+
+"""
+
+import numpy as np
+
+from xtgeo import RoxUtils
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_cube_roxapi(self, project, name, folder=None):  # pragma: no cover
+    """Import (transfer) a Cube via ROXAR API container to XTGeo.
+
+    .. versionadded:: 2.1
+    """
+    rox = RoxUtils(project, readonly=True)
+
+    proj = rox.project
+
+    _roxapi_import_cube(self, rox, proj, name, folder)
+
+
+def _roxapi_import_cube(self, rox, proj, name, folder):  # pragma: no cover
+    """Short summary.
+
+    Args:
+        proj (object): RMS magic project.
+        name (str): Name of cube.
+        folder (str): Cube folder in RMS.
+
+    """
+    # note that name must be in brackets
+    path = [name]
+    if folder is not None:
+        fld = folder.split("/")
+        path = fld + path
+
+    if path not in proj.seismic.data.keys():
+        raise ValueError(f"Path {path} is not within RMS Seismic Cube container")
+    try:
+        rcube = proj.seismic.data[path]
+        _roxapi_cube_to_xtgeo(self, rox, rcube)
+    except KeyError as emsg:
+        logger.error(emsg)
+        raise
+
+
+def _roxapi_cube_to_xtgeo(self, rox, rcube):  # pragma: no cover
+    """Tranforming cube from ROXAPI to XTGeo object."""
+    logger.info("Cube from roxapi to xtgeo...")
+
+    # roxrotation is cube rotation clockwise from azimuth but not consistent
+    roxrotation = rcube.orientation
+
+    roxhandedness = str(rcube.handedness)
+
+    self._xori, self._yori = rcube.origin
+    self._zori = rcube.first_z
+    self._zinc = rcube.sample_rate
+    self._ncol, self._nrow, self._nlay = rcube.dimensions
+    self._xinc, self._yinc = rcube.increment
+
+    self._rotation = roxrotation * -1
+
+    if self._rotation < 0:
+        self._rotation += 360
+    elif self._rotation > 360:
+        self._rotation -= 360
+
+    self._yflip = 1
+    if roxhandedness == "right":
+        self._yflip = -1
+
+    ilstart = rcube.get_inline(0)
+    xlstart = rcube.get_crossline(0)
+    ilincr, xlincr = rcube.inline_crossline_increment
+
+    self._ilines = np.array(
+        range(ilstart, self._ncol + ilstart, ilincr), dtype=np.int32
+    )
+    self._xlines = np.array(
+        range(xlstart, self._nrow + xlstart, xlincr), dtype=np.int32
+    )
+
+    # roxar API does not store traceid codes, assume 1
+    self._traceidcodes = np.ones((self._ncol, self._nrow), dtype=np.int32)
+
+    if rcube.is_empty:
+        xtg.warn("Cube has no data; assume 0")
+    else:
+        self.values = rcube.get_values()
+
+
+def export_cube_roxapi(
+    self, project, name, folder=None, domain="time", compression=("wavelet", 5)
+):  # pragma: no cover
+    """Export (store) a Seismic cube to RMS via ROXAR API spec."""
+    rox = RoxUtils(project, readonly=False)
+
+    logger.debug("TODO: compression %s", compression)
+
+    _roxapi_export_cube(
+        self,
+        rox.project,
+        rox,
+        name,
+        folder=folder,
+        domain=domain,
+        compression=compression,
+    )
+
+    if rox._roxexternal:
+        rox.project.save()
+
+    rox.safe_close()
+
+
+def _roxapi_export_cube(
+    self, proj, rox, name, folder=None, domain="time", compression=("wavelet", 5)
+):  # type: ignore # pragma: no cover
+    roxar = None
+    try:
+        import roxar  # type: ignore
+    except ImportError:
+        pass
+
+    logger.info(
+        "There are issues with compression %s, hence it is ignored", compression
+    )
+
+    path = []
+    if folder is not None:
+        fld = folder.split("/")
+        path = fld + path
+
+    rcube = proj.seismic.data.create_cube(name, path=path)
+
+    # populate
+    origin = (float(self.xori), float(self.yori))
+    first_z = self.zori
+    increment = (self.xinc, self.yinc)
+    sample_rate = self.zinc
+    rotation = self.rotation
+    vertical_domain = roxar.VerticalDomain.time
+    if domain == "depth":
+        vertical_domain = roxar.VerticalDomain.depth
+
+    values = self.values.copy()  # copy() needed?
+
+    handedness = roxar.Direction.left
+    if self.yflip == -1:
+        handedness = roxar.Direction.right
+
+    # inline xline vector
+    ilstart = self.ilines[0]
+    xlstart = self.xlines[0]
+    ilincr = self.ilines[1] - self.ilines[0]
+    xlincr = self.xlines[1] - self.xlines[0]
+
+    rcube.set_seismic(
+        values,
+        origin,
+        increment,
+        first_z,
+        sample_rate,
+        rotation * -1,
+        vertical_domain=vertical_domain,
+        handedness=handedness,
+        inline_crossline_start=(ilstart, xlstart),
+        inline_crossline_increment=(ilincr, xlincr),
+    )
```

## xtgeo/cube/_cube_utils.py

 * *Ordering differences only*

```diff
@@ -1,289 +1,289 @@
-"""Cube utilities (basic low level)"""
-import warnings
-
-import numpy as np
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo import XTGeoCLibError
-from xtgeo.common import XTGeoDialog
-from xtgeo.common.calc import _swap_axes
-
-xtg = XTGeoDialog()
-
-
-logger = xtg.functionlogger(__name__)
-# pylint: disable=protected-access
-
-
-def swapaxes(self):
-    """Pure numpy/python version"""
-    self._rotation, self._yflip, swapped_values = _swap_axes(
-        self._rotation,
-        self._yflip,
-        values=self._values,
-        traceidcodes=self._traceidcodes,
-    )
-    self._ncol, self._nrow = self._nrow, self._ncol
-    self._xinc, self._yinc = self._yinc, self._xinc
-    self.values = swapped_values["values"]
-    self._traceidcodes = swapped_values["traceidcodes"]
-
-
-def thinning(self, icol, jrow, klay):
-    inputs = [icol, jrow, klay]
-    ranges = [self.nrow, self.ncol, self.nlay]
-
-    for inum, ixc in enumerate(inputs):
-        if not isinstance(ixc, int):
-            raise ValueError(f"Some input is not integer: {inputs}")
-        if ixc > ranges[inum] / 2:
-            raise ValueError(
-                f"Input numbers <{inputs}> are too large compared to existing "
-                f"ranges <{ranges}>"
-            )
-
-    # just simple numpy operations, and changing some cube props
-
-    val = self.values.copy()
-
-    val = val[::icol, ::jrow, ::klay]
-    self._ncol = val.shape[0]
-    self._nrow = val.shape[1]
-    self._nlay = val.shape[2]
-    self._xinc *= icol
-    self._yinc *= jrow
-    self._zinc *= klay
-    self._ilines = self._ilines[::icol]
-    self._xlines = self._xlines[::jrow]
-    self._traceidcodes = self._traceidcodes[::icol, ::jrow]
-
-    self.values = val
-
-
-def cropping(self, icols, jrows, klays):
-    """Cropping, where inputs are tuples"""
-
-    icol1, icol2 = icols
-    jrow1, jrow2 = jrows
-    klay1, klay2 = klays
-
-    val = self.values.copy()
-    ncol = self.ncol
-    nrow = self.nrow
-    nlay = self.nlay
-
-    val = val[
-        0 + icol1 : ncol - icol2, 0 + jrow1 : nrow - jrow2, 0 + klay1 : nlay - klay2
-    ]
-
-    self._ncol = val.shape[0]
-    self._nrow = val.shape[1]
-    self._nlay = val.shape[2]
-
-    self._ilines = self._ilines[0 + icol1 : ncol - icol2]
-    self._xlines = self._xlines[0 + jrow1 : nrow - jrow2]
-    self.traceidcodes = self.traceidcodes[
-        0 + icol1 : ncol - icol2, 0 + jrow1 : nrow - jrow2
-    ]
-
-    # 1 + .., since the following routine as 1 as base for i j
-    ier, xpp, ypp = _cxtgeo.cube_xy_from_ij(
-        1 + icol1,
-        1 + jrow1,
-        self.xori,
-        self.xinc,
-        self.yori,
-        self.yinc,
-        ncol,
-        nrow,
-        self.yflip,
-        self.rotation,
-        0,
-    )
-
-    if ier != 0:
-        raise RuntimeError(f"Unexpected error, code is {ier}")
-
-    # get new X Y origins
-    self._xori = xpp
-    self._yori = ypp
-    self._zori = self.zori + klay1 * self.zinc
-
-    self.values = val
-
-
-def resample(self, other, sampling="nearest", outside_value=None):
-    """Resample another cube to the current self"""
-    # TODO: traceidcodes
-
-    values1a = self.values.reshape(-1)
-    values2a = other.values.reshape(-1)
-
-    logger.info("Resampling, using %s...", sampling)
-
-    ier = _cxtgeo.cube_resample_cube(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self.xori,
-        self.xinc,
-        self.yori,
-        self.yinc,
-        self.zori,
-        self.zinc,
-        self.rotation,
-        self.yflip,
-        values1a,
-        other.ncol,
-        other.nrow,
-        other.nlay,
-        other.xori,
-        other.xinc,
-        other.yori,
-        other.yinc,
-        other.zori,
-        other.zinc,
-        other.rotation,
-        other.yflip,
-        values2a,
-        1 if sampling == "trilinear" else 0,
-        0 if outside_value is None else 1,
-        0 if outside_value is None else outside_value,
-    )
-    if ier == -4:
-        warnings.warn("Less than 10% of origonal cube sampled", RuntimeWarning)
-    elif ier != 0:
-        raise XTGeoCLibError("cube_resample_cube failed to complete")
-
-
-def get_xy_value_from_ij(self, iloc, jloc, ixline=False, zerobased=False):
-    """Find X Y value from I J index, or corresponding inline/xline"""
-    # assumes that inline follows I and xlines follows J
-
-    iuse = iloc
-    juse = jloc
-
-    if zerobased:
-        iuse = iuse + 1
-        juse = juse + 1
-
-    if ixline:
-        ilst = self.ilines.tolist()
-        jlst = self.xlines.tolist()
-        iuse = ilst.index(iloc) + 1
-        juse = jlst.index(jloc) + 1
-
-    if 1 <= iuse <= self.ncol and 1 <= juse <= self.nrow:
-        ier, xval, yval = _cxtgeo.cube_xy_from_ij(
-            iuse,
-            juse,
-            self.xori,
-            self.xinc,
-            self.yori,
-            self.yinc,
-            self.ncol,
-            self.nrow,
-            self._yflip,
-            self.rotation,
-            0,
-        )
-        if ier != 0:
-            raise XTGeoCLibError(f"cube_xy_from_ij failed with error code: {ier}")
-
-    else:
-        raise ValueError("Index i and/or j out of bounds")
-
-    return xval, yval
-
-
-def get_randomline(
-    self,
-    fencespec,
-    zmin=None,
-    zmax=None,
-    zincrement=None,
-    hincrement=None,
-    atleast=5,
-    nextend=2,
-    sampling="nearest",
-):
-    """Get a random line from a fence spesification"""
-
-    if isinstance(fencespec, xtgeo.Polygons):
-        logger.info("Estimate hincrement from Polygons instance...")
-        fencespec = _get_randomline_fence(self, fencespec, hincrement, atleast, nextend)
-        logger.info("Estimate hincrement from Polygons instance... DONE")
-
-    if not len(fencespec.shape) == 2:
-        raise ValueError("Fence is not a 2D numpy")
-
-    xcoords = fencespec[:, 0]
-    ycoords = fencespec[:, 1]
-    hcoords = fencespec[:, 3]
-
-    for ino in range(hcoords.shape[0] - 1):
-        dhv = hcoords[ino + 1] - hcoords[ino]
-        logger.info("Delta H along well path: %s", dhv)
-
-    zcubemax = self._zori + (self._nlay - 1) * self._zinc
-    if zmin is None or zmin < self._zori:
-        zmin = self._zori
-
-    if zmax is None or zmax > zcubemax:
-        zmax = zcubemax
-
-    if zincrement is None:
-        zincrement = self._zinc / 2.0
-
-    nzsam = int((zmax - zmin) / zincrement) + 1
-
-    nsamples = xcoords.shape[0] * nzsam
-
-    option = 0
-    if sampling == "trilinear":
-        option = 1
-
-    _ier, values = _cxtgeo.cube_get_randomline(
-        xcoords,
-        ycoords,
-        zmin,
-        zmax,
-        nzsam,
-        self._xori,
-        self._xinc,
-        self._yori,
-        self._yinc,
-        self._zori,
-        self._zinc,
-        self._rotation,
-        self._yflip,
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._values.reshape(-1),
-        nsamples,
-        option,
-    )
-
-    values[values > xtgeo.UNDEF_LIMIT] = np.nan
-    arr = values.reshape((xcoords.shape[0], nzsam)).T
-
-    return (hcoords[0], hcoords[-1], zmin, zmax, arr)
-
-
-def _get_randomline_fence(self, fencespec, hincrement, atleast, nextend):
-    """Compute a resampled fence from a Polygons instance"""
-
-    if hincrement is None:
-        avgdxdy = 0.5 * (self.xinc + self.yinc)
-        distance = 0.5 * avgdxdy
-    else:
-        distance = hincrement
-
-    logger.info("Getting fence from a Polygons instance...")
-    fspec = fencespec.get_fence(
-        distance=distance, atleast=atleast, nextend=nextend, asnumpy=True
-    )
-    logger.info("Getting fence from a Polygons instance... DONE")
-    return fspec
+"""Cube utilities (basic low level)"""
+import warnings
+
+import numpy as np
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo import XTGeoCLibError
+from xtgeo.common import XTGeoDialog
+from xtgeo.common.calc import _swap_axes
+
+xtg = XTGeoDialog()
+
+
+logger = xtg.functionlogger(__name__)
+# pylint: disable=protected-access
+
+
+def swapaxes(self):
+    """Pure numpy/python version"""
+    self._rotation, self._yflip, swapped_values = _swap_axes(
+        self._rotation,
+        self._yflip,
+        values=self._values,
+        traceidcodes=self._traceidcodes,
+    )
+    self._ncol, self._nrow = self._nrow, self._ncol
+    self._xinc, self._yinc = self._yinc, self._xinc
+    self.values = swapped_values["values"]
+    self._traceidcodes = swapped_values["traceidcodes"]
+
+
+def thinning(self, icol, jrow, klay):
+    inputs = [icol, jrow, klay]
+    ranges = [self.nrow, self.ncol, self.nlay]
+
+    for inum, ixc in enumerate(inputs):
+        if not isinstance(ixc, int):
+            raise ValueError(f"Some input is not integer: {inputs}")
+        if ixc > ranges[inum] / 2:
+            raise ValueError(
+                f"Input numbers <{inputs}> are too large compared to existing "
+                f"ranges <{ranges}>"
+            )
+
+    # just simple numpy operations, and changing some cube props
+
+    val = self.values.copy()
+
+    val = val[::icol, ::jrow, ::klay]
+    self._ncol = val.shape[0]
+    self._nrow = val.shape[1]
+    self._nlay = val.shape[2]
+    self._xinc *= icol
+    self._yinc *= jrow
+    self._zinc *= klay
+    self._ilines = self._ilines[::icol]
+    self._xlines = self._xlines[::jrow]
+    self._traceidcodes = self._traceidcodes[::icol, ::jrow]
+
+    self.values = val
+
+
+def cropping(self, icols, jrows, klays):
+    """Cropping, where inputs are tuples"""
+
+    icol1, icol2 = icols
+    jrow1, jrow2 = jrows
+    klay1, klay2 = klays
+
+    val = self.values.copy()
+    ncol = self.ncol
+    nrow = self.nrow
+    nlay = self.nlay
+
+    val = val[
+        0 + icol1 : ncol - icol2, 0 + jrow1 : nrow - jrow2, 0 + klay1 : nlay - klay2
+    ]
+
+    self._ncol = val.shape[0]
+    self._nrow = val.shape[1]
+    self._nlay = val.shape[2]
+
+    self._ilines = self._ilines[0 + icol1 : ncol - icol2]
+    self._xlines = self._xlines[0 + jrow1 : nrow - jrow2]
+    self.traceidcodes = self.traceidcodes[
+        0 + icol1 : ncol - icol2, 0 + jrow1 : nrow - jrow2
+    ]
+
+    # 1 + .., since the following routine as 1 as base for i j
+    ier, xpp, ypp = _cxtgeo.cube_xy_from_ij(
+        1 + icol1,
+        1 + jrow1,
+        self.xori,
+        self.xinc,
+        self.yori,
+        self.yinc,
+        ncol,
+        nrow,
+        self.yflip,
+        self.rotation,
+        0,
+    )
+
+    if ier != 0:
+        raise RuntimeError(f"Unexpected error, code is {ier}")
+
+    # get new X Y origins
+    self._xori = xpp
+    self._yori = ypp
+    self._zori = self.zori + klay1 * self.zinc
+
+    self.values = val
+
+
+def resample(self, other, sampling="nearest", outside_value=None):
+    """Resample another cube to the current self"""
+    # TODO: traceidcodes
+
+    values1a = self.values.reshape(-1)
+    values2a = other.values.reshape(-1)
+
+    logger.info("Resampling, using %s...", sampling)
+
+    ier = _cxtgeo.cube_resample_cube(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self.xori,
+        self.xinc,
+        self.yori,
+        self.yinc,
+        self.zori,
+        self.zinc,
+        self.rotation,
+        self.yflip,
+        values1a,
+        other.ncol,
+        other.nrow,
+        other.nlay,
+        other.xori,
+        other.xinc,
+        other.yori,
+        other.yinc,
+        other.zori,
+        other.zinc,
+        other.rotation,
+        other.yflip,
+        values2a,
+        1 if sampling == "trilinear" else 0,
+        0 if outside_value is None else 1,
+        0 if outside_value is None else outside_value,
+    )
+    if ier == -4:
+        warnings.warn("Less than 10% of origonal cube sampled", RuntimeWarning)
+    elif ier != 0:
+        raise XTGeoCLibError("cube_resample_cube failed to complete")
+
+
+def get_xy_value_from_ij(self, iloc, jloc, ixline=False, zerobased=False):
+    """Find X Y value from I J index, or corresponding inline/xline"""
+    # assumes that inline follows I and xlines follows J
+
+    iuse = iloc
+    juse = jloc
+
+    if zerobased:
+        iuse = iuse + 1
+        juse = juse + 1
+
+    if ixline:
+        ilst = self.ilines.tolist()
+        jlst = self.xlines.tolist()
+        iuse = ilst.index(iloc) + 1
+        juse = jlst.index(jloc) + 1
+
+    if 1 <= iuse <= self.ncol and 1 <= juse <= self.nrow:
+        ier, xval, yval = _cxtgeo.cube_xy_from_ij(
+            iuse,
+            juse,
+            self.xori,
+            self.xinc,
+            self.yori,
+            self.yinc,
+            self.ncol,
+            self.nrow,
+            self._yflip,
+            self.rotation,
+            0,
+        )
+        if ier != 0:
+            raise XTGeoCLibError(f"cube_xy_from_ij failed with error code: {ier}")
+
+    else:
+        raise ValueError("Index i and/or j out of bounds")
+
+    return xval, yval
+
+
+def get_randomline(
+    self,
+    fencespec,
+    zmin=None,
+    zmax=None,
+    zincrement=None,
+    hincrement=None,
+    atleast=5,
+    nextend=2,
+    sampling="nearest",
+):
+    """Get a random line from a fence spesification"""
+
+    if isinstance(fencespec, xtgeo.Polygons):
+        logger.info("Estimate hincrement from Polygons instance...")
+        fencespec = _get_randomline_fence(self, fencespec, hincrement, atleast, nextend)
+        logger.info("Estimate hincrement from Polygons instance... DONE")
+
+    if not len(fencespec.shape) == 2:
+        raise ValueError("Fence is not a 2D numpy")
+
+    xcoords = fencespec[:, 0]
+    ycoords = fencespec[:, 1]
+    hcoords = fencespec[:, 3]
+
+    for ino in range(hcoords.shape[0] - 1):
+        dhv = hcoords[ino + 1] - hcoords[ino]
+        logger.info("Delta H along well path: %s", dhv)
+
+    zcubemax = self._zori + (self._nlay - 1) * self._zinc
+    if zmin is None or zmin < self._zori:
+        zmin = self._zori
+
+    if zmax is None or zmax > zcubemax:
+        zmax = zcubemax
+
+    if zincrement is None:
+        zincrement = self._zinc / 2.0
+
+    nzsam = int((zmax - zmin) / zincrement) + 1
+
+    nsamples = xcoords.shape[0] * nzsam
+
+    option = 0
+    if sampling == "trilinear":
+        option = 1
+
+    _ier, values = _cxtgeo.cube_get_randomline(
+        xcoords,
+        ycoords,
+        zmin,
+        zmax,
+        nzsam,
+        self._xori,
+        self._xinc,
+        self._yori,
+        self._yinc,
+        self._zori,
+        self._zinc,
+        self._rotation,
+        self._yflip,
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._values.reshape(-1),
+        nsamples,
+        option,
+    )
+
+    values[values > xtgeo.UNDEF_LIMIT] = np.nan
+    arr = values.reshape((xcoords.shape[0], nzsam)).T
+
+    return (hcoords[0], hcoords[-1], zmin, zmax, arr)
+
+
+def _get_randomline_fence(self, fencespec, hincrement, atleast, nextend):
+    """Compute a resampled fence from a Polygons instance"""
+
+    if hincrement is None:
+        avgdxdy = 0.5 * (self.xinc + self.yinc)
+        distance = 0.5 * avgdxdy
+    else:
+        distance = hincrement
+
+    logger.info("Getting fence from a Polygons instance...")
+    fspec = fencespec.get_fence(
+        distance=distance, atleast=atleast, nextend=nextend, asnumpy=True
+    )
+    logger.info("Getting fence from a Polygons instance... DONE")
+    return fspec
```

## xtgeo/cube/cube1.py

 * *Ordering differences only*

```diff
@@ -1,1068 +1,1068 @@
-# coding: utf-8
-"""Module for a seismic (or whatever) cube."""
-import functools
-import numbers
-import os.path
-import pathlib
-import tempfile
-import warnings
-
-import deprecation
-import numpy as np
-
-import xtgeo
-import xtgeo.common.sys as xtgeosys
-from xtgeo.common import XTGDescription, XTGeoDialog
-from xtgeo.cube import _cube_export, _cube_import, _cube_roxapi, _cube_utils
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def _data_reader_factory(fformat):
-    if fformat == "segy":
-        return _cube_import.import_segy
-    elif fformat == "storm":
-        return _cube_import.import_stormcube
-    elif fformat == "xtg":
-        return _cube_import.import_xtgregcube
-    else:
-        raise ValueError(f"File format fformat={fformat} is not supported")
-
-
-def cube_from_file(mfile, fformat="guess"):
-    """This makes an instance of a Cube directly from file import.
-
-    Args:
-        mfile (str): Name of file
-        fformat (str): See :meth:`Cube.from_file`
-
-    Example::
-
-        >>> import xtgeo
-        >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-    """
-    return Cube._read_file(mfile, fformat)
-
-
-def cube_from_roxar(project, name, folder=None):
-    """This makes an instance of a Cube directly from roxar input.
-
-    The folder is a string on form "a" or "a/b" if subfolders are present
-
-    Example::
-
-        import xtgeo
-        mycube = xtgeo.cube_from_roxar(project, "DepthCube")
-
-    """
-    obj = Cube(ncol=9, nrow=9, nlay=9, xinc=9.99, yinc=9.99, zinc=9.99)
-
-    obj.from_roxar(project, name, folder=folder)
-
-    return obj
-
-
-def allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of Cube and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        # Checking if we are doing an initialization
-        # from file and raise a deprecation warning if
-        # we are.
-        if args and isinstance(args[0], (str, pathlib.Path)):
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "mcube = xtgeo.cube_from_file('some_name.gri') instead",
-                DeprecationWarning,
-            )
-            cfile = args[0]
-            if len(args) > 1:
-                fformat = args[1]
-            else:
-                fformat = kwargs.get("fformat", None)
-            mfile = xtgeosys._XTGeoFile(cfile)
-            if fformat is None or fformat == "guess":
-                fformat = mfile.detect_fformat(suffixonly=True)
-            else:
-                fformat = mfile.generic_format_by_proposal(fformat)  # default
-            kwargs = _data_reader_factory(fformat)(mfile)
-            kwargs["filesrc"] = mfile.file
-            return func(cls, **kwargs)
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-def allow_deprecated_default_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of Cube and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        _deprecation_msg = (
-            "{} is a required argument and will no "
-            "longer be defaulted in xtgeo version 4.0"
-        )
-        if "ncol" not in kwargs and len(args) < 1:
-            warnings.warn(_deprecation_msg.format("ncol"), DeprecationWarning)
-            kwargs["ncol"] = 5
-        if "nrow" not in kwargs and len(args) < 2:
-            warnings.warn(_deprecation_msg.format("nrow"), DeprecationWarning)
-            kwargs["nrow"] = 3
-        if "nlay" not in kwargs and len(args) < 3:
-            warnings.warn(_deprecation_msg.format("nlay"), DeprecationWarning)
-            kwargs["nlay"] = 2
-        if "xinc" not in kwargs and len(args) < 4:
-            warnings.warn(_deprecation_msg.format("xinc"), DeprecationWarning)
-            kwargs["xinc"] = 25.0
-        if "yinc" not in kwargs and len(args) < 5:
-            warnings.warn(_deprecation_msg.format("yinc"), DeprecationWarning)
-            kwargs["yinc"] = 25.0
-        if "zinc" not in kwargs and len(args) < 6:
-            warnings.warn(_deprecation_msg.format("zinc"), DeprecationWarning)
-            kwargs["zinc"] = 2.0
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-class Cube:  # pylint: disable=too-many-public-methods
-    """Class for a (seismic) cube in the XTGeo framework.
-
-    The values are stored as a 3D numpy array (4 bytes; float32 is default),
-    with internal C ordering (nlay fastest).
-
-    See :func:`xtgeo.cube_from_file` for importing cubes from e.g. segy files.
-
-    See also Cube section in documentation: docs/datamodel.rst
-
-    Examples::
-
-        >>> import xtgeo
-        >>> # a user defined cube:
-        >>> mycube = xtgeo.Cube(
-        ...     xori=100.0,
-        ...     yori=200.0,
-        ...     zori=150.0,
-        ...     ncol=40,
-        ...     nrow=30,
-        ...     nlay=10,
-        ...     rotation=30,
-        ...     values=0
-        ... )
-
-    Args:
-      xori: Origin in Easting coordinate
-      yori: Origin in Northing coordinate
-      zori: Origin in Depth coordinate, where depth is positive down
-      ncol: Number of columns
-      nrow: Number of columns
-      nlay: Number of layers, starting from top
-      rotation: Cube rotation, X axis is applied and "school-wise" rotation,
-                     anti-clock in degrees
-      values: Numpy array with shape (ncol, nrow, nlay), C order, np.float32
-      ilines: 1D numpy array with ncol elements, aka INLINES array, defaults to arange
-      xlines: 1D numpy array with nrow elements, aka XLINES array, defaults to arange
-      segyfile: Name of source segyfile if any
-      filesrc: String: Source file if any
-      yflip: Normally 1; if -1 Y axis is flipped --> from left-handed (1) to
-                     right handed (-1). Right handed cubes are common.
-
-    """
-
-    @allow_deprecated_init
-    @allow_deprecated_default_init
-    def __init__(
-        self,
-        ncol,
-        nrow,
-        nlay,
-        xinc,
-        yinc,
-        zinc,
-        xori=0.0,
-        yori=0.0,
-        zori=0.0,
-        yflip=1,
-        values=0.0,
-        rotation=0.0,
-        zflip=1,
-        ilines=None,
-        xlines=None,
-        traceidcodes=None,
-        segyfile=None,
-        filesrc=None,
-    ):
-        """Initiate a Cube instance."""
-
-        self._reset(
-            xori=xori,
-            yori=yori,
-            zori=zori,
-            ncol=ncol,
-            nrow=nrow,
-            nlay=nlay,
-            xinc=xinc,
-            yinc=yinc,
-            zinc=zinc,
-            yflip=yflip,
-            values=values,
-            rotation=rotation,
-            zflip=zflip,
-            ilines=ilines,
-            xlines=xlines,
-            traceidcodes=traceidcodes,
-            segyfile=segyfile,
-            filesrc=filesrc,
-        )
-
-    def _reset(
-        self,
-        ncol=5,
-        nrow=3,
-        nlay=2,
-        xinc=25.0,
-        yinc=25.0,
-        zinc=2.0,
-        xori=0.0,
-        yori=0.0,
-        zori=0.0,
-        yflip=1,
-        values=0.0,
-        rotation=0.0,
-        zflip=1,
-        ilines=None,
-        xlines=None,
-        traceidcodes=None,
-        segyfile=None,
-        filesrc=None,
-    ):
-        self._filesrc = filesrc
-        self._xori = xori
-        self._yori = yori
-        self._zori = zori
-        self._ncol = ncol
-        self._nrow = nrow
-        self._nlay = nlay
-        self._xinc = xinc
-        self._yinc = yinc
-        self._zinc = zinc
-        self._yflip = yflip
-        self._zflip = zflip  # currently not in use
-        self._rotation = rotation
-
-        self._values = None
-        self.values = values  # "values" is intentional over "_values"; cf. values()
-
-        if ilines is None:
-            self._ilines = ilines or np.array(range(1, self._ncol + 1), dtype=np.int32)
-        else:
-            self._ilines = ilines
-        if xlines is None:
-            self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
-        else:
-            self._xlines = xlines
-        if traceidcodes is None:
-            self._traceidcodes = np.ones((self._ncol, self._nrow), dtype=np.int32)
-        else:
-            self._traceidcodes = traceidcodes
-        self._segyfile = segyfile
-        self.undef = xtgeo.UNDEF
-
-        self._metadata = xtgeo.MetaDataRegularCube()
-        self._metadata.required = self
-
-    def __repr__(self):
-        """The __repr__ method."""
-        avg = self.values.mean()
-        dsc = (
-            f"{self.__class__} (ncol={self.ncol!r}, nrow={self.nrow!r}, "
-            f"nlay={self.nlay!r}, original file: {self._filesrc}), "
-            f"average {avg}, ID=<{id(self)}>"
-        )
-        return dsc
-
-    def __str__(self):
-        """The __str__ method for pretty print."""
-        return self.describe(flush=False)
-
-    @property
-    def metadata(self):
-        """Return metadata object instance of type MetaDataRegularSurface."""
-        return self._metadata
-
-    @metadata.setter
-    def metadata(self, obj):
-        # The current metadata object can be replaced. This is a bit dangerous so
-        # further check must be done to validate. TODO.
-        if not isinstance(obj, xtgeo.MetaDataRegularCube):
-            raise ValueError("Input obj not an instance of MetaDataRegularCube")
-
-        self._metadata = obj  # checking is currently missing! TODO
-
-    @property
-    def ncol(self):
-        """The NCOL (NX or I dir) number (read-only)."""
-        return self._ncol
-
-    @property
-    def nrow(self):
-        """The NROW (NY or J dir) number (read-only)."""
-        return self._nrow
-
-    @property
-    def nlay(self):
-        """The NLAY (or NZ or K dir) number (read-only)."""
-        return self._nlay
-
-    @property
-    def dimensions(self):
-        """3-tuple: The cube dimensions as a tuple of 3 integers (read only)."""
-        return (self._ncol, self._nrow, self._nlay)
-
-    @property
-    def xori(self):
-        """The XORI (origin corner) coordinate."""
-        return self._xori
-
-    @xori.setter
-    def xori(self, val):
-        logger.warning("Changing xori is risky!")
-        self._xori = val
-
-    @property
-    def yori(self):
-        """The YORI (origin corner) coordinate."""
-        return self._yori
-
-    @yori.setter
-    def yori(self, val):
-        logger.warning("Changing yori is risky!")
-        self._yori = val
-
-    @property
-    def zori(self):
-        """The ZORI (origin corner) coordinate."""
-        return self._zori
-
-    @zori.setter
-    def zori(self, val):
-        logger.warning("Changing zori is risky!")
-        self._zori = val
-
-    @property
-    def xinc(self):
-        """The XINC (increment X) as property."""
-        return self._xinc
-
-    @xinc.setter
-    def xinc(self, val):
-        logger.warning("Changing xinc is risky!")
-        self._xinc = val
-
-    @property
-    def yinc(self):
-        """The YINC (increment Y)."""
-        return self._yinc
-
-    @yinc.setter
-    def yinc(self, val):
-        logger.warning("Changing yinc is risky!")
-        self._yinc = val
-
-    @property
-    def zinc(self):
-        """The ZINC (increment Z)."""
-        return self._zinc
-
-    @zinc.setter
-    def zinc(self, val):
-        logger.warning("Changing zinc is risky!")
-        self._zinc = val
-
-    @property
-    def rotation(self):
-        """The rotation, anticlock from X axis in degrees."""
-        return self._rotation
-
-    @rotation.setter
-    def rotation(self, val):
-        logger.warning("Changing rotation is risky!")
-        self._rotation = val
-
-    @property
-    def ilines(self):
-        """The inlines numbering vector."""
-        return self._ilines
-
-    @ilines.setter
-    def ilines(self, values):
-        self._ilines = values
-
-    @property
-    def xlines(self):
-        """The xlines numbering vector."""
-        return self._xlines
-
-    @xlines.setter
-    def xlines(self, values):
-        self._xlines = values
-
-    @property
-    def zslices(self):
-        """Return the time/depth slices as an int array (read only)."""
-        # This is a derived property
-        zslices = range(
-            int(self.zori), int(self.zori + self.nlay * self.zinc), int(self.zinc)
-        )
-        zslices = np.array(zslices)
-        return zslices
-
-    @property
-    def traceidcodes(self):
-        """The trace identifaction codes array (ncol, nrow)."""
-        return self._traceidcodes
-
-    @traceidcodes.setter
-    def traceidcodes(self, values):
-        if isinstance(values, (int, str)):
-            self._traceidcodes = np.full((self.ncol, self.nrow), values, dtype=np.int32)
-        else:
-            if isinstance(values, list):
-                values = np.array(values, np.int32)
-            self._traceidcodes = values.reshape(self.ncol, self.nrow)
-
-    @property
-    def yflip(self):
-        """The YFLIP indicator, 1 is normal, -1 means Y flipped.
-
-        YFLIP = 1 means a LEFT HANDED coordinate system with Z axis
-        positive down, while inline (col) follow East (X) and xline (rows)
-        follows North (Y), when rotation is zero.
-        """
-        return self._yflip
-
-    @property
-    def zflip(self):
-        """The ZFLIP indicator, 1 is normal, -1 means Z flipped.
-
-        ZFLIP = 1 and YFLIP = 1 means a LEFT HANDED coordinate system with Z axis
-        positive down, while inline (col) follow East (X) and xline (rows)
-        follows North (Y), when rotation is zero.
-        """
-        return self._zflip
-
-    @property
-    def segyfile(self):
-        """The input segy file name (str), if any (or None) (read-only)."""
-        return self._segyfile
-
-    @property
-    def filesrc(self):
-        """The input file name (str), if any (or None) (read-only)."""
-        return self._filesrc
-
-    @filesrc.setter
-    def filesrc(self, name):
-        self._filesrc = name
-
-    @property
-    def values(self):
-        """The values, as a 3D numpy (ncol, nrow, nlay), 4 byte float."""
-        return self._values
-
-    @values.setter
-    def values(self, values):
-        self._ensure_correct_values(values)
-
-    # =========================================================================
-    # Describe
-    # =========================================================================
-
-    def generate_hash(self, hashmethod="md5"):
-        """Return a unique hash ID for current instance.
-
-        See :meth:`~xtgeo.common.sys.generic_hash()` for documentation.
-
-        .. versionadded:: 2.14
-        """
-        required = (
-            "ncol",
-            "nrow",
-            "nlay",
-            "xori",
-            "yori",
-            "zori",
-            "xinc",
-            "yinc",
-            "zinc",
-            "yflip",
-            "zflip",
-            "rotation",
-            "values",
-            "ilines",
-            "xlines",
-            "traceidcodes",
-        )
-
-        gid = ""
-        for req in required:
-            gid += f"{getattr(self, '_' + req)}"
-
-        return xtgeosys.generic_hash(gid, hashmethod=hashmethod)
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout or return.
-
-        Args:
-            flush (bool): If True, description is printed to stdout.
-        """
-        dsc = XTGDescription()
-        dsc.title("Description of Cube instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("File source", self._filesrc)
-        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
-        dsc.txt("Origins XORI, YORI, ZORI", self.xori, self.yori, self.zori)
-        dsc.txt("Increments XINC YINC ZINC", self.xinc, self.yinc, self.zinc)
-        dsc.txt("Rotation (anti-clock from X)", self.rotation)
-        dsc.txt("YFLIP flag", self.yflip)
-        np.set_printoptions(threshold=16)
-        dsc.txt("Inlines vector", self._ilines)
-        dsc.txt("Xlines vector", self._xlines)
-        dsc.txt("Time or depth slices vector", self.zslices)
-        dsc.txt("Values", self._values.reshape(-1), self._values.dtype)
-        np.set_printoptions(threshold=1000)
-        dsc.txt(
-            "Values, mean, stdev, minimum, maximum",
-            self.values.mean(),
-            self.values.std(),
-            self.values.min(),
-            self.values.max(),
-        )
-        dsc.txt("Trace ID codes", self._traceidcodes.reshape(-1))
-        msize = float(self.values.size * 4) / (1024 * 1024 * 1024)
-        dsc.txt("Minimum memory usage of array (GB)", msize)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    # ==================================================================================
-    # Copy, swapping, cropping, thinning...
-    # ==================================================================================
-
-    def copy(self):
-        """Deep copy of a Cube() object to another instance.
-
-
-        >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-        >>> mycube2 = mycube.copy()
-
-        """
-        xcube = Cube(
-            ncol=self.ncol,
-            nrow=self.nrow,
-            nlay=self.nlay,
-            xinc=self.xinc,
-            yinc=self.yinc,
-            zinc=self.zinc,
-            xori=self.xori,
-            yori=self.yori,
-            zori=self.zori,
-            yflip=self.yflip,
-            segyfile=self.segyfile,
-            rotation=self.rotation,
-            values=self.values.copy(),
-        )
-
-        xcube.filesrc = self._filesrc
-
-        xcube.ilines = self._ilines.copy()
-        xcube.xlines = self._xlines.copy()
-        xcube.traceidcodes = self._traceidcodes.copy()
-        xcube.metadata.required = xcube
-
-        return xcube
-
-    def swapaxes(self):
-        """Swap the axes inline vs xline, keep origin."""
-        _cube_utils.swapaxes(self)
-
-    def resample(self, incube, sampling="nearest", outside_value=None):
-        """Resample a Cube object into this instance.
-
-        Args:
-            incube (Cube): A XTGeo Cube instance
-            sampling (str): Sampling algorithm: 'nearest' for nearest node
-                of 'trilinear' for trilinear interpoltion (more correct but
-                slower)
-            outside_value (None or float). If None, keep original, otherwise
-                use this value
-
-        Raises:
-            ValueError: If cubes do not overlap
-
-        Example:
-
-            >>> import xtgeo
-            >>> mycube1 = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-            >>> mycube2 = xtgeo.Cube(
-            ...     xori=777574,
-            ...     yori=6736507,
-            ...     zori=1000,
-            ...     xinc=10,
-            ...     yinc=10,
-            ...     zinc=4,
-            ...     ncol=100,
-            ...     nrow=100,
-            ...     nlay=100,
-            ...     yflip=mycube1.yflip,
-            ...     rotation=mycube1.rotation
-            ... )
-            >>> mycube2.resample(mycube1)
-
-        """
-        _cube_utils.resample(
-            self, incube, sampling=sampling, outside_value=outside_value
-        )
-
-    def do_thinning(self, icol, jrow, klay):
-        """Thinning the cube by removing every N column, row and/or layer.
-
-        Args:
-            icol (int): Thinning factor for columns (usually inlines)
-            jrow (int): Thinning factor for rows (usually xlines)
-            klay (int): Thinning factor for layers
-
-        Raises:
-            ValueError: If icol, jrow or klay are out of reasonable range
-
-        Example:
-
-            >>> mycube1 = Cube(cube_dir + "/ib_test_cube2.segy")
-            >>> mycube1.do_thinning(2, 2, 1)  # keep every second column, row
-            >>> mycube1.to_file(outdir + '/mysegy_smaller.segy')
-
-        """
-        _cube_utils.thinning(self, icol, jrow, klay)
-
-    def do_cropping(self, icols, jrows, klays, mode="edges"):
-        """Cropping the cube by removing rows, columns, layers.
-
-        Note that input boundary checking is currently lacking, and this
-        is a currently a user responsibility!
-
-        The 'mode' is used to determine to different 'approaches' on
-        cropping. Examples for icols and mode 'edges':
-        Here the tuple (N, M) will cut N first rows and M last rows.
-
-        However, if mode is 'inclusive' then, it defines the range
-        of rows to be included, and the numbering now shall be the
-        INLINE, XLINE and DEPTH/TIME mode.
-
-        Args:
-            icols (int tuple): Cropping front, end of rows, or inclusive range
-            jrows (int tuple): Cropping front, end of columns, or
-                inclusive range
-            klays (int tuple ): Cropping top, base layers, or inclusive range.
-            mode (str): 'Default is 'edges'; alternative is 'inclusive'
-
-        Example:
-            Crop 10 columns from front, 2 from back, then 20 rows in front,
-            40 in back, then no cropping of layers::
-
-            >>> import xtgeo
-            >>> mycube1 = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-            >>> mycube2 = mycube1.copy()
-            >>> mycube1.do_cropping((10, 2), (20, 40), (0, 0))
-            >>> mycube1.to_file(outdir + '/mysegy_smaller.segy')
-
-            In stead, do cropping as 'inclusive' where inlines, xlines, slices
-            arrays are known::
-
-            >>> mycube2.do_cropping((11, 32), (112, 114), (150, 200))
-
-        """
-        useicols = icols
-        usejrows = jrows
-        useklays = klays
-
-        if mode == "inclusive":
-            # transfer to 'numbers to row/col/lay to remove' in front end ...
-            useicols = (
-                icols[0] - self._ilines[0],
-                self._ilines[self._ncol - 1] - icols[1],
-            )
-            usejrows = (
-                jrows[0] - self._xlines[0],
-                self._xlines[self._nrow - 1] - jrows[1],
-            )
-            ntop = int((klays[0] - self.zori) / self.zinc)
-            nbot = int((self.zori + self.nlay * self.zinc - klays[1] - 1) / (self.zinc))
-            useklays = (ntop, nbot)
-
-        logger.info(
-            "Cropping at all cube sides: %s %s %s", useicols, usejrows, useklays
-        )
-        _cube_utils.cropping(self, useicols, usejrows, useklays)
-
-    def values_dead_traces(self, newvalue):
-        """Set values for traces flagged as dead.
-
-        Dead traces have traceidcodes 2 and corresponding values in the cube
-        will here receive a constant value to mimic "undefined".
-
-        Args:
-            newvalue (float): Set cube values to newvalues where traceid is 2.
-
-        Return:
-            oldvalue (float): The estimated simple 'average' of old value will
-                be returned as (max + min)/2. If no dead traces, return None.
-        """
-        logger.info("Set values for dead traces, if any")
-
-        if 2 in self._traceidcodes:
-            minval = self._values[self._traceidcodes == 2].min()
-            maxval = self._values[self._traceidcodes == 2].max()
-            # a bit weird calculation of mean but kept for backward compatibility
-            self._values[self._traceidcodes == 2] = newvalue
-            return 0.5 * (minval + maxval)
-
-        return None
-
-    def get_xy_value_from_ij(self, iloc, jloc, ixline=False, zerobased=False):
-        """Returns x, y coordinate from a single i j location.
-
-        Args:
-            iloc (int): I (col) location (base is 1)
-            jloc (int): J (row) location (base is 1)
-            ixline (bool): If True, then input locations are inline and xline position
-            zerobased (bool): If True, first index is 0, else it is 1. This does not
-                apply when ixline is set to True.
-
-        Returns:
-            The X, Y coordinate pair.
-        """
-        xval, yval = _cube_utils.get_xy_value_from_ij(
-            self, iloc, jloc, ixline=ixline, zerobased=zerobased
-        )
-        return xval, yval
-
-    # =========================================================================
-    # Cube extractions, e.g. XSection
-    # =========================================================================
-
-    def get_randomline(
-        self,
-        fencespec,
-        zmin=None,
-        zmax=None,
-        zincrement=None,
-        hincrement=None,
-        atleast=5,
-        nextend=2,
-        sampling="nearest",
-    ):
-        """Get a randomline from a fence spesification.
-
-        This randomline will be a 2D numpy with depth/time on the vertical
-        axis, and length along as horizontal axis. Undefined values will have
-        the np.nan value.
-
-        The input fencespec is either a 2D numpy where each row is X, Y, Z, HLEN,
-        where X, Y are UTM coordinates, Z is depth/time, and HLEN is a
-        length along the fence, or a Polygons instance.
-
-        If input fencspec is a numpy 2D, it is important that the HLEN array
-        has a constant increment and ideally a sampling that is less than the
-        Cube resolution. If a Polygons() instance, this is automated!
-
-        Args:
-            fencespec (:obj:`~numpy.ndarray` or :class:`~xtgeo.xyz.polygons.Polygons`):
-                2D numpy with X, Y, Z, HLEN as rows or a xtgeo Polygons() object.
-            zmin (float): Minimum Z (default is Cube Z minima/origin)
-            zmax (float): Maximum Z (default is Cube Z maximum)
-            zincrement (float): Sampling vertically, default is Cube ZINC/2
-            hincrement (float or bool): Resampling horizontally. This applies only
-                if the fencespec is a Polygons() instance. If None (default),
-                the distance will be deduced automatically.
-            atleast (int): Minimum number of horizontal samples (only if
-                fencespec is a Polygons instance)
-            nextend (int): Extend with nextend * hincrement in both ends (only if
-                fencespec is a Polygons instance)
-            sampling (str): Algorithm, 'nearest' or 'trilinear' (first is
-                faster, second is more precise for continuous fields)
-
-        Returns:
-            A tuple: (hmin, hmax, vmin, vmax, ndarray2d)
-
-        Raises:
-            ValueError: Input fence is not according to spec.
-
-        .. versionchanged:: 2.1 support for Polygons() as fencespec, and keywords
-           hincrement, atleast and sampling
-
-        .. seealso::
-           Class :class:`~xtgeo.xyz.polygons.Polygons`
-              The method :meth:`~xtgeo.xyz.polygons.Polygons.get_fence()` which can be
-              used to pregenerate `fencespec`
-
-        """
-        if not isinstance(fencespec, (np.ndarray, xtgeo.Polygons)):
-            raise ValueError(
-                "fencespec must be a numpy or a Polygons() object. "
-                f"Current type is {type(fencespec)}"
-            )
-        logger.info("Getting randomline...")
-        res = _cube_utils.get_randomline(
-            self,
-            fencespec,
-            zmin=zmin,
-            zmax=zmax,
-            zincrement=zincrement,
-            hincrement=hincrement,
-            atleast=atleast,
-            nextend=nextend,
-            sampling=sampling,
-        )
-        logger.info("Getting randomline... DONE")
-        return res
-
-    # =========================================================================
-    # Import and export
-    # =========================================================================
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.cube_from_file() instead",
-    )
-    def from_file(self, sfile, fformat="guess", engine=None):
-        """Deprecated, see :func:`cube_from_file`."""
-        mfile = xtgeosys._XTGeoFile(sfile)
-        if fformat is None or fformat == "guess":
-            fformat = mfile.detect_fformat(suffixonly=True)
-        else:
-            fformat = mfile.generic_format_by_proposal(fformat)  # default
-
-        if engine is not None:
-            warnings.warn(
-                "The engine parameter is deprecated, and has no effect.",
-                DeprecationWarning,
-            )
-
-        kwargs = _data_reader_factory(fformat)(mfile)
-        kwargs["filesrc"] = mfile.file
-        self._reset(**kwargs)
-
-    @classmethod
-    def _read_file(cls, sfile, fformat="guess"):
-        """Import cube data from file.
-
-        If fformat is not provided, the file type will be guessed based
-        on file extension (e.g. segy og sgy for SEGY format)
-
-        Args:
-            sfile (str): Filename (as string or pathlib.Path instance).
-            fformat (str): file format guess/segy/rms_regular/xtgregcube
-                where 'guess' is default. Regard 'xtgrecube' format as experimental.
-            deadtraces (float): Set 'dead' trace values to this value (SEGY
-                only). Default is UNDEF value (a very large number).
-
-        Raises:
-            OSError: if the file cannot be read (e.g. not found)
-            ValueError: Input is invalid
-
-        Example::
-
-            >>> zz = Cube()
-            >>> zz.from_file(cube_dir + "/ib_test_cube2.segy")
-
-
-        """
-        mfile = xtgeosys._XTGeoFile(sfile)
-        if fformat is None or fformat == "guess":
-            fformat = mfile.detect_fformat(suffixonly=True)
-        else:
-            fformat = mfile.generic_format_by_proposal(fformat)  # default
-        kwargs = _data_reader_factory(fformat)(mfile)
-        kwargs["filesrc"] = mfile.file
-        return cls(**kwargs)
-
-    def to_file(self, sfile, fformat="segy", pristine=False, engine="xtgeo"):
-        """Export cube data to file.
-
-        Args:
-            sfile (str): Filename
-            fformat (str, optional): file format 'segy' (default) or
-                'rms_regular'
-            pristine (bool): If True, make SEGY from scratch.
-            engine (str): Which "engine" to use.
-
-        Example::
-            >>> import xtgeo
-            >>> zz = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-            >>> zz.to_file(outdir + '/some.rmsreg')
-        """
-        fobj = xtgeosys._XTGeoFile(sfile, mode="wb")
-
-        fobj.check_folder(raiseerror=OSError)
-
-        if fformat == "segy":
-            _cube_export.export_segy(self, fobj.name, pristine=pristine, engine=engine)
-        elif fformat == "rms_regular":
-            _cube_export.export_rmsreg(self, fobj.name)
-        elif fformat == "xtgregcube":
-            _cube_export.export_xtgregcube(self, fobj.name)
-        else:
-            raise ValueError(f"File format fformat={fformat} is not supported")
-
-    def from_roxar(self, project, name, folder=None):  # pragma: no cover
-        """Import (transfer) a cube from a Roxar seismic object to XTGeo.
-
-        Args:
-            project (str): Inside RMS use the magic 'project', else use
-                path to RMS project
-            name (str): Name of cube within RMS project.
-            folder (str): Folder name in in RMS if present; use '/' to seperate
-                subfolders
-
-        Raises:
-            To be described...
-
-        Example::
-
-            zz = Cube()
-            zz.from_roxar(project, 'truth_reek_seismic_depth_2000', folder="alt/depth")
-
-        """
-        _cube_roxapi.import_cube_roxapi(self, project, name, folder=folder)
-
-        self._metadata.required = self
-
-    def to_roxar(
-        self, project, name, folder=None, domain="time", compression=("wavelet", 5)
-    ):  # pragma: no cover
-        """Export (transfer) a cube from a XTGeo cube object to Roxar data.
-
-        Note:
-            When project is file path (direct access, outside RMS) then
-            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-            will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str or roxar._project): Inside RMS use the magic 'project',
-                else use path to RMS project, or a project reference
-            name (str): Name of cube (seismic data) within RMS project.
-            folder (str): Cubes may be stored under a folder in the tree, use '/'
-                to seperate subfolders.
-            domain (str): 'time' (default) or 'depth'
-            compression (tuple): description to come...
-
-        Raises:
-            To be described...
-
-        Example::
-
-            zz = xtgeo.cube.Cube('myfile.segy')
-            zz.to_roxar(project, 'reek_cube')
-
-            # alternative
-            zz = xtgeo.cube_from_file('myfile.segy')
-            zz.to_roxar(project, 'reek_cube')
-
-        """
-        _cube_roxapi.export_cube_roxapi(
-            self, project, name, folder=folder, domain=domain, compression=compression
-        )
-
-    @staticmethod
-    def scan_segy_traces(sfile, outfile=None):
-        """Scan a SEGY file traces and print limits info to STDOUT or file.
-
-        Args:
-            sfile (str): Name of SEGY file
-            outfile: File where store scanned trace info, if empty or None
-                output goes to STDOUT.
-        """
-        oflag = False
-        # if outfile is none, it means that you want to plot on STDOUT
-        if outfile is None:
-            fx = tempfile.NamedTemporaryFile(delete=False, prefix="tmpxgeo")
-            fx.close()
-            outfile = fx.name
-            logger.info("TMP file name is %s", outfile)
-            oflag = True
-
-        _cube_import._scan_segy_trace(sfile, outfile=outfile)
-
-        if oflag:
-            # pass
-            logger.info("OUTPUT to screen...")
-            with open(outfile, "r") as out:
-                for line in out:
-                    print(line.rstrip("\r\n"))
-            os.remove(outfile)
-
-    @staticmethod
-    def scan_segy_header(sfile, outfile=None):
-        """Scan a SEGY file header and print info to screen or file.
-
-        Args:
-            sfile (str): Name of SEGY file
-            outfile (str): File where store header info, if empty or None
-                output goes to screen (STDOUT).
-        """
-        flag = False
-        # if outfile is none, it means that you want to print on STDOUT
-        if outfile is None:
-            fc = tempfile.NamedTemporaryFile(delete=False, prefix="tmpxtgeo")
-            fc.close()
-            outfile = fc.name
-            logger.info("TMP file name is %s", outfile)
-            flag = True
-
-        _cube_import._scan_segy_header(sfile, outfile=outfile)
-
-        if flag:
-            logger.info("OUTPUT to screen...")
-            with open(outfile, "r") as out:
-                for line in out:
-                    print(line.rstrip("\r\n"))
-            os.remove(outfile)
-
-    def _ensure_correct_values(self, values):
-        """Ensures that values is a 3D numpy (ncol, nrow, nlay), C order.
-
-        Args:
-            values (array-like or scalar): Values to process.
-
-        Return:
-            Nothing, self._values will be updated inplace
-
-        """
-        if values is None or values is False:
-            self._ensure_correct_values(0.0)
-            return
-
-        if isinstance(values, numbers.Number):
-            self._values = np.zeros(self.dimensions, dtype=np.float32) + values
-            return
-
-        if isinstance(values, np.ndarray):
-            values = values.reshape(self.dimensions)
-
-            if not values.data.c_contiguous:
-                values = np.ascontiguousarray(values)
-
-        if isinstance(values, (list, tuple)):
-            values = np.array(values, dtype=np.float32).reshape(self.dimensions)
-
-        self._values = values
+# coding: utf-8
+"""Module for a seismic (or whatever) cube."""
+import functools
+import numbers
+import os.path
+import pathlib
+import tempfile
+import warnings
+
+import deprecation
+import numpy as np
+
+import xtgeo
+import xtgeo.common.sys as xtgeosys
+from xtgeo.common import XTGDescription, XTGeoDialog
+from xtgeo.cube import _cube_export, _cube_import, _cube_roxapi, _cube_utils
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def _data_reader_factory(fformat):
+    if fformat == "segy":
+        return _cube_import.import_segy
+    elif fformat == "storm":
+        return _cube_import.import_stormcube
+    elif fformat == "xtg":
+        return _cube_import.import_xtgregcube
+    else:
+        raise ValueError(f"File format fformat={fformat} is not supported")
+
+
+def cube_from_file(mfile, fformat="guess"):
+    """This makes an instance of a Cube directly from file import.
+
+    Args:
+        mfile (str): Name of file
+        fformat (str): See :meth:`Cube.from_file`
+
+    Example::
+
+        >>> import xtgeo
+        >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+    """
+    return Cube._read_file(mfile, fformat)
+
+
+def cube_from_roxar(project, name, folder=None):
+    """This makes an instance of a Cube directly from roxar input.
+
+    The folder is a string on form "a" or "a/b" if subfolders are present
+
+    Example::
+
+        import xtgeo
+        mycube = xtgeo.cube_from_roxar(project, "DepthCube")
+
+    """
+    obj = Cube(ncol=9, nrow=9, nlay=9, xinc=9.99, yinc=9.99, zinc=9.99)
+
+    obj.from_roxar(project, name, folder=folder)
+
+    return obj
+
+
+def allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of Cube and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        # Checking if we are doing an initialization
+        # from file and raise a deprecation warning if
+        # we are.
+        if args and isinstance(args[0], (str, pathlib.Path)):
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "mcube = xtgeo.cube_from_file('some_name.gri') instead",
+                DeprecationWarning,
+            )
+            cfile = args[0]
+            if len(args) > 1:
+                fformat = args[1]
+            else:
+                fformat = kwargs.get("fformat", None)
+            mfile = xtgeosys._XTGeoFile(cfile)
+            if fformat is None or fformat == "guess":
+                fformat = mfile.detect_fformat(suffixonly=True)
+            else:
+                fformat = mfile.generic_format_by_proposal(fformat)  # default
+            kwargs = _data_reader_factory(fformat)(mfile)
+            kwargs["filesrc"] = mfile.file
+            return func(cls, **kwargs)
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+def allow_deprecated_default_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of Cube and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        _deprecation_msg = (
+            "{} is a required argument and will no "
+            "longer be defaulted in xtgeo version 4.0"
+        )
+        if "ncol" not in kwargs and len(args) < 1:
+            warnings.warn(_deprecation_msg.format("ncol"), DeprecationWarning)
+            kwargs["ncol"] = 5
+        if "nrow" not in kwargs and len(args) < 2:
+            warnings.warn(_deprecation_msg.format("nrow"), DeprecationWarning)
+            kwargs["nrow"] = 3
+        if "nlay" not in kwargs and len(args) < 3:
+            warnings.warn(_deprecation_msg.format("nlay"), DeprecationWarning)
+            kwargs["nlay"] = 2
+        if "xinc" not in kwargs and len(args) < 4:
+            warnings.warn(_deprecation_msg.format("xinc"), DeprecationWarning)
+            kwargs["xinc"] = 25.0
+        if "yinc" not in kwargs and len(args) < 5:
+            warnings.warn(_deprecation_msg.format("yinc"), DeprecationWarning)
+            kwargs["yinc"] = 25.0
+        if "zinc" not in kwargs and len(args) < 6:
+            warnings.warn(_deprecation_msg.format("zinc"), DeprecationWarning)
+            kwargs["zinc"] = 2.0
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+class Cube:  # pylint: disable=too-many-public-methods
+    """Class for a (seismic) cube in the XTGeo framework.
+
+    The values are stored as a 3D numpy array (4 bytes; float32 is default),
+    with internal C ordering (nlay fastest).
+
+    See :func:`xtgeo.cube_from_file` for importing cubes from e.g. segy files.
+
+    See also Cube section in documentation: docs/datamodel.rst
+
+    Examples::
+
+        >>> import xtgeo
+        >>> # a user defined cube:
+        >>> mycube = xtgeo.Cube(
+        ...     xori=100.0,
+        ...     yori=200.0,
+        ...     zori=150.0,
+        ...     ncol=40,
+        ...     nrow=30,
+        ...     nlay=10,
+        ...     rotation=30,
+        ...     values=0
+        ... )
+
+    Args:
+      xori: Origin in Easting coordinate
+      yori: Origin in Northing coordinate
+      zori: Origin in Depth coordinate, where depth is positive down
+      ncol: Number of columns
+      nrow: Number of columns
+      nlay: Number of layers, starting from top
+      rotation: Cube rotation, X axis is applied and "school-wise" rotation,
+                     anti-clock in degrees
+      values: Numpy array with shape (ncol, nrow, nlay), C order, np.float32
+      ilines: 1D numpy array with ncol elements, aka INLINES array, defaults to arange
+      xlines: 1D numpy array with nrow elements, aka XLINES array, defaults to arange
+      segyfile: Name of source segyfile if any
+      filesrc: String: Source file if any
+      yflip: Normally 1; if -1 Y axis is flipped --> from left-handed (1) to
+                     right handed (-1). Right handed cubes are common.
+
+    """
+
+    @allow_deprecated_init
+    @allow_deprecated_default_init
+    def __init__(
+        self,
+        ncol,
+        nrow,
+        nlay,
+        xinc,
+        yinc,
+        zinc,
+        xori=0.0,
+        yori=0.0,
+        zori=0.0,
+        yflip=1,
+        values=0.0,
+        rotation=0.0,
+        zflip=1,
+        ilines=None,
+        xlines=None,
+        traceidcodes=None,
+        segyfile=None,
+        filesrc=None,
+    ):
+        """Initiate a Cube instance."""
+
+        self._reset(
+            xori=xori,
+            yori=yori,
+            zori=zori,
+            ncol=ncol,
+            nrow=nrow,
+            nlay=nlay,
+            xinc=xinc,
+            yinc=yinc,
+            zinc=zinc,
+            yflip=yflip,
+            values=values,
+            rotation=rotation,
+            zflip=zflip,
+            ilines=ilines,
+            xlines=xlines,
+            traceidcodes=traceidcodes,
+            segyfile=segyfile,
+            filesrc=filesrc,
+        )
+
+    def _reset(
+        self,
+        ncol=5,
+        nrow=3,
+        nlay=2,
+        xinc=25.0,
+        yinc=25.0,
+        zinc=2.0,
+        xori=0.0,
+        yori=0.0,
+        zori=0.0,
+        yflip=1,
+        values=0.0,
+        rotation=0.0,
+        zflip=1,
+        ilines=None,
+        xlines=None,
+        traceidcodes=None,
+        segyfile=None,
+        filesrc=None,
+    ):
+        self._filesrc = filesrc
+        self._xori = xori
+        self._yori = yori
+        self._zori = zori
+        self._ncol = ncol
+        self._nrow = nrow
+        self._nlay = nlay
+        self._xinc = xinc
+        self._yinc = yinc
+        self._zinc = zinc
+        self._yflip = yflip
+        self._zflip = zflip  # currently not in use
+        self._rotation = rotation
+
+        self._values = None
+        self.values = values  # "values" is intentional over "_values"; cf. values()
+
+        if ilines is None:
+            self._ilines = ilines or np.array(range(1, self._ncol + 1), dtype=np.int32)
+        else:
+            self._ilines = ilines
+        if xlines is None:
+            self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
+        else:
+            self._xlines = xlines
+        if traceidcodes is None:
+            self._traceidcodes = np.ones((self._ncol, self._nrow), dtype=np.int32)
+        else:
+            self._traceidcodes = traceidcodes
+        self._segyfile = segyfile
+        self.undef = xtgeo.UNDEF
+
+        self._metadata = xtgeo.MetaDataRegularCube()
+        self._metadata.required = self
+
+    def __repr__(self):
+        """The __repr__ method."""
+        avg = self.values.mean()
+        dsc = (
+            f"{self.__class__} (ncol={self.ncol!r}, nrow={self.nrow!r}, "
+            f"nlay={self.nlay!r}, original file: {self._filesrc}), "
+            f"average {avg}, ID=<{id(self)}>"
+        )
+        return dsc
+
+    def __str__(self):
+        """The __str__ method for pretty print."""
+        return self.describe(flush=False)
+
+    @property
+    def metadata(self):
+        """Return metadata object instance of type MetaDataRegularSurface."""
+        return self._metadata
+
+    @metadata.setter
+    def metadata(self, obj):
+        # The current metadata object can be replaced. This is a bit dangerous so
+        # further check must be done to validate. TODO.
+        if not isinstance(obj, xtgeo.MetaDataRegularCube):
+            raise ValueError("Input obj not an instance of MetaDataRegularCube")
+
+        self._metadata = obj  # checking is currently missing! TODO
+
+    @property
+    def ncol(self):
+        """The NCOL (NX or I dir) number (read-only)."""
+        return self._ncol
+
+    @property
+    def nrow(self):
+        """The NROW (NY or J dir) number (read-only)."""
+        return self._nrow
+
+    @property
+    def nlay(self):
+        """The NLAY (or NZ or K dir) number (read-only)."""
+        return self._nlay
+
+    @property
+    def dimensions(self):
+        """3-tuple: The cube dimensions as a tuple of 3 integers (read only)."""
+        return (self._ncol, self._nrow, self._nlay)
+
+    @property
+    def xori(self):
+        """The XORI (origin corner) coordinate."""
+        return self._xori
+
+    @xori.setter
+    def xori(self, val):
+        logger.warning("Changing xori is risky!")
+        self._xori = val
+
+    @property
+    def yori(self):
+        """The YORI (origin corner) coordinate."""
+        return self._yori
+
+    @yori.setter
+    def yori(self, val):
+        logger.warning("Changing yori is risky!")
+        self._yori = val
+
+    @property
+    def zori(self):
+        """The ZORI (origin corner) coordinate."""
+        return self._zori
+
+    @zori.setter
+    def zori(self, val):
+        logger.warning("Changing zori is risky!")
+        self._zori = val
+
+    @property
+    def xinc(self):
+        """The XINC (increment X) as property."""
+        return self._xinc
+
+    @xinc.setter
+    def xinc(self, val):
+        logger.warning("Changing xinc is risky!")
+        self._xinc = val
+
+    @property
+    def yinc(self):
+        """The YINC (increment Y)."""
+        return self._yinc
+
+    @yinc.setter
+    def yinc(self, val):
+        logger.warning("Changing yinc is risky!")
+        self._yinc = val
+
+    @property
+    def zinc(self):
+        """The ZINC (increment Z)."""
+        return self._zinc
+
+    @zinc.setter
+    def zinc(self, val):
+        logger.warning("Changing zinc is risky!")
+        self._zinc = val
+
+    @property
+    def rotation(self):
+        """The rotation, anticlock from X axis in degrees."""
+        return self._rotation
+
+    @rotation.setter
+    def rotation(self, val):
+        logger.warning("Changing rotation is risky!")
+        self._rotation = val
+
+    @property
+    def ilines(self):
+        """The inlines numbering vector."""
+        return self._ilines
+
+    @ilines.setter
+    def ilines(self, values):
+        self._ilines = values
+
+    @property
+    def xlines(self):
+        """The xlines numbering vector."""
+        return self._xlines
+
+    @xlines.setter
+    def xlines(self, values):
+        self._xlines = values
+
+    @property
+    def zslices(self):
+        """Return the time/depth slices as an int array (read only)."""
+        # This is a derived property
+        zslices = range(
+            int(self.zori), int(self.zori + self.nlay * self.zinc), int(self.zinc)
+        )
+        zslices = np.array(zslices)
+        return zslices
+
+    @property
+    def traceidcodes(self):
+        """The trace identifaction codes array (ncol, nrow)."""
+        return self._traceidcodes
+
+    @traceidcodes.setter
+    def traceidcodes(self, values):
+        if isinstance(values, (int, str)):
+            self._traceidcodes = np.full((self.ncol, self.nrow), values, dtype=np.int32)
+        else:
+            if isinstance(values, list):
+                values = np.array(values, np.int32)
+            self._traceidcodes = values.reshape(self.ncol, self.nrow)
+
+    @property
+    def yflip(self):
+        """The YFLIP indicator, 1 is normal, -1 means Y flipped.
+
+        YFLIP = 1 means a LEFT HANDED coordinate system with Z axis
+        positive down, while inline (col) follow East (X) and xline (rows)
+        follows North (Y), when rotation is zero.
+        """
+        return self._yflip
+
+    @property
+    def zflip(self):
+        """The ZFLIP indicator, 1 is normal, -1 means Z flipped.
+
+        ZFLIP = 1 and YFLIP = 1 means a LEFT HANDED coordinate system with Z axis
+        positive down, while inline (col) follow East (X) and xline (rows)
+        follows North (Y), when rotation is zero.
+        """
+        return self._zflip
+
+    @property
+    def segyfile(self):
+        """The input segy file name (str), if any (or None) (read-only)."""
+        return self._segyfile
+
+    @property
+    def filesrc(self):
+        """The input file name (str), if any (or None) (read-only)."""
+        return self._filesrc
+
+    @filesrc.setter
+    def filesrc(self, name):
+        self._filesrc = name
+
+    @property
+    def values(self):
+        """The values, as a 3D numpy (ncol, nrow, nlay), 4 byte float."""
+        return self._values
+
+    @values.setter
+    def values(self, values):
+        self._ensure_correct_values(values)
+
+    # =========================================================================
+    # Describe
+    # =========================================================================
+
+    def generate_hash(self, hashmethod="md5"):
+        """Return a unique hash ID for current instance.
+
+        See :meth:`~xtgeo.common.sys.generic_hash()` for documentation.
+
+        .. versionadded:: 2.14
+        """
+        required = (
+            "ncol",
+            "nrow",
+            "nlay",
+            "xori",
+            "yori",
+            "zori",
+            "xinc",
+            "yinc",
+            "zinc",
+            "yflip",
+            "zflip",
+            "rotation",
+            "values",
+            "ilines",
+            "xlines",
+            "traceidcodes",
+        )
+
+        gid = ""
+        for req in required:
+            gid += f"{getattr(self, '_' + req)}"
+
+        return xtgeosys.generic_hash(gid, hashmethod=hashmethod)
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout or return.
+
+        Args:
+            flush (bool): If True, description is printed to stdout.
+        """
+        dsc = XTGDescription()
+        dsc.title("Description of Cube instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("File source", self._filesrc)
+        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
+        dsc.txt("Origins XORI, YORI, ZORI", self.xori, self.yori, self.zori)
+        dsc.txt("Increments XINC YINC ZINC", self.xinc, self.yinc, self.zinc)
+        dsc.txt("Rotation (anti-clock from X)", self.rotation)
+        dsc.txt("YFLIP flag", self.yflip)
+        np.set_printoptions(threshold=16)
+        dsc.txt("Inlines vector", self._ilines)
+        dsc.txt("Xlines vector", self._xlines)
+        dsc.txt("Time or depth slices vector", self.zslices)
+        dsc.txt("Values", self._values.reshape(-1), self._values.dtype)
+        np.set_printoptions(threshold=1000)
+        dsc.txt(
+            "Values, mean, stdev, minimum, maximum",
+            self.values.mean(),
+            self.values.std(),
+            self.values.min(),
+            self.values.max(),
+        )
+        dsc.txt("Trace ID codes", self._traceidcodes.reshape(-1))
+        msize = float(self.values.size * 4) / (1024 * 1024 * 1024)
+        dsc.txt("Minimum memory usage of array (GB)", msize)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    # ==================================================================================
+    # Copy, swapping, cropping, thinning...
+    # ==================================================================================
+
+    def copy(self):
+        """Deep copy of a Cube() object to another instance.
+
+
+        >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+        >>> mycube2 = mycube.copy()
+
+        """
+        xcube = Cube(
+            ncol=self.ncol,
+            nrow=self.nrow,
+            nlay=self.nlay,
+            xinc=self.xinc,
+            yinc=self.yinc,
+            zinc=self.zinc,
+            xori=self.xori,
+            yori=self.yori,
+            zori=self.zori,
+            yflip=self.yflip,
+            segyfile=self.segyfile,
+            rotation=self.rotation,
+            values=self.values.copy(),
+        )
+
+        xcube.filesrc = self._filesrc
+
+        xcube.ilines = self._ilines.copy()
+        xcube.xlines = self._xlines.copy()
+        xcube.traceidcodes = self._traceidcodes.copy()
+        xcube.metadata.required = xcube
+
+        return xcube
+
+    def swapaxes(self):
+        """Swap the axes inline vs xline, keep origin."""
+        _cube_utils.swapaxes(self)
+
+    def resample(self, incube, sampling="nearest", outside_value=None):
+        """Resample a Cube object into this instance.
+
+        Args:
+            incube (Cube): A XTGeo Cube instance
+            sampling (str): Sampling algorithm: 'nearest' for nearest node
+                of 'trilinear' for trilinear interpoltion (more correct but
+                slower)
+            outside_value (None or float). If None, keep original, otherwise
+                use this value
+
+        Raises:
+            ValueError: If cubes do not overlap
+
+        Example:
+
+            >>> import xtgeo
+            >>> mycube1 = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+            >>> mycube2 = xtgeo.Cube(
+            ...     xori=777574,
+            ...     yori=6736507,
+            ...     zori=1000,
+            ...     xinc=10,
+            ...     yinc=10,
+            ...     zinc=4,
+            ...     ncol=100,
+            ...     nrow=100,
+            ...     nlay=100,
+            ...     yflip=mycube1.yflip,
+            ...     rotation=mycube1.rotation
+            ... )
+            >>> mycube2.resample(mycube1)
+
+        """
+        _cube_utils.resample(
+            self, incube, sampling=sampling, outside_value=outside_value
+        )
+
+    def do_thinning(self, icol, jrow, klay):
+        """Thinning the cube by removing every N column, row and/or layer.
+
+        Args:
+            icol (int): Thinning factor for columns (usually inlines)
+            jrow (int): Thinning factor for rows (usually xlines)
+            klay (int): Thinning factor for layers
+
+        Raises:
+            ValueError: If icol, jrow or klay are out of reasonable range
+
+        Example:
+
+            >>> mycube1 = Cube(cube_dir + "/ib_test_cube2.segy")
+            >>> mycube1.do_thinning(2, 2, 1)  # keep every second column, row
+            >>> mycube1.to_file(outdir + '/mysegy_smaller.segy')
+
+        """
+        _cube_utils.thinning(self, icol, jrow, klay)
+
+    def do_cropping(self, icols, jrows, klays, mode="edges"):
+        """Cropping the cube by removing rows, columns, layers.
+
+        Note that input boundary checking is currently lacking, and this
+        is a currently a user responsibility!
+
+        The 'mode' is used to determine to different 'approaches' on
+        cropping. Examples for icols and mode 'edges':
+        Here the tuple (N, M) will cut N first rows and M last rows.
+
+        However, if mode is 'inclusive' then, it defines the range
+        of rows to be included, and the numbering now shall be the
+        INLINE, XLINE and DEPTH/TIME mode.
+
+        Args:
+            icols (int tuple): Cropping front, end of rows, or inclusive range
+            jrows (int tuple): Cropping front, end of columns, or
+                inclusive range
+            klays (int tuple ): Cropping top, base layers, or inclusive range.
+            mode (str): 'Default is 'edges'; alternative is 'inclusive'
+
+        Example:
+            Crop 10 columns from front, 2 from back, then 20 rows in front,
+            40 in back, then no cropping of layers::
+
+            >>> import xtgeo
+            >>> mycube1 = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+            >>> mycube2 = mycube1.copy()
+            >>> mycube1.do_cropping((10, 2), (20, 40), (0, 0))
+            >>> mycube1.to_file(outdir + '/mysegy_smaller.segy')
+
+            In stead, do cropping as 'inclusive' where inlines, xlines, slices
+            arrays are known::
+
+            >>> mycube2.do_cropping((11, 32), (112, 114), (150, 200))
+
+        """
+        useicols = icols
+        usejrows = jrows
+        useklays = klays
+
+        if mode == "inclusive":
+            # transfer to 'numbers to row/col/lay to remove' in front end ...
+            useicols = (
+                icols[0] - self._ilines[0],
+                self._ilines[self._ncol - 1] - icols[1],
+            )
+            usejrows = (
+                jrows[0] - self._xlines[0],
+                self._xlines[self._nrow - 1] - jrows[1],
+            )
+            ntop = int((klays[0] - self.zori) / self.zinc)
+            nbot = int((self.zori + self.nlay * self.zinc - klays[1] - 1) / (self.zinc))
+            useklays = (ntop, nbot)
+
+        logger.info(
+            "Cropping at all cube sides: %s %s %s", useicols, usejrows, useklays
+        )
+        _cube_utils.cropping(self, useicols, usejrows, useklays)
+
+    def values_dead_traces(self, newvalue):
+        """Set values for traces flagged as dead.
+
+        Dead traces have traceidcodes 2 and corresponding values in the cube
+        will here receive a constant value to mimic "undefined".
+
+        Args:
+            newvalue (float): Set cube values to newvalues where traceid is 2.
+
+        Return:
+            oldvalue (float): The estimated simple 'average' of old value will
+                be returned as (max + min)/2. If no dead traces, return None.
+        """
+        logger.info("Set values for dead traces, if any")
+
+        if 2 in self._traceidcodes:
+            minval = self._values[self._traceidcodes == 2].min()
+            maxval = self._values[self._traceidcodes == 2].max()
+            # a bit weird calculation of mean but kept for backward compatibility
+            self._values[self._traceidcodes == 2] = newvalue
+            return 0.5 * (minval + maxval)
+
+        return None
+
+    def get_xy_value_from_ij(self, iloc, jloc, ixline=False, zerobased=False):
+        """Returns x, y coordinate from a single i j location.
+
+        Args:
+            iloc (int): I (col) location (base is 1)
+            jloc (int): J (row) location (base is 1)
+            ixline (bool): If True, then input locations are inline and xline position
+            zerobased (bool): If True, first index is 0, else it is 1. This does not
+                apply when ixline is set to True.
+
+        Returns:
+            The X, Y coordinate pair.
+        """
+        xval, yval = _cube_utils.get_xy_value_from_ij(
+            self, iloc, jloc, ixline=ixline, zerobased=zerobased
+        )
+        return xval, yval
+
+    # =========================================================================
+    # Cube extractions, e.g. XSection
+    # =========================================================================
+
+    def get_randomline(
+        self,
+        fencespec,
+        zmin=None,
+        zmax=None,
+        zincrement=None,
+        hincrement=None,
+        atleast=5,
+        nextend=2,
+        sampling="nearest",
+    ):
+        """Get a randomline from a fence spesification.
+
+        This randomline will be a 2D numpy with depth/time on the vertical
+        axis, and length along as horizontal axis. Undefined values will have
+        the np.nan value.
+
+        The input fencespec is either a 2D numpy where each row is X, Y, Z, HLEN,
+        where X, Y are UTM coordinates, Z is depth/time, and HLEN is a
+        length along the fence, or a Polygons instance.
+
+        If input fencspec is a numpy 2D, it is important that the HLEN array
+        has a constant increment and ideally a sampling that is less than the
+        Cube resolution. If a Polygons() instance, this is automated!
+
+        Args:
+            fencespec (:obj:`~numpy.ndarray` or :class:`~xtgeo.xyz.polygons.Polygons`):
+                2D numpy with X, Y, Z, HLEN as rows or a xtgeo Polygons() object.
+            zmin (float): Minimum Z (default is Cube Z minima/origin)
+            zmax (float): Maximum Z (default is Cube Z maximum)
+            zincrement (float): Sampling vertically, default is Cube ZINC/2
+            hincrement (float or bool): Resampling horizontally. This applies only
+                if the fencespec is a Polygons() instance. If None (default),
+                the distance will be deduced automatically.
+            atleast (int): Minimum number of horizontal samples (only if
+                fencespec is a Polygons instance)
+            nextend (int): Extend with nextend * hincrement in both ends (only if
+                fencespec is a Polygons instance)
+            sampling (str): Algorithm, 'nearest' or 'trilinear' (first is
+                faster, second is more precise for continuous fields)
+
+        Returns:
+            A tuple: (hmin, hmax, vmin, vmax, ndarray2d)
+
+        Raises:
+            ValueError: Input fence is not according to spec.
+
+        .. versionchanged:: 2.1 support for Polygons() as fencespec, and keywords
+           hincrement, atleast and sampling
+
+        .. seealso::
+           Class :class:`~xtgeo.xyz.polygons.Polygons`
+              The method :meth:`~xtgeo.xyz.polygons.Polygons.get_fence()` which can be
+              used to pregenerate `fencespec`
+
+        """
+        if not isinstance(fencespec, (np.ndarray, xtgeo.Polygons)):
+            raise ValueError(
+                "fencespec must be a numpy or a Polygons() object. "
+                f"Current type is {type(fencespec)}"
+            )
+        logger.info("Getting randomline...")
+        res = _cube_utils.get_randomline(
+            self,
+            fencespec,
+            zmin=zmin,
+            zmax=zmax,
+            zincrement=zincrement,
+            hincrement=hincrement,
+            atleast=atleast,
+            nextend=nextend,
+            sampling=sampling,
+        )
+        logger.info("Getting randomline... DONE")
+        return res
+
+    # =========================================================================
+    # Import and export
+    # =========================================================================
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.cube_from_file() instead",
+    )
+    def from_file(self, sfile, fformat="guess", engine=None):
+        """Deprecated, see :func:`cube_from_file`."""
+        mfile = xtgeosys._XTGeoFile(sfile)
+        if fformat is None or fformat == "guess":
+            fformat = mfile.detect_fformat(suffixonly=True)
+        else:
+            fformat = mfile.generic_format_by_proposal(fformat)  # default
+
+        if engine is not None:
+            warnings.warn(
+                "The engine parameter is deprecated, and has no effect.",
+                DeprecationWarning,
+            )
+
+        kwargs = _data_reader_factory(fformat)(mfile)
+        kwargs["filesrc"] = mfile.file
+        self._reset(**kwargs)
+
+    @classmethod
+    def _read_file(cls, sfile, fformat="guess"):
+        """Import cube data from file.
+
+        If fformat is not provided, the file type will be guessed based
+        on file extension (e.g. segy og sgy for SEGY format)
+
+        Args:
+            sfile (str): Filename (as string or pathlib.Path instance).
+            fformat (str): file format guess/segy/rms_regular/xtgregcube
+                where 'guess' is default. Regard 'xtgrecube' format as experimental.
+            deadtraces (float): Set 'dead' trace values to this value (SEGY
+                only). Default is UNDEF value (a very large number).
+
+        Raises:
+            OSError: if the file cannot be read (e.g. not found)
+            ValueError: Input is invalid
+
+        Example::
+
+            >>> zz = Cube()
+            >>> zz.from_file(cube_dir + "/ib_test_cube2.segy")
+
+
+        """
+        mfile = xtgeosys._XTGeoFile(sfile)
+        if fformat is None or fformat == "guess":
+            fformat = mfile.detect_fformat(suffixonly=True)
+        else:
+            fformat = mfile.generic_format_by_proposal(fformat)  # default
+        kwargs = _data_reader_factory(fformat)(mfile)
+        kwargs["filesrc"] = mfile.file
+        return cls(**kwargs)
+
+    def to_file(self, sfile, fformat="segy", pristine=False, engine="xtgeo"):
+        """Export cube data to file.
+
+        Args:
+            sfile (str): Filename
+            fformat (str, optional): file format 'segy' (default) or
+                'rms_regular'
+            pristine (bool): If True, make SEGY from scratch.
+            engine (str): Which "engine" to use.
+
+        Example::
+            >>> import xtgeo
+            >>> zz = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+            >>> zz.to_file(outdir + '/some.rmsreg')
+        """
+        fobj = xtgeosys._XTGeoFile(sfile, mode="wb")
+
+        fobj.check_folder(raiseerror=OSError)
+
+        if fformat == "segy":
+            _cube_export.export_segy(self, fobj.name, pristine=pristine, engine=engine)
+        elif fformat == "rms_regular":
+            _cube_export.export_rmsreg(self, fobj.name)
+        elif fformat == "xtgregcube":
+            _cube_export.export_xtgregcube(self, fobj.name)
+        else:
+            raise ValueError(f"File format fformat={fformat} is not supported")
+
+    def from_roxar(self, project, name, folder=None):  # pragma: no cover
+        """Import (transfer) a cube from a Roxar seismic object to XTGeo.
+
+        Args:
+            project (str): Inside RMS use the magic 'project', else use
+                path to RMS project
+            name (str): Name of cube within RMS project.
+            folder (str): Folder name in in RMS if present; use '/' to seperate
+                subfolders
+
+        Raises:
+            To be described...
+
+        Example::
+
+            zz = Cube()
+            zz.from_roxar(project, 'truth_reek_seismic_depth_2000', folder="alt/depth")
+
+        """
+        _cube_roxapi.import_cube_roxapi(self, project, name, folder=folder)
+
+        self._metadata.required = self
+
+    def to_roxar(
+        self, project, name, folder=None, domain="time", compression=("wavelet", 5)
+    ):  # pragma: no cover
+        """Export (transfer) a cube from a XTGeo cube object to Roxar data.
+
+        Note:
+            When project is file path (direct access, outside RMS) then
+            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+            will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str or roxar._project): Inside RMS use the magic 'project',
+                else use path to RMS project, or a project reference
+            name (str): Name of cube (seismic data) within RMS project.
+            folder (str): Cubes may be stored under a folder in the tree, use '/'
+                to seperate subfolders.
+            domain (str): 'time' (default) or 'depth'
+            compression (tuple): description to come...
+
+        Raises:
+            To be described...
+
+        Example::
+
+            zz = xtgeo.cube.Cube('myfile.segy')
+            zz.to_roxar(project, 'reek_cube')
+
+            # alternative
+            zz = xtgeo.cube_from_file('myfile.segy')
+            zz.to_roxar(project, 'reek_cube')
+
+        """
+        _cube_roxapi.export_cube_roxapi(
+            self, project, name, folder=folder, domain=domain, compression=compression
+        )
+
+    @staticmethod
+    def scan_segy_traces(sfile, outfile=None):
+        """Scan a SEGY file traces and print limits info to STDOUT or file.
+
+        Args:
+            sfile (str): Name of SEGY file
+            outfile: File where store scanned trace info, if empty or None
+                output goes to STDOUT.
+        """
+        oflag = False
+        # if outfile is none, it means that you want to plot on STDOUT
+        if outfile is None:
+            fx = tempfile.NamedTemporaryFile(delete=False, prefix="tmpxgeo")
+            fx.close()
+            outfile = fx.name
+            logger.info("TMP file name is %s", outfile)
+            oflag = True
+
+        _cube_import._scan_segy_trace(sfile, outfile=outfile)
+
+        if oflag:
+            # pass
+            logger.info("OUTPUT to screen...")
+            with open(outfile, "r") as out:
+                for line in out:
+                    print(line.rstrip("\r\n"))
+            os.remove(outfile)
+
+    @staticmethod
+    def scan_segy_header(sfile, outfile=None):
+        """Scan a SEGY file header and print info to screen or file.
+
+        Args:
+            sfile (str): Name of SEGY file
+            outfile (str): File where store header info, if empty or None
+                output goes to screen (STDOUT).
+        """
+        flag = False
+        # if outfile is none, it means that you want to print on STDOUT
+        if outfile is None:
+            fc = tempfile.NamedTemporaryFile(delete=False, prefix="tmpxtgeo")
+            fc.close()
+            outfile = fc.name
+            logger.info("TMP file name is %s", outfile)
+            flag = True
+
+        _cube_import._scan_segy_header(sfile, outfile=outfile)
+
+        if flag:
+            logger.info("OUTPUT to screen...")
+            with open(outfile, "r") as out:
+                for line in out:
+                    print(line.rstrip("\r\n"))
+            os.remove(outfile)
+
+    def _ensure_correct_values(self, values):
+        """Ensures that values is a 3D numpy (ncol, nrow, nlay), C order.
+
+        Args:
+            values (array-like or scalar): Values to process.
+
+        Return:
+            Nothing, self._values will be updated inplace
+
+        """
+        if values is None or values is False:
+            self._ensure_correct_values(0.0)
+            return
+
+        if isinstance(values, numbers.Number):
+            self._values = np.zeros(self.dimensions, dtype=np.float32) + values
+            return
+
+        if isinstance(values, np.ndarray):
+            values = values.reshape(self.dimensions)
+
+            if not values.data.c_contiguous:
+                values = np.ascontiguousarray(values)
+
+        if isinstance(values, (list, tuple)):
+            values = np.array(values, dtype=np.float32).reshape(self.dimensions)
+
+        self._values = values
```

## xtgeo/cxtgeo/__init__.py

```diff
@@ -1 +1 @@
-00000000: 0d0a                                     ..
+00000000: 0a                                       .
```

## xtgeo/grid3d/__init__.py

 * *Ordering differences only*

```diff
@@ -1,15 +1,15 @@
-# -*- coding: utf-8 -*-
-# flake8: noqa
-"""The XTGeo grid3d package"""
-
-
-from xtgeo.common.exceptions import (
-    DateNotFoundError,
-    KeywordFoundNoDateError,
-    KeywordNotFoundError,
-)
-
-from ._ecl_grid import GridRelative, Units
-from .grid import Grid
-from .grid_property import GridProperty
-from .grid_properties import GridProperties
+# -*- coding: utf-8 -*-
+# flake8: noqa
+"""The XTGeo grid3d package"""
+
+
+from xtgeo.common.exceptions import (
+    DateNotFoundError,
+    KeywordFoundNoDateError,
+    KeywordNotFoundError,
+)
+
+from ._ecl_grid import GridRelative, Units
+from .grid import Grid
+from .grid_property import GridProperty
+from .grid_properties import GridProperties
```

## xtgeo/grid3d/_ecl_grid.py

 * *Ordering differences only*

```diff
@@ -1,788 +1,788 @@
-import warnings
-from abc import ABC, abstractmethod
-from dataclasses import astuple, dataclass, fields
-from enum import Enum, auto, unique
-from typing import Any, List, Optional, Tuple, Union
-
-import numpy as np
-from scipy.constants import foot
-
-from ._grdecl_format import match_keyword
-
-
-@unique
-class Units(Enum):
-    METRES = auto()
-    CM = auto()
-    FEET = auto()
-
-    def conversion_factor(self, other):
-        "Conversion factor from one unit to another"
-        result = 1.0
-        if self == other:
-            return result
-        if other == Units.FEET:
-            result *= 1 / foot
-        if other == Units.CM:
-            result *= 1e2
-        if self == Units.FEET:
-            result *= foot
-        if self == Units.CM:
-            result *= 1e-2
-        return result
-
-    def to_grdecl(self):
-        return self.name
-
-    def to_bgrdecl(self):
-        return self.to_grdecl().ljust(8)
-
-    @classmethod
-    def from_grdecl(cls, unit_string):
-        if match_keyword(unit_string, "METRES"):
-            return cls.METRES
-        if match_keyword(unit_string, "FEET"):
-            return cls.FEET
-        if match_keyword(unit_string, "CM"):
-            return cls.CM
-        raise ValueError(f"Unknown unit string {unit_string}")
-
-    @classmethod
-    def from_bgrdecl(cls, unit_string):
-        if isinstance(unit_string, bytes):
-            return cls.from_grdecl(unit_string.decode("ascii"))
-        return cls.from_grdecl(unit_string)
-
-
-@unique
-class GridRelative(Enum):
-    """GridRelative is the second value given GRIDUNIT keyword.
-
-    MAP means map relative units, while
-    leaving it blank means relative to the origin given by the
-    MAPAXES keyword.
-    """
-
-    MAP = auto()
-    ORIGIN = auto()
-
-    def to_grdecl(self) -> str:
-        if self == GridRelative.MAP:
-            return "MAP"
-        else:
-            return ""
-
-    def to_bgrdecl(self) -> str:
-        return self.to_grdecl().ljust(8)
-
-    @classmethod
-    def from_grdecl(cls, unit_string: str):
-        if match_keyword(unit_string, "MAP"):
-            return cls.MAP
-        else:
-            return cls.ORIGIN
-
-    @classmethod
-    def from_bgrdecl(cls, unit_string):
-        if isinstance(unit_string, bytes):
-            return cls.from_grdecl(unit_string.decode("ascii"))
-        return cls.from_grdecl(unit_string)
-
-
-@dataclass
-class GrdeclKeyword:
-    """An abstract grdecl keyword.
-
-    Gives a general implementation of to/from grdecl which recurses on
-    fields. Ie. a dataclass such as
-    >>> class A(GrdeclKeyword):
-    ...     ...
-    >>> class B(GrdeclKeyword):
-    ...     ...
-
-    >>> @dataclass
-    ... class MyKeyword(GrdeclKeyword):
-    ...     field1: A
-    ...     field2: B
-
-    will have a to_grdecl method that will be similar to
-
-    >>> def to_grdecl(self):
-    ...     return [self.field1.to_grdecl(), self.field2.to_grdecl]
-
-    Similarly from_grdecl will call fields from_grdecl
-    to construct the object
-
-    >>> @classmethod
-    ... def from_grdecl(cls, values):
-    ...     return cls(A.from_grdecl(values[0]), B.from_grdecl(values[1]))
-    """
-
-    def to_grdecl(self) -> List[Any]:
-        """Convert the keyword to list of grdecl keyword values.
-        Returns:
-            list of values of the given keyword. ie. The
-            keyword read from "SPECGRID 1 1 1 F" should return
-            [1,1,1,CoordinateType.CYLINDRICAL]
-        """
-        return [value.to_grdecl() for value in astuple(self)]
-
-    def to_bgrdecl(self) -> List[Any]:
-        return [value.to_bgrdecl() for value in astuple(self)]
-
-    @classmethod
-    def from_bgrdecl(cls, values):
-        object_types = [f.type for f in fields(cls)]
-        return cls(*[typ.from_bgrdecl(val) for val, typ in zip(values, object_types)])
-
-    @classmethod
-    def from_grdecl(cls, values):
-        """Convert list of grdecl keyword values to a keyword.
-        Args:
-            values(list): list of values given after the keyword in
-                the grdecl file.
-        Returns:
-            A GrdeclKeyword constructed from the given values.
-        """
-        object_types = [f.type for f in fields(cls)]
-        return cls(*[typ.from_grdecl(val) for val, typ in zip(values, object_types)])
-
-
-@unique
-class Order(Enum):
-    """Either increasing or decreasing.
-
-    Used for the grdecl keywords INC and DEC
-    respectively.
-    """
-
-    INCREASING = auto()
-    DECREASING = auto()
-
-    def to_grdecl(self) -> str:
-        return str(self.name)[0:3]
-
-    def to_bgrdecl(self) -> str:
-        return self.to_grdecl().ljust(8)
-
-    @classmethod
-    def from_grdecl(cls, order_string):
-        if match_keyword(order_string, "INC"):
-            return cls.INCREASING
-        if match_keyword(order_string, "DEC"):
-            return cls.DECREASING
-
-    @classmethod
-    def from_bgrdecl(cls, unit_string: Union[bytes, str]):
-        if isinstance(unit_string, bytes):
-            return cls.from_grdecl(unit_string.decode("ascii"))
-        return cls.from_grdecl(unit_string)
-
-
-@unique
-class Handedness(Enum):
-    """The handedness of an orientation.
-
-    Eiter left handed or right handed.  Used for the grdecl keywords LEFT and
-    RIGHT.
-    """
-
-    LEFT = auto()
-    RIGHT = auto()
-
-    def to_grdecl(self) -> str:
-        return self.name
-
-    def to_bgrdecl(self) -> str:
-        return self.to_grdecl().ljust(8)
-
-    @classmethod
-    def from_grdecl(cls, orientation_string: str):
-        if match_keyword(orientation_string, "LEFT"):
-            return cls.LEFT
-        if match_keyword(orientation_string, "RIGHT"):
-            return cls.RIGHT
-        raise ValueError(f"Unknown handedness string {orientation_string}")
-
-    @classmethod
-    def from_bgrdecl(cls, unit_string: Union[bytes, str]):
-        if isinstance(unit_string, bytes):
-            return cls.from_grdecl(unit_string.decode("ascii"))
-        return cls.from_grdecl(unit_string)
-
-
-@unique
-class Orientation(Enum):
-    """Either up or down, for the grdecl keywords UP and DOWN."""
-
-    UP = auto()
-    DOWN = auto()
-
-    def to_grdecl(self) -> str:
-        return self.name
-
-    def to_bgrdecl(self) -> str:
-        return self.to_grdecl().ljust(8)
-
-    @classmethod
-    def from_grdecl(cls, orientation_string: str):
-        if match_keyword(orientation_string, "UP"):
-            return cls.UP
-        if match_keyword(orientation_string, "DOWN"):
-            return cls.DOWN
-        raise ValueError(f"Unknown orientation string {orientation_string}")
-
-    @classmethod
-    def from_bgrdecl(cls, unit_string: Union[bytes, str]):
-        if isinstance(unit_string, bytes):
-            return cls.from_grdecl(unit_string.decode("ascii"))
-        return cls.from_grdecl(unit_string)
-
-
-@dataclass
-class GdOrient(GrdeclKeyword):
-    """The GDORIENT keyword gives the orientation of the grid.
-
-    The three first values is either increasing or decreasing
-    depending on whether the corresponding dimension has increasing
-    or decreasing coordinates. Then comes the direction of the z dimension,
-    and finally the handedness of the orientation. Defaults to
-    "GDORIENT INC INC INC DOWN RIGHT /".
-    """
-
-    i_order: Order = Order.INCREASING
-    j_order: Order = Order.INCREASING
-    k_order: Order = Order.INCREASING
-    z_direction: Orientation = Orientation.DOWN
-    handedness: Handedness = Handedness.RIGHT
-
-
-@dataclass
-class GridUnit(GrdeclKeyword):
-    """Defines the units used for grid dimensions.
-
-    The first value is a string describing the units used, defaults to METRES,
-    known accepted other units are FIELD and LAB. The last value describes
-    whether the measurements are relative to the map or to the origin of
-    MAPAXES.
-    """
-
-    unit: Units = Units.METRES
-    grid_relative: GridRelative = GridRelative.ORIGIN
-
-
-@dataclass
-class MapAxes(GrdeclKeyword):
-    """The mapaxes keyword gives the local coordinate system of the map.
-
-    The map coordinate system is given by a point on the y line, the origin and
-    a point on the x line. ie. The usual coordinate system is given by "MAPAXES
-    0 1 0 0 1 0 /" where the two first values is a point on the y line, the
-    middle two values is the origin, and the last two values is a point on the
-    x line.
-    """
-
-    y_line: Tuple[float, float] = (0.0, 1.0)
-    origin: Tuple[float, float] = (0.0, 0.0)
-    x_line: Tuple[float, float] = (1.0, 0.0)
-
-    def to_grdecl(self) -> List[float]:
-        return list(self.y_line) + list(self.origin) + list(self.x_line)
-
-    def to_bgrdecl(self) -> List[float]:
-        return np.array(self.to_grdecl(), dtype=np.float32)
-
-    def in_units(self, old_units, new_units):
-        factor = old_units.conversion_factor(new_units)
-        y_line = (self.y_line[0] * factor, self.y_line[1] * factor)
-        x_line = (self.x_line[0] * factor, self.x_line[1] * factor)
-        origin = (self.origin[0] * factor, self.origin[1] * factor)
-        return MapAxes(y_line, origin, x_line)
-
-    @classmethod
-    def from_bgrdecl(cls, values: List[Union[float, str]]):
-        return cls.from_grdecl(values)
-
-    @classmethod
-    def from_grdecl(cls, values: List[Union[float, str]]):
-        if len(values) != 6:
-            raise ValueError("MAPAXES must contain 6 values")
-        return cls(
-            (float(values[0]), float(values[1])),
-            (float(values[2]), float(values[3])),
-            (float(values[4]), float(values[5])),
-        )
-
-
-@unique
-class CoordinateType(Enum):
-    """The coordinate system type given in the SPECGRID keyword.
-
-    This is given by either T or F in the last value of SPECGRID, meaning
-    either cylindrical or cartesian coordinates respectively.
-    """
-
-    CARTESIAN = auto()
-    CYLINDRICAL = auto()
-
-    def to_grdecl(self) -> str:
-        if self == CoordinateType.CARTESIAN:
-            return "F"
-        else:
-            return "T"
-
-    def to_bgrdecl(self) -> int:
-        if self == CoordinateType.CARTESIAN:
-            return 0
-        else:
-            return 1
-
-    @classmethod
-    def from_bgrdecl(cls, coord_value: int):
-        if coord_value == 0:
-            return cls.CARTESIAN
-        else:
-            return cls.CYLINDRICAL
-
-    @classmethod
-    def from_grdecl(cls, coord_string: str):
-        if match_keyword(coord_string, "F"):
-            return cls.CARTESIAN
-        if match_keyword(coord_string, "T"):
-            return cls.CYLINDRICAL
-        raise ValueError(f"Unknown coordinate type {coord_string}")
-
-
-def transform_xtgeo_coord_by_mapaxes(mapaxes: MapAxes, coord: np.ndarray):
-    """Transforms xtgeo coord values by mapaxes.
-
-    The mapaxes keyword in a grdecl file defines a new coordinate system by
-    which x and y values are to be interpreted. The given xtgeo coord
-    values are transformed from the local coordinate system defined by
-    mapaxes to global coordinates.
-    """
-    x_point = mapaxes.x_line
-    y_point = mapaxes.y_line
-    origin = mapaxes.origin
-
-    x_axis = np.array(x_point) - origin
-    y_axis = np.array(y_point) - origin
-
-    x_unit = x_axis / np.linalg.norm(x_axis)
-    y_unit = y_axis / np.linalg.norm(y_axis)
-
-    coord[:, :, (0, 1)] = (
-        origin
-        + coord[:, :, 0, np.newaxis] * x_unit
-        + coord[:, :, 1, np.newaxis] * y_unit
-    )
-    coord[:, :, (3, 4)] = (
-        origin
-        + coord[:, :, 3, np.newaxis] * x_unit
-        + coord[:, :, 4, np.newaxis] * y_unit
-    )
-
-    return coord
-
-
-def inverse_transform_xtgeo_coord_by_mapaxes(mapaxes: MapAxes, coord: np.ndarray):
-    """Inversely transforms xtgeo coord values by mapaxes.
-
-    The inverse operation of transform_xtgeo_coord_by_mapaxes.
-    """
-    x_point = mapaxes.x_line
-    y_point = mapaxes.y_line
-    origin = mapaxes.origin
-
-    x_axis = np.array(x_point) - origin
-    y_axis = np.array(y_point) - origin
-
-    x_unit = x_axis / np.linalg.norm(x_axis)
-    y_unit = y_axis / np.linalg.norm(y_axis)
-
-    coord[:, :, (0, 1)] -= np.array(origin)
-    coord[:, :, (3, 4)] -= np.array(origin)
-
-    inv_transform = np.linalg.inv(np.transpose([x_unit, y_unit]))
-
-    # The following index manipulation is
-    # an optimized version of
-
-    # nx, ny, _ = coord.shape
-    # for i in range(nx):
-    #    for j in range(ny):
-    #        coord[i, j, (0, 1)] = inv_transform @ coord[i, j, (0, 1)]
-    #        coord[i, j, (3, 4)] = inv_transform @ coord[i, j, (3, 4)]
-    coord[:, :, (0, 1)] = (
-        inv_transform[np.newaxis, np.newaxis, :, :] @ coord[:, :, (0, 1), np.newaxis]
-    )[:, :, :, 0]
-    coord[:, :, (3, 4)] = (
-        inv_transform[np.newaxis, np.newaxis, :, :] @ coord[:, :, (3, 4), np.newaxis]
-    )[:, :, :, 0]
-    return coord
-
-
-class EclGrid(ABC):
-    """
-    The main keywords that describe a grdecl grid is COORD, ZCORN and ACTNUM.
-
-    The grid is made up of nx*ny*nz cells in three corresponding dimensions.
-    The number of cells in each direction is described in the SPECGRID keyword.
-
-    The values in COORD, ZCORN and ACTNUM are stored flattened in F-order and
-    have dimensions (nx+1,ny+1,6), (nx,2,ny,2,nz,2), and (nx,ny,nz) respectively.
-
-    COORD and ZCORN descibe a corner point geometry for the grid. There is a
-    straight line from the bottom to the top of the grid on which the corners
-    of each grid lie. COORD describe the top and bottom (x,y,z) values of these
-    corner lines, hence, it contains six floats for each corner line.
-
-    ZCORN has 8 values for each grid, which describes the z-value (height) at
-    which that cells corners intersect with the corresponding corner line. The
-    order of corners is  "left" before "right" in the second dimension of
-    ZCORN, "near"  before "far" in the fourth dimension , and "upper" before
-    "bottom" in the last dimension. Note that this orientation assumes,
-    increasing first dimension as to the "right", increasing second dimension
-    towards "far", and increasing third dimension as towards "bottom".
-
-    The topology is such that, assuming no gaps between cells, the (i,j,k)th
-    cell and the (i+1,j+1,k+1)th cell share the upper near left corner of the
-    (i+1,j+1,k+1)th cell which is the lower far right corner of the (i,j,k)th
-    cell.
-
-    ACTNUM describes the active status of each cell. For simulations without
-    dual porosity or thermal, 0 means inactive, 1 means active and other values
-    are not used. For dual porosity, 0 means inactive, 1 means matrix only,
-    2 means fracture only, and 3 means both fracture and matrix. For thermal
-    simulations, 0 means inactive, 1 means active, 2 means rock volume only,
-    3 means pore volume only.
-    """
-
-    @property
-    @abstractmethod
-    def coord(self) -> np.ndarray:
-        pass
-
-    @property
-    @abstractmethod
-    def zcorn(self) -> np.ndarray:
-        pass
-
-    @property
-    @abstractmethod
-    def actnum(self) -> Optional[np.ndarray]:
-        pass
-
-    def __eq__(self, other) -> bool:
-        if not isinstance(other, EclGrid):
-            return False
-        return (
-            (
-                (self.actnum is None and other.actnum is None)
-                or np.array_equal(self.actnum, other.actnum)
-            )
-            and np.array_equal(self.coord, other.coord)
-            and np.array_equal(self.zcorn, other.zcorn)
-        )
-
-    @property
-    @abstractmethod
-    def is_map_relative(self) -> bool:
-        pass
-
-    @property
-    @abstractmethod
-    def mapaxes(self) -> Optional[MapAxes]:
-        pass
-
-    @property
-    @abstractmethod
-    def dimensions(self) -> Tuple[int, int, int]:
-        pass
-
-    @property
-    @abstractmethod
-    def map_axis_units(self) -> Units:
-        pass
-
-    @property
-    @abstractmethod
-    def grid_units(self) -> Units:
-        pass
-
-    @abstractmethod
-    def _check_xtgeo_compatible(self):
-        pass
-
-    def convert_grid_units(self, units):
-        """Converts the units of the grid
-        Args:
-            units: The unit to convert to.
-
-        After convert_grid_units is called, `EclGrid.grid_units == units`.
-
-        """
-        old_grid_units = self.grid_units
-        factor = old_grid_units.conversion_factor(units)
-        self.coord *= factor
-        self.zcorn *= factor
-        self.grid_units = units
-
-    @staticmethod
-    def valid_mapaxes(mapaxes: MapAxes) -> bool:
-        y_line = mapaxes.y_line
-        x_line = mapaxes.x_line
-        origin = mapaxes.origin
-        x_axis = np.array(x_line) - origin
-        y_axis = np.array(y_line) - origin
-
-        return np.linalg.norm(x_axis) > 1e-5 and np.linalg.norm(y_axis) > 1e-5
-
-    def _relative_to_transform(self, xtgeo_coord, relative_to=GridRelative.MAP):
-        """Handle relative transform of xtgeo_coord()."""
-        mapaxes = self.mapaxes
-        has_mapaxes = True
-        if self.mapaxes is None:
-            mapaxes = MapAxes()
-            has_mapaxes = False
-        axis_units = self.map_axis_units
-
-        has_axis_units = True
-        if axis_units is None:
-            axis_units = self.grid_units
-            has_axis_units = False
-
-        if has_mapaxes and not has_axis_units:
-            warnings.warn(
-                "Axis units specification is missing in input, assuming that no "
-                "unit conversion is necessary"
-            )
-
-        if relative_to == GridRelative.MAP and not self.is_map_relative:
-            xtgeo_coord *= self.grid_units.conversion_factor(axis_units)
-            xtgeo_coord = transform_xtgeo_coord_by_mapaxes(mapaxes, xtgeo_coord)
-
-        elif relative_to == GridRelative.ORIGIN and self.is_map_relative:
-            mapaxes = mapaxes.in_units(axis_units, self.grid_units)
-            xtgeo_coord = inverse_transform_xtgeo_coord_by_mapaxes(mapaxes, xtgeo_coord)
-
-        return xtgeo_coord
-
-    def xtgeo_coord(self, relative_to=GridRelative.MAP):
-        """
-        Args:
-            relative_to: Specifies the axis system the coords should be
-            relative to, either map or grid. Defaults to map. If relative_to is
-            GridRelative.MAP then the resulting units are that of map_axis_units.
-        Returns:
-            coord in xtgeo format.
-        """
-        self._check_xtgeo_compatible()
-        nx, ny, _ = self.dimensions
-
-        xtgeo_coord = (
-            np.swapaxes(self.coord.reshape((ny + 1, nx + 1, 6)), 0, 1)
-            .astype(np.float64)
-            .copy()
-        )
-        xtgeo_coord = self._relative_to_transform(xtgeo_coord, relative_to)
-        return np.ascontiguousarray(xtgeo_coord)
-
-    def xtgeo_actnum(self):
-        """
-        Returns:
-            actnum in xtgeo format.
-        """
-        self._check_xtgeo_compatible()
-        nx, ny, nz = self.dimensions
-        if self.actnum is None:
-            return np.ones(shape=(nx, ny, nz), dtype=np.int32)
-        activity_number = self.actnum.reshape((nx, ny, nz), order="F")
-        return np.ascontiguousarray(activity_number)
-
-    def xtgeo_zcorn(self, relative_to=GridRelative.MAP):
-        """
-            relative_to: Specifies the axis system the zcorn should be
-            relative to, either map or origin. Defaults to map. For zcorn
-            this only affects which units zcorn will be in, grid units for
-            relative to origin, map units for relative to map.
-        Returns:
-            zcorn in xtgeo format.
-        """
-        self._check_xtgeo_compatible()
-        nx, ny, nz = self.dimensions
-        zcorn = self.zcorn.reshape((2, nx, 2, ny, 2, nz), order="F")
-
-        if not np.allclose(
-            zcorn[:, :, :, :, 1, : nz - 1], zcorn[:, :, :, :, 0, 1:], atol=1e-2
-        ):
-            warnings.warn(
-                "An Eclipse style grid with vertical ZCORN splits "
-                "or overlaps between vertical neighbouring cells is detected. XTGeo "
-                "will import the grid as if the cell layers are connected, "
-                "hence check result carefully. "
-                "(Note also that this check both active and inactive cells!)",
-                UserWarning,
-            )
-
-        result = np.zeros((nx + 1, ny + 1, nz + 1, 4), dtype=np.float32)
-
-        # xtgeo uses 4 z values per i,j,k to mean the 4 z values of
-        # adjacent cells for the cornerline at position i,j,k assuming
-        # no difference in z values between upper and lower cells. In
-        # the order sw,se,nw,ne.
-
-        # In grdecl, there are 8 zvalues per i,j,k meaning the z values
-        # of each corner for the cell at i,j,k. In
-        # the order "left" (west) before "right" (east) , "near" (south)
-        # before "far" (north) , "upper" before "bottom"
-
-        # set the nw value of cornerline i+1,j to
-        # the near right corner of cell i,j
-        result[1:, :ny, 0:nz, 2] = zcorn[1, :, 0, :, 0, :]
-        result[1:, :ny, nz, 2] = zcorn[1, :, 0, :, 1, nz - 1]
-
-        # set the ne value of cornerline i,j to
-        # the near left corner of cell i,j
-        result[:nx, :ny, 0:nz, 3] = zcorn[0, :, 0, :, 0, :]
-        result[:nx, :ny, nz, 3] = zcorn[0, :, 0, :, 1, nz - 1]
-
-        # set the sw value of cornerline i+1,j+1 to
-        # the far right corner of cell i,j to
-        result[1:, 1:, 0:nz, 0] = zcorn[1, :, 1, :, 0, :]
-        result[1:, 1:, nz, 0] = zcorn[1, :, 1, :, 1, nz - 1]
-
-        # set the se value of cornerline i,j+1 to
-        # the far left corner of cell i,j
-        result[:nx, 1:, 0:nz, 1] = zcorn[0, :, 1, :, 0, :]
-        result[:nx, 1:, nz, 1] = zcorn[0, :, 1, :, 1, nz - 1]
-
-        self.duplicate_insignificant_xtgeo_zcorn(result)
-
-        axis_units = self.map_axis_units
-        if axis_units is None:
-            axis_units = self.grid_units
-        if relative_to == GridRelative.MAP and not self.is_map_relative:
-            result *= self.grid_units.conversion_factor(self.map_axis_units)
-
-        return np.ascontiguousarray(result)
-
-    def duplicate_insignificant_xtgeo_zcorn(self, zcorn: np.ndarray):
-        """Duplicates values on the faces and corners of the grid.
-
-        The xtgeo format has 4 z values for all cornerlines, refering
-        to the z value for the corresponding corner of the cell that is
-        sw, se, nw and ne of the cornerline. However, for the cornerlines
-        that are on the boundary of the grid, there might be no such cell, ie.
-        north of the northernmost cornerlines there are no cells. These are
-        then duplicated of corresponding cells in the opposite direction.
-
-        """
-        nx, ny, nz = self.dimensions
-
-        # south of the sw->se face is duplicate
-        # of the north values
-        zcorn[1:nx, 0, :, 0] = zcorn[1:nx, 0, :, 2]
-        zcorn[1:nx, 0, :, 1] = zcorn[1:nx, 0, :, 3]
-
-        # vertical sw corner line is duplicates of
-        # the ne value
-        zcorn[0, 0, :, 0] = zcorn[0, 0, :, 3]
-        zcorn[0, 0, :, 1] = zcorn[0, 0, :, 3]
-        zcorn[0, 0, :, 2] = zcorn[0, 0, :, 3]
-
-        # east values of the se->ne face
-        # is duplicates of the corresponding
-        # west values
-        zcorn[nx, 1:ny, :, 1] = zcorn[nx, 1:ny, :, 0]
-        zcorn[nx, 1:ny, :, 3] = zcorn[nx, 1:ny, :, 2]
-
-        # vertical se corner line is all duplicates
-        # of its nw value
-        zcorn[nx, 0, :, 0] = zcorn[nx, 0, :, 2]
-        zcorn[nx, 0, :, 1] = zcorn[nx, 0, :, 2]
-        zcorn[nx, 0, :, 3] = zcorn[nx, 0, :, 2]
-
-        # north values of the nw->ne face is duplicates
-        # of the corresponding south values
-        zcorn[1:nx, ny, :, 2] = zcorn[1:nx, ny, :, 0]
-        zcorn[1:nx, ny, :, 3] = zcorn[1:nx, ny, :, 1]
-
-        # vertical nw corner line is all duplicates
-        # of the se value
-        zcorn[0, ny, :, 0] = zcorn[0, ny, :, 1]
-        zcorn[0, ny, :, 2] = zcorn[0, ny, :, 1]
-        zcorn[0, ny, :, 3] = zcorn[0, ny, :, 1]
-
-        # west values of the sw->nw face is duplicates
-        # of corresponding east values
-        zcorn[0, 1:ny, :, 0] = zcorn[0, 1:ny, :, 1]
-        zcorn[0, 1:ny, :, 2] = zcorn[0, 1:ny, :, 3]
-
-        # vertical ne corner line is all duplicates
-        # of the sw value
-        zcorn[nx, ny, :, 1] = zcorn[nx, ny, :, 0]
-        zcorn[nx, ny, :, 2] = zcorn[nx, ny, :, 0]
-        zcorn[nx, ny, :, 3] = zcorn[nx, ny, :, 0]
-
-    @classmethod
-    @abstractmethod
-    def default_settings_grid(
-        cls,
-        coord: np.ndarray,
-        zcorn: np.ndarray,
-        actnum: Optional[np.ndarray],
-        size: Tuple[int, int, int],
-    ):
-        pass
-
-    @classmethod
-    def from_xtgeo_grid(cls, xtgeo_grid):
-        xtgeo_grid._xtgformat2()
-
-        nx, ny, nz = xtgeo_grid.dimensions
-        actnum = xtgeo_grid._actnumsv.reshape(nx, ny, nz)
-        actnum = actnum.ravel(order="F")
-        if np.all(actnum == 1):
-            actnum = None
-        coord = np.ascontiguousarray(np.swapaxes(xtgeo_grid._coordsv, 0, 1).ravel())
-        zcorn = np.zeros((2, nx, 2, ny, 2, nz))
-        xtgeo_zcorn = xtgeo_grid._zcornsv.reshape((nx + 1, ny + 1, nz + 1, 4))
-
-        # This is the reverse operation of that of xtgeo_zcorn,
-        # see that function for description of operations.
-
-        # set the nw value of cornerline i+1,j to
-        # the near right corner of cell i,j
-        zcorn[1, :, 0, :, 1, :] = xtgeo_zcorn[1:, :ny, 1:, 2]
-        zcorn[1, :, 0, :, 0, :] = xtgeo_zcorn[1:, :ny, :nz, 2]
-
-        # set the ne value of cornerline i,j to
-        # the near left corner of cell i,j
-        zcorn[0, :, 0, :, 1, :] = xtgeo_zcorn[:nx, :ny, 1:, 3]
-        zcorn[0, :, 0, :, 0, :] = xtgeo_zcorn[:nx, :ny, :nz, 3]
-
-        # set the sw value of cornerline i+1,j+1 to
-        # the far right corner of cell i,j to
-        zcorn[1, :, 1, :, 1, :] = xtgeo_zcorn[1:, 1:, 1:, 0]
-        zcorn[1, :, 1, :, 0, :] = xtgeo_zcorn[1:, 1:, :nz, 0]
-
-        # set the se value of cornerline i,j+1 to
-        # the far left corner of cell i,j
-        zcorn[0, :, 1, :, 1, :] = xtgeo_zcorn[:nx, 1:, 1:, 1]
-        zcorn[0, :, 1, :, 0, :] = xtgeo_zcorn[:nx, 1:, :nz, 1]
-
-        zcorn = zcorn.ravel(order="F")
-
-        result = cls.default_settings_grid(
-            coord=coord,
-            zcorn=zcorn,
-            actnum=actnum,
-            size=(nx, ny, nz),
-        )
-
-        if xtgeo_grid.units is not None:
-            result.grid_units = xtgeo_grid.units
-            result.map_axis_units = xtgeo_grid.units
-
-        return result
+import warnings
+from abc import ABC, abstractmethod
+from dataclasses import astuple, dataclass, fields
+from enum import Enum, auto, unique
+from typing import Any, List, Optional, Tuple, Union
+
+import numpy as np
+from scipy.constants import foot
+
+from ._grdecl_format import match_keyword
+
+
+@unique
+class Units(Enum):
+    METRES = auto()
+    CM = auto()
+    FEET = auto()
+
+    def conversion_factor(self, other):
+        "Conversion factor from one unit to another"
+        result = 1.0
+        if self == other:
+            return result
+        if other == Units.FEET:
+            result *= 1 / foot
+        if other == Units.CM:
+            result *= 1e2
+        if self == Units.FEET:
+            result *= foot
+        if self == Units.CM:
+            result *= 1e-2
+        return result
+
+    def to_grdecl(self):
+        return self.name
+
+    def to_bgrdecl(self):
+        return self.to_grdecl().ljust(8)
+
+    @classmethod
+    def from_grdecl(cls, unit_string):
+        if match_keyword(unit_string, "METRES"):
+            return cls.METRES
+        if match_keyword(unit_string, "FEET"):
+            return cls.FEET
+        if match_keyword(unit_string, "CM"):
+            return cls.CM
+        raise ValueError(f"Unknown unit string {unit_string}")
+
+    @classmethod
+    def from_bgrdecl(cls, unit_string):
+        if isinstance(unit_string, bytes):
+            return cls.from_grdecl(unit_string.decode("ascii"))
+        return cls.from_grdecl(unit_string)
+
+
+@unique
+class GridRelative(Enum):
+    """GridRelative is the second value given GRIDUNIT keyword.
+
+    MAP means map relative units, while
+    leaving it blank means relative to the origin given by the
+    MAPAXES keyword.
+    """
+
+    MAP = auto()
+    ORIGIN = auto()
+
+    def to_grdecl(self) -> str:
+        if self == GridRelative.MAP:
+            return "MAP"
+        else:
+            return ""
+
+    def to_bgrdecl(self) -> str:
+        return self.to_grdecl().ljust(8)
+
+    @classmethod
+    def from_grdecl(cls, unit_string: str):
+        if match_keyword(unit_string, "MAP"):
+            return cls.MAP
+        else:
+            return cls.ORIGIN
+
+    @classmethod
+    def from_bgrdecl(cls, unit_string):
+        if isinstance(unit_string, bytes):
+            return cls.from_grdecl(unit_string.decode("ascii"))
+        return cls.from_grdecl(unit_string)
+
+
+@dataclass
+class GrdeclKeyword:
+    """An abstract grdecl keyword.
+
+    Gives a general implementation of to/from grdecl which recurses on
+    fields. Ie. a dataclass such as
+    >>> class A(GrdeclKeyword):
+    ...     ...
+    >>> class B(GrdeclKeyword):
+    ...     ...
+
+    >>> @dataclass
+    ... class MyKeyword(GrdeclKeyword):
+    ...     field1: A
+    ...     field2: B
+
+    will have a to_grdecl method that will be similar to
+
+    >>> def to_grdecl(self):
+    ...     return [self.field1.to_grdecl(), self.field2.to_grdecl]
+
+    Similarly from_grdecl will call fields from_grdecl
+    to construct the object
+
+    >>> @classmethod
+    ... def from_grdecl(cls, values):
+    ...     return cls(A.from_grdecl(values[0]), B.from_grdecl(values[1]))
+    """
+
+    def to_grdecl(self) -> List[Any]:
+        """Convert the keyword to list of grdecl keyword values.
+        Returns:
+            list of values of the given keyword. ie. The
+            keyword read from "SPECGRID 1 1 1 F" should return
+            [1,1,1,CoordinateType.CYLINDRICAL]
+        """
+        return [value.to_grdecl() for value in astuple(self)]
+
+    def to_bgrdecl(self) -> List[Any]:
+        return [value.to_bgrdecl() for value in astuple(self)]
+
+    @classmethod
+    def from_bgrdecl(cls, values):
+        object_types = [f.type for f in fields(cls)]
+        return cls(*[typ.from_bgrdecl(val) for val, typ in zip(values, object_types)])
+
+    @classmethod
+    def from_grdecl(cls, values):
+        """Convert list of grdecl keyword values to a keyword.
+        Args:
+            values(list): list of values given after the keyword in
+                the grdecl file.
+        Returns:
+            A GrdeclKeyword constructed from the given values.
+        """
+        object_types = [f.type for f in fields(cls)]
+        return cls(*[typ.from_grdecl(val) for val, typ in zip(values, object_types)])
+
+
+@unique
+class Order(Enum):
+    """Either increasing or decreasing.
+
+    Used for the grdecl keywords INC and DEC
+    respectively.
+    """
+
+    INCREASING = auto()
+    DECREASING = auto()
+
+    def to_grdecl(self) -> str:
+        return str(self.name)[0:3]
+
+    def to_bgrdecl(self) -> str:
+        return self.to_grdecl().ljust(8)
+
+    @classmethod
+    def from_grdecl(cls, order_string):
+        if match_keyword(order_string, "INC"):
+            return cls.INCREASING
+        if match_keyword(order_string, "DEC"):
+            return cls.DECREASING
+
+    @classmethod
+    def from_bgrdecl(cls, unit_string: Union[bytes, str]):
+        if isinstance(unit_string, bytes):
+            return cls.from_grdecl(unit_string.decode("ascii"))
+        return cls.from_grdecl(unit_string)
+
+
+@unique
+class Handedness(Enum):
+    """The handedness of an orientation.
+
+    Eiter left handed or right handed.  Used for the grdecl keywords LEFT and
+    RIGHT.
+    """
+
+    LEFT = auto()
+    RIGHT = auto()
+
+    def to_grdecl(self) -> str:
+        return self.name
+
+    def to_bgrdecl(self) -> str:
+        return self.to_grdecl().ljust(8)
+
+    @classmethod
+    def from_grdecl(cls, orientation_string: str):
+        if match_keyword(orientation_string, "LEFT"):
+            return cls.LEFT
+        if match_keyword(orientation_string, "RIGHT"):
+            return cls.RIGHT
+        raise ValueError(f"Unknown handedness string {orientation_string}")
+
+    @classmethod
+    def from_bgrdecl(cls, unit_string: Union[bytes, str]):
+        if isinstance(unit_string, bytes):
+            return cls.from_grdecl(unit_string.decode("ascii"))
+        return cls.from_grdecl(unit_string)
+
+
+@unique
+class Orientation(Enum):
+    """Either up or down, for the grdecl keywords UP and DOWN."""
+
+    UP = auto()
+    DOWN = auto()
+
+    def to_grdecl(self) -> str:
+        return self.name
+
+    def to_bgrdecl(self) -> str:
+        return self.to_grdecl().ljust(8)
+
+    @classmethod
+    def from_grdecl(cls, orientation_string: str):
+        if match_keyword(orientation_string, "UP"):
+            return cls.UP
+        if match_keyword(orientation_string, "DOWN"):
+            return cls.DOWN
+        raise ValueError(f"Unknown orientation string {orientation_string}")
+
+    @classmethod
+    def from_bgrdecl(cls, unit_string: Union[bytes, str]):
+        if isinstance(unit_string, bytes):
+            return cls.from_grdecl(unit_string.decode("ascii"))
+        return cls.from_grdecl(unit_string)
+
+
+@dataclass
+class GdOrient(GrdeclKeyword):
+    """The GDORIENT keyword gives the orientation of the grid.
+
+    The three first values is either increasing or decreasing
+    depending on whether the corresponding dimension has increasing
+    or decreasing coordinates. Then comes the direction of the z dimension,
+    and finally the handedness of the orientation. Defaults to
+    "GDORIENT INC INC INC DOWN RIGHT /".
+    """
+
+    i_order: Order = Order.INCREASING
+    j_order: Order = Order.INCREASING
+    k_order: Order = Order.INCREASING
+    z_direction: Orientation = Orientation.DOWN
+    handedness: Handedness = Handedness.RIGHT
+
+
+@dataclass
+class GridUnit(GrdeclKeyword):
+    """Defines the units used for grid dimensions.
+
+    The first value is a string describing the units used, defaults to METRES,
+    known accepted other units are FIELD and LAB. The last value describes
+    whether the measurements are relative to the map or to the origin of
+    MAPAXES.
+    """
+
+    unit: Units = Units.METRES
+    grid_relative: GridRelative = GridRelative.ORIGIN
+
+
+@dataclass
+class MapAxes(GrdeclKeyword):
+    """The mapaxes keyword gives the local coordinate system of the map.
+
+    The map coordinate system is given by a point on the y line, the origin and
+    a point on the x line. ie. The usual coordinate system is given by "MAPAXES
+    0 1 0 0 1 0 /" where the two first values is a point on the y line, the
+    middle two values is the origin, and the last two values is a point on the
+    x line.
+    """
+
+    y_line: Tuple[float, float] = (0.0, 1.0)
+    origin: Tuple[float, float] = (0.0, 0.0)
+    x_line: Tuple[float, float] = (1.0, 0.0)
+
+    def to_grdecl(self) -> List[float]:
+        return list(self.y_line) + list(self.origin) + list(self.x_line)
+
+    def to_bgrdecl(self) -> List[float]:
+        return np.array(self.to_grdecl(), dtype=np.float32)
+
+    def in_units(self, old_units, new_units):
+        factor = old_units.conversion_factor(new_units)
+        y_line = (self.y_line[0] * factor, self.y_line[1] * factor)
+        x_line = (self.x_line[0] * factor, self.x_line[1] * factor)
+        origin = (self.origin[0] * factor, self.origin[1] * factor)
+        return MapAxes(y_line, origin, x_line)
+
+    @classmethod
+    def from_bgrdecl(cls, values: List[Union[float, str]]):
+        return cls.from_grdecl(values)
+
+    @classmethod
+    def from_grdecl(cls, values: List[Union[float, str]]):
+        if len(values) != 6:
+            raise ValueError("MAPAXES must contain 6 values")
+        return cls(
+            (float(values[0]), float(values[1])),
+            (float(values[2]), float(values[3])),
+            (float(values[4]), float(values[5])),
+        )
+
+
+@unique
+class CoordinateType(Enum):
+    """The coordinate system type given in the SPECGRID keyword.
+
+    This is given by either T or F in the last value of SPECGRID, meaning
+    either cylindrical or cartesian coordinates respectively.
+    """
+
+    CARTESIAN = auto()
+    CYLINDRICAL = auto()
+
+    def to_grdecl(self) -> str:
+        if self == CoordinateType.CARTESIAN:
+            return "F"
+        else:
+            return "T"
+
+    def to_bgrdecl(self) -> int:
+        if self == CoordinateType.CARTESIAN:
+            return 0
+        else:
+            return 1
+
+    @classmethod
+    def from_bgrdecl(cls, coord_value: int):
+        if coord_value == 0:
+            return cls.CARTESIAN
+        else:
+            return cls.CYLINDRICAL
+
+    @classmethod
+    def from_grdecl(cls, coord_string: str):
+        if match_keyword(coord_string, "F"):
+            return cls.CARTESIAN
+        if match_keyword(coord_string, "T"):
+            return cls.CYLINDRICAL
+        raise ValueError(f"Unknown coordinate type {coord_string}")
+
+
+def transform_xtgeo_coord_by_mapaxes(mapaxes: MapAxes, coord: np.ndarray):
+    """Transforms xtgeo coord values by mapaxes.
+
+    The mapaxes keyword in a grdecl file defines a new coordinate system by
+    which x and y values are to be interpreted. The given xtgeo coord
+    values are transformed from the local coordinate system defined by
+    mapaxes to global coordinates.
+    """
+    x_point = mapaxes.x_line
+    y_point = mapaxes.y_line
+    origin = mapaxes.origin
+
+    x_axis = np.array(x_point) - origin
+    y_axis = np.array(y_point) - origin
+
+    x_unit = x_axis / np.linalg.norm(x_axis)
+    y_unit = y_axis / np.linalg.norm(y_axis)
+
+    coord[:, :, (0, 1)] = (
+        origin
+        + coord[:, :, 0, np.newaxis] * x_unit
+        + coord[:, :, 1, np.newaxis] * y_unit
+    )
+    coord[:, :, (3, 4)] = (
+        origin
+        + coord[:, :, 3, np.newaxis] * x_unit
+        + coord[:, :, 4, np.newaxis] * y_unit
+    )
+
+    return coord
+
+
+def inverse_transform_xtgeo_coord_by_mapaxes(mapaxes: MapAxes, coord: np.ndarray):
+    """Inversely transforms xtgeo coord values by mapaxes.
+
+    The inverse operation of transform_xtgeo_coord_by_mapaxes.
+    """
+    x_point = mapaxes.x_line
+    y_point = mapaxes.y_line
+    origin = mapaxes.origin
+
+    x_axis = np.array(x_point) - origin
+    y_axis = np.array(y_point) - origin
+
+    x_unit = x_axis / np.linalg.norm(x_axis)
+    y_unit = y_axis / np.linalg.norm(y_axis)
+
+    coord[:, :, (0, 1)] -= np.array(origin)
+    coord[:, :, (3, 4)] -= np.array(origin)
+
+    inv_transform = np.linalg.inv(np.transpose([x_unit, y_unit]))
+
+    # The following index manipulation is
+    # an optimized version of
+
+    # nx, ny, _ = coord.shape
+    # for i in range(nx):
+    #    for j in range(ny):
+    #        coord[i, j, (0, 1)] = inv_transform @ coord[i, j, (0, 1)]
+    #        coord[i, j, (3, 4)] = inv_transform @ coord[i, j, (3, 4)]
+    coord[:, :, (0, 1)] = (
+        inv_transform[np.newaxis, np.newaxis, :, :] @ coord[:, :, (0, 1), np.newaxis]
+    )[:, :, :, 0]
+    coord[:, :, (3, 4)] = (
+        inv_transform[np.newaxis, np.newaxis, :, :] @ coord[:, :, (3, 4), np.newaxis]
+    )[:, :, :, 0]
+    return coord
+
+
+class EclGrid(ABC):
+    """
+    The main keywords that describe a grdecl grid is COORD, ZCORN and ACTNUM.
+
+    The grid is made up of nx*ny*nz cells in three corresponding dimensions.
+    The number of cells in each direction is described in the SPECGRID keyword.
+
+    The values in COORD, ZCORN and ACTNUM are stored flattened in F-order and
+    have dimensions (nx+1,ny+1,6), (nx,2,ny,2,nz,2), and (nx,ny,nz) respectively.
+
+    COORD and ZCORN descibe a corner point geometry for the grid. There is a
+    straight line from the bottom to the top of the grid on which the corners
+    of each grid lie. COORD describe the top and bottom (x,y,z) values of these
+    corner lines, hence, it contains six floats for each corner line.
+
+    ZCORN has 8 values for each grid, which describes the z-value (height) at
+    which that cells corners intersect with the corresponding corner line. The
+    order of corners is  "left" before "right" in the second dimension of
+    ZCORN, "near"  before "far" in the fourth dimension , and "upper" before
+    "bottom" in the last dimension. Note that this orientation assumes,
+    increasing first dimension as to the "right", increasing second dimension
+    towards "far", and increasing third dimension as towards "bottom".
+
+    The topology is such that, assuming no gaps between cells, the (i,j,k)th
+    cell and the (i+1,j+1,k+1)th cell share the upper near left corner of the
+    (i+1,j+1,k+1)th cell which is the lower far right corner of the (i,j,k)th
+    cell.
+
+    ACTNUM describes the active status of each cell. For simulations without
+    dual porosity or thermal, 0 means inactive, 1 means active and other values
+    are not used. For dual porosity, 0 means inactive, 1 means matrix only,
+    2 means fracture only, and 3 means both fracture and matrix. For thermal
+    simulations, 0 means inactive, 1 means active, 2 means rock volume only,
+    3 means pore volume only.
+    """
+
+    @property
+    @abstractmethod
+    def coord(self) -> np.ndarray:
+        pass
+
+    @property
+    @abstractmethod
+    def zcorn(self) -> np.ndarray:
+        pass
+
+    @property
+    @abstractmethod
+    def actnum(self) -> Optional[np.ndarray]:
+        pass
+
+    def __eq__(self, other) -> bool:
+        if not isinstance(other, EclGrid):
+            return False
+        return (
+            (
+                (self.actnum is None and other.actnum is None)
+                or np.array_equal(self.actnum, other.actnum)
+            )
+            and np.array_equal(self.coord, other.coord)
+            and np.array_equal(self.zcorn, other.zcorn)
+        )
+
+    @property
+    @abstractmethod
+    def is_map_relative(self) -> bool:
+        pass
+
+    @property
+    @abstractmethod
+    def mapaxes(self) -> Optional[MapAxes]:
+        pass
+
+    @property
+    @abstractmethod
+    def dimensions(self) -> Tuple[int, int, int]:
+        pass
+
+    @property
+    @abstractmethod
+    def map_axis_units(self) -> Units:
+        pass
+
+    @property
+    @abstractmethod
+    def grid_units(self) -> Units:
+        pass
+
+    @abstractmethod
+    def _check_xtgeo_compatible(self):
+        pass
+
+    def convert_grid_units(self, units):
+        """Converts the units of the grid
+        Args:
+            units: The unit to convert to.
+
+        After convert_grid_units is called, `EclGrid.grid_units == units`.
+
+        """
+        old_grid_units = self.grid_units
+        factor = old_grid_units.conversion_factor(units)
+        self.coord *= factor
+        self.zcorn *= factor
+        self.grid_units = units
+
+    @staticmethod
+    def valid_mapaxes(mapaxes: MapAxes) -> bool:
+        y_line = mapaxes.y_line
+        x_line = mapaxes.x_line
+        origin = mapaxes.origin
+        x_axis = np.array(x_line) - origin
+        y_axis = np.array(y_line) - origin
+
+        return np.linalg.norm(x_axis) > 1e-5 and np.linalg.norm(y_axis) > 1e-5
+
+    def _relative_to_transform(self, xtgeo_coord, relative_to=GridRelative.MAP):
+        """Handle relative transform of xtgeo_coord()."""
+        mapaxes = self.mapaxes
+        has_mapaxes = True
+        if self.mapaxes is None:
+            mapaxes = MapAxes()
+            has_mapaxes = False
+        axis_units = self.map_axis_units
+
+        has_axis_units = True
+        if axis_units is None:
+            axis_units = self.grid_units
+            has_axis_units = False
+
+        if has_mapaxes and not has_axis_units:
+            warnings.warn(
+                "Axis units specification is missing in input, assuming that no "
+                "unit conversion is necessary"
+            )
+
+        if relative_to == GridRelative.MAP and not self.is_map_relative:
+            xtgeo_coord *= self.grid_units.conversion_factor(axis_units)
+            xtgeo_coord = transform_xtgeo_coord_by_mapaxes(mapaxes, xtgeo_coord)
+
+        elif relative_to == GridRelative.ORIGIN and self.is_map_relative:
+            mapaxes = mapaxes.in_units(axis_units, self.grid_units)
+            xtgeo_coord = inverse_transform_xtgeo_coord_by_mapaxes(mapaxes, xtgeo_coord)
+
+        return xtgeo_coord
+
+    def xtgeo_coord(self, relative_to=GridRelative.MAP):
+        """
+        Args:
+            relative_to: Specifies the axis system the coords should be
+            relative to, either map or grid. Defaults to map. If relative_to is
+            GridRelative.MAP then the resulting units are that of map_axis_units.
+        Returns:
+            coord in xtgeo format.
+        """
+        self._check_xtgeo_compatible()
+        nx, ny, _ = self.dimensions
+
+        xtgeo_coord = (
+            np.swapaxes(self.coord.reshape((ny + 1, nx + 1, 6)), 0, 1)
+            .astype(np.float64)
+            .copy()
+        )
+        xtgeo_coord = self._relative_to_transform(xtgeo_coord, relative_to)
+        return np.ascontiguousarray(xtgeo_coord)
+
+    def xtgeo_actnum(self):
+        """
+        Returns:
+            actnum in xtgeo format.
+        """
+        self._check_xtgeo_compatible()
+        nx, ny, nz = self.dimensions
+        if self.actnum is None:
+            return np.ones(shape=(nx, ny, nz), dtype=np.int32)
+        activity_number = self.actnum.reshape((nx, ny, nz), order="F")
+        return np.ascontiguousarray(activity_number)
+
+    def xtgeo_zcorn(self, relative_to=GridRelative.MAP):
+        """
+            relative_to: Specifies the axis system the zcorn should be
+            relative to, either map or origin. Defaults to map. For zcorn
+            this only affects which units zcorn will be in, grid units for
+            relative to origin, map units for relative to map.
+        Returns:
+            zcorn in xtgeo format.
+        """
+        self._check_xtgeo_compatible()
+        nx, ny, nz = self.dimensions
+        zcorn = self.zcorn.reshape((2, nx, 2, ny, 2, nz), order="F")
+
+        if not np.allclose(
+            zcorn[:, :, :, :, 1, : nz - 1], zcorn[:, :, :, :, 0, 1:], atol=1e-2
+        ):
+            warnings.warn(
+                "An Eclipse style grid with vertical ZCORN splits "
+                "or overlaps between vertical neighbouring cells is detected. XTGeo "
+                "will import the grid as if the cell layers are connected, "
+                "hence check result carefully. "
+                "(Note also that this check both active and inactive cells!)",
+                UserWarning,
+            )
+
+        result = np.zeros((nx + 1, ny + 1, nz + 1, 4), dtype=np.float32)
+
+        # xtgeo uses 4 z values per i,j,k to mean the 4 z values of
+        # adjacent cells for the cornerline at position i,j,k assuming
+        # no difference in z values between upper and lower cells. In
+        # the order sw,se,nw,ne.
+
+        # In grdecl, there are 8 zvalues per i,j,k meaning the z values
+        # of each corner for the cell at i,j,k. In
+        # the order "left" (west) before "right" (east) , "near" (south)
+        # before "far" (north) , "upper" before "bottom"
+
+        # set the nw value of cornerline i+1,j to
+        # the near right corner of cell i,j
+        result[1:, :ny, 0:nz, 2] = zcorn[1, :, 0, :, 0, :]
+        result[1:, :ny, nz, 2] = zcorn[1, :, 0, :, 1, nz - 1]
+
+        # set the ne value of cornerline i,j to
+        # the near left corner of cell i,j
+        result[:nx, :ny, 0:nz, 3] = zcorn[0, :, 0, :, 0, :]
+        result[:nx, :ny, nz, 3] = zcorn[0, :, 0, :, 1, nz - 1]
+
+        # set the sw value of cornerline i+1,j+1 to
+        # the far right corner of cell i,j to
+        result[1:, 1:, 0:nz, 0] = zcorn[1, :, 1, :, 0, :]
+        result[1:, 1:, nz, 0] = zcorn[1, :, 1, :, 1, nz - 1]
+
+        # set the se value of cornerline i,j+1 to
+        # the far left corner of cell i,j
+        result[:nx, 1:, 0:nz, 1] = zcorn[0, :, 1, :, 0, :]
+        result[:nx, 1:, nz, 1] = zcorn[0, :, 1, :, 1, nz - 1]
+
+        self.duplicate_insignificant_xtgeo_zcorn(result)
+
+        axis_units = self.map_axis_units
+        if axis_units is None:
+            axis_units = self.grid_units
+        if relative_to == GridRelative.MAP and not self.is_map_relative:
+            result *= self.grid_units.conversion_factor(self.map_axis_units)
+
+        return np.ascontiguousarray(result)
+
+    def duplicate_insignificant_xtgeo_zcorn(self, zcorn: np.ndarray):
+        """Duplicates values on the faces and corners of the grid.
+
+        The xtgeo format has 4 z values for all cornerlines, refering
+        to the z value for the corresponding corner of the cell that is
+        sw, se, nw and ne of the cornerline. However, for the cornerlines
+        that are on the boundary of the grid, there might be no such cell, ie.
+        north of the northernmost cornerlines there are no cells. These are
+        then duplicated of corresponding cells in the opposite direction.
+
+        """
+        nx, ny, nz = self.dimensions
+
+        # south of the sw->se face is duplicate
+        # of the north values
+        zcorn[1:nx, 0, :, 0] = zcorn[1:nx, 0, :, 2]
+        zcorn[1:nx, 0, :, 1] = zcorn[1:nx, 0, :, 3]
+
+        # vertical sw corner line is duplicates of
+        # the ne value
+        zcorn[0, 0, :, 0] = zcorn[0, 0, :, 3]
+        zcorn[0, 0, :, 1] = zcorn[0, 0, :, 3]
+        zcorn[0, 0, :, 2] = zcorn[0, 0, :, 3]
+
+        # east values of the se->ne face
+        # is duplicates of the corresponding
+        # west values
+        zcorn[nx, 1:ny, :, 1] = zcorn[nx, 1:ny, :, 0]
+        zcorn[nx, 1:ny, :, 3] = zcorn[nx, 1:ny, :, 2]
+
+        # vertical se corner line is all duplicates
+        # of its nw value
+        zcorn[nx, 0, :, 0] = zcorn[nx, 0, :, 2]
+        zcorn[nx, 0, :, 1] = zcorn[nx, 0, :, 2]
+        zcorn[nx, 0, :, 3] = zcorn[nx, 0, :, 2]
+
+        # north values of the nw->ne face is duplicates
+        # of the corresponding south values
+        zcorn[1:nx, ny, :, 2] = zcorn[1:nx, ny, :, 0]
+        zcorn[1:nx, ny, :, 3] = zcorn[1:nx, ny, :, 1]
+
+        # vertical nw corner line is all duplicates
+        # of the se value
+        zcorn[0, ny, :, 0] = zcorn[0, ny, :, 1]
+        zcorn[0, ny, :, 2] = zcorn[0, ny, :, 1]
+        zcorn[0, ny, :, 3] = zcorn[0, ny, :, 1]
+
+        # west values of the sw->nw face is duplicates
+        # of corresponding east values
+        zcorn[0, 1:ny, :, 0] = zcorn[0, 1:ny, :, 1]
+        zcorn[0, 1:ny, :, 2] = zcorn[0, 1:ny, :, 3]
+
+        # vertical ne corner line is all duplicates
+        # of the sw value
+        zcorn[nx, ny, :, 1] = zcorn[nx, ny, :, 0]
+        zcorn[nx, ny, :, 2] = zcorn[nx, ny, :, 0]
+        zcorn[nx, ny, :, 3] = zcorn[nx, ny, :, 0]
+
+    @classmethod
+    @abstractmethod
+    def default_settings_grid(
+        cls,
+        coord: np.ndarray,
+        zcorn: np.ndarray,
+        actnum: Optional[np.ndarray],
+        size: Tuple[int, int, int],
+    ):
+        pass
+
+    @classmethod
+    def from_xtgeo_grid(cls, xtgeo_grid):
+        xtgeo_grid._xtgformat2()
+
+        nx, ny, nz = xtgeo_grid.dimensions
+        actnum = xtgeo_grid._actnumsv.reshape(nx, ny, nz)
+        actnum = actnum.ravel(order="F")
+        if np.all(actnum == 1):
+            actnum = None
+        coord = np.ascontiguousarray(np.swapaxes(xtgeo_grid._coordsv, 0, 1).ravel())
+        zcorn = np.zeros((2, nx, 2, ny, 2, nz))
+        xtgeo_zcorn = xtgeo_grid._zcornsv.reshape((nx + 1, ny + 1, nz + 1, 4))
+
+        # This is the reverse operation of that of xtgeo_zcorn,
+        # see that function for description of operations.
+
+        # set the nw value of cornerline i+1,j to
+        # the near right corner of cell i,j
+        zcorn[1, :, 0, :, 1, :] = xtgeo_zcorn[1:, :ny, 1:, 2]
+        zcorn[1, :, 0, :, 0, :] = xtgeo_zcorn[1:, :ny, :nz, 2]
+
+        # set the ne value of cornerline i,j to
+        # the near left corner of cell i,j
+        zcorn[0, :, 0, :, 1, :] = xtgeo_zcorn[:nx, :ny, 1:, 3]
+        zcorn[0, :, 0, :, 0, :] = xtgeo_zcorn[:nx, :ny, :nz, 3]
+
+        # set the sw value of cornerline i+1,j+1 to
+        # the far right corner of cell i,j to
+        zcorn[1, :, 1, :, 1, :] = xtgeo_zcorn[1:, 1:, 1:, 0]
+        zcorn[1, :, 1, :, 0, :] = xtgeo_zcorn[1:, 1:, :nz, 0]
+
+        # set the se value of cornerline i,j+1 to
+        # the far left corner of cell i,j
+        zcorn[0, :, 1, :, 1, :] = xtgeo_zcorn[:nx, 1:, 1:, 1]
+        zcorn[0, :, 1, :, 0, :] = xtgeo_zcorn[:nx, 1:, :nz, 1]
+
+        zcorn = zcorn.ravel(order="F")
+
+        result = cls.default_settings_grid(
+            coord=coord,
+            zcorn=zcorn,
+            actnum=actnum,
+            size=(nx, ny, nz),
+        )
+
+        if xtgeo_grid.units is not None:
+            result.grid_units = xtgeo_grid.units
+            result.map_axis_units = xtgeo_grid.units
+
+        return result
```

## xtgeo/grid3d/_ecl_inte_head.py

 * *Ordering differences only*

```diff
@@ -1,140 +1,140 @@
-import warnings
-
-import numpy as np
-
-from ._ecl_output_file import Phases, Simulator, TypeOfGrid, UnitSystem
-
-
-class InteHead:
-    """Contains the values for the INTEHEAD array in ecl restart
-    and init files.
-
-    Output files from eclipse and opm flow will contain sections
-    starting with keyword-array headers. One of these is the INTEHEAD
-    keyword. The values in the array are integers, and the meaning
-    of each index is described in the e.g. the OPM user manual (2021-04 rev_01
-    section D.6-D.7).
-
-    The length of the array is not specified, meaning some values are missing,
-    the InteHead class creates a lookup for these values:
-
-    >>> intehead = InteHead(np.array([0,1,2,3,4,5,6,7,8]))
-    >>> intehead.num_x
-    8
-    >>> # The year field is missing in the input
-    >>> intehead.year is None
-    True
-    """
-
-    def __init__(self, values: np.ndarray):
-        """Create an InteHead from the corresponding array.
-
-        Args:
-            values: Array of values following the INTEHEAD keyword
-                in an ECL restart or init file.
-        """
-        self.values = values
-
-    def __eq__(self, other):
-        if not isinstance(other, InteHead):
-            return False
-
-        return np.array_equal(self.values, other.values)
-
-    def __repr__(self):
-        return f"InteHead(values={self.values})"
-
-    def __str__(self):
-        return self.__repr__()
-
-    def _optional_index_lookup(self, index, constructor=(lambda x: x)):
-        """Looks up the value at the given index, returning None if out of bound.
-
-        Args:
-            index: The index in the value array to look up
-            constructor: Constructor function to wrap non-None values in, defaults
-                to identity.
-        Returns:
-            value at the index, None if out of bounds.
-        """
-        if len(self.values) > index:
-            return constructor(self.values[index])
-        return None
-
-    @property
-    def unit_system(self) -> UnitSystem:
-        """
-        The unit system used in the file.
-        """
-        return self._optional_index_lookup(2, UnitSystem)
-
-    @property
-    def num_x(self):
-        """The number of columns (x direction) of cells"""
-        return self._optional_index_lookup(8)
-
-    @property
-    def num_y(self):
-        """The number of rows (y direction) of cells"""
-        return self._optional_index_lookup(9)
-
-    @property
-    def num_z(self):
-        """The number of layers (z direction) of cells"""
-        return self._optional_index_lookup(10)
-
-    @property
-    def num_active(self):
-        """The number of active cells"""
-        return self._optional_index_lookup(11)
-
-    @property
-    def phases(self):
-        """The phase system used for simulation"""
-        if any([ids in str(self.simulator) for ids in ["300", "INTERSECT"]]):
-            # item 14 in E300 runs is number of tracers, not IPHS; assume oil/wat/gas
-            # item 14 in INTERSECT is always(?) undef., not IPHS; assume oil/wat/gas
-            return Phases.OIL_WATER_GAS
-
-        return self._optional_index_lookup(14, Phases)
-
-    @property
-    def day(self):
-        """The simulated time calendar day
-
-        (e.g. 3rd of april 2018)
-
-        """
-        return self._optional_index_lookup(64)
-
-    @property
-    def month(self):
-        """The simulated time calendar month
-
-        4 for simulation being in month 4.
-
-        """
-        return self._optional_index_lookup(65)
-
-    @property
-    def year(self):
-        """The simulated time calendar month
-
-        e.g. 2018 for simulation being done in year 2018
-        """
-        return self._optional_index_lookup(66)
-
-    @property
-    def simulator(self) -> Simulator:
-        """The simulator used for producing the run, or integer code if unknown"""
-        s_code = self._optional_index_lookup(94)
-        try:
-            return Simulator(s_code)
-        except ValueError:
-            warnings.warn(f"Unknown simulator code {s_code}")
-            return s_code
-
-    @property
-    def type_of_grid(self):
-        """The type of grid used in the simulation"""
-        return self._optional_index_lookup(13, TypeOfGrid.alternate_code)
+import warnings
+
+import numpy as np
+
+from ._ecl_output_file import Phases, Simulator, TypeOfGrid, UnitSystem
+
+
+class InteHead:
+    """Contains the values for the INTEHEAD array in ecl restart
+    and init files.
+
+    Output files from eclipse and opm flow will contain sections
+    starting with keyword-array headers. One of these is the INTEHEAD
+    keyword. The values in the array are integers, and the meaning
+    of each index is described in the e.g. the OPM user manual (2021-04 rev_01
+    section D.6-D.7).
+
+    The length of the array is not specified, meaning some values are missing,
+    the InteHead class creates a lookup for these values:
+
+    >>> intehead = InteHead(np.array([0,1,2,3,4,5,6,7,8]))
+    >>> intehead.num_x
+    8
+    >>> # The year field is missing in the input
+    >>> intehead.year is None
+    True
+    """
+
+    def __init__(self, values: np.ndarray):
+        """Create an InteHead from the corresponding array.
+
+        Args:
+            values: Array of values following the INTEHEAD keyword
+                in an ECL restart or init file.
+        """
+        self.values = values
+
+    def __eq__(self, other):
+        if not isinstance(other, InteHead):
+            return False
+
+        return np.array_equal(self.values, other.values)
+
+    def __repr__(self):
+        return f"InteHead(values={self.values})"
+
+    def __str__(self):
+        return self.__repr__()
+
+    def _optional_index_lookup(self, index, constructor=(lambda x: x)):
+        """Looks up the value at the given index, returning None if out of bound.
+
+        Args:
+            index: The index in the value array to look up
+            constructor: Constructor function to wrap non-None values in, defaults
+                to identity.
+        Returns:
+            value at the index, None if out of bounds.
+        """
+        if len(self.values) > index:
+            return constructor(self.values[index])
+        return None
+
+    @property
+    def unit_system(self) -> UnitSystem:
+        """
+        The unit system used in the file.
+        """
+        return self._optional_index_lookup(2, UnitSystem)
+
+    @property
+    def num_x(self):
+        """The number of columns (x direction) of cells"""
+        return self._optional_index_lookup(8)
+
+    @property
+    def num_y(self):
+        """The number of rows (y direction) of cells"""
+        return self._optional_index_lookup(9)
+
+    @property
+    def num_z(self):
+        """The number of layers (z direction) of cells"""
+        return self._optional_index_lookup(10)
+
+    @property
+    def num_active(self):
+        """The number of active cells"""
+        return self._optional_index_lookup(11)
+
+    @property
+    def phases(self):
+        """The phase system used for simulation"""
+        if any([ids in str(self.simulator) for ids in ["300", "INTERSECT"]]):
+            # item 14 in E300 runs is number of tracers, not IPHS; assume oil/wat/gas
+            # item 14 in INTERSECT is always(?) undef., not IPHS; assume oil/wat/gas
+            return Phases.OIL_WATER_GAS
+
+        return self._optional_index_lookup(14, Phases)
+
+    @property
+    def day(self):
+        """The simulated time calendar day
+
+        (e.g. 3rd of april 2018)
+
+        """
+        return self._optional_index_lookup(64)
+
+    @property
+    def month(self):
+        """The simulated time calendar month
+
+        4 for simulation being in month 4.
+
+        """
+        return self._optional_index_lookup(65)
+
+    @property
+    def year(self):
+        """The simulated time calendar month
+
+        e.g. 2018 for simulation being done in year 2018
+        """
+        return self._optional_index_lookup(66)
+
+    @property
+    def simulator(self) -> Simulator:
+        """The simulator used for producing the run, or integer code if unknown"""
+        s_code = self._optional_index_lookup(94)
+        try:
+            return Simulator(s_code)
+        except ValueError:
+            warnings.warn(f"Unknown simulator code {s_code}")
+            return s_code
+
+    @property
+    def type_of_grid(self):
+        """The type of grid used in the simulation"""
+        return self._optional_index_lookup(13, TypeOfGrid.alternate_code)
```

## xtgeo/grid3d/_ecl_logi_head.py

 * *Ordering differences only*

```diff
@@ -1,70 +1,70 @@
-from dataclasses import dataclass
-from typing import List, Optional
-
-from ._ecl_output_file import Simulator
-
-
-def lookup_optional_code(values, index):
-    if len(values) <= index:
-        return None
-    return values[index]
-
-
-@dataclass
-class LogiHead:
-    """Contains the values for the LOGIHEAD array in restart and init files.
-
-    Output files from eclipse and opm flow will contain sections
-    starting with keyword-array headers. One of these is the LOGIHEAD
-    keyword. The values in the array are booleans, and the meaning
-    of each index is described in the e.g. the OPM user manual (2021-04 rev_01
-    section D.6-D.7).
-
-    The length of the array is not specified, meaning some values are missing,
-    the InteHead class creates a lookup for these values:
-
-    Generally, the field describe whether an option has been enabled in the
-    model, ie. if logihead.dual_porosity is True then the model uses the
-    dual porosity feature.
-
-    >>> logihead = LogiHead.from_file_values([True, True, False], Simulator.ECLIPSE_100)
-    >>> logihead.dissolved_gas
-    True
-    >>> # Whether coal bed methane is used is missing
-    >>> logihead.coal_bed_methane is None
-    True
-
-    """
-
-    dissolved_gas: Optional[bool] = None
-    vaporized_oil: Optional[bool] = None
-    directional: Optional[bool] = None
-    radial: Optional[bool] = None
-    reversible: Optional[bool] = None
-    hysterisis: Optional[bool] = None
-    dual_porosity: Optional[bool] = None
-    end_point_scaling: Optional[bool] = None
-    directional_end_point_scaling: Optional[bool] = None
-    reversible_end_point_scaling: Optional[bool] = None
-    alternate_end_point_scaling: Optional[bool] = None
-    miscible_displacement: Optional[bool] = None
-    scale_water_pressure1: Optional[bool] = None
-    scale_water_pressure2: Optional[bool] = None
-    coal_bed_methane: Optional[bool] = None
-
-    @classmethod
-    def from_file_values(cls, values: List[bool], simulator: Simulator):
-        """Construct a LogiHead from the array following the LOGIHEAD keyword
-        Args:
-            values: The iterable of boolean values following the LOGIHEAD keyword
-            simulator: The meaning of each field is simulator dependent, so
-                the simulator must be given.
-        """
-        if simulator == Simulator.ECLIPSE_100:
-            # Weirdly, eclipse_100 outputs reversible and radial flags
-            # in swapped order.
-            indices = [0, 1, 2, 4, 3, 6, 14, 16, 17, 18, 19, 35, 55, 56, 127]
-        else:
-            indices = [0, 1, 2, 3, 4, 6, 14, 16, 17, 18, 19, 35, 55, 56, 127]
-
-        return cls(*[lookup_optional_code(values, i) for i in indices])
+from dataclasses import dataclass
+from typing import List, Optional
+
+from ._ecl_output_file import Simulator
+
+
+def lookup_optional_code(values, index):
+    if len(values) <= index:
+        return None
+    return values[index]
+
+
+@dataclass
+class LogiHead:
+    """Contains the values for the LOGIHEAD array in restart and init files.
+
+    Output files from eclipse and opm flow will contain sections
+    starting with keyword-array headers. One of these is the LOGIHEAD
+    keyword. The values in the array are booleans, and the meaning
+    of each index is described in the e.g. the OPM user manual (2021-04 rev_01
+    section D.6-D.7).
+
+    The length of the array is not specified, meaning some values are missing,
+    the InteHead class creates a lookup for these values:
+
+    Generally, the field describe whether an option has been enabled in the
+    model, ie. if logihead.dual_porosity is True then the model uses the
+    dual porosity feature.
+
+    >>> logihead = LogiHead.from_file_values([True, True, False], Simulator.ECLIPSE_100)
+    >>> logihead.dissolved_gas
+    True
+    >>> # Whether coal bed methane is used is missing
+    >>> logihead.coal_bed_methane is None
+    True
+
+    """
+
+    dissolved_gas: Optional[bool] = None
+    vaporized_oil: Optional[bool] = None
+    directional: Optional[bool] = None
+    radial: Optional[bool] = None
+    reversible: Optional[bool] = None
+    hysterisis: Optional[bool] = None
+    dual_porosity: Optional[bool] = None
+    end_point_scaling: Optional[bool] = None
+    directional_end_point_scaling: Optional[bool] = None
+    reversible_end_point_scaling: Optional[bool] = None
+    alternate_end_point_scaling: Optional[bool] = None
+    miscible_displacement: Optional[bool] = None
+    scale_water_pressure1: Optional[bool] = None
+    scale_water_pressure2: Optional[bool] = None
+    coal_bed_methane: Optional[bool] = None
+
+    @classmethod
+    def from_file_values(cls, values: List[bool], simulator: Simulator):
+        """Construct a LogiHead from the array following the LOGIHEAD keyword
+        Args:
+            values: The iterable of boolean values following the LOGIHEAD keyword
+            simulator: The meaning of each field is simulator dependent, so
+                the simulator must be given.
+        """
+        if simulator == Simulator.ECLIPSE_100:
+            # Weirdly, eclipse_100 outputs reversible and radial flags
+            # in swapped order.
+            indices = [0, 1, 2, 4, 3, 6, 14, 16, 17, 18, 19, 35, 55, 56, 127]
+        else:
+            indices = [0, 1, 2, 3, 4, 6, 14, 16, 17, 18, 19, 35, 55, 56, 127]
+
+        return cls(*[lookup_optional_code(values, i) for i in indices])
```

## xtgeo/grid3d/_ecl_output_file.py

 * *Ordering differences only*

```diff
@@ -1,78 +1,78 @@
-from enum import Enum, unique
-
-
-@unique
-class TypeOfGrid(Enum):
-    """
-    A Grid has three possible data layout formats, UNSTRUCTURED, CORNER_POINT,
-    BLOCK_CENTER and COMPOSITE (meaning combination of the two former). Only
-    CORNER_POINT layout is supported by XTGeo.
-    """
-
-    COMPOSITE = 0
-    CORNER_POINT = 1
-    UNSTRUCTURED = 2
-    BLOCK_CENTER = 3
-
-    @classmethod
-    def alternate_code(cls, code):
-        """Converts from alternate code to TypeOfGrid member.
-
-        weirdly, type of grid sometimes (For instance init's INTHEAD and
-        FILEHEAD) have an alternate integer code for type of grid.
-        """
-        if code == 0:
-            type_of_grid = cls.CORNER_POINT
-        elif code == 1:
-            type_of_grid = cls.UNSTRUCTURED
-        elif code == 2:
-            type_of_grid = cls.COMPOSITE
-        elif code == 3:
-            type_of_grid = cls.BLOCK_CENTER
-        else:
-            raise ValueError(f"Unknown grid type {code}")
-        return type_of_grid
-
-    @property
-    def alternate_value(self):
-        """Inverse of alternate_code."""
-        alternate_value = 0
-        if self == TypeOfGrid.CORNER_POINT:
-            alternate_value = 0
-        elif self == TypeOfGrid.UNSTRUCTURED:
-            alternate_value = 1
-        elif self == TypeOfGrid.COMPOSITE:
-            alternate_value = 2
-        elif self == TypeOfGrid.BLOCK_CENTER:
-            alternate_value = 3
-        else:
-            raise ValueError(f"Unknown grid type {self}")
-        return alternate_value
-
-
-@unique
-class UnitSystem(Enum):
-    METRIC = 1
-    FIELD = 2
-    LAB = 3
-
-
-@unique
-class Phases(Enum):
-    E300_GENERIC = 0
-    OIL = 1
-    WATER = 2
-    OIL_WATER = 3
-    GAS = 4
-    OIL_GAS = 5
-    GAS_WATER = 6
-    OIL_WATER_GAS = 7
-
-
-@unique
-class Simulator(Enum):
-    ECLIPSE_100 = 100
-    ECLIPSE_300 = 300
-    ECLIPSE_300_THERMAL = 500
-    INTERSECT = 700
-    FRONTSIM = 800
+from enum import Enum, unique
+
+
+@unique
+class TypeOfGrid(Enum):
+    """
+    A Grid has three possible data layout formats, UNSTRUCTURED, CORNER_POINT,
+    BLOCK_CENTER and COMPOSITE (meaning combination of the two former). Only
+    CORNER_POINT layout is supported by XTGeo.
+    """
+
+    COMPOSITE = 0
+    CORNER_POINT = 1
+    UNSTRUCTURED = 2
+    BLOCK_CENTER = 3
+
+    @classmethod
+    def alternate_code(cls, code):
+        """Converts from alternate code to TypeOfGrid member.
+
+        weirdly, type of grid sometimes (For instance init's INTHEAD and
+        FILEHEAD) have an alternate integer code for type of grid.
+        """
+        if code == 0:
+            type_of_grid = cls.CORNER_POINT
+        elif code == 1:
+            type_of_grid = cls.UNSTRUCTURED
+        elif code == 2:
+            type_of_grid = cls.COMPOSITE
+        elif code == 3:
+            type_of_grid = cls.BLOCK_CENTER
+        else:
+            raise ValueError(f"Unknown grid type {code}")
+        return type_of_grid
+
+    @property
+    def alternate_value(self):
+        """Inverse of alternate_code."""
+        alternate_value = 0
+        if self == TypeOfGrid.CORNER_POINT:
+            alternate_value = 0
+        elif self == TypeOfGrid.UNSTRUCTURED:
+            alternate_value = 1
+        elif self == TypeOfGrid.COMPOSITE:
+            alternate_value = 2
+        elif self == TypeOfGrid.BLOCK_CENTER:
+            alternate_value = 3
+        else:
+            raise ValueError(f"Unknown grid type {self}")
+        return alternate_value
+
+
+@unique
+class UnitSystem(Enum):
+    METRIC = 1
+    FIELD = 2
+    LAB = 3
+
+
+@unique
+class Phases(Enum):
+    E300_GENERIC = 0
+    OIL = 1
+    WATER = 2
+    OIL_WATER = 3
+    GAS = 4
+    OIL_GAS = 5
+    GAS_WATER = 6
+    OIL_WATER_GAS = 7
+
+
+@unique
+class Simulator(Enum):
+    ECLIPSE_100 = 100
+    ECLIPSE_300 = 300
+    ECLIPSE_300_THERMAL = 500
+    INTERSECT = 700
+    FRONTSIM = 800
```

## xtgeo/grid3d/_egrid.py

 * *Ordering differences only*

```diff
@@ -1,1007 +1,1007 @@
-"""
-The egrid fileformat is a file outputted by reservoir simulators such as opm
-flow containing the grid geometry. The layout of cell data and units is similar
-to grdecl files, but there is additional metadata.
-
-The data is layed out similarly to other ecl output files, see the ecl_data_io
-module.
-
-There is an alternate data layout (in addition to that of grdecl files), called
-unstructured, which is not widely supported. XTGeo does not currently support
-that format.
-
-egrid files like other ecl files contain tuples of keywords and list of data values
-of one type (An array with a name). The enums in this file generally describe
-a range of values for a position in one of these lists, the dataclasses describe
-the values of one keyword or a collection of those, named a file section.
-
-The following egrid file contents (as keyword/array pairs)::
-
-  ("FILEHEAD", [2001,3,0,3,0,0,0])
-  ("GRIDUNIT", "METRES   ")
-
-is represented by::
-
-    EGridHead(
-        Filehead(2001,3,3,TypeOfGrid.CORNER_POINT,RockModel(0),GridFormat(0)),
-        GridUnit("METRES   ")
-    )
-
-Where ``EGridHead`` is a section of the file, ``Filehead`` and ``GridUnit`` are
-keywords.
-
-Generally, the data layout of these objects map 1-to-1 with some section of an
-valid egrid file.
-
-keywords implement the `to_egrid` and `from_egrid` functions
-which should satisfy::
-
-    GridHead.from_egrid(x).to_egrid() == x
-
-These convert to and from the object representation and the keyword/array
-pairs, ie.
-
->>> grid_head_contents = [0]*100
->>> head = GridHead.from_egrid(grid_head_contents)
->>> head
-GridHead(type_of_grid=<TypeOfGrid.COMPOSITE...
->>> head.to_egrid().tolist() == grid_head_contents
-True
-"""
-import warnings
-from dataclasses import dataclass
-from enum import Enum, unique
-from itertools import chain
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Iterable,
-    List,
-    Optional,
-    Sequence,
-    Set,
-    Tuple,
-    Union,
-)
-
-import numpy as np
-from ecl_data_io import Format, lazy_read, write
-
-from ._ecl_grid import (
-    CoordinateType,
-    EclGrid,
-    GdOrient,
-    GridRelative,
-    GridUnit,
-    MapAxes,
-    Units,
-)
-from ._ecl_output_file import TypeOfGrid
-
-
-class EGridFileFormatError(ValueError):
-    """
-    Exception raised when an file unexpectedly does not conform to the egrid
-    format.
-    """
-
-    pass
-
-
-@unique
-class RockModel(Enum):
-    """
-    Type of rock model.
-    """
-
-    SINGLE_PERMEABILITY_POROSITY = 0
-    DUAL_POROSITY = 1
-    DUAL_PERMEABILITY = 2
-
-
-@unique
-class GridFormat(Enum):
-    """
-    The format of the "original grid", ie., what
-    method was used to construct the values in the file.
-    """
-
-    UNKNOWN = 0
-    IRREGULAR_CORNER_POINT = 1
-    REGULAR_CARTESIAN = 2
-
-
-@dataclass
-class Filehead:
-    """
-    The first keyword in an egrid file is the FILEHEAD
-    keyword, containing metadata about the file and its
-    content.
-    """
-
-    version_number: int
-    year: int
-    version_bound: int
-    type_of_grid: TypeOfGrid
-    rock_model: RockModel
-    grid_format: GridFormat
-
-    @classmethod
-    def from_egrid(cls, values: List[int]):
-        """
-        Construct a Filehead given the list of values following
-        the FILEHEAD keyword.
-        Args:
-            values(List[int]): list of values following the FILEHEAD keyword,
-                expected to contain at least 7 values (normally 100).
-        Returns:
-            A Filhead constructed from the given values.
-        """
-        if len(values) < 7:
-            raise ValueError(f"Filehead given too few values, {len(values)} < 7")
-        return cls(
-            version_number=values[0],
-            year=values[1],
-            version_bound=values[3],
-            type_of_grid=TypeOfGrid.alternate_code(values[4]),
-            rock_model=RockModel(values[5]),
-            grid_format=GridFormat(values[6]),
-        )
-
-    def to_egrid(self) -> np.ndarray:
-        """
-        Returns:
-            List of values, as layed out after the FILEHEAD keyword for
-            the given filehead.
-        """
-        # The data is expected to consist of
-        # 100 integers, but only a subset is used.
-        result = np.zeros((100,), dtype=np.int32)
-        result[0] = self.version_number
-        result[1] = self.year
-        result[3] = self.version_bound
-        result[4] = self.type_of_grid.alternate_value
-        result[5] = self.rock_model.value
-        result[6] = self.grid_format.value
-        return result
-
-
-@dataclass
-class GridHead:
-    """
-    Both for lgr (see LGRSection) and the global grid (see GlobalGrid)
-    the GRIDHEAD keyword indicates the start of the grid layout for that
-    section.
-    """
-
-    type_of_grid: TypeOfGrid
-    num_x: int
-    num_y: int
-    num_z: int
-    grid_reference_number: int
-    numres: int
-    nseg: int
-    coordinate_type: CoordinateType
-    lgr_start: Tuple[int, int, int]
-    lgr_end: Tuple[int, int, int]
-
-    @classmethod
-    def from_egrid(cls, values: Sequence[int]):
-        if len(values) < 33:
-            raise ValueError(
-                f"Too few arguments to GridHead.from_egrid {len(values)} < 33"
-            )
-        return cls(
-            type_of_grid=TypeOfGrid(values[0]),
-            num_x=values[1],
-            num_y=values[2],
-            num_z=values[3],
-            grid_reference_number=values[4],
-            numres=values[24],
-            nseg=values[25],
-            coordinate_type=CoordinateType.from_bgrdecl(values[26]),
-            lgr_start=(values[27], values[28], values[29]),
-            lgr_end=(values[30], values[31], values[32]),
-        )
-
-    def to_egrid(self) -> np.ndarray:
-        # The data is expected to consist of
-        # 100 integers, but only a subset is used.
-        result = np.zeros((100,), dtype=np.int32)
-        result[0] = self.type_of_grid.value
-        result[1] = self.num_x
-        result[2] = self.num_y
-        result[3] = self.num_z
-        result[4] = self.grid_reference_number
-        result[24] = self.numres
-        result[25] = self.nseg
-        result[26] = self.coordinate_type.to_bgrdecl()
-        result[[27, 28, 29]] = np.array(self.lgr_start)
-        result[[30, 31, 32]] = np.array(self.lgr_end)
-        return result
-
-
-@dataclass
-class EGridSubGrid:
-    """
-    Both the LGR sections and the global grid contain a grid which is in the
-    general format of a eclipse grid. EGridSubGrid contain the common implementation.
-    """
-
-    grid_head: Optional[GridHead]
-    coord: np.ndarray
-    zcorn: np.ndarray
-    actnum: Optional[np.ndarray] = None
-
-    def __eq__(self, other):
-        return (
-            self.grid_head == other.grid_head
-            and np.array_equal(self.actnum, other.actnum)
-            and np.array_equal(self.coord, other.coord)
-            and np.array_equal(self.zcorn, other.zcorn)
-        )
-
-    def _check_xtgeo_compatible(self):
-        if self.grid_head.coordinate_type == CoordinateType.CYLINDRICAL:
-            raise NotImplementedError(
-                "Xtgeo does not currently support cylindrical coordinate systems"
-            )
-        if self.grid_head.numres < 1:
-            warnings.warn(
-                "EGrid file given with numres < 1, which is invalid, so assuming"
-                " instead that the file contains exactly one reservoir. XTGeo"
-                " prior to version 2.14 would output grids with numres == 0. If"
-                " the file was created with an older version of XTGeo, consider"
-                " importing the file with version 2.15 and re-exporting."
-            )
-            self.grid_head.numres = 1
-        if self.grid_head.numres > 1:
-            raise NotImplementedError(
-                "Xtgeo does not currently support multiple reservoirs"
-            )
-
-    @property
-    def dimensions(self) -> Tuple[int, int, int]:
-        return (
-            int(self.grid_head.num_x),
-            int(self.grid_head.num_y),
-            int(self.grid_head.num_z),
-        )
-
-    def to_egrid(self) -> List[Tuple[str, Any]]:
-        result = [
-            ("GRIDHEAD", self.grid_head.to_egrid()),
-            ("COORD   ", self.coord.astype(np.float32)),
-            ("ZCORN   ", self.zcorn.astype(np.float32)),
-        ]
-        if self.actnum is not None:
-            result.append(("ACTNUM  ", self.actnum.astype(np.int32)))
-        return result
-
-
-@dataclass
-class LGRSection(EGridSubGrid):
-    """
-    An Egrid file can contain multiple LGR (Local Grid Refinement) sections
-    which define a subgrid with finer layout.
-    """
-
-    name: Optional[str] = None
-    parent: Optional[str] = None
-    grid_parent: Optional[str] = None
-    hostnum: Optional[np.ndarray] = None
-    boxorig: Optional[Tuple[int, int, int]] = None
-    coord_sys: Optional[MapAxes] = None
-
-    def __eq__(self, other):
-        if not isinstance(other, LGRSection):
-            return False
-        return (
-            super().__eq__(other)
-            and self.name == other.name
-            and self.parent == other.parent
-            and self.grid_parent == other.grid_parent
-            and np.array_equal(self.hostnum, other.hostnum)
-            and self.boxorig == other.boxorig
-            and self.coord_sys == other.coord_sys
-        )
-
-    def __post_init__(self):
-        if self.name is None:
-            raise TypeError("Missing parameter to LGRSection: name")
-
-    def to_egrid(self) -> List[Tuple[str, Any]]:
-        result_dict = dict(super().to_egrid())
-        result_dict["LGR     "] = [self.name]
-        if self.parent is not None:
-            result_dict["LGRPARNT"] = [self.parent]
-        if self.grid_parent is not None:
-            result_dict["LGRSGRID"] = [self.grid_parent]
-        if self.hostnum is not None:
-            result_dict["HOSTNUM "] = self.hostnum
-        if self.boxorig is not None:
-            result_dict["BOXORIG "] = np.array(self.boxorig, dtype=np.int32)
-        if self.coord_sys is not None:
-            result_dict["COORDSYS"] = self.coord_sys.to_bgrdecl()
-        result_dict["ENDGRID "] = np.array([], dtype=np.int32)
-        result_dict["ENDLGR  "] = np.array([], dtype=np.int32)
-        result = []
-        order = [
-            "LGR     ",
-            "LGRPARNT",
-            "LGRSGRID",
-            "GRIDHEAD",
-            "BOXORIG ",
-            "COORD   ",
-            "COORDSYS",
-            "ZCORN   ",
-            "ACTNUM  ",
-            "HOSTNUM ",
-            "ENDGRID ",
-            "ENDLGR  ",
-        ]
-        for kw in order:
-            if kw in result_dict:
-                result.append((kw, result_dict[kw]))
-        return result
-
-
-@dataclass
-class GlobalGrid(EGridSubGrid):
-    """
-    The global grid contains the layout of the grid before
-    refinements, and the sectioning into grid coarsening
-    through the optional corsnum keyword.
-    """
-
-    coord_sys: Optional[MapAxes] = None
-    boxorig: Optional[Tuple[int, int, int]] = None
-    corsnum: Optional[np.ndarray] = None
-
-    def _check_xtgeo_compatible(self):
-        super()._check_xtgeo_compatible()
-        if self.corsnum is not None:
-            warnings.warn(
-                "egrid file given with coarsening, this is not directly supported "
-                " by xtgeo. Instead grid is imported without coarsening."
-            )
-
-        if self.coord_sys is not None:
-            warnings.warn(
-                "egrid file given with coordinate definition for global "
-                "grid, this is not directly supported by xtgeo. Instead "
-                "grid is imported without converting by local coordsys."
-            )
-
-    def __eq__(self, other):
-        if not isinstance(other, GlobalGrid):
-            return False
-        return (
-            super().__eq__(other)
-            and self.coord_sys == other.coord_sys
-            and self.boxorig == other.boxorig
-            and np.array_equal(self.corsnum, other.corsnum)
-        )
-
-    def to_egrid(self) -> List[Tuple[str, Any]]:
-        result_dict = dict(super().to_egrid())
-        if self.coord_sys is not None:
-            result_dict["COORDSYS"] = self.coord_sys.to_bgrdecl()
-        if self.boxorig is not None:
-            result_dict["BOXORIG "] = np.array(self.boxorig, dtype=np.int32)
-        if self.corsnum is not None:
-            result_dict["CORSNUM "] = self.corsnum
-        result_dict["ENDGRID "] = np.array([], dtype=np.int32)
-        result = []
-        order = [
-            "GRIDHEAD",
-            "BOXORIG ",
-            "COORD   ",
-            "COORDSYS",
-            "ZCORN   ",
-            "ACTNUM  ",
-            "CORSNUM ",
-            "ENDGRID ",
-        ]
-        for kw in order:
-            if kw in result_dict:
-                result.append((kw, result_dict[kw]))
-        return result
-
-
-@dataclass
-class NNCHead:
-    """
-    The NNCHead keyword denotes the start of a
-    NNCSection and contains the number of nncs and
-    the grid number of the grid where the NNCs applies.
-    """
-
-    num_nnc: int
-    grid_identifier: int
-
-    @classmethod
-    def from_egrid(cls, values: List[int]):
-        return cls(*values[0:2])
-
-    def to_egrid(self) -> np.ndarray:
-        result = np.zeros((10,), dtype=np.int32)
-        result[0] = self.num_nnc
-        result[1] = self.grid_identifier
-        return result
-
-
-@dataclass
-class NNCSection:
-    """The NNCSection's describe non-neighboor connections in the grid.
-
-    See, for instance, OPM user manual 2021-4 Rev. 1 Table D1.1 and 6.3.5.
-
-    Args:
-        nnchead: The nnc header
-        upstream_nnc: list of cells (by index) for the upstream nnc.
-        downstream_nnc: list of cells (by index) for the downstream nnc
-            to be connected to the corresponding cell in upstream_nnc.
-        nncl: list of LGR cells (by index) to be connected to the global grid.
-        nncg: list of global cells (by index) connected to the corresponding
-         LGR cells in nncl.
-
-    """
-
-    nnchead: NNCHead
-    upstream_nnc: np.ndarray
-    downstream_nnc: np.ndarray
-    nncl: Optional[np.ndarray] = None
-    nncg: Optional[np.ndarray] = None
-
-    def __eq__(self, other):
-        if not isinstance(other, NNCSection):
-            return False
-        return (
-            self.nnchead == other.nnchead
-            and np.array_equal(self.upstream_nnc, other.upstream_nnc)
-            and np.array_equal(self.downstream_nnc, other.downstream_nnc)
-            and np.array_equal(self.nncl, other.nncl)
-            and np.array_equal(self.nncg, other.nncg)
-        )
-
-    def to_egrid(self) -> List[Tuple[str, Any]]:
-        result = [
-            ("NNCHEAD ", self.nnchead.to_egrid()),
-            ("NNC1    ", self.upstream_nnc),
-            ("NNC2    ", self.downstream_nnc),
-        ]
-        if self.nncl is not None:
-            result.append(("NNCL    ", self.nncl))
-        if self.nncg is not None:
-            result.append(("NNCG    ", self.nncg))
-        return result
-
-
-@dataclass
-class AmalgamationSection:
-    """The AmalgamationSection's describe the amalgamation of two LGR's.
-
-    See, for instance, OPM user manual 2021-4 Rev. 1 Table D1.1 and 6.3.5.
-
-    Args:
-    lgr_idxs: The indexes of the LGR's to be amalgamated
-    nna1: indecies in the first lgr connected in the amalgamation.
-    nna2: indecies in the second lgr connected in the amalgamation, to
-        the corresponding cell in nna1.
-
-    """
-
-    lgr_idxs: Tuple[int, int]
-    nna1: Optional[np.ndarray]
-    nna2: Optional[np.ndarray]
-
-    def __eq__(self, other):
-        if not isinstance(other, AmalgamationSection):
-            return False
-        return (
-            self.lgr_idxs == other.lgr_idxs
-            and np.array_equal(self.nna1, other.nna1)
-            and np.array_equal(self.nna2, other.nna2)
-        )
-
-    def to_egrid(self) -> List[Tuple[str, Any]]:
-        return [
-            ("NNCHEADA", np.array(self.lgr_idxs, np.int32)),
-            ("NNA1    ", self.nna1),
-            ("NNA2    ", self.nna2),
-        ]
-
-
-@dataclass
-class EGridHead:
-    """The EGridHead section occurs once at the start of an EGrid file."""
-
-    file_head: Filehead
-    mapunits: Optional[Units] = None
-    mapaxes: Optional[MapAxes] = None
-    gridunit: Optional[GridUnit] = None
-    gdorient: Optional[GdOrient] = None
-
-    def to_egrid(self) -> List[Tuple[str, Any]]:
-        result = [
-            ("FILEHEAD", self.file_head.to_egrid()),
-        ]
-        if self.mapunits is not None:
-            result.append(("MAPUNITS", [self.mapunits.to_bgrdecl()]))
-        if self.mapaxes is not None:
-            result.append(("MAPAXES ", self.mapaxes.to_bgrdecl()))
-        if self.gridunit is not None:
-            result.append(("GRIDUNIT", self.gridunit.to_bgrdecl()))
-        if self.gdorient is not None:
-            result.append(("GDORIENT", self.gdorient.to_bgrdecl()))
-        return result
-
-
-@dataclass
-class EGrid(EclGrid):
-    """Contains the data of an EGRID file.
-
-    Args:
-        egrid_head: The file header starting with the FILEHEAD keyword and
-            contains optional information about units, map relative location, and
-            orientation.
-        global_grid: The global grid
-        lgr_sections: List of local grid refinements.
-        nnc_sections: Describe non-neighboring sections as a list of either
-            NNCSections or AmalgamationSection's.
-    """
-
-    egrid_head: EGridHead
-    global_grid: GlobalGrid
-    lgr_sections: List[LGRSection]
-    # The nnc_sections are kept as one list which can consist of both
-    # NNCSection and AmalgamationSection as these occur interspersed in the
-    # file. The order seems to be sorted by LGR index. Keeping them in
-    # one list keeps the data layout of EGrid 1-to-1 with the contents
-    # of the file.
-    nnc_sections: List[Union[NNCSection, AmalgamationSection]]
-
-    @classmethod
-    def default_settings_grid(
-        cls,
-        coord: np.ndarray,
-        zcorn: np.ndarray,
-        actnum: Optional[np.ndarray],
-        size: Tuple[int, int, int],
-    ):
-        grid_head = GridHead(
-            TypeOfGrid.CORNER_POINT,
-            *size,
-            1,
-            1,
-            1,
-            CoordinateType.CARTESIAN,
-            (0, 0, 0),
-            (0, 0, 0),
-        )
-        global_grid = GlobalGrid(
-            grid_head,
-            coord,
-            zcorn,
-            actnum,
-        )
-        return EGrid(
-            EGridHead(
-                Filehead(
-                    3,
-                    2007,
-                    3,
-                    TypeOfGrid.CORNER_POINT,
-                    RockModel.SINGLE_PERMEABILITY_POROSITY,
-                    GridFormat.IRREGULAR_CORNER_POINT,
-                ),
-                gridunit=GridUnit(),
-            ),
-            global_grid,
-            [],
-            [],
-        )
-
-    @property
-    def coord(self) -> np.ndarray:
-        return self.global_grid.coord
-
-    @coord.setter
-    def coord(self, value: np.ndarray):
-        self.global_grid.coord = value
-
-    @property
-    def zcorn(self) -> np.ndarray:
-        return self.global_grid.zcorn
-
-    @zcorn.setter
-    def zcorn(self, value: np.ndarray):
-        self.global_grid.zcorn = value
-
-    @property
-    def actnum(self) -> Optional[np.ndarray]:
-        return self.global_grid.actnum
-
-    @classmethod
-    def from_file(cls, filelike, fileformat: str = None):
-        """
-        Read an egrid file
-        Args:
-            filelike (str,Path,stream): The egrid file to be read.
-            file_format (None or str): The format of the file (either "egrid"
-                or "fegrid") None means guess.
-        Returns:
-            EGrid with the contents of the file.
-        """
-        file_format = None
-        if fileformat == "egrid":
-            file_format = Format.UNFORMATTED
-        elif fileformat == "fegrid":
-            file_format = Format.FORMATTED
-        elif fileformat is not None:
-            raise ValueError(f"Unrecognized egrid file format {fileformat}")
-        return EGridReader(filelike, file_format=file_format).read()
-
-    def to_file(self, filelike, fileformat: str = "egrid"):
-        """
-        write the EGrid to file.
-        Args:
-            filelike (str,Path,stream): The egrid file to write to.
-            file_format (ecl_data_io.Format): The format of the file.
-        """
-        file_format = None
-        if fileformat == "egrid":
-            file_format = Format.UNFORMATTED
-        elif fileformat == "fegrid":
-            file_format = Format.FORMATTED
-        elif fileformat is not None:
-            raise ValueError(f"Unrecognized egrid file format {fileformat}")
-        contents = []
-        contents += self.egrid_head.to_egrid()
-        contents += self.global_grid.to_egrid()
-        for lgr in self.lgr_sections:
-            contents += lgr.to_egrid()
-        for nnc in self.nnc_sections:
-            contents += nnc.to_egrid()
-        write(filelike, contents, file_format)
-
-    def _check_xtgeo_compatible(self):
-        self.global_grid._check_xtgeo_compatible()
-        if self.lgr_sections:
-            warnings.warn(
-                "UserWarning: egrid file contains local grid refinements (LGR). "
-                "LGR's are not directly supported, only the global grid is "
-                "imported."
-            )
-
-    @property
-    def is_map_relative(self) -> bool:
-        if self.egrid_head.gridunit is None:
-            return False
-        return self.egrid_head.gridunit.grid_relative == GridRelative.MAP
-
-    @property
-    def mapaxes(self) -> Optional[MapAxes]:
-        return self.egrid_head.mapaxes
-
-    @mapaxes.setter
-    def mapaxes(self, value):
-        self.egrid_head.mapaxes = value
-
-    @property
-    def dimensions(self) -> Tuple[int, int, int]:
-        return self.global_grid.dimensions
-
-    @property
-    def map_axis_units(self) -> Units:
-        return self.egrid_head.mapunits
-
-    @map_axis_units.setter
-    def map_axis_units(self, value):
-        self.egrid_head.mapunits = value
-
-    @property
-    def grid_units(self) -> Units:
-        return self.egrid_head.gridunit.unit
-
-    @grid_units.setter
-    def grid_units(self, value):
-        self.egrid_head.gridunit.unit = value
-
-    @classmethod
-    def from_xtgeo_grid(cls, xtgeo_grid):
-        default_grid = super().from_xtgeo_grid(xtgeo_grid)
-
-        default_grid.global_grid.coord = default_grid.global_grid.coord.astype(
-            np.float32
-        )
-        default_grid.global_grid.zcorn = default_grid.global_grid.zcorn.astype(
-            np.float32
-        )
-        if xtgeo_grid._dualporo:
-            default_grid.rock_model = RockModel.DUAL_POROSITY
-        if xtgeo_grid._dualperm:
-            default_grid.rock_model = RockModel.DUAL_PERMEABILITY
-
-        if default_grid.egrid_head.gridunit is None:
-            warnings.warn(
-                "Unitless xtgeo grid converted to egrid. Assuming meters as unit."
-            )
-            default_grid.egrid_head.gridunit = GridUnit()
-        return default_grid
-
-
-keyword_translation = {
-    "FILEHEAD": "file_head",
-    "MAPUNITS": "mapunits",
-    "MAPAXES ": "mapaxes",
-    "GRIDUNIT": "gridunit",
-    "GDORIENT": "gdorient",
-    "LGR     ": "name",
-    "GRIDHEAD": "grid_head",
-    "HOSTNUM ": "hostnum",
-    "BOXORIG ": "boxorig",
-    "COORDSYS": "coord_sys",
-    "LGRPARNT": "parent",
-    "LGRSGRID": "grid_parent",
-    "COORD   ": "coord",
-    "ZCORN   ": "zcorn",
-    "ACTNUM  ": "actnum",
-    "NNCHEAD ": "nnchead",
-    "NNC1    ": "upstream_nnc",
-    "NNC2    ": "downstream_nnc",
-    "NNCL    ": "nncl",
-    "NNCG    ": "nncg",
-    "NNCHEADA": "lgr_idxs",
-    "NNA1    ": "nna1",
-    "NNA2    ": "nna2",
-    "CORSNUM ": "corsnum",
-}
-
-
-class EGridReader:
-    """
-    The EGridReader reads an egrid file through the `read` method.
-
-    Args:
-        filelike (str, Path, stream): The egrid file to read from.
-        file_format (None or ecl_data_io.Format): The format of the file,
-            None means guess.
-
-    """
-
-    def __init__(self, filelike, file_format: Format = None):
-        self.filelike = filelike
-        self.keyword_generator = lazy_read(filelike, file_format)
-
-    def read_section(
-        self,
-        keyword_factories: Dict[str, Callable],
-        required_keywords: Set[str],
-        stop_keywords: Iterable[str],
-        skip_keywords: Iterable[str] = [],
-        keyword_visitors: Iterable[Callable] = [],
-    ):
-        """
-        Read a general egrid file section.
-        Args:
-            keyword_factories (dict[str, func]): The function used
-                to construct a section member.
-            required_keywords (List[str]): List of keywords that are required
-                for the given section.
-            stop_keywords (List[str]): List of keywords which when read ends
-                the section. The keyword generator will be at the first keyword
-                in stop_keywords after read_section is called.
-            skip_keywords (List[str]): List of keywords that does not
-                have a factory, which should just be skipped.
-            keyword_visitors (List[func]): List of functions that
-                "visit" each keyword. Each of these functions are called
-                for each keyword, value pair and can be used to
-                preprocess the data.
-
-        Returns:
-            dictionary of parameters for the constructor of the given section.
-        """
-        results = {}
-        i = 0
-        while True:
-            try:
-                entry = next(self.keyword_generator)
-            except StopIteration:
-                break
-            kw = entry.read_keyword()
-            if kw in skip_keywords:
-                continue
-            if kw in stop_keywords and i > 0:
-                # Optional keywords were possibly omitted and
-                # we have reached the global grid section
-                # push back the grid head of the global grid
-                # and proceed
-                self.keyword_generator = chain([entry], self.keyword_generator)
-                break
-            if kw in results:
-                raise EGridFileFormatError(f"Duplicate keyword {kw} in {self.filelike}")
-            try:
-                factory = keyword_factories[kw]
-            except KeyError as err:
-                raise EGridFileFormatError(f"Unknown egrid keyword {kw}") from err
-            try:
-                value = factory(entry.read_array())
-                results[kw] = value
-            except (ValueError, IndexError, TypeError) as err:
-                raise EGridFileFormatError(f"Incorrect values in keyword {kw}") from err
-            for visit in keyword_visitors:
-                visit(kw, value)
-            i += 1
-
-        missing_keywords = required_keywords.difference(results.keys())
-        params = {keyword_translation[kw]: v for kw, v in results.items()}
-        if missing_keywords:
-            raise EGridFileFormatError(f"Missing required keywords {missing_keywords}")
-        return params
-
-    def read_header(self) -> EGridHead:
-        """
-        Reads the EGrid header from the start of the stream. Ensures
-        that the keyword_generator is at the first GRIDHEAD keyword
-        after the header.
-        """
-        params = self.read_section(
-            keyword_factories={
-                "FILEHEAD": Filehead.from_egrid,
-                "MAPUNITS": lambda x: Units.from_bgrdecl(x[0]),
-                "MAPAXES ": MapAxes.from_bgrdecl,
-                "GRIDUNIT": GridUnit.from_bgrdecl,
-                "GDORIENT": GdOrient.from_bgrdecl,
-            },
-            required_keywords={"FILEHEAD"},
-            stop_keywords=["GRIDHEAD"],
-        )
-        return EGridHead(**params)
-
-    def read_global_grid(self) -> GlobalGrid:
-        """
-        Reads the global grid section from the start of the keyword_generator,
-        ensures the keyword_generator is at the keyword after the first ENDGRID
-        keyword encountered.
-        """
-
-        def check_gridhead(kw: str, value):
-            if kw == "GRIDHEAD" and value.type_of_grid != TypeOfGrid.CORNER_POINT:
-                raise NotImplementedError(
-                    "XTGeo does not support unstructured or mixed grids."
-                )
-
-        params = self.read_section(
-            keyword_factories={
-                "GRIDHEAD": GridHead.from_egrid,
-                "BOXORIG ": tuple,
-                "COORDSYS": MapAxes.from_bgrdecl,
-                "COORD   ": lambda x: np.array(x, dtype=np.float32),
-                "ZCORN   ": lambda x: np.array(x, dtype=np.float32),
-                "ACTNUM  ": lambda x: np.array(x, dtype=np.int32),
-                "CORSNUM ": lambda x: np.array(x, dtype=np.int32),
-            },
-            required_keywords={"GRIDHEAD", "COORD   ", "ZCORN   "},
-            stop_keywords=["ENDGRID "],
-            keyword_visitors=[check_gridhead],
-        )
-        try:
-            entry = next(self.keyword_generator)
-        except StopIteration as err:
-            raise EGridFileFormatError(
-                "Did not read ENDGRID after global grid"
-            ) from err
-        if entry.read_keyword() != "ENDGRID ":
-            raise EGridFileFormatError("Did not read ENDGRID after global grid")
-        return GlobalGrid(**params)
-
-    def read_subsections(self) -> Tuple[List[LGRSection], List[NNCSection]]:
-        """
-        Reads lgr and nnc subsections from the start of the keyword_generator.
-        """
-        lgr_sections = []
-        nnc_sections = []
-        while True:
-            try:
-                entry = next(self.keyword_generator)
-            except StopIteration:
-                break
-            self.keyword_generator = chain([entry], self.keyword_generator)
-            keyword = entry.read_keyword().rstrip()
-            if keyword == "LGR":
-                lgr_sections.append(self.read_lgr_subsection())
-            elif keyword == "NNCHEAD":
-                nnc_sections.append(self.read_nnc_subsection())
-            elif keyword == "NNCHEADA":
-                nnc_sections.append(self.read_amalgamation_subsection())
-            else:
-                raise EGridFileFormatError(
-                    f"egrid subsection started with unexpected keyword {keyword}"
-                )
-        return lgr_sections, nnc_sections
-
-    def read_lgr_subsection(self) -> LGRSection:
-        """
-        Reads one lgr subsection from the start of the keyword generator.
-        After read_lgr_subsection is called, The keyword_generator is at the
-        keyword after the first ENDLGR keyword encountered, or end of stream.
-        """
-        params = self.read_section(
-            keyword_factories={
-                "LGR     ": lambda x: x[0].decode("ascii"),
-                "LGRPARNT": lambda x: x[0].decode("ascii"),
-                "LGRSGRID": lambda x: x[0].decode("ascii"),
-                "GRIDHEAD": GridHead.from_egrid,
-                "BOXORIG ": tuple,
-                "COORDSYS": MapAxes.from_bgrdecl,
-                "COORD   ": lambda x: np.array(x, dtype=np.float32),
-                "ZCORN   ": lambda x: np.array(x, dtype=np.float32),
-                "ACTNUM  ": lambda x: np.array(x, dtype=np.int32),
-                "HOSTNUM ": lambda x: np.array(x, dtype=np.int32),
-            },
-            required_keywords={
-                "LGR     ",
-                "GRIDHEAD",
-                "COORD   ",
-                "ZCORN   ",
-                "HOSTNUM ",
-            },
-            skip_keywords=["ENDGRID "],
-            stop_keywords=["ENDLGR  "],
-        )
-        try:
-            entry = next(self.keyword_generator)
-        except StopIteration as err:
-            raise EGridFileFormatError("Did not read ENDLGR after lgr section") from err
-        if entry.read_keyword() != "ENDLGR  ":
-            raise EGridFileFormatError("Did not read ENDLGR after lgr section")
-        return LGRSection(**params)
-
-    def read_nnc_subsection(self) -> NNCSection:
-        """
-        Reads one nnc subsection from the start of the keyword generator.
-        After read_nncsubsection is called, The keyword_generator is
-        at the next NNCHEAD, NNCHEADA or LGR keyword, or end of stream.
-        """
-        params = self.read_section(
-            keyword_factories={
-                "NNCHEAD ": NNCHead.from_egrid,
-                "NNC1    ": lambda x: np.array(x, dtype=np.int32),
-                "NNC2    ": lambda x: np.array(x, dtype=np.int32),
-                "NNCL    ": lambda x: np.array(x, dtype=np.int32),
-                "NNCG    ": lambda x: np.array(x, dtype=np.int32),
-            },
-            required_keywords={"NNCHEAD ", "NNC1    ", "NNC2    "},
-            stop_keywords=["NNCHEAD ", "LGR     ", "NNCHEADA"],
-        )
-        return NNCSection(**params)
-
-    def read_amalgamation_subsection(self) -> AmalgamationSection:
-        """
-        Reads one amalgamation subsection from the start of the keyword
-        generator. After read_nncsubsection is called, The keyword_generator is
-        at the next NNCHEAD, NNCHEADA or LGR keyword, or end of stream.
-        """
-        params = self.read_section(
-            keyword_factories={
-                "NNCHEADA": lambda x: tuple(x[0:2]),
-                "NNA1    ": lambda x: np.array(x, dtype=np.int32),
-                "NNA2    ": lambda x: np.array(x, dtype=np.int32),
-            },
-            required_keywords={"NNCHEADA", "NNA1    ", "NNA2    "},
-            stop_keywords=["NNCHEAD ", "LGR     ", "NNCHEADA"],
-        )
-        return AmalgamationSection(**params)
-
-    def read(self) -> EGrid:
-        header = self.read_header()
-        if header.file_head.type_of_grid != TypeOfGrid.CORNER_POINT:
-            raise NotImplementedError(
-                "XTGeo does not support unstructured or mixed grids."
-            )
-        global_grid = self.read_global_grid()
-        lgr_sections, nnc_sections = self.read_subsections()
-        return EGrid(header, global_grid, lgr_sections, nnc_sections)
+"""
+The egrid fileformat is a file outputted by reservoir simulators such as opm
+flow containing the grid geometry. The layout of cell data and units is similar
+to grdecl files, but there is additional metadata.
+
+The data is layed out similarly to other ecl output files, see the ecl_data_io
+module.
+
+There is an alternate data layout (in addition to that of grdecl files), called
+unstructured, which is not widely supported. XTGeo does not currently support
+that format.
+
+egrid files like other ecl files contain tuples of keywords and list of data values
+of one type (An array with a name). The enums in this file generally describe
+a range of values for a position in one of these lists, the dataclasses describe
+the values of one keyword or a collection of those, named a file section.
+
+The following egrid file contents (as keyword/array pairs)::
+
+  ("FILEHEAD", [2001,3,0,3,0,0,0])
+  ("GRIDUNIT", "METRES   ")
+
+is represented by::
+
+    EGridHead(
+        Filehead(2001,3,3,TypeOfGrid.CORNER_POINT,RockModel(0),GridFormat(0)),
+        GridUnit("METRES   ")
+    )
+
+Where ``EGridHead`` is a section of the file, ``Filehead`` and ``GridUnit`` are
+keywords.
+
+Generally, the data layout of these objects map 1-to-1 with some section of an
+valid egrid file.
+
+keywords implement the `to_egrid` and `from_egrid` functions
+which should satisfy::
+
+    GridHead.from_egrid(x).to_egrid() == x
+
+These convert to and from the object representation and the keyword/array
+pairs, ie.
+
+>>> grid_head_contents = [0]*100
+>>> head = GridHead.from_egrid(grid_head_contents)
+>>> head
+GridHead(type_of_grid=<TypeOfGrid.COMPOSITE...
+>>> head.to_egrid().tolist() == grid_head_contents
+True
+"""
+import warnings
+from dataclasses import dataclass
+from enum import Enum, unique
+from itertools import chain
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Iterable,
+    List,
+    Optional,
+    Sequence,
+    Set,
+    Tuple,
+    Union,
+)
+
+import numpy as np
+from ecl_data_io import Format, lazy_read, write
+
+from ._ecl_grid import (
+    CoordinateType,
+    EclGrid,
+    GdOrient,
+    GridRelative,
+    GridUnit,
+    MapAxes,
+    Units,
+)
+from ._ecl_output_file import TypeOfGrid
+
+
+class EGridFileFormatError(ValueError):
+    """
+    Exception raised when an file unexpectedly does not conform to the egrid
+    format.
+    """
+
+    pass
+
+
+@unique
+class RockModel(Enum):
+    """
+    Type of rock model.
+    """
+
+    SINGLE_PERMEABILITY_POROSITY = 0
+    DUAL_POROSITY = 1
+    DUAL_PERMEABILITY = 2
+
+
+@unique
+class GridFormat(Enum):
+    """
+    The format of the "original grid", ie., what
+    method was used to construct the values in the file.
+    """
+
+    UNKNOWN = 0
+    IRREGULAR_CORNER_POINT = 1
+    REGULAR_CARTESIAN = 2
+
+
+@dataclass
+class Filehead:
+    """
+    The first keyword in an egrid file is the FILEHEAD
+    keyword, containing metadata about the file and its
+    content.
+    """
+
+    version_number: int
+    year: int
+    version_bound: int
+    type_of_grid: TypeOfGrid
+    rock_model: RockModel
+    grid_format: GridFormat
+
+    @classmethod
+    def from_egrid(cls, values: List[int]):
+        """
+        Construct a Filehead given the list of values following
+        the FILEHEAD keyword.
+        Args:
+            values(List[int]): list of values following the FILEHEAD keyword,
+                expected to contain at least 7 values (normally 100).
+        Returns:
+            A Filhead constructed from the given values.
+        """
+        if len(values) < 7:
+            raise ValueError(f"Filehead given too few values, {len(values)} < 7")
+        return cls(
+            version_number=values[0],
+            year=values[1],
+            version_bound=values[3],
+            type_of_grid=TypeOfGrid.alternate_code(values[4]),
+            rock_model=RockModel(values[5]),
+            grid_format=GridFormat(values[6]),
+        )
+
+    def to_egrid(self) -> np.ndarray:
+        """
+        Returns:
+            List of values, as layed out after the FILEHEAD keyword for
+            the given filehead.
+        """
+        # The data is expected to consist of
+        # 100 integers, but only a subset is used.
+        result = np.zeros((100,), dtype=np.int32)
+        result[0] = self.version_number
+        result[1] = self.year
+        result[3] = self.version_bound
+        result[4] = self.type_of_grid.alternate_value
+        result[5] = self.rock_model.value
+        result[6] = self.grid_format.value
+        return result
+
+
+@dataclass
+class GridHead:
+    """
+    Both for lgr (see LGRSection) and the global grid (see GlobalGrid)
+    the GRIDHEAD keyword indicates the start of the grid layout for that
+    section.
+    """
+
+    type_of_grid: TypeOfGrid
+    num_x: int
+    num_y: int
+    num_z: int
+    grid_reference_number: int
+    numres: int
+    nseg: int
+    coordinate_type: CoordinateType
+    lgr_start: Tuple[int, int, int]
+    lgr_end: Tuple[int, int, int]
+
+    @classmethod
+    def from_egrid(cls, values: Sequence[int]):
+        if len(values) < 33:
+            raise ValueError(
+                f"Too few arguments to GridHead.from_egrid {len(values)} < 33"
+            )
+        return cls(
+            type_of_grid=TypeOfGrid(values[0]),
+            num_x=values[1],
+            num_y=values[2],
+            num_z=values[3],
+            grid_reference_number=values[4],
+            numres=values[24],
+            nseg=values[25],
+            coordinate_type=CoordinateType.from_bgrdecl(values[26]),
+            lgr_start=(values[27], values[28], values[29]),
+            lgr_end=(values[30], values[31], values[32]),
+        )
+
+    def to_egrid(self) -> np.ndarray:
+        # The data is expected to consist of
+        # 100 integers, but only a subset is used.
+        result = np.zeros((100,), dtype=np.int32)
+        result[0] = self.type_of_grid.value
+        result[1] = self.num_x
+        result[2] = self.num_y
+        result[3] = self.num_z
+        result[4] = self.grid_reference_number
+        result[24] = self.numres
+        result[25] = self.nseg
+        result[26] = self.coordinate_type.to_bgrdecl()
+        result[[27, 28, 29]] = np.array(self.lgr_start)
+        result[[30, 31, 32]] = np.array(self.lgr_end)
+        return result
+
+
+@dataclass
+class EGridSubGrid:
+    """
+    Both the LGR sections and the global grid contain a grid which is in the
+    general format of a eclipse grid. EGridSubGrid contain the common implementation.
+    """
+
+    grid_head: Optional[GridHead]
+    coord: np.ndarray
+    zcorn: np.ndarray
+    actnum: Optional[np.ndarray] = None
+
+    def __eq__(self, other):
+        return (
+            self.grid_head == other.grid_head
+            and np.array_equal(self.actnum, other.actnum)
+            and np.array_equal(self.coord, other.coord)
+            and np.array_equal(self.zcorn, other.zcorn)
+        )
+
+    def _check_xtgeo_compatible(self):
+        if self.grid_head.coordinate_type == CoordinateType.CYLINDRICAL:
+            raise NotImplementedError(
+                "Xtgeo does not currently support cylindrical coordinate systems"
+            )
+        if self.grid_head.numres < 1:
+            warnings.warn(
+                "EGrid file given with numres < 1, which is invalid, so assuming"
+                " instead that the file contains exactly one reservoir. XTGeo"
+                " prior to version 2.14 would output grids with numres == 0. If"
+                " the file was created with an older version of XTGeo, consider"
+                " importing the file with version 2.15 and re-exporting."
+            )
+            self.grid_head.numres = 1
+        if self.grid_head.numres > 1:
+            raise NotImplementedError(
+                "Xtgeo does not currently support multiple reservoirs"
+            )
+
+    @property
+    def dimensions(self) -> Tuple[int, int, int]:
+        return (
+            int(self.grid_head.num_x),
+            int(self.grid_head.num_y),
+            int(self.grid_head.num_z),
+        )
+
+    def to_egrid(self) -> List[Tuple[str, Any]]:
+        result = [
+            ("GRIDHEAD", self.grid_head.to_egrid()),
+            ("COORD   ", self.coord.astype(np.float32)),
+            ("ZCORN   ", self.zcorn.astype(np.float32)),
+        ]
+        if self.actnum is not None:
+            result.append(("ACTNUM  ", self.actnum.astype(np.int32)))
+        return result
+
+
+@dataclass
+class LGRSection(EGridSubGrid):
+    """
+    An Egrid file can contain multiple LGR (Local Grid Refinement) sections
+    which define a subgrid with finer layout.
+    """
+
+    name: Optional[str] = None
+    parent: Optional[str] = None
+    grid_parent: Optional[str] = None
+    hostnum: Optional[np.ndarray] = None
+    boxorig: Optional[Tuple[int, int, int]] = None
+    coord_sys: Optional[MapAxes] = None
+
+    def __eq__(self, other):
+        if not isinstance(other, LGRSection):
+            return False
+        return (
+            super().__eq__(other)
+            and self.name == other.name
+            and self.parent == other.parent
+            and self.grid_parent == other.grid_parent
+            and np.array_equal(self.hostnum, other.hostnum)
+            and self.boxorig == other.boxorig
+            and self.coord_sys == other.coord_sys
+        )
+
+    def __post_init__(self):
+        if self.name is None:
+            raise TypeError("Missing parameter to LGRSection: name")
+
+    def to_egrid(self) -> List[Tuple[str, Any]]:
+        result_dict = dict(super().to_egrid())
+        result_dict["LGR     "] = [self.name]
+        if self.parent is not None:
+            result_dict["LGRPARNT"] = [self.parent]
+        if self.grid_parent is not None:
+            result_dict["LGRSGRID"] = [self.grid_parent]
+        if self.hostnum is not None:
+            result_dict["HOSTNUM "] = self.hostnum
+        if self.boxorig is not None:
+            result_dict["BOXORIG "] = np.array(self.boxorig, dtype=np.int32)
+        if self.coord_sys is not None:
+            result_dict["COORDSYS"] = self.coord_sys.to_bgrdecl()
+        result_dict["ENDGRID "] = np.array([], dtype=np.int32)
+        result_dict["ENDLGR  "] = np.array([], dtype=np.int32)
+        result = []
+        order = [
+            "LGR     ",
+            "LGRPARNT",
+            "LGRSGRID",
+            "GRIDHEAD",
+            "BOXORIG ",
+            "COORD   ",
+            "COORDSYS",
+            "ZCORN   ",
+            "ACTNUM  ",
+            "HOSTNUM ",
+            "ENDGRID ",
+            "ENDLGR  ",
+        ]
+        for kw in order:
+            if kw in result_dict:
+                result.append((kw, result_dict[kw]))
+        return result
+
+
+@dataclass
+class GlobalGrid(EGridSubGrid):
+    """
+    The global grid contains the layout of the grid before
+    refinements, and the sectioning into grid coarsening
+    through the optional corsnum keyword.
+    """
+
+    coord_sys: Optional[MapAxes] = None
+    boxorig: Optional[Tuple[int, int, int]] = None
+    corsnum: Optional[np.ndarray] = None
+
+    def _check_xtgeo_compatible(self):
+        super()._check_xtgeo_compatible()
+        if self.corsnum is not None:
+            warnings.warn(
+                "egrid file given with coarsening, this is not directly supported "
+                " by xtgeo. Instead grid is imported without coarsening."
+            )
+
+        if self.coord_sys is not None:
+            warnings.warn(
+                "egrid file given with coordinate definition for global "
+                "grid, this is not directly supported by xtgeo. Instead "
+                "grid is imported without converting by local coordsys."
+            )
+
+    def __eq__(self, other):
+        if not isinstance(other, GlobalGrid):
+            return False
+        return (
+            super().__eq__(other)
+            and self.coord_sys == other.coord_sys
+            and self.boxorig == other.boxorig
+            and np.array_equal(self.corsnum, other.corsnum)
+        )
+
+    def to_egrid(self) -> List[Tuple[str, Any]]:
+        result_dict = dict(super().to_egrid())
+        if self.coord_sys is not None:
+            result_dict["COORDSYS"] = self.coord_sys.to_bgrdecl()
+        if self.boxorig is not None:
+            result_dict["BOXORIG "] = np.array(self.boxorig, dtype=np.int32)
+        if self.corsnum is not None:
+            result_dict["CORSNUM "] = self.corsnum
+        result_dict["ENDGRID "] = np.array([], dtype=np.int32)
+        result = []
+        order = [
+            "GRIDHEAD",
+            "BOXORIG ",
+            "COORD   ",
+            "COORDSYS",
+            "ZCORN   ",
+            "ACTNUM  ",
+            "CORSNUM ",
+            "ENDGRID ",
+        ]
+        for kw in order:
+            if kw in result_dict:
+                result.append((kw, result_dict[kw]))
+        return result
+
+
+@dataclass
+class NNCHead:
+    """
+    The NNCHead keyword denotes the start of a
+    NNCSection and contains the number of nncs and
+    the grid number of the grid where the NNCs applies.
+    """
+
+    num_nnc: int
+    grid_identifier: int
+
+    @classmethod
+    def from_egrid(cls, values: List[int]):
+        return cls(*values[0:2])
+
+    def to_egrid(self) -> np.ndarray:
+        result = np.zeros((10,), dtype=np.int32)
+        result[0] = self.num_nnc
+        result[1] = self.grid_identifier
+        return result
+
+
+@dataclass
+class NNCSection:
+    """The NNCSection's describe non-neighboor connections in the grid.
+
+    See, for instance, OPM user manual 2021-4 Rev. 1 Table D1.1 and 6.3.5.
+
+    Args:
+        nnchead: The nnc header
+        upstream_nnc: list of cells (by index) for the upstream nnc.
+        downstream_nnc: list of cells (by index) for the downstream nnc
+            to be connected to the corresponding cell in upstream_nnc.
+        nncl: list of LGR cells (by index) to be connected to the global grid.
+        nncg: list of global cells (by index) connected to the corresponding
+         LGR cells in nncl.
+
+    """
+
+    nnchead: NNCHead
+    upstream_nnc: np.ndarray
+    downstream_nnc: np.ndarray
+    nncl: Optional[np.ndarray] = None
+    nncg: Optional[np.ndarray] = None
+
+    def __eq__(self, other):
+        if not isinstance(other, NNCSection):
+            return False
+        return (
+            self.nnchead == other.nnchead
+            and np.array_equal(self.upstream_nnc, other.upstream_nnc)
+            and np.array_equal(self.downstream_nnc, other.downstream_nnc)
+            and np.array_equal(self.nncl, other.nncl)
+            and np.array_equal(self.nncg, other.nncg)
+        )
+
+    def to_egrid(self) -> List[Tuple[str, Any]]:
+        result = [
+            ("NNCHEAD ", self.nnchead.to_egrid()),
+            ("NNC1    ", self.upstream_nnc),
+            ("NNC2    ", self.downstream_nnc),
+        ]
+        if self.nncl is not None:
+            result.append(("NNCL    ", self.nncl))
+        if self.nncg is not None:
+            result.append(("NNCG    ", self.nncg))
+        return result
+
+
+@dataclass
+class AmalgamationSection:
+    """The AmalgamationSection's describe the amalgamation of two LGR's.
+
+    See, for instance, OPM user manual 2021-4 Rev. 1 Table D1.1 and 6.3.5.
+
+    Args:
+    lgr_idxs: The indexes of the LGR's to be amalgamated
+    nna1: indecies in the first lgr connected in the amalgamation.
+    nna2: indecies in the second lgr connected in the amalgamation, to
+        the corresponding cell in nna1.
+
+    """
+
+    lgr_idxs: Tuple[int, int]
+    nna1: Optional[np.ndarray]
+    nna2: Optional[np.ndarray]
+
+    def __eq__(self, other):
+        if not isinstance(other, AmalgamationSection):
+            return False
+        return (
+            self.lgr_idxs == other.lgr_idxs
+            and np.array_equal(self.nna1, other.nna1)
+            and np.array_equal(self.nna2, other.nna2)
+        )
+
+    def to_egrid(self) -> List[Tuple[str, Any]]:
+        return [
+            ("NNCHEADA", np.array(self.lgr_idxs, np.int32)),
+            ("NNA1    ", self.nna1),
+            ("NNA2    ", self.nna2),
+        ]
+
+
+@dataclass
+class EGridHead:
+    """The EGridHead section occurs once at the start of an EGrid file."""
+
+    file_head: Filehead
+    mapunits: Optional[Units] = None
+    mapaxes: Optional[MapAxes] = None
+    gridunit: Optional[GridUnit] = None
+    gdorient: Optional[GdOrient] = None
+
+    def to_egrid(self) -> List[Tuple[str, Any]]:
+        result = [
+            ("FILEHEAD", self.file_head.to_egrid()),
+        ]
+        if self.mapunits is not None:
+            result.append(("MAPUNITS", [self.mapunits.to_bgrdecl()]))
+        if self.mapaxes is not None:
+            result.append(("MAPAXES ", self.mapaxes.to_bgrdecl()))
+        if self.gridunit is not None:
+            result.append(("GRIDUNIT", self.gridunit.to_bgrdecl()))
+        if self.gdorient is not None:
+            result.append(("GDORIENT", self.gdorient.to_bgrdecl()))
+        return result
+
+
+@dataclass
+class EGrid(EclGrid):
+    """Contains the data of an EGRID file.
+
+    Args:
+        egrid_head: The file header starting with the FILEHEAD keyword and
+            contains optional information about units, map relative location, and
+            orientation.
+        global_grid: The global grid
+        lgr_sections: List of local grid refinements.
+        nnc_sections: Describe non-neighboring sections as a list of either
+            NNCSections or AmalgamationSection's.
+    """
+
+    egrid_head: EGridHead
+    global_grid: GlobalGrid
+    lgr_sections: List[LGRSection]
+    # The nnc_sections are kept as one list which can consist of both
+    # NNCSection and AmalgamationSection as these occur interspersed in the
+    # file. The order seems to be sorted by LGR index. Keeping them in
+    # one list keeps the data layout of EGrid 1-to-1 with the contents
+    # of the file.
+    nnc_sections: List[Union[NNCSection, AmalgamationSection]]
+
+    @classmethod
+    def default_settings_grid(
+        cls,
+        coord: np.ndarray,
+        zcorn: np.ndarray,
+        actnum: Optional[np.ndarray],
+        size: Tuple[int, int, int],
+    ):
+        grid_head = GridHead(
+            TypeOfGrid.CORNER_POINT,
+            *size,
+            1,
+            1,
+            1,
+            CoordinateType.CARTESIAN,
+            (0, 0, 0),
+            (0, 0, 0),
+        )
+        global_grid = GlobalGrid(
+            grid_head,
+            coord,
+            zcorn,
+            actnum,
+        )
+        return EGrid(
+            EGridHead(
+                Filehead(
+                    3,
+                    2007,
+                    3,
+                    TypeOfGrid.CORNER_POINT,
+                    RockModel.SINGLE_PERMEABILITY_POROSITY,
+                    GridFormat.IRREGULAR_CORNER_POINT,
+                ),
+                gridunit=GridUnit(),
+            ),
+            global_grid,
+            [],
+            [],
+        )
+
+    @property
+    def coord(self) -> np.ndarray:
+        return self.global_grid.coord
+
+    @coord.setter
+    def coord(self, value: np.ndarray):
+        self.global_grid.coord = value
+
+    @property
+    def zcorn(self) -> np.ndarray:
+        return self.global_grid.zcorn
+
+    @zcorn.setter
+    def zcorn(self, value: np.ndarray):
+        self.global_grid.zcorn = value
+
+    @property
+    def actnum(self) -> Optional[np.ndarray]:
+        return self.global_grid.actnum
+
+    @classmethod
+    def from_file(cls, filelike, fileformat: str = None):
+        """
+        Read an egrid file
+        Args:
+            filelike (str,Path,stream): The egrid file to be read.
+            file_format (None or str): The format of the file (either "egrid"
+                or "fegrid") None means guess.
+        Returns:
+            EGrid with the contents of the file.
+        """
+        file_format = None
+        if fileformat == "egrid":
+            file_format = Format.UNFORMATTED
+        elif fileformat == "fegrid":
+            file_format = Format.FORMATTED
+        elif fileformat is not None:
+            raise ValueError(f"Unrecognized egrid file format {fileformat}")
+        return EGridReader(filelike, file_format=file_format).read()
+
+    def to_file(self, filelike, fileformat: str = "egrid"):
+        """
+        write the EGrid to file.
+        Args:
+            filelike (str,Path,stream): The egrid file to write to.
+            file_format (ecl_data_io.Format): The format of the file.
+        """
+        file_format = None
+        if fileformat == "egrid":
+            file_format = Format.UNFORMATTED
+        elif fileformat == "fegrid":
+            file_format = Format.FORMATTED
+        elif fileformat is not None:
+            raise ValueError(f"Unrecognized egrid file format {fileformat}")
+        contents = []
+        contents += self.egrid_head.to_egrid()
+        contents += self.global_grid.to_egrid()
+        for lgr in self.lgr_sections:
+            contents += lgr.to_egrid()
+        for nnc in self.nnc_sections:
+            contents += nnc.to_egrid()
+        write(filelike, contents, file_format)
+
+    def _check_xtgeo_compatible(self):
+        self.global_grid._check_xtgeo_compatible()
+        if self.lgr_sections:
+            warnings.warn(
+                "UserWarning: egrid file contains local grid refinements (LGR). "
+                "LGR's are not directly supported, only the global grid is "
+                "imported."
+            )
+
+    @property
+    def is_map_relative(self) -> bool:
+        if self.egrid_head.gridunit is None:
+            return False
+        return self.egrid_head.gridunit.grid_relative == GridRelative.MAP
+
+    @property
+    def mapaxes(self) -> Optional[MapAxes]:
+        return self.egrid_head.mapaxes
+
+    @mapaxes.setter
+    def mapaxes(self, value):
+        self.egrid_head.mapaxes = value
+
+    @property
+    def dimensions(self) -> Tuple[int, int, int]:
+        return self.global_grid.dimensions
+
+    @property
+    def map_axis_units(self) -> Units:
+        return self.egrid_head.mapunits
+
+    @map_axis_units.setter
+    def map_axis_units(self, value):
+        self.egrid_head.mapunits = value
+
+    @property
+    def grid_units(self) -> Units:
+        return self.egrid_head.gridunit.unit
+
+    @grid_units.setter
+    def grid_units(self, value):
+        self.egrid_head.gridunit.unit = value
+
+    @classmethod
+    def from_xtgeo_grid(cls, xtgeo_grid):
+        default_grid = super().from_xtgeo_grid(xtgeo_grid)
+
+        default_grid.global_grid.coord = default_grid.global_grid.coord.astype(
+            np.float32
+        )
+        default_grid.global_grid.zcorn = default_grid.global_grid.zcorn.astype(
+            np.float32
+        )
+        if xtgeo_grid._dualporo:
+            default_grid.rock_model = RockModel.DUAL_POROSITY
+        if xtgeo_grid._dualperm:
+            default_grid.rock_model = RockModel.DUAL_PERMEABILITY
+
+        if default_grid.egrid_head.gridunit is None:
+            warnings.warn(
+                "Unitless xtgeo grid converted to egrid. Assuming meters as unit."
+            )
+            default_grid.egrid_head.gridunit = GridUnit()
+        return default_grid
+
+
+keyword_translation = {
+    "FILEHEAD": "file_head",
+    "MAPUNITS": "mapunits",
+    "MAPAXES ": "mapaxes",
+    "GRIDUNIT": "gridunit",
+    "GDORIENT": "gdorient",
+    "LGR     ": "name",
+    "GRIDHEAD": "grid_head",
+    "HOSTNUM ": "hostnum",
+    "BOXORIG ": "boxorig",
+    "COORDSYS": "coord_sys",
+    "LGRPARNT": "parent",
+    "LGRSGRID": "grid_parent",
+    "COORD   ": "coord",
+    "ZCORN   ": "zcorn",
+    "ACTNUM  ": "actnum",
+    "NNCHEAD ": "nnchead",
+    "NNC1    ": "upstream_nnc",
+    "NNC2    ": "downstream_nnc",
+    "NNCL    ": "nncl",
+    "NNCG    ": "nncg",
+    "NNCHEADA": "lgr_idxs",
+    "NNA1    ": "nna1",
+    "NNA2    ": "nna2",
+    "CORSNUM ": "corsnum",
+}
+
+
+class EGridReader:
+    """
+    The EGridReader reads an egrid file through the `read` method.
+
+    Args:
+        filelike (str, Path, stream): The egrid file to read from.
+        file_format (None or ecl_data_io.Format): The format of the file,
+            None means guess.
+
+    """
+
+    def __init__(self, filelike, file_format: Format = None):
+        self.filelike = filelike
+        self.keyword_generator = lazy_read(filelike, file_format)
+
+    def read_section(
+        self,
+        keyword_factories: Dict[str, Callable],
+        required_keywords: Set[str],
+        stop_keywords: Iterable[str],
+        skip_keywords: Iterable[str] = [],
+        keyword_visitors: Iterable[Callable] = [],
+    ):
+        """
+        Read a general egrid file section.
+        Args:
+            keyword_factories (dict[str, func]): The function used
+                to construct a section member.
+            required_keywords (List[str]): List of keywords that are required
+                for the given section.
+            stop_keywords (List[str]): List of keywords which when read ends
+                the section. The keyword generator will be at the first keyword
+                in stop_keywords after read_section is called.
+            skip_keywords (List[str]): List of keywords that does not
+                have a factory, which should just be skipped.
+            keyword_visitors (List[func]): List of functions that
+                "visit" each keyword. Each of these functions are called
+                for each keyword, value pair and can be used to
+                preprocess the data.
+
+        Returns:
+            dictionary of parameters for the constructor of the given section.
+        """
+        results = {}
+        i = 0
+        while True:
+            try:
+                entry = next(self.keyword_generator)
+            except StopIteration:
+                break
+            kw = entry.read_keyword()
+            if kw in skip_keywords:
+                continue
+            if kw in stop_keywords and i > 0:
+                # Optional keywords were possibly omitted and
+                # we have reached the global grid section
+                # push back the grid head of the global grid
+                # and proceed
+                self.keyword_generator = chain([entry], self.keyword_generator)
+                break
+            if kw in results:
+                raise EGridFileFormatError(f"Duplicate keyword {kw} in {self.filelike}")
+            try:
+                factory = keyword_factories[kw]
+            except KeyError as err:
+                raise EGridFileFormatError(f"Unknown egrid keyword {kw}") from err
+            try:
+                value = factory(entry.read_array())
+                results[kw] = value
+            except (ValueError, IndexError, TypeError) as err:
+                raise EGridFileFormatError(f"Incorrect values in keyword {kw}") from err
+            for visit in keyword_visitors:
+                visit(kw, value)
+            i += 1
+
+        missing_keywords = required_keywords.difference(results.keys())
+        params = {keyword_translation[kw]: v for kw, v in results.items()}
+        if missing_keywords:
+            raise EGridFileFormatError(f"Missing required keywords {missing_keywords}")
+        return params
+
+    def read_header(self) -> EGridHead:
+        """
+        Reads the EGrid header from the start of the stream. Ensures
+        that the keyword_generator is at the first GRIDHEAD keyword
+        after the header.
+        """
+        params = self.read_section(
+            keyword_factories={
+                "FILEHEAD": Filehead.from_egrid,
+                "MAPUNITS": lambda x: Units.from_bgrdecl(x[0]),
+                "MAPAXES ": MapAxes.from_bgrdecl,
+                "GRIDUNIT": GridUnit.from_bgrdecl,
+                "GDORIENT": GdOrient.from_bgrdecl,
+            },
+            required_keywords={"FILEHEAD"},
+            stop_keywords=["GRIDHEAD"],
+        )
+        return EGridHead(**params)
+
+    def read_global_grid(self) -> GlobalGrid:
+        """
+        Reads the global grid section from the start of the keyword_generator,
+        ensures the keyword_generator is at the keyword after the first ENDGRID
+        keyword encountered.
+        """
+
+        def check_gridhead(kw: str, value):
+            if kw == "GRIDHEAD" and value.type_of_grid != TypeOfGrid.CORNER_POINT:
+                raise NotImplementedError(
+                    "XTGeo does not support unstructured or mixed grids."
+                )
+
+        params = self.read_section(
+            keyword_factories={
+                "GRIDHEAD": GridHead.from_egrid,
+                "BOXORIG ": tuple,
+                "COORDSYS": MapAxes.from_bgrdecl,
+                "COORD   ": lambda x: np.array(x, dtype=np.float32),
+                "ZCORN   ": lambda x: np.array(x, dtype=np.float32),
+                "ACTNUM  ": lambda x: np.array(x, dtype=np.int32),
+                "CORSNUM ": lambda x: np.array(x, dtype=np.int32),
+            },
+            required_keywords={"GRIDHEAD", "COORD   ", "ZCORN   "},
+            stop_keywords=["ENDGRID "],
+            keyword_visitors=[check_gridhead],
+        )
+        try:
+            entry = next(self.keyword_generator)
+        except StopIteration as err:
+            raise EGridFileFormatError(
+                "Did not read ENDGRID after global grid"
+            ) from err
+        if entry.read_keyword() != "ENDGRID ":
+            raise EGridFileFormatError("Did not read ENDGRID after global grid")
+        return GlobalGrid(**params)
+
+    def read_subsections(self) -> Tuple[List[LGRSection], List[NNCSection]]:
+        """
+        Reads lgr and nnc subsections from the start of the keyword_generator.
+        """
+        lgr_sections = []
+        nnc_sections = []
+        while True:
+            try:
+                entry = next(self.keyword_generator)
+            except StopIteration:
+                break
+            self.keyword_generator = chain([entry], self.keyword_generator)
+            keyword = entry.read_keyword().rstrip()
+            if keyword == "LGR":
+                lgr_sections.append(self.read_lgr_subsection())
+            elif keyword == "NNCHEAD":
+                nnc_sections.append(self.read_nnc_subsection())
+            elif keyword == "NNCHEADA":
+                nnc_sections.append(self.read_amalgamation_subsection())
+            else:
+                raise EGridFileFormatError(
+                    f"egrid subsection started with unexpected keyword {keyword}"
+                )
+        return lgr_sections, nnc_sections
+
+    def read_lgr_subsection(self) -> LGRSection:
+        """
+        Reads one lgr subsection from the start of the keyword generator.
+        After read_lgr_subsection is called, The keyword_generator is at the
+        keyword after the first ENDLGR keyword encountered, or end of stream.
+        """
+        params = self.read_section(
+            keyword_factories={
+                "LGR     ": lambda x: x[0].decode("ascii"),
+                "LGRPARNT": lambda x: x[0].decode("ascii"),
+                "LGRSGRID": lambda x: x[0].decode("ascii"),
+                "GRIDHEAD": GridHead.from_egrid,
+                "BOXORIG ": tuple,
+                "COORDSYS": MapAxes.from_bgrdecl,
+                "COORD   ": lambda x: np.array(x, dtype=np.float32),
+                "ZCORN   ": lambda x: np.array(x, dtype=np.float32),
+                "ACTNUM  ": lambda x: np.array(x, dtype=np.int32),
+                "HOSTNUM ": lambda x: np.array(x, dtype=np.int32),
+            },
+            required_keywords={
+                "LGR     ",
+                "GRIDHEAD",
+                "COORD   ",
+                "ZCORN   ",
+                "HOSTNUM ",
+            },
+            skip_keywords=["ENDGRID "],
+            stop_keywords=["ENDLGR  "],
+        )
+        try:
+            entry = next(self.keyword_generator)
+        except StopIteration as err:
+            raise EGridFileFormatError("Did not read ENDLGR after lgr section") from err
+        if entry.read_keyword() != "ENDLGR  ":
+            raise EGridFileFormatError("Did not read ENDLGR after lgr section")
+        return LGRSection(**params)
+
+    def read_nnc_subsection(self) -> NNCSection:
+        """
+        Reads one nnc subsection from the start of the keyword generator.
+        After read_nncsubsection is called, The keyword_generator is
+        at the next NNCHEAD, NNCHEADA or LGR keyword, or end of stream.
+        """
+        params = self.read_section(
+            keyword_factories={
+                "NNCHEAD ": NNCHead.from_egrid,
+                "NNC1    ": lambda x: np.array(x, dtype=np.int32),
+                "NNC2    ": lambda x: np.array(x, dtype=np.int32),
+                "NNCL    ": lambda x: np.array(x, dtype=np.int32),
+                "NNCG    ": lambda x: np.array(x, dtype=np.int32),
+            },
+            required_keywords={"NNCHEAD ", "NNC1    ", "NNC2    "},
+            stop_keywords=["NNCHEAD ", "LGR     ", "NNCHEADA"],
+        )
+        return NNCSection(**params)
+
+    def read_amalgamation_subsection(self) -> AmalgamationSection:
+        """
+        Reads one amalgamation subsection from the start of the keyword
+        generator. After read_nncsubsection is called, The keyword_generator is
+        at the next NNCHEAD, NNCHEADA or LGR keyword, or end of stream.
+        """
+        params = self.read_section(
+            keyword_factories={
+                "NNCHEADA": lambda x: tuple(x[0:2]),
+                "NNA1    ": lambda x: np.array(x, dtype=np.int32),
+                "NNA2    ": lambda x: np.array(x, dtype=np.int32),
+            },
+            required_keywords={"NNCHEADA", "NNA1    ", "NNA2    "},
+            stop_keywords=["NNCHEAD ", "LGR     ", "NNCHEADA"],
+        )
+        return AmalgamationSection(**params)
+
+    def read(self) -> EGrid:
+        header = self.read_header()
+        if header.file_head.type_of_grid != TypeOfGrid.CORNER_POINT:
+            raise NotImplementedError(
+                "XTGeo does not support unstructured or mixed grids."
+            )
+        global_grid = self.read_global_grid()
+        lgr_sections, nnc_sections = self.read_subsections()
+        return EGrid(header, global_grid, lgr_sections, nnc_sections)
```

## xtgeo/grid3d/_find_gridprop_in_eclrun.py

 * *Ordering differences only*

```diff
@@ -1,632 +1,632 @@
-import functools
-import itertools
-import operator
-import pathlib
-import warnings
-from pathlib import Path
-from typing import Dict, List, Optional, Union
-
-import ecl_data_io as eclio
-import numpy as np
-from typing_extensions import Literal
-
-import xtgeo
-
-from ._ecl_inte_head import InteHead
-from ._ecl_logi_head import LogiHead
-from ._ecl_output_file import Phases
-from ._grdecl_format import match_keyword
-
-sat_keys = ["SOIL", "SGAS", "SWAT"]
-
-
-def filter_lgr(generator):
-    try:
-        while True:
-            entry = next(generator)
-            if entry.read_keyword() == "LGR":
-                warnings.warn(
-                    "Found LGR in ecl run file. "
-                    "LGR's are not directly supported, "
-                    "instead only global values are imported."
-                )
-                while entry.read_keyword() != "ENDLGR":
-                    entry = next(generator)
-            else:
-                yield entry
-    except StopIteration:
-        return
-
-
-# Based on which phases are present some saturation values are given default
-# values, e.g. if phases=Phases.OIL, the saturation of oil ("SOIL") is 1.0 and
-# all other phases are 0.0.
-DEFAULT_SATURATIONS = {
-    Phases.OIL: {
-        "SOIL": 1.0,
-        "SWAT": 0.0,
-        "SGAS": 0.0,
-    },
-    Phases.GAS: {
-        "SOIL": 0.0,
-        "SWAT": 0.0,
-        "SGAS": 1.0,
-    },
-    Phases.WATER: {
-        "SOIL": 0.0,
-        "SWAT": 1.0,
-        "SGAS": 0.0,
-    },
-    Phases.OIL_WATER: {
-        "SGAS": 0.0,
-    },
-    Phases.OIL_GAS: {
-        "SWAT": 0.0,
-    },
-    Phases.GAS_WATER: {
-        "SOIL": 0.0,
-    },
-    Phases.OIL_WATER_GAS: dict(),
-    Phases.E300_GENERIC: dict(),
-}
-
-
-def remainder_saturations(saturations):
-    """Infers remainder saturations based on sum(saturations.values()) == 1.
-
-    >>> remainder_saturations({'SWAT': 0.5, 'SGAS': 0.5})
-    {'SOIL': 0.0}
-
-    Args:
-        saturations: Dictionary of phases, such as returned by
-            :meth:`default_saturations()`.
-
-    Returns:
-        dictionary of saturation values that can be inferred.
-    """
-    if all(k in saturations for k in sat_keys):
-        return dict()
-    if any(k not in sat_keys for k in saturations):
-        raise ValueError(f"Unknown saturation keys: {list(saturations.keys())}")
-    rest = sum(saturations.values())
-    if len(saturations) == 2 or np.allclose(rest, 1.0):
-        missing = set(sat_keys).difference(set(saturations.keys()))
-        return {m: 1.0 - rest for m in missing}
-    return dict()
-
-
-def peek_headers(generator):
-    """Reads header from a ecl_data_io keyword generator without consuming keywords.
-
-    Args:
-        generator: keyword generator such as returned by ecl_data_io.lazy_read
-
-    Returns:
-        Tuple of inthead, logihead, and a modified generator which contains all original
-        keywords.
-    """
-
-    def read_headers(generator):
-        intehead_array = None
-        logihead_array = None
-        while intehead_array is None or logihead_array is None:
-            entry = next(generator)
-            kw = entry.read_keyword()
-            if match_keyword(kw, "LOGIHEAD"):
-                logihead_array = entry.read_array()
-            if match_keyword(kw, "INTEHEAD"):
-                intehead_array = entry.read_array()
-
-        intehead = InteHead(intehead_array)
-        logihead = LogiHead.from_file_values(logihead_array, intehead.simulator)
-        return intehead, logihead
-
-    header_generator, generator = itertools.tee(generator)
-    try:
-        intehead, logihead = read_headers(header_generator)
-    except StopIteration as stopit:
-        raise ValueError("Reached end of file without reading headers") from stopit
-    return intehead, logihead, generator
-
-
-def get_fetch_names(name: str) -> List[str]:
-    """Given a gridproperty name, give list of supporting keyword names.
-
-    >>> get_fetch_names('PORO')
-    ['PORO']
-    >>> get_fetch_names('SWAT')
-    ['SOIL', 'SGAS', 'SWAT']
-
-    Args:
-        name: The name of a grid property
-    Returns:
-        List of grid properties that must be fetched from the file.
-
-    """
-    if any(match_keyword(name, saturation_keyword) for saturation_keyword in sat_keys):
-        fetch_names = sat_keys
-    else:
-        fetch_names = [name]
-    return fetch_names
-
-
-def read_values(generator, intehead, names, lengths="all"):
-    """Read the given list of parameter values from the generator.
-
-    Reads the given list of values from the generator. Some saturation
-    values may be inferred from the invariant sum(saturations.values()) == 1.0
-    (see  :meth:`remainder_saturations()`.
-
-    """
-
-    def flatten(lst):
-        return functools.reduce(operator.iconcat, lst, [])
-
-    fetch_names = set(flatten(list(get_fetch_names(name) for name in names)))
-    defaulted = list()
-    if names == "all":
-        values = dict()
-    else:
-        values = DEFAULT_SATURATIONS[intehead.phases].copy()
-        defaulted = list(values.keys())
-
-    for entry in generator:
-        if all(name in values for name in fetch_names):
-            break
-        kw = entry.read_keyword()
-        if lengths != "all":
-            if entry.read_length() not in lengths:
-                continue
-        if names == "all":
-            key = kw.rstrip()
-            array = entry.read_array()
-            if np.issubdtype(array.dtype, np.number) or np.issubdtype(
-                array.dtype, bool
-            ):
-                values[key] = entry.read_array()
-        else:
-            matched = [name for name in fetch_names if match_keyword(kw, name)]
-            if len(matched) == 1:
-                if matched[0] in values and matched[0] not in defaulted:
-                    raise ValueError(f"Found duplicate keyword {matched[0]}")
-                values[matched[0]] = entry.read_array()
-            elif len(matched) > 1:
-                # This should not happen if get_fetch_names and
-                # match_keyword work as intended
-                raise ValueError(f"Ambiguous keywords {matched} matched vs {kw}")
-
-    if names == "all":
-        # A more consistent behavior would be to include saturations calculated
-        # with remainder_saturations when names=="all" aswell, however, we do
-        # not include those to keep backwards compatability.
-        # TODO: deprecate this behavior
-        return values
-    else:
-        values.update(
-            **remainder_saturations({k: values[k] for k in sat_keys if k in values})
-        )
-        return {name: values[name] for name in names if name in values}
-
-
-def check_grid_match(intehead: InteHead, logihead: LogiHead, grid):
-    """Checks that the init/restart headers matches the grid
-
-    Checks that the values given in the headers are compatible with
-    the grid.
-    """
-    dimensions = intehead.num_x, intehead.num_y, intehead.num_z
-
-    if logihead.dual_porosity:
-        dimensions = intehead.num_x, intehead.num_y, intehead.num_z // 2
-
-    if logihead.dual_porosity != grid.dualporo:
-        raise ValueError("Grid dual poro status does not match output file")
-
-    if dimensions != grid.dimensions:
-        raise ValueError(
-            "Grid dimensions do not match dimensions given in output file,"
-            f" {dimensions} vs {grid.dimensions}"
-        )
-
-
-def expand_scalar_values(value, num_cells, dualporo: bool) -> np.ndarray:
-    """Convert from scalar value to filled array of expected size.
-    Args:
-        value: The potentially scalar value
-        num_cells: The number of cells in the grid
-        dualporo: Whether the model has dual porosity
-    Returns:
-        If value is an array, then returns that array, otherwise
-        calls np.full with the shape determined by dualporo status and
-        number of cells.
-
-    """
-    if dualporo:
-        return np.full(fill_value=value, shape=num_cells * 2)
-    return np.full(fill_value=value, shape=num_cells)
-
-
-def pick_dualporo_values(
-    values: np.ndarray, actind: np.ndarray, num_cells: int, fracture: bool
-) -> np.ndarray:
-    """From array of values in an ecl run file, give the fracture or matrix values.
-
-    Args:
-        values: Array of values from an ecl run file.
-        actind: Array of the indecies of active cells.
-        num_cells: Total number of cells.
-        fracture: Whether to give the fracture or matrix values.
-    Returns:
-        Array of either fracture or matrix values from the input values.
-    """
-    active_size = len(actind)
-    if len(values) == 2 * num_cells:
-        indsize = num_cells
-    else:
-        indsize = active_size
-    if fracture:
-        return values[-indsize:]
-    return values[:indsize]
-
-
-def valid_gridprop_lengths(grid):
-    num_cells = np.prod(grid.dimensions)
-    if grid.dualporo:
-        num_fracture = len(grid.get_dualactnum_indices(fracture=True))
-        num_matrix = len(grid.get_dualactnum_indices(fracture=False))
-        return [2 * num_cells, num_fracture + num_matrix]
-    else:
-        num_active = len(grid.get_actnum_indices())
-        return [num_cells, num_active]
-
-
-def match_values_to_active_cells(
-    values,
-    actind,
-    num_cells,
-) -> np.ndarray:
-    """Expands array of ecl run values to be one-to-one with cells.
-
-    In the ecl run file, the values might be only those for active cells.
-    This funtion expands those values to one for each cell with non-active
-    indecies given the xtgeo.UNDEF/xtgeo.UNDEF_INT value.
-
-    Args:
-        values: Array of values from an ecl run file.
-        actind: Array of the indecies of active cells.
-        num_cells: Total number of cells.
-    Returns:
-        Array of input values, but guaranteed num_cells length.
-
-    """
-    if len(values) != len(actind):
-        raise ValueError(
-            f"Unexpected shape of values in init file: {np.asarray(values).shape}, "
-            f"expected to match grid dimensions {num_cells} or "
-            f"number of active cells {len(actind)}"
-        )
-
-    if np.issubdtype(values.dtype, np.integer):
-        undef = xtgeo.UNDEF_INT
-    else:
-        undef = xtgeo.UNDEF
-    result = np.full(fill_value=undef, shape=num_cells, dtype=values.dtype)
-    result[actind] = values
-    return result
-
-
-def make_gridprop_values(values, grid, fracture):
-    """Converts values given in init or restart file to one suitable for GridProperty.
-
-    Args:
-    values: The array read from the file
-    grid: The grid from the ecl run
-    fracture: Whether to get the fracture or matrix values
-
-    Returns:
-        Masked array of values indexed by cell
-    """
-    num_cells = np.prod(grid.dimensions)
-    if np.isscalar(values):
-        values = expand_scalar_values(values, num_cells, grid.dualporo)
-
-    if grid.dualporo:
-        actind = grid.get_dualactnum_indices(fracture=fracture, order="F")
-        values = pick_dualporo_values(values, actind, num_cells, fracture)
-    else:
-        actind = grid.get_actnum_indices(order="F")
-
-    if len(values) != num_cells:
-        values = match_values_to_active_cells(values, actind, num_cells)
-
-    values = values.reshape(grid.dimensions, order="F")
-
-    if grid.dualporo:
-        if fracture:
-            values[grid._dualactnum.values == 1] = 0.0
-        else:
-            values[grid._dualactnum.values == 2] = 0.0
-
-    return np.ma.masked_where(grid.get_actnum().values < 1, values)
-
-
-def date_from_intehead(intehead: InteHead) -> Optional[int]:
-    """Returns date format for use in GridProperty name given intehead."""
-    if any(val is None for val in [intehead.day, intehead.month, intehead.year]):
-        return None
-    return intehead.day + intehead.month * 100 + intehead.year * 10000
-
-
-def gridprop_params(values, name, date, grid, fracture):
-    """Make dictionary of GridProperty parameters from imported values."""
-    result = dict()
-    result["name"] = name
-    result["date"] = str(date) if date is not None else None
-    result["fracture"] = fracture
-
-    result["ncol"], result["nrow"], result["nlay"] = grid.dimensions
-    result["dualporo"] = grid.dualporo
-    result["dualperm"] = grid.dualperm
-
-    result["values"] = make_gridprop_values(values, grid, fracture)
-
-    if np.issubdtype(result["values"].dtype, np.integer):
-        uniq = np.unique(values).tolist()
-        codes = dict(zip(uniq, uniq))
-        codes = {key: str(val) for key, val in codes.items()}
-        result["codes"] = codes
-        result["values"] = result["values"].astype(np.int32)
-        result["discrete"] = True
-    else:
-        result["codes"] = dict()
-        result["values"] = result["values"].astype(np.float64)
-        result["discrete"] = False
-    return result
-
-
-def get_actnum_from_porv(init_filelike, grid):
-    """Override actnum value based on the cell pore volume.
-
-    Args:
-        init_filelike: The init file
-        grid: The grid used by the simulator to produce the init file.
-    Returns:
-        None
-    """
-    generator = filter_lgr(eclio.lazy_read(init_filelike))
-    intehead, logihead, generator = peek_headers(generator)
-
-    if "INTERSECT" not in str(intehead.simulator):
-        # no need to continue if other simulators than INTERSECT
-        return
-
-    check_grid_match(intehead, logihead, grid)
-    porv = read_values(
-        generator, intehead, ["PORV"], lengths=valid_gridprop_lengths(grid)
-    )
-
-    original_active = grid.nactive
-    if porv:
-        if grid.dualporo:
-            num_cells = np.prod(grid.dimensions)
-            actnum_matrix = np.where(
-                porv["PORV"][:num_cells].reshape(grid.dimensions, order="F") > 0.0, 1, 0
-            )
-            actnum_fracture = np.where(
-                porv["PORV"][num_cells:].reshape(grid.dimensions, order="F") > 0.0, 2, 0
-            )
-            grid._dualactnum.values = actnum_matrix + actnum_fracture
-        else:
-            acttmp = grid.get_actnum().copy()
-            acttmp.values = np.where(
-                porv["PORV"].reshape(grid.dimensions, order="F") > 0.0, 1, 0
-            )
-            grid.set_actnum(acttmp)
-
-    new_active = grid.nactive
-    if new_active != original_active:
-        warnings.warn(
-            "The original active cells (ACTNUM) has been overrided by PORV criteria "
-            "from INIT file (purpose: fix simulator INTERSECT issue)",
-            UserWarning,
-        )
-
-
-def find_gridprop_from_init_file(
-    init_filelike,
-    names: Union[List[str], Literal["all"]],
-    grid,
-    fracture: bool = False,
-) -> List[Dict]:
-    """Finds all parameters in a init matching names.
-
-    Note: Does not check that all names are found.
-
-    Args:
-        init_filelike: The init file
-        names: List of property names to be imported. Can also,
-            be set to "all" to import all parameters.
-        grid: The grid used by the simulator to produce the init file.
-        fracture: If a dual porosity module, indicates that the fracture
-            (as apposed to the matrix) grid property should be imported.
-    Returns:
-        List of GridProperty parameters matching the names.
-
-    """
-    init_stream = not isinstance(init_filelike, (str, Path))
-    if init_stream:
-        orig_pos = init_filelike.tell()
-    get_actnum_from_porv(init_filelike, grid)
-    if init_stream:
-        init_filelike.seek(orig_pos, 0)
-    generator = filter_lgr(eclio.lazy_read(init_filelike))
-    intehead, logihead, generator = peek_headers(generator)
-
-    check_grid_match(intehead, logihead, grid)
-
-    date = date_from_intehead(intehead)
-    return [
-        gridprop_params(v, name, date, grid, fracture)
-        for name, v in read_values(
-            generator, intehead, names, lengths=valid_gridprop_lengths(grid)
-        ).items()
-    ]
-
-
-def section_generator(generator):
-    """Sections the generator as delimited by "SEQNUM" keyword.
-
-    Unified restart files will repeat properties in sections, one for each
-    date. The "SEQNUM" keyword indicates a new section. section_generator
-    takes a keyword generator and returns a generator over sections.
-
-    Note: It does so in a lighweight manner so that a section
-    is emptied once the next section is requested, and the original
-    generator is iterated as you progress in the sections.
-    """
-    try:
-        first_seq = next(generator)
-        if not match_keyword(first_seq.read_keyword(), "SEQNUM"):
-            raise ValueError("Restart file did not start with SEQNUM.")
-    except StopIteration:
-        return
-    while True:
-        this_section = itertools.takewhile(
-            lambda x: not match_keyword(x.read_keyword(), "SEQNUM"), generator
-        )
-        yield this_section
-        # empty the section iterator
-        for _ in this_section:
-            pass
-        # peek ahead to see if there are more elements
-        # and stop if there are not
-        try:
-            entry = next(generator)
-            generator = itertools.chain(iter([entry]), generator)
-        except StopIteration:
-            return
-
-
-def find_gridprops_from_restart_file_sections(
-    sections,
-    names: Union[List[str], Literal["all"]],
-    dates: Union[List[int], Literal["all", "last", "first"]],
-    grid,
-    fracture: bool = False,
-) -> List[Dict]:
-    """Finds list of parameters from an sections generator.
-
-    See :meth:`section_generator` for suitable input generator.
-
-    When there are multiple steps/properties matching the given
-    date, the first property matching the date is selected.
-
-    Args:
-        sections: Section generator such as returned by :meth:`section_generator`.
-        names: List of property names to be imported. Can also,
-            be set to "all" to import all parameters.
-        dates: List of xtgeo style dates (e.g. int(19990101)), can also
-            be "all", "last" and "first".
-        grid: The grid used by the simulator to produce the init file.
-        fracture: If a dual porosity module, indicates that the fracture
-            (as apposed to the matrix) grid property should be imported.
-    Returns:
-        List of GridProperty parameters matching the names.
-    """
-    first_date = None
-    last_date = None
-    read_properties = dict()
-    for section in sections:
-        intehead, logihead, section = peek_headers(section)
-        check_grid_match(intehead, logihead, grid)
-        date = date_from_intehead(intehead)
-
-        if dates not in ("all", "first", "last"):
-            if date not in dates:
-                continue
-
-        section_properties = {
-            (name, date): gridprop_params(v, name, date, grid, fracture)
-            for name, v in read_values(
-                section, intehead, names, lengths=valid_gridprop_lengths(grid)
-            ).items()
-        }
-
-        if dates == "first":
-            if first_date is None:
-                first_date = date
-            elif date != first_date:
-                break
-
-        elif dates == "last":
-            if date != last_date:
-                last_date = date
-                read_properties = dict()
-
-        for key in section_properties:
-            if key not in read_properties:
-                read_properties[key] = section_properties[key]
-    return list(read_properties.values())
-
-
-def find_gridprops_from_restart_file(
-    restart_filelike,
-    names: Union[List[str], Literal["all"]],
-    dates: Union[List[int], Literal["all", "first", "last"]],
-    grid,
-    fracture: bool = False,
-    fformat: eclio.Format = eclio.Format.UNFORMATTED,
-) -> List[Dict]:
-    """Finds all parameters in a restart file matching the given names and dates.
-
-    Note: Does not check that all names are found.
-
-    Args:
-        restart_filelike: The restart file.
-        names: List of property names to be imported. Can also,
-            be set to "all" to import all parameters.
-        dates: List of xtgeo style dates (e.g. int(19990101)), can also
-            be "all", "last" and "first".
-        grid: The grid used by the simulator to produce the restart file.
-        fracture: If a dual porosity module, indicates that the fracture
-            (as apposed to the matrix) grid property should be imported.
-    Returns:
-        List of GridProperty parameters matching the names and dates.
-    """
-    close = False
-    if isinstance(restart_filelike, (pathlib.Path, str)):
-        if fformat == eclio.Format.UNFORMATTED:
-            filehandle = open(restart_filelike, "rb")
-            close = True
-            assume_init_file = (str(restart_filelike)).replace(".UNRST", ".INIT")
-        elif fformat == eclio.Format.FORMATTED:
-            filehandle = open(restart_filelike, "rt", encoding="ascii")
-            close = True
-            assume_init_file = (str(restart_filelike)).replace(".FUNRST", ".FINIT")
-        else:
-            raise ValueError(f"Unsupported restart file format {fformat}")
-
-        # for INTERSECT, the ACTNUM in the grid is unreliable (bug); hence the ACTNUM is
-        # overriden by reading PORV from the INIT file; note that this will not work
-        # if input is a stream
-        if (Path(assume_init_file)).is_file():
-            get_actnum_from_porv(assume_init_file, grid)
-
-    else:
-        # If restart_filelike is not a filename/path we assume
-        # its a stream
-        filehandle = restart_filelike
-        close = False
-    try:
-        generator = section_generator(filter_lgr(eclio.lazy_read(filehandle)))
-        read_properties = find_gridprops_from_restart_file_sections(
-            generator,
-            names,
-            dates,
-            grid,
-            fracture,
-        )
-    finally:
-        if close:
-            filehandle.close()
-    return read_properties
+import functools
+import itertools
+import operator
+import pathlib
+import warnings
+from pathlib import Path
+from typing import Dict, List, Optional, Union
+
+import ecl_data_io as eclio
+import numpy as np
+from typing_extensions import Literal
+
+import xtgeo
+
+from ._ecl_inte_head import InteHead
+from ._ecl_logi_head import LogiHead
+from ._ecl_output_file import Phases
+from ._grdecl_format import match_keyword
+
+sat_keys = ["SOIL", "SGAS", "SWAT"]
+
+
+def filter_lgr(generator):
+    try:
+        while True:
+            entry = next(generator)
+            if entry.read_keyword() == "LGR":
+                warnings.warn(
+                    "Found LGR in ecl run file. "
+                    "LGR's are not directly supported, "
+                    "instead only global values are imported."
+                )
+                while entry.read_keyword() != "ENDLGR":
+                    entry = next(generator)
+            else:
+                yield entry
+    except StopIteration:
+        return
+
+
+# Based on which phases are present some saturation values are given default
+# values, e.g. if phases=Phases.OIL, the saturation of oil ("SOIL") is 1.0 and
+# all other phases are 0.0.
+DEFAULT_SATURATIONS = {
+    Phases.OIL: {
+        "SOIL": 1.0,
+        "SWAT": 0.0,
+        "SGAS": 0.0,
+    },
+    Phases.GAS: {
+        "SOIL": 0.0,
+        "SWAT": 0.0,
+        "SGAS": 1.0,
+    },
+    Phases.WATER: {
+        "SOIL": 0.0,
+        "SWAT": 1.0,
+        "SGAS": 0.0,
+    },
+    Phases.OIL_WATER: {
+        "SGAS": 0.0,
+    },
+    Phases.OIL_GAS: {
+        "SWAT": 0.0,
+    },
+    Phases.GAS_WATER: {
+        "SOIL": 0.0,
+    },
+    Phases.OIL_WATER_GAS: dict(),
+    Phases.E300_GENERIC: dict(),
+}
+
+
+def remainder_saturations(saturations):
+    """Infers remainder saturations based on sum(saturations.values()) == 1.
+
+    >>> remainder_saturations({'SWAT': 0.5, 'SGAS': 0.5})
+    {'SOIL': 0.0}
+
+    Args:
+        saturations: Dictionary of phases, such as returned by
+            :meth:`default_saturations()`.
+
+    Returns:
+        dictionary of saturation values that can be inferred.
+    """
+    if all(k in saturations for k in sat_keys):
+        return dict()
+    if any(k not in sat_keys for k in saturations):
+        raise ValueError(f"Unknown saturation keys: {list(saturations.keys())}")
+    rest = sum(saturations.values())
+    if len(saturations) == 2 or np.allclose(rest, 1.0):
+        missing = set(sat_keys).difference(set(saturations.keys()))
+        return {m: 1.0 - rest for m in missing}
+    return dict()
+
+
+def peek_headers(generator):
+    """Reads header from a ecl_data_io keyword generator without consuming keywords.
+
+    Args:
+        generator: keyword generator such as returned by ecl_data_io.lazy_read
+
+    Returns:
+        Tuple of inthead, logihead, and a modified generator which contains all original
+        keywords.
+    """
+
+    def read_headers(generator):
+        intehead_array = None
+        logihead_array = None
+        while intehead_array is None or logihead_array is None:
+            entry = next(generator)
+            kw = entry.read_keyword()
+            if match_keyword(kw, "LOGIHEAD"):
+                logihead_array = entry.read_array()
+            if match_keyword(kw, "INTEHEAD"):
+                intehead_array = entry.read_array()
+
+        intehead = InteHead(intehead_array)
+        logihead = LogiHead.from_file_values(logihead_array, intehead.simulator)
+        return intehead, logihead
+
+    header_generator, generator = itertools.tee(generator)
+    try:
+        intehead, logihead = read_headers(header_generator)
+    except StopIteration as stopit:
+        raise ValueError("Reached end of file without reading headers") from stopit
+    return intehead, logihead, generator
+
+
+def get_fetch_names(name: str) -> List[str]:
+    """Given a gridproperty name, give list of supporting keyword names.
+
+    >>> get_fetch_names('PORO')
+    ['PORO']
+    >>> get_fetch_names('SWAT')
+    ['SOIL', 'SGAS', 'SWAT']
+
+    Args:
+        name: The name of a grid property
+    Returns:
+        List of grid properties that must be fetched from the file.
+
+    """
+    if any(match_keyword(name, saturation_keyword) for saturation_keyword in sat_keys):
+        fetch_names = sat_keys
+    else:
+        fetch_names = [name]
+    return fetch_names
+
+
+def read_values(generator, intehead, names, lengths="all"):
+    """Read the given list of parameter values from the generator.
+
+    Reads the given list of values from the generator. Some saturation
+    values may be inferred from the invariant sum(saturations.values()) == 1.0
+    (see  :meth:`remainder_saturations()`.
+
+    """
+
+    def flatten(lst):
+        return functools.reduce(operator.iconcat, lst, [])
+
+    fetch_names = set(flatten(list(get_fetch_names(name) for name in names)))
+    defaulted = list()
+    if names == "all":
+        values = dict()
+    else:
+        values = DEFAULT_SATURATIONS[intehead.phases].copy()
+        defaulted = list(values.keys())
+
+    for entry in generator:
+        if all(name in values for name in fetch_names):
+            break
+        kw = entry.read_keyword()
+        if lengths != "all":
+            if entry.read_length() not in lengths:
+                continue
+        if names == "all":
+            key = kw.rstrip()
+            array = entry.read_array()
+            if np.issubdtype(array.dtype, np.number) or np.issubdtype(
+                array.dtype, bool
+            ):
+                values[key] = entry.read_array()
+        else:
+            matched = [name for name in fetch_names if match_keyword(kw, name)]
+            if len(matched) == 1:
+                if matched[0] in values and matched[0] not in defaulted:
+                    raise ValueError(f"Found duplicate keyword {matched[0]}")
+                values[matched[0]] = entry.read_array()
+            elif len(matched) > 1:
+                # This should not happen if get_fetch_names and
+                # match_keyword work as intended
+                raise ValueError(f"Ambiguous keywords {matched} matched vs {kw}")
+
+    if names == "all":
+        # A more consistent behavior would be to include saturations calculated
+        # with remainder_saturations when names=="all" aswell, however, we do
+        # not include those to keep backwards compatability.
+        # TODO: deprecate this behavior
+        return values
+    else:
+        values.update(
+            **remainder_saturations({k: values[k] for k in sat_keys if k in values})
+        )
+        return {name: values[name] for name in names if name in values}
+
+
+def check_grid_match(intehead: InteHead, logihead: LogiHead, grid):
+    """Checks that the init/restart headers matches the grid
+
+    Checks that the values given in the headers are compatible with
+    the grid.
+    """
+    dimensions = intehead.num_x, intehead.num_y, intehead.num_z
+
+    if logihead.dual_porosity:
+        dimensions = intehead.num_x, intehead.num_y, intehead.num_z // 2
+
+    if logihead.dual_porosity != grid.dualporo:
+        raise ValueError("Grid dual poro status does not match output file")
+
+    if dimensions != grid.dimensions:
+        raise ValueError(
+            "Grid dimensions do not match dimensions given in output file,"
+            f" {dimensions} vs {grid.dimensions}"
+        )
+
+
+def expand_scalar_values(value, num_cells, dualporo: bool) -> np.ndarray:
+    """Convert from scalar value to filled array of expected size.
+    Args:
+        value: The potentially scalar value
+        num_cells: The number of cells in the grid
+        dualporo: Whether the model has dual porosity
+    Returns:
+        If value is an array, then returns that array, otherwise
+        calls np.full with the shape determined by dualporo status and
+        number of cells.
+
+    """
+    if dualporo:
+        return np.full(fill_value=value, shape=num_cells * 2)
+    return np.full(fill_value=value, shape=num_cells)
+
+
+def pick_dualporo_values(
+    values: np.ndarray, actind: np.ndarray, num_cells: int, fracture: bool
+) -> np.ndarray:
+    """From array of values in an ecl run file, give the fracture or matrix values.
+
+    Args:
+        values: Array of values from an ecl run file.
+        actind: Array of the indecies of active cells.
+        num_cells: Total number of cells.
+        fracture: Whether to give the fracture or matrix values.
+    Returns:
+        Array of either fracture or matrix values from the input values.
+    """
+    active_size = len(actind)
+    if len(values) == 2 * num_cells:
+        indsize = num_cells
+    else:
+        indsize = active_size
+    if fracture:
+        return values[-indsize:]
+    return values[:indsize]
+
+
+def valid_gridprop_lengths(grid):
+    num_cells = np.prod(grid.dimensions)
+    if grid.dualporo:
+        num_fracture = len(grid.get_dualactnum_indices(fracture=True))
+        num_matrix = len(grid.get_dualactnum_indices(fracture=False))
+        return [2 * num_cells, num_fracture + num_matrix]
+    else:
+        num_active = len(grid.get_actnum_indices())
+        return [num_cells, num_active]
+
+
+def match_values_to_active_cells(
+    values,
+    actind,
+    num_cells,
+) -> np.ndarray:
+    """Expands array of ecl run values to be one-to-one with cells.
+
+    In the ecl run file, the values might be only those for active cells.
+    This funtion expands those values to one for each cell with non-active
+    indecies given the xtgeo.UNDEF/xtgeo.UNDEF_INT value.
+
+    Args:
+        values: Array of values from an ecl run file.
+        actind: Array of the indecies of active cells.
+        num_cells: Total number of cells.
+    Returns:
+        Array of input values, but guaranteed num_cells length.
+
+    """
+    if len(values) != len(actind):
+        raise ValueError(
+            f"Unexpected shape of values in init file: {np.asarray(values).shape}, "
+            f"expected to match grid dimensions {num_cells} or "
+            f"number of active cells {len(actind)}"
+        )
+
+    if np.issubdtype(values.dtype, np.integer):
+        undef = xtgeo.UNDEF_INT
+    else:
+        undef = xtgeo.UNDEF
+    result = np.full(fill_value=undef, shape=num_cells, dtype=values.dtype)
+    result[actind] = values
+    return result
+
+
+def make_gridprop_values(values, grid, fracture):
+    """Converts values given in init or restart file to one suitable for GridProperty.
+
+    Args:
+    values: The array read from the file
+    grid: The grid from the ecl run
+    fracture: Whether to get the fracture or matrix values
+
+    Returns:
+        Masked array of values indexed by cell
+    """
+    num_cells = np.prod(grid.dimensions)
+    if np.isscalar(values):
+        values = expand_scalar_values(values, num_cells, grid.dualporo)
+
+    if grid.dualporo:
+        actind = grid.get_dualactnum_indices(fracture=fracture, order="F")
+        values = pick_dualporo_values(values, actind, num_cells, fracture)
+    else:
+        actind = grid.get_actnum_indices(order="F")
+
+    if len(values) != num_cells:
+        values = match_values_to_active_cells(values, actind, num_cells)
+
+    values = values.reshape(grid.dimensions, order="F")
+
+    if grid.dualporo:
+        if fracture:
+            values[grid._dualactnum.values == 1] = 0.0
+        else:
+            values[grid._dualactnum.values == 2] = 0.0
+
+    return np.ma.masked_where(grid.get_actnum().values < 1, values)
+
+
+def date_from_intehead(intehead: InteHead) -> Optional[int]:
+    """Returns date format for use in GridProperty name given intehead."""
+    if any(val is None for val in [intehead.day, intehead.month, intehead.year]):
+        return None
+    return intehead.day + intehead.month * 100 + intehead.year * 10000
+
+
+def gridprop_params(values, name, date, grid, fracture):
+    """Make dictionary of GridProperty parameters from imported values."""
+    result = dict()
+    result["name"] = name
+    result["date"] = str(date) if date is not None else None
+    result["fracture"] = fracture
+
+    result["ncol"], result["nrow"], result["nlay"] = grid.dimensions
+    result["dualporo"] = grid.dualporo
+    result["dualperm"] = grid.dualperm
+
+    result["values"] = make_gridprop_values(values, grid, fracture)
+
+    if np.issubdtype(result["values"].dtype, np.integer):
+        uniq = np.unique(values).tolist()
+        codes = dict(zip(uniq, uniq))
+        codes = {key: str(val) for key, val in codes.items()}
+        result["codes"] = codes
+        result["values"] = result["values"].astype(np.int32)
+        result["discrete"] = True
+    else:
+        result["codes"] = dict()
+        result["values"] = result["values"].astype(np.float64)
+        result["discrete"] = False
+    return result
+
+
+def get_actnum_from_porv(init_filelike, grid):
+    """Override actnum value based on the cell pore volume.
+
+    Args:
+        init_filelike: The init file
+        grid: The grid used by the simulator to produce the init file.
+    Returns:
+        None
+    """
+    generator = filter_lgr(eclio.lazy_read(init_filelike))
+    intehead, logihead, generator = peek_headers(generator)
+
+    if "INTERSECT" not in str(intehead.simulator):
+        # no need to continue if other simulators than INTERSECT
+        return
+
+    check_grid_match(intehead, logihead, grid)
+    porv = read_values(
+        generator, intehead, ["PORV"], lengths=valid_gridprop_lengths(grid)
+    )
+
+    original_active = grid.nactive
+    if porv:
+        if grid.dualporo:
+            num_cells = np.prod(grid.dimensions)
+            actnum_matrix = np.where(
+                porv["PORV"][:num_cells].reshape(grid.dimensions, order="F") > 0.0, 1, 0
+            )
+            actnum_fracture = np.where(
+                porv["PORV"][num_cells:].reshape(grid.dimensions, order="F") > 0.0, 2, 0
+            )
+            grid._dualactnum.values = actnum_matrix + actnum_fracture
+        else:
+            acttmp = grid.get_actnum().copy()
+            acttmp.values = np.where(
+                porv["PORV"].reshape(grid.dimensions, order="F") > 0.0, 1, 0
+            )
+            grid.set_actnum(acttmp)
+
+    new_active = grid.nactive
+    if new_active != original_active:
+        warnings.warn(
+            "The original active cells (ACTNUM) has been overrided by PORV criteria "
+            "from INIT file (purpose: fix simulator INTERSECT issue)",
+            UserWarning,
+        )
+
+
+def find_gridprop_from_init_file(
+    init_filelike,
+    names: Union[List[str], Literal["all"]],
+    grid,
+    fracture: bool = False,
+) -> List[Dict]:
+    """Finds all parameters in a init matching names.
+
+    Note: Does not check that all names are found.
+
+    Args:
+        init_filelike: The init file
+        names: List of property names to be imported. Can also,
+            be set to "all" to import all parameters.
+        grid: The grid used by the simulator to produce the init file.
+        fracture: If a dual porosity module, indicates that the fracture
+            (as apposed to the matrix) grid property should be imported.
+    Returns:
+        List of GridProperty parameters matching the names.
+
+    """
+    init_stream = not isinstance(init_filelike, (str, Path))
+    if init_stream:
+        orig_pos = init_filelike.tell()
+    get_actnum_from_porv(init_filelike, grid)
+    if init_stream:
+        init_filelike.seek(orig_pos, 0)
+    generator = filter_lgr(eclio.lazy_read(init_filelike))
+    intehead, logihead, generator = peek_headers(generator)
+
+    check_grid_match(intehead, logihead, grid)
+
+    date = date_from_intehead(intehead)
+    return [
+        gridprop_params(v, name, date, grid, fracture)
+        for name, v in read_values(
+            generator, intehead, names, lengths=valid_gridprop_lengths(grid)
+        ).items()
+    ]
+
+
+def section_generator(generator):
+    """Sections the generator as delimited by "SEQNUM" keyword.
+
+    Unified restart files will repeat properties in sections, one for each
+    date. The "SEQNUM" keyword indicates a new section. section_generator
+    takes a keyword generator and returns a generator over sections.
+
+    Note: It does so in a lighweight manner so that a section
+    is emptied once the next section is requested, and the original
+    generator is iterated as you progress in the sections.
+    """
+    try:
+        first_seq = next(generator)
+        if not match_keyword(first_seq.read_keyword(), "SEQNUM"):
+            raise ValueError("Restart file did not start with SEQNUM.")
+    except StopIteration:
+        return
+    while True:
+        this_section = itertools.takewhile(
+            lambda x: not match_keyword(x.read_keyword(), "SEQNUM"), generator
+        )
+        yield this_section
+        # empty the section iterator
+        for _ in this_section:
+            pass
+        # peek ahead to see if there are more elements
+        # and stop if there are not
+        try:
+            entry = next(generator)
+            generator = itertools.chain(iter([entry]), generator)
+        except StopIteration:
+            return
+
+
+def find_gridprops_from_restart_file_sections(
+    sections,
+    names: Union[List[str], Literal["all"]],
+    dates: Union[List[int], Literal["all", "last", "first"]],
+    grid,
+    fracture: bool = False,
+) -> List[Dict]:
+    """Finds list of parameters from an sections generator.
+
+    See :meth:`section_generator` for suitable input generator.
+
+    When there are multiple steps/properties matching the given
+    date, the first property matching the date is selected.
+
+    Args:
+        sections: Section generator such as returned by :meth:`section_generator`.
+        names: List of property names to be imported. Can also,
+            be set to "all" to import all parameters.
+        dates: List of xtgeo style dates (e.g. int(19990101)), can also
+            be "all", "last" and "first".
+        grid: The grid used by the simulator to produce the init file.
+        fracture: If a dual porosity module, indicates that the fracture
+            (as apposed to the matrix) grid property should be imported.
+    Returns:
+        List of GridProperty parameters matching the names.
+    """
+    first_date = None
+    last_date = None
+    read_properties = dict()
+    for section in sections:
+        intehead, logihead, section = peek_headers(section)
+        check_grid_match(intehead, logihead, grid)
+        date = date_from_intehead(intehead)
+
+        if dates not in ("all", "first", "last"):
+            if date not in dates:
+                continue
+
+        section_properties = {
+            (name, date): gridprop_params(v, name, date, grid, fracture)
+            for name, v in read_values(
+                section, intehead, names, lengths=valid_gridprop_lengths(grid)
+            ).items()
+        }
+
+        if dates == "first":
+            if first_date is None:
+                first_date = date
+            elif date != first_date:
+                break
+
+        elif dates == "last":
+            if date != last_date:
+                last_date = date
+                read_properties = dict()
+
+        for key in section_properties:
+            if key not in read_properties:
+                read_properties[key] = section_properties[key]
+    return list(read_properties.values())
+
+
+def find_gridprops_from_restart_file(
+    restart_filelike,
+    names: Union[List[str], Literal["all"]],
+    dates: Union[List[int], Literal["all", "first", "last"]],
+    grid,
+    fracture: bool = False,
+    fformat: eclio.Format = eclio.Format.UNFORMATTED,
+) -> List[Dict]:
+    """Finds all parameters in a restart file matching the given names and dates.
+
+    Note: Does not check that all names are found.
+
+    Args:
+        restart_filelike: The restart file.
+        names: List of property names to be imported. Can also,
+            be set to "all" to import all parameters.
+        dates: List of xtgeo style dates (e.g. int(19990101)), can also
+            be "all", "last" and "first".
+        grid: The grid used by the simulator to produce the restart file.
+        fracture: If a dual porosity module, indicates that the fracture
+            (as apposed to the matrix) grid property should be imported.
+    Returns:
+        List of GridProperty parameters matching the names and dates.
+    """
+    close = False
+    if isinstance(restart_filelike, (pathlib.Path, str)):
+        if fformat == eclio.Format.UNFORMATTED:
+            filehandle = open(restart_filelike, "rb")
+            close = True
+            assume_init_file = (str(restart_filelike)).replace(".UNRST", ".INIT")
+        elif fformat == eclio.Format.FORMATTED:
+            filehandle = open(restart_filelike, "rt", encoding="ascii")
+            close = True
+            assume_init_file = (str(restart_filelike)).replace(".FUNRST", ".FINIT")
+        else:
+            raise ValueError(f"Unsupported restart file format {fformat}")
+
+        # for INTERSECT, the ACTNUM in the grid is unreliable (bug); hence the ACTNUM is
+        # overriden by reading PORV from the INIT file; note that this will not work
+        # if input is a stream
+        if (Path(assume_init_file)).is_file():
+            get_actnum_from_porv(assume_init_file, grid)
+
+    else:
+        # If restart_filelike is not a filename/path we assume
+        # its a stream
+        filehandle = restart_filelike
+        close = False
+    try:
+        generator = section_generator(filter_lgr(eclio.lazy_read(filehandle)))
+        read_properties = find_gridprops_from_restart_file_sections(
+            generator,
+            names,
+            dates,
+            grid,
+            fracture,
+        )
+    finally:
+        if close:
+            filehandle.close()
+    return read_properties
```

## xtgeo/grid3d/_grdecl_format.py

 * *Ordering differences only*

```diff
@@ -1,257 +1,257 @@
-import warnings
-from contextlib import contextmanager
-
-import xtgeo
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def split_line(line):
-    """
-    split a keyword line inside a grdecl file. This splits the values of a
-    'simple' keyword into tokens. ie.
-
-    >>> list(split_line("3 1.0 3*4 PORO 3*INC 'HELLO WORLD  ' 3*'NAME'"))
-    ['3', '1.0', '3*4', 'PORO', '3*INC', "'HELLO WORLD  '", "3*'NAME'"]
-
-    note that we do not require string literals to have delimiting space at the
-    end, but at the start. This is to be permissive at the end (as there is no
-    formal requirement for spaces at end of string literals), but no space at
-    the start of a string literal might indicate a repeating count.
-
-    >>> list(split_line("3'hello world'4"))
-    ["3'hello world'", '4']
-
-    """
-    value = ""
-    inside_str = False
-    for char in line:
-        if char == "'":
-            # Either the start or
-            # the end of a string literal
-            if inside_str:
-                yield value + char
-                value = ""
-                inside_str = False
-            else:
-                inside_str = True
-                value += char
-        elif inside_str:
-            # inside a string literal
-            value += char
-        elif value and value[-1] == "-" and char == "-":
-            # a comment
-            value = value[0:-1]
-            break
-        elif char.isspace():
-            # delimiting space
-            if value:
-                yield value
-                value = ""
-        else:
-            value += char
-    if value:
-        yield value
-
-
-def split_line_no_string(line):
-    """
-    Same as split_line, but does not handle string literals, instead
-    its quite a bit faster.
-    """
-    for w in line.split():
-        if w.startswith("--"):
-            return
-        yield w
-
-
-def match_keyword(kw1, kw2):
-    """
-    Perhaps surprisingly, the eclipse input format considers keywords
-    as 8 character strings with space denoting end. So PORO, 'PORO ', and
-    'PORO    ' are all considered the same keyword.
-
-    Note that spaces may also occur inside e.g. tracer keywords, hence 'G1 F' vs 'G1 S'
-    are different keywords.
-
-    >>> match_keyword("PORO", "PORO ")
-    True
-    >>> match_keyword("PORO", "PERM")
-    False
-    >>> match_keyword("MORETHAN8LETTERS1)", "MORETHAN8LETTER2")
-    True
-    >>> match_keyword("G1 F", "G1 S")
-    False
-
-    """
-    return kw1[0:8].rstrip() == kw2[0:8].rstrip()
-
-
-def interpret_token(val):
-    """
-    Interpret a eclipse token, tries to interpret the
-    value in the following order:
-    * string literal
-    * keyword
-    * repreated keyword
-    * number
-
-    If the token cannot be matched, we default to returning
-    the uninterpreted token.
-
-    >>> interpret_token("3")
-    ['3']
-    >>> interpret_token("1.0")
-    ['1.0']
-    >>> interpret_token("'hello'")
-    ['hello']
-    >>> interpret_token("PORO")
-    ['PORO']
-    >>> interpret_token("3PORO")
-    ['3PORO']
-    >>> interpret_token("3*PORO")
-    ['PORO', 'PORO', 'PORO']
-    >>> interpret_token("3*'PORO '")
-    ['PORO ', 'PORO ', 'PORO ']
-    >>> interpret_token("3'PORO '")
-    ["3'PORO '"]
-
-    """
-    if val[0] == "'" and val[-1] == "'":
-        # A string literal
-        return [val[1:-1]]
-    if val[0].isalpha():
-        # A keyword
-        return [val]
-    if "*" in val:
-        multiplicand, value = val.split("*")
-        return interpret_token(value) * int(multiplicand)
-    return [val]
-
-
-IGNORE_ALL = None
-
-
-@contextmanager
-def open_grdecl(
-    grdecl_file,
-    keywords,
-    simple_keywords=[],
-    max_len=None,
-    ignore=IGNORE_ALL,
-    strict=True,
-):
-    """Generates tuples of keyword and values in records of a grdecl file.
-
-    The format of the file must be that of the GRID section of a eclipse input
-    DATA file.
-
-    The records looked for must be "simple" ie.  start with the keyword, be
-    followed by single word values and ended by a slash ('/').
-
-    .. code-block:: none
-
-        KEYWORD
-        value value value /
-
-    reading the above file with :code:`open_grdecl("filename.grdecl",
-    keywords="KEYWORD")` will generate :code:`[("KEYWORD", ["value", "value",
-    "value"])]`
-
-    open_grdecl does not follow includes, obey skips, parse MESSAGE commands or
-    make exception for groups and subrecords.
-
-    Raises:
-        ValueError: when end of file is reached without terminating a keyword,
-            or the file contains an unrecognized (or ignored) keyword.
-
-    Args:
-        keywords (List[str]): Which keywords to look for, these are expected to
-        be at the start of a line in the file  and the respective values
-        following on subsequent lines separated by whitespace. Reading of a
-        keyword is completed by a final '\'. See example above.
-
-        simple_keywords (List[str]): Similar to keywords, but faster and
-        cannot contain any string literals, such as the GRIDUNIT keyword
-        which can be followed by the string literal 'METRES '.
-
-        max_len (int): The maximum significant length of a keyword (Eclipse
-        uses 8) ignore (List[str]): Keywords that have no associated data, and
-        should be ignored, e.g. ECHO. Defaults to ignore all keywords that are
-        not part of the results.
-
-        ignore (List[str]): list of unmatched keywords to ignore, defaults to
-        ignoring all unmatched keywords. Any keyword not ignored and not in
-        the list of keywords looked for will give an error unless strict=False.
-        Although a keyword is ignored, if it has trailing values on new lines
-        those are interpreted as keywords, in order to ignore keywords with
-        trailing values, use strict=False and filter warnings. Alternatively,
-        add it to the list of expected keywords.
-
-        strict (boolean): Whether unmatched keywords should raise an error or
-        a warning.
-    """
-
-    def read_grdecl(grdecl_stream):
-        words = []
-        keyword = None
-        line_splitter = split_line
-
-        line_no = 1
-        line = grdecl_stream.readline()
-
-        while line:
-            if line is None:
-                break
-
-            if keyword is None:
-                if max_len:
-                    snubbed = line[0 : min(max_len, len(line))]
-                else:
-                    snubbed = line
-                simple_matched_keywords = [
-                    kw for kw in simple_keywords if match_keyword(kw, snubbed)
-                ]
-                matched_keywords = [kw for kw in keywords if match_keyword(kw, snubbed)]
-                if matched_keywords or simple_matched_keywords:
-                    if matched_keywords:
-                        keyword = matched_keywords[0]
-                        line_splitter = split_line
-                    else:
-                        keyword = simple_matched_keywords[0]
-                        line_splitter = split_line_no_string
-                    logger.debug("Keyword %s found on line %d", keyword, line_no)
-                elif (
-                    list(split_line(line))  # Not an empty line
-                    and ignore is not IGNORE_ALL  # Not ignoring all
-                    and not any(
-                        match_keyword(snubbed, i) for i in ignore
-                    )  # Not ignoring this
-                ):
-                    if strict:
-                        raise ValueError(
-                            f"Unrecognized keyword {repr(line)} on line {line_no}"
-                        )
-                    else:
-                        warnings.warn(
-                            f"Unrecognized keyword {repr(line)} on line {line_no}"
-                        )
-
-            else:
-                for word in line_splitter(line):
-                    if word == "/":
-                        yield (keyword, words)
-                        keyword = None
-                        words = []
-                        break
-                    words += interpret_token(word)
-            line = grdecl_stream.readline()
-            line_no += 1
-
-        if keyword is not None:
-            raise ValueError(f"Reached end of stream while reading {keyword}")
-
-    with open(grdecl_file, "r") as stream:
-        yield read_grdecl(stream)
+import warnings
+from contextlib import contextmanager
+
+import xtgeo
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def split_line(line):
+    """
+    split a keyword line inside a grdecl file. This splits the values of a
+    'simple' keyword into tokens. ie.
+
+    >>> list(split_line("3 1.0 3*4 PORO 3*INC 'HELLO WORLD  ' 3*'NAME'"))
+    ['3', '1.0', '3*4', 'PORO', '3*INC', "'HELLO WORLD  '", "3*'NAME'"]
+
+    note that we do not require string literals to have delimiting space at the
+    end, but at the start. This is to be permissive at the end (as there is no
+    formal requirement for spaces at end of string literals), but no space at
+    the start of a string literal might indicate a repeating count.
+
+    >>> list(split_line("3'hello world'4"))
+    ["3'hello world'", '4']
+
+    """
+    value = ""
+    inside_str = False
+    for char in line:
+        if char == "'":
+            # Either the start or
+            # the end of a string literal
+            if inside_str:
+                yield value + char
+                value = ""
+                inside_str = False
+            else:
+                inside_str = True
+                value += char
+        elif inside_str:
+            # inside a string literal
+            value += char
+        elif value and value[-1] == "-" and char == "-":
+            # a comment
+            value = value[0:-1]
+            break
+        elif char.isspace():
+            # delimiting space
+            if value:
+                yield value
+                value = ""
+        else:
+            value += char
+    if value:
+        yield value
+
+
+def split_line_no_string(line):
+    """
+    Same as split_line, but does not handle string literals, instead
+    its quite a bit faster.
+    """
+    for w in line.split():
+        if w.startswith("--"):
+            return
+        yield w
+
+
+def match_keyword(kw1, kw2):
+    """
+    Perhaps surprisingly, the eclipse input format considers keywords
+    as 8 character strings with space denoting end. So PORO, 'PORO ', and
+    'PORO    ' are all considered the same keyword.
+
+    Note that spaces may also occur inside e.g. tracer keywords, hence 'G1 F' vs 'G1 S'
+    are different keywords.
+
+    >>> match_keyword("PORO", "PORO ")
+    True
+    >>> match_keyword("PORO", "PERM")
+    False
+    >>> match_keyword("MORETHAN8LETTERS1)", "MORETHAN8LETTER2")
+    True
+    >>> match_keyword("G1 F", "G1 S")
+    False
+
+    """
+    return kw1[0:8].rstrip() == kw2[0:8].rstrip()
+
+
+def interpret_token(val):
+    """
+    Interpret a eclipse token, tries to interpret the
+    value in the following order:
+    * string literal
+    * keyword
+    * repreated keyword
+    * number
+
+    If the token cannot be matched, we default to returning
+    the uninterpreted token.
+
+    >>> interpret_token("3")
+    ['3']
+    >>> interpret_token("1.0")
+    ['1.0']
+    >>> interpret_token("'hello'")
+    ['hello']
+    >>> interpret_token("PORO")
+    ['PORO']
+    >>> interpret_token("3PORO")
+    ['3PORO']
+    >>> interpret_token("3*PORO")
+    ['PORO', 'PORO', 'PORO']
+    >>> interpret_token("3*'PORO '")
+    ['PORO ', 'PORO ', 'PORO ']
+    >>> interpret_token("3'PORO '")
+    ["3'PORO '"]
+
+    """
+    if val[0] == "'" and val[-1] == "'":
+        # A string literal
+        return [val[1:-1]]
+    if val[0].isalpha():
+        # A keyword
+        return [val]
+    if "*" in val:
+        multiplicand, value = val.split("*")
+        return interpret_token(value) * int(multiplicand)
+    return [val]
+
+
+IGNORE_ALL = None
+
+
+@contextmanager
+def open_grdecl(
+    grdecl_file,
+    keywords,
+    simple_keywords=[],
+    max_len=None,
+    ignore=IGNORE_ALL,
+    strict=True,
+):
+    """Generates tuples of keyword and values in records of a grdecl file.
+
+    The format of the file must be that of the GRID section of a eclipse input
+    DATA file.
+
+    The records looked for must be "simple" ie.  start with the keyword, be
+    followed by single word values and ended by a slash ('/').
+
+    .. code-block:: none
+
+        KEYWORD
+        value value value /
+
+    reading the above file with :code:`open_grdecl("filename.grdecl",
+    keywords="KEYWORD")` will generate :code:`[("KEYWORD", ["value", "value",
+    "value"])]`
+
+    open_grdecl does not follow includes, obey skips, parse MESSAGE commands or
+    make exception for groups and subrecords.
+
+    Raises:
+        ValueError: when end of file is reached without terminating a keyword,
+            or the file contains an unrecognized (or ignored) keyword.
+
+    Args:
+        keywords (List[str]): Which keywords to look for, these are expected to
+        be at the start of a line in the file  and the respective values
+        following on subsequent lines separated by whitespace. Reading of a
+        keyword is completed by a final '\'. See example above.
+
+        simple_keywords (List[str]): Similar to keywords, but faster and
+        cannot contain any string literals, such as the GRIDUNIT keyword
+        which can be followed by the string literal 'METRES '.
+
+        max_len (int): The maximum significant length of a keyword (Eclipse
+        uses 8) ignore (List[str]): Keywords that have no associated data, and
+        should be ignored, e.g. ECHO. Defaults to ignore all keywords that are
+        not part of the results.
+
+        ignore (List[str]): list of unmatched keywords to ignore, defaults to
+        ignoring all unmatched keywords. Any keyword not ignored and not in
+        the list of keywords looked for will give an error unless strict=False.
+        Although a keyword is ignored, if it has trailing values on new lines
+        those are interpreted as keywords, in order to ignore keywords with
+        trailing values, use strict=False and filter warnings. Alternatively,
+        add it to the list of expected keywords.
+
+        strict (boolean): Whether unmatched keywords should raise an error or
+        a warning.
+    """
+
+    def read_grdecl(grdecl_stream):
+        words = []
+        keyword = None
+        line_splitter = split_line
+
+        line_no = 1
+        line = grdecl_stream.readline()
+
+        while line:
+            if line is None:
+                break
+
+            if keyword is None:
+                if max_len:
+                    snubbed = line[0 : min(max_len, len(line))]
+                else:
+                    snubbed = line
+                simple_matched_keywords = [
+                    kw for kw in simple_keywords if match_keyword(kw, snubbed)
+                ]
+                matched_keywords = [kw for kw in keywords if match_keyword(kw, snubbed)]
+                if matched_keywords or simple_matched_keywords:
+                    if matched_keywords:
+                        keyword = matched_keywords[0]
+                        line_splitter = split_line
+                    else:
+                        keyword = simple_matched_keywords[0]
+                        line_splitter = split_line_no_string
+                    logger.debug("Keyword %s found on line %d", keyword, line_no)
+                elif (
+                    list(split_line(line))  # Not an empty line
+                    and ignore is not IGNORE_ALL  # Not ignoring all
+                    and not any(
+                        match_keyword(snubbed, i) for i in ignore
+                    )  # Not ignoring this
+                ):
+                    if strict:
+                        raise ValueError(
+                            f"Unrecognized keyword {repr(line)} on line {line_no}"
+                        )
+                    else:
+                        warnings.warn(
+                            f"Unrecognized keyword {repr(line)} on line {line_no}"
+                        )
+
+            else:
+                for word in line_splitter(line):
+                    if word == "/":
+                        yield (keyword, words)
+                        keyword = None
+                        words = []
+                        break
+                    words += interpret_token(word)
+            line = grdecl_stream.readline()
+            line_no += 1
+
+        if keyword is not None:
+            raise ValueError(f"Reached end of stream while reading {keyword}")
+
+    with open(grdecl_file, "r") as stream:
+        yield read_grdecl(stream)
```

## xtgeo/grid3d/_grdecl_grid.py

 * *Ordering differences only*

```diff
@@ -1,380 +1,380 @@
-"""
-Datastructure for the contents of grdecl files.
-
-The grdecl file format is not specified in a strict manner
-but in the most general sense it is a file that can be included
-into the GRID section of a eclipse input file.
-
-However, it is nearly impossible to support such a file format completely,
-instead we narrow it down to the following subset of keywords:
-
-    * COORD
-    * ZCORN
-    * ACTNUM
-    * MAPAXES
-    * GRIDUNIT
-    * SPECGRID
-    * GDORIENT
-
-And ignore ECHO and NOECHO keywords. see _grid_format for the details
-of how these keywords are layed out in a text file, and see GrdeclGrid
-for how the grid geometry is interpreted from these keywords.
-"""
-from dataclasses import dataclass
-from typing import Optional, Tuple
-
-import numpy as np
-from ecl_data_io import Format, lazy_read, write
-
-from ._ecl_grid import (
-    CoordinateType,
-    EclGrid,
-    GdOrient,
-    GrdeclKeyword,
-    GridRelative,
-    GridUnit,
-    MapAxes,
-    Units,
-)
-from ._grdecl_format import IGNORE_ALL, open_grdecl
-
-
-@dataclass
-class SpecGrid(GrdeclKeyword):
-    """The SPECGRID keyword gives the size of the grid.
-
-    The 3 first values is the number of cells in each dimension
-    of the grid. The next is the number of reservoirs in the file and
-    the last is the type of coordinates, see CoordinateType.
-
-    example:
-        "SPECGRID 10 10 10 1 T /" meaning 10x10x10 grid with 1 reservoir
-        and cylindrical coordinates.
-
-    """
-
-    ndivix: int = 1
-    ndiviy: int = 1
-    ndiviz: int = 1
-    numres: int = 1
-    coordinate_type: CoordinateType = CoordinateType.CARTESIAN
-
-    def to_grdecl(self):
-        return [
-            self.ndivix,
-            self.ndiviy,
-            self.ndiviz,
-            self.numres,
-            self.coordinate_type.to_grdecl(),
-        ]
-
-    def to_bgrdecl(self):
-        return np.array(
-            [
-                self.ndivix,
-                self.ndiviy,
-                self.ndiviz,
-                self.numres,
-                self.coordinate_type.to_bgrdecl(),
-            ],
-            dtype=np.int32,
-        )
-
-    @classmethod
-    def from_bgrdecl(cls, values):
-        ivalues = [int(v) for v in values[:4]]
-        if len(values) < 5:
-            return cls(*ivalues)
-        if len(values) == 5:
-            return cls(*ivalues, CoordinateType.from_bgrdecl(values[-1]))
-        raise ValueError("SPECGRID should have at most 5 values")
-
-    @classmethod
-    def from_grdecl(cls, values):
-        ivalues = [int(v) for v in values[:4]]
-        if len(values) < 5:
-            return cls(*ivalues)
-        if len(values) == 5:
-            return cls(*ivalues, CoordinateType.from_grdecl(values[-1]))
-        raise ValueError("SPECGRID should have at most 5 values")
-
-
-class GrdeclGrid(EclGrid):
-    """
-    The main keywords that describe a grdecl grid is COORD, ZCORN and ACTNUM
-    and are described in xtgeo.grid3d._ecl_grid.
-    The remaining fields (SPECGRID, MAPAXES, MAPUNITS, GRIDUNIT, GDORIENT)
-    describe units, orientation and dimensions, see corresponding dataclasses.
-    The number of cells in each direction is described in the SPECGRID keyword.
-    """
-
-    def __init__(
-        self,
-        coord: np.ndarray,
-        zcorn: np.ndarray,
-        specgrid: SpecGrid,
-        actnum: Optional[np.ndarray] = None,
-        mapaxes: Optional[MapAxes] = None,
-        mapunits: Optional[Units] = None,
-        gridunit: Optional[GridUnit] = None,
-        gdorient: Optional[GdOrient] = None,
-    ):
-        self._coord = coord
-        self._zcorn = zcorn
-        self.specgrid = specgrid
-        self._actnum = actnum
-        self._mapaxes = mapaxes
-        self.mapunits = mapunits
-        self.gridunit = gridunit
-        self.gdorient = gdorient
-
-    def __str__(self):
-        return (
-            "GrdeclGrid("
-            f"coord={self._coord}, "
-            f"zcorn={self._zcorn}, "
-            f"specgrid={self.specgrid}, "
-            f"actnum={self._actnum}, "
-            f"mapaxes={self._mapaxes}, "
-            f"mapunits={self.mapunits}, "
-            f"gridunit={self.gridunit}, "
-            f"gdorient={self.gdorient})"
-        )
-
-    def __repr__(self):
-        return str(self)
-
-    @property
-    def mapaxes(self) -> Optional[MapAxes]:
-        return self._mapaxes
-
-    @mapaxes.setter
-    def mapaxes(self, value):
-        self._mapaxes = value
-
-    @property
-    def coord(self) -> np.ndarray:
-        return self._coord
-
-    @coord.setter
-    def coord(self, value):
-        self._coord = value
-
-    @property
-    def zcorn(self) -> np.ndarray:
-        return self._zcorn
-
-    @zcorn.setter
-    def zcorn(self, value):
-        self._zcorn = value
-
-    @property
-    def actnum(self) -> Optional[np.ndarray]:
-        return self._actnum
-
-    @classmethod
-    def default_settings_grid(
-        cls,
-        coord: np.ndarray,
-        zcorn: np.ndarray,
-        actnum: Optional[np.ndarray],
-        size: Tuple[int, int, int],
-    ):
-        return cls(coord, zcorn, SpecGrid(*size), actnum)
-
-    def __eq__(self, other):
-        if not isinstance(other, GrdeclGrid):
-            return False
-        return (
-            self.specgrid == other.specgrid
-            and self.mapaxes == other.mapaxes
-            and self.mapunits == other.mapunits
-            and self.gridunit == other.gridunit
-            and self.gdorient == other.gdorient
-            and np.array_equal(self.actnum, other.actnum)
-            and np.array_equal(self.coord, other.coord)
-            and np.array_equal(self.zcorn, other.zcorn)
-        )
-
-    @property
-    def dimensions(self):
-        return (self.specgrid.ndivix, self.specgrid.ndiviy, self.specgrid.ndiviz)
-
-    @property
-    def is_map_relative(self) -> bool:
-        return (
-            self.gridunit is not None
-            and self.gridunit.grid_relative == GridRelative.MAP
-        )
-
-    @property
-    def map_axis_units(self):
-        if self.mapunits is None:
-            return Units.METRES
-        return self.mapunits
-
-    @map_axis_units.setter
-    def map_axis_units(self, value):
-        self.mapunits = value
-
-    @property
-    def grid_units(self):
-        if self.gridunit is None:
-            return Units.METRES
-        return self.gridunit.unit
-
-    @grid_units.setter
-    def grid_units(self, value):
-        if self.gridunit is None and value != Units.METRES:
-            self.gridunit = GridUnit(unit=value)
-        elif self.gridunit is not None:
-            self.gridunit.unit = value
-
-    @classmethod
-    def from_file(cls, filename, fileformat="grdecl"):
-        """
-        write the grdeclgrid to a file.
-        :param filename: path to file to write.
-        :param fileformat: Either "grdecl" or "bgrdecl" to
-            indicate binary or ascii format.
-        """
-        if fileformat == "grdecl":
-            return cls._from_grdecl_file(filename)
-        if fileformat == "bgrdecl":
-            return cls._from_bgrdecl_file(filename)
-        raise ValueError(b"Unknown grdecl file format {fileformat}")
-
-    @classmethod
-    def _from_bgrdecl_file(cls, filename, fileformat=None):
-        keyword_factories = {
-            "COORD": lambda x: np.array(x, dtype=np.float32),
-            "ZCORN": lambda x: np.array(x, dtype=np.float32),
-            "ACTNUM": lambda x: np.array(x, dtype=np.int32),
-            "MAPAXES": MapAxes.from_bgrdecl,
-            "MAPUNITS": lambda x: Units.from_bgrdecl(x[0]),
-            "GRIDUNIT": GridUnit.from_bgrdecl,
-            "SPECGRID": SpecGrid.from_bgrdecl,
-            "GDORIENT": GdOrient.from_bgrdecl,
-        }
-        results = {}
-        for entry in lazy_read(filename, fileformat=fileformat):
-            if len(results) == len(keyword_factories):
-                break
-            kw = entry.read_keyword().rstrip()
-            if kw in results:
-                raise ValueError(f"Duplicate keyword {kw} in {filename}")
-            try:
-                factory = keyword_factories[kw]
-            except KeyError as e:
-                raise ValueError(f"Unknown grdecl keyword {kw}") from e
-            results[kw.lower()] = factory(entry.read_array())
-        return cls(**results)
-
-    @classmethod
-    def _from_grdecl_file(cls, filename):
-        keyword_factories = {
-            "COORD": lambda x: np.array(x, dtype=np.float32),
-            "ZCORN": lambda x: np.array(x, dtype=np.float32),
-            "ACTNUM": lambda x: np.array(x, dtype=np.int32),
-            "MAPAXES": MapAxes.from_grdecl,
-            "MAPUNITS": lambda x: Units.from_grdecl(x[0]),
-            "GRIDUNIT": GridUnit.from_grdecl,
-            "SPECGRID": SpecGrid.from_grdecl,
-            "GDORIENT": GdOrient.from_grdecl,
-        }
-        results = {}
-        with open_grdecl(
-            filename,
-            keywords=["MAPAXES", "MAPUNITS", "GRIDUNIT", "SPECGRID", "GDORIENT"],
-            simple_keywords=["COORD", "ZCORN", "ACTNUM"],
-            max_len=None,
-            ignore=IGNORE_ALL,
-            strict=False,
-        ) as keyword_generator:
-            for kw, values in keyword_generator:
-                if len(results) == len(keyword_factories):
-                    break
-                if kw in results:
-                    raise ValueError(f"Duplicate keyword {kw} in {filename}")
-                try:
-                    factory = keyword_factories[kw]
-                except KeyError as e:
-                    raise ValueError(f"Unknown grdecl keyword {kw}") from e
-                results[kw.lower()] = factory(values)
-        return cls(**results)
-
-    def to_file(self, filename, fileformat="grdecl"):
-        """
-        write the grdeclgrid to a file.
-        :param filename: path to file to write.
-        :param fileformat: Either "grdecl" or "bgrdecl" to
-            indicate binary or ascii format.
-        """
-        if fileformat == "grdecl":
-            return self._to_grdecl_file(filename)
-        if fileformat == "bgrdecl":
-            return self._to_bgrdecl_file(filename)
-        raise ValueError(b"Unknown grdecl file format {fileformat}")
-
-    def _to_grdecl_file(self, filename):
-        with open(filename, "w") as filestream:
-            keywords = [
-                ("SPECGRID", self.specgrid.to_grdecl()),
-                ("MAPAXES", self.mapaxes.to_grdecl() if self.mapaxes else None),
-                ("MAPUNITS", [self.mapunits.to_grdecl()] if self.mapunits else None),
-                ("GRIDUNIT", self.gridunit.to_grdecl() if self.gridunit else None),
-                ("GDORIENT", self.gdorient.to_grdecl() if self.gdorient else None),
-                ("COORD", self.coord),
-                ("ZCORN", self.zcorn),
-                ("ACTNUM", self.actnum),
-            ]
-            for kw, values in keywords:
-                if values is None:
-                    continue
-                filestream.write(f"{kw}\n")
-                numcolumns = 0
-                for value in values:
-                    numcolumns += 1
-                    filestream.write(f" {value}")
-
-                    if numcolumns >= 6:  # 6 should ensure < 128 character width total
-                        filestream.write("\n")
-                        numcolumns = 0
-
-                filestream.write("\n /\n")
-
-    def _to_bgrdecl_file(self, filename, fileformat=Format.UNFORMATTED):
-        contents = filter(
-            lambda x: x[1] is not None,
-            [
-                ("SPECGRID", self.specgrid.to_bgrdecl()),
-                ("MAPAXES ", self.mapaxes.to_bgrdecl() if self.mapaxes else None),
-                ("MAPUNITS", [self.mapunits.to_bgrdecl()] if self.mapunits else None),
-                ("GRIDUNIT", self.gridunit.to_bgrdecl() if self.gridunit else None),
-                ("GDORIENT", self.gdorient.to_bgrdecl() if self.gdorient else None),
-                ("COORD   ", self.coord.astype(np.float32)),
-                ("ZCORN   ", self.zcorn.astype(np.float32)),
-                (
-                    "ACTNUM  ",
-                    self.actnum.astype(np.int32) if self.actnum is not None else None,
-                ),
-            ],
-        )
-        write(
-            filename,
-            contents,
-        )
-
-    def _check_xtgeo_compatible(self):
-        if self.specgrid.coordinate_type == CoordinateType.CYLINDRICAL:
-            raise NotImplementedError(
-                "Xtgeo does not currently support cylindrical coordinate systems"
-            )
-        if self.specgrid.numres != 1:
-            raise NotImplementedError(
-                "Xtgeo does not currently support multiple reservoirs"
-            )
-        if self.gridunit and self.gridunit.grid_relative == GridRelative.MAP:
-            raise NotImplementedError(
-                "Xtgeo does not currently support conversion of map relative grid units"
-            )
+"""
+Datastructure for the contents of grdecl files.
+
+The grdecl file format is not specified in a strict manner
+but in the most general sense it is a file that can be included
+into the GRID section of a eclipse input file.
+
+However, it is nearly impossible to support such a file format completely,
+instead we narrow it down to the following subset of keywords:
+
+    * COORD
+    * ZCORN
+    * ACTNUM
+    * MAPAXES
+    * GRIDUNIT
+    * SPECGRID
+    * GDORIENT
+
+And ignore ECHO and NOECHO keywords. see _grid_format for the details
+of how these keywords are layed out in a text file, and see GrdeclGrid
+for how the grid geometry is interpreted from these keywords.
+"""
+from dataclasses import dataclass
+from typing import Optional, Tuple
+
+import numpy as np
+from ecl_data_io import Format, lazy_read, write
+
+from ._ecl_grid import (
+    CoordinateType,
+    EclGrid,
+    GdOrient,
+    GrdeclKeyword,
+    GridRelative,
+    GridUnit,
+    MapAxes,
+    Units,
+)
+from ._grdecl_format import IGNORE_ALL, open_grdecl
+
+
+@dataclass
+class SpecGrid(GrdeclKeyword):
+    """The SPECGRID keyword gives the size of the grid.
+
+    The 3 first values is the number of cells in each dimension
+    of the grid. The next is the number of reservoirs in the file and
+    the last is the type of coordinates, see CoordinateType.
+
+    example:
+        "SPECGRID 10 10 10 1 T /" meaning 10x10x10 grid with 1 reservoir
+        and cylindrical coordinates.
+
+    """
+
+    ndivix: int = 1
+    ndiviy: int = 1
+    ndiviz: int = 1
+    numres: int = 1
+    coordinate_type: CoordinateType = CoordinateType.CARTESIAN
+
+    def to_grdecl(self):
+        return [
+            self.ndivix,
+            self.ndiviy,
+            self.ndiviz,
+            self.numres,
+            self.coordinate_type.to_grdecl(),
+        ]
+
+    def to_bgrdecl(self):
+        return np.array(
+            [
+                self.ndivix,
+                self.ndiviy,
+                self.ndiviz,
+                self.numres,
+                self.coordinate_type.to_bgrdecl(),
+            ],
+            dtype=np.int32,
+        )
+
+    @classmethod
+    def from_bgrdecl(cls, values):
+        ivalues = [int(v) for v in values[:4]]
+        if len(values) < 5:
+            return cls(*ivalues)
+        if len(values) == 5:
+            return cls(*ivalues, CoordinateType.from_bgrdecl(values[-1]))
+        raise ValueError("SPECGRID should have at most 5 values")
+
+    @classmethod
+    def from_grdecl(cls, values):
+        ivalues = [int(v) for v in values[:4]]
+        if len(values) < 5:
+            return cls(*ivalues)
+        if len(values) == 5:
+            return cls(*ivalues, CoordinateType.from_grdecl(values[-1]))
+        raise ValueError("SPECGRID should have at most 5 values")
+
+
+class GrdeclGrid(EclGrid):
+    """
+    The main keywords that describe a grdecl grid is COORD, ZCORN and ACTNUM
+    and are described in xtgeo.grid3d._ecl_grid.
+    The remaining fields (SPECGRID, MAPAXES, MAPUNITS, GRIDUNIT, GDORIENT)
+    describe units, orientation and dimensions, see corresponding dataclasses.
+    The number of cells in each direction is described in the SPECGRID keyword.
+    """
+
+    def __init__(
+        self,
+        coord: np.ndarray,
+        zcorn: np.ndarray,
+        specgrid: SpecGrid,
+        actnum: Optional[np.ndarray] = None,
+        mapaxes: Optional[MapAxes] = None,
+        mapunits: Optional[Units] = None,
+        gridunit: Optional[GridUnit] = None,
+        gdorient: Optional[GdOrient] = None,
+    ):
+        self._coord = coord
+        self._zcorn = zcorn
+        self.specgrid = specgrid
+        self._actnum = actnum
+        self._mapaxes = mapaxes
+        self.mapunits = mapunits
+        self.gridunit = gridunit
+        self.gdorient = gdorient
+
+    def __str__(self):
+        return (
+            "GrdeclGrid("
+            f"coord={self._coord}, "
+            f"zcorn={self._zcorn}, "
+            f"specgrid={self.specgrid}, "
+            f"actnum={self._actnum}, "
+            f"mapaxes={self._mapaxes}, "
+            f"mapunits={self.mapunits}, "
+            f"gridunit={self.gridunit}, "
+            f"gdorient={self.gdorient})"
+        )
+
+    def __repr__(self):
+        return str(self)
+
+    @property
+    def mapaxes(self) -> Optional[MapAxes]:
+        return self._mapaxes
+
+    @mapaxes.setter
+    def mapaxes(self, value):
+        self._mapaxes = value
+
+    @property
+    def coord(self) -> np.ndarray:
+        return self._coord
+
+    @coord.setter
+    def coord(self, value):
+        self._coord = value
+
+    @property
+    def zcorn(self) -> np.ndarray:
+        return self._zcorn
+
+    @zcorn.setter
+    def zcorn(self, value):
+        self._zcorn = value
+
+    @property
+    def actnum(self) -> Optional[np.ndarray]:
+        return self._actnum
+
+    @classmethod
+    def default_settings_grid(
+        cls,
+        coord: np.ndarray,
+        zcorn: np.ndarray,
+        actnum: Optional[np.ndarray],
+        size: Tuple[int, int, int],
+    ):
+        return cls(coord, zcorn, SpecGrid(*size), actnum)
+
+    def __eq__(self, other):
+        if not isinstance(other, GrdeclGrid):
+            return False
+        return (
+            self.specgrid == other.specgrid
+            and self.mapaxes == other.mapaxes
+            and self.mapunits == other.mapunits
+            and self.gridunit == other.gridunit
+            and self.gdorient == other.gdorient
+            and np.array_equal(self.actnum, other.actnum)
+            and np.array_equal(self.coord, other.coord)
+            and np.array_equal(self.zcorn, other.zcorn)
+        )
+
+    @property
+    def dimensions(self):
+        return (self.specgrid.ndivix, self.specgrid.ndiviy, self.specgrid.ndiviz)
+
+    @property
+    def is_map_relative(self) -> bool:
+        return (
+            self.gridunit is not None
+            and self.gridunit.grid_relative == GridRelative.MAP
+        )
+
+    @property
+    def map_axis_units(self):
+        if self.mapunits is None:
+            return Units.METRES
+        return self.mapunits
+
+    @map_axis_units.setter
+    def map_axis_units(self, value):
+        self.mapunits = value
+
+    @property
+    def grid_units(self):
+        if self.gridunit is None:
+            return Units.METRES
+        return self.gridunit.unit
+
+    @grid_units.setter
+    def grid_units(self, value):
+        if self.gridunit is None and value != Units.METRES:
+            self.gridunit = GridUnit(unit=value)
+        elif self.gridunit is not None:
+            self.gridunit.unit = value
+
+    @classmethod
+    def from_file(cls, filename, fileformat="grdecl"):
+        """
+        write the grdeclgrid to a file.
+        :param filename: path to file to write.
+        :param fileformat: Either "grdecl" or "bgrdecl" to
+            indicate binary or ascii format.
+        """
+        if fileformat == "grdecl":
+            return cls._from_grdecl_file(filename)
+        if fileformat == "bgrdecl":
+            return cls._from_bgrdecl_file(filename)
+        raise ValueError(b"Unknown grdecl file format {fileformat}")
+
+    @classmethod
+    def _from_bgrdecl_file(cls, filename, fileformat=None):
+        keyword_factories = {
+            "COORD": lambda x: np.array(x, dtype=np.float32),
+            "ZCORN": lambda x: np.array(x, dtype=np.float32),
+            "ACTNUM": lambda x: np.array(x, dtype=np.int32),
+            "MAPAXES": MapAxes.from_bgrdecl,
+            "MAPUNITS": lambda x: Units.from_bgrdecl(x[0]),
+            "GRIDUNIT": GridUnit.from_bgrdecl,
+            "SPECGRID": SpecGrid.from_bgrdecl,
+            "GDORIENT": GdOrient.from_bgrdecl,
+        }
+        results = {}
+        for entry in lazy_read(filename, fileformat=fileformat):
+            if len(results) == len(keyword_factories):
+                break
+            kw = entry.read_keyword().rstrip()
+            if kw in results:
+                raise ValueError(f"Duplicate keyword {kw} in {filename}")
+            try:
+                factory = keyword_factories[kw]
+            except KeyError as e:
+                raise ValueError(f"Unknown grdecl keyword {kw}") from e
+            results[kw.lower()] = factory(entry.read_array())
+        return cls(**results)
+
+    @classmethod
+    def _from_grdecl_file(cls, filename):
+        keyword_factories = {
+            "COORD": lambda x: np.array(x, dtype=np.float32),
+            "ZCORN": lambda x: np.array(x, dtype=np.float32),
+            "ACTNUM": lambda x: np.array(x, dtype=np.int32),
+            "MAPAXES": MapAxes.from_grdecl,
+            "MAPUNITS": lambda x: Units.from_grdecl(x[0]),
+            "GRIDUNIT": GridUnit.from_grdecl,
+            "SPECGRID": SpecGrid.from_grdecl,
+            "GDORIENT": GdOrient.from_grdecl,
+        }
+        results = {}
+        with open_grdecl(
+            filename,
+            keywords=["MAPAXES", "MAPUNITS", "GRIDUNIT", "SPECGRID", "GDORIENT"],
+            simple_keywords=["COORD", "ZCORN", "ACTNUM"],
+            max_len=None,
+            ignore=IGNORE_ALL,
+            strict=False,
+        ) as keyword_generator:
+            for kw, values in keyword_generator:
+                if len(results) == len(keyword_factories):
+                    break
+                if kw in results:
+                    raise ValueError(f"Duplicate keyword {kw} in {filename}")
+                try:
+                    factory = keyword_factories[kw]
+                except KeyError as e:
+                    raise ValueError(f"Unknown grdecl keyword {kw}") from e
+                results[kw.lower()] = factory(values)
+        return cls(**results)
+
+    def to_file(self, filename, fileformat="grdecl"):
+        """
+        write the grdeclgrid to a file.
+        :param filename: path to file to write.
+        :param fileformat: Either "grdecl" or "bgrdecl" to
+            indicate binary or ascii format.
+        """
+        if fileformat == "grdecl":
+            return self._to_grdecl_file(filename)
+        if fileformat == "bgrdecl":
+            return self._to_bgrdecl_file(filename)
+        raise ValueError(b"Unknown grdecl file format {fileformat}")
+
+    def _to_grdecl_file(self, filename):
+        with open(filename, "w") as filestream:
+            keywords = [
+                ("SPECGRID", self.specgrid.to_grdecl()),
+                ("MAPAXES", self.mapaxes.to_grdecl() if self.mapaxes else None),
+                ("MAPUNITS", [self.mapunits.to_grdecl()] if self.mapunits else None),
+                ("GRIDUNIT", self.gridunit.to_grdecl() if self.gridunit else None),
+                ("GDORIENT", self.gdorient.to_grdecl() if self.gdorient else None),
+                ("COORD", self.coord),
+                ("ZCORN", self.zcorn),
+                ("ACTNUM", self.actnum),
+            ]
+            for kw, values in keywords:
+                if values is None:
+                    continue
+                filestream.write(f"{kw}\n")
+                numcolumns = 0
+                for value in values:
+                    numcolumns += 1
+                    filestream.write(f" {value}")
+
+                    if numcolumns >= 6:  # 6 should ensure < 128 character width total
+                        filestream.write("\n")
+                        numcolumns = 0
+
+                filestream.write("\n /\n")
+
+    def _to_bgrdecl_file(self, filename, fileformat=Format.UNFORMATTED):
+        contents = filter(
+            lambda x: x[1] is not None,
+            [
+                ("SPECGRID", self.specgrid.to_bgrdecl()),
+                ("MAPAXES ", self.mapaxes.to_bgrdecl() if self.mapaxes else None),
+                ("MAPUNITS", [self.mapunits.to_bgrdecl()] if self.mapunits else None),
+                ("GRIDUNIT", self.gridunit.to_bgrdecl() if self.gridunit else None),
+                ("GDORIENT", self.gdorient.to_bgrdecl() if self.gdorient else None),
+                ("COORD   ", self.coord.astype(np.float32)),
+                ("ZCORN   ", self.zcorn.astype(np.float32)),
+                (
+                    "ACTNUM  ",
+                    self.actnum.astype(np.int32) if self.actnum is not None else None,
+                ),
+            ],
+        )
+        write(
+            filename,
+            contents,
+        )
+
+    def _check_xtgeo_compatible(self):
+        if self.specgrid.coordinate_type == CoordinateType.CYLINDRICAL:
+            raise NotImplementedError(
+                "Xtgeo does not currently support cylindrical coordinate systems"
+            )
+        if self.specgrid.numres != 1:
+            raise NotImplementedError(
+                "Xtgeo does not currently support multiple reservoirs"
+            )
+        if self.gridunit and self.gridunit.grid_relative == GridRelative.MAP:
+            raise NotImplementedError(
+                "Xtgeo does not currently support conversion of map relative grid units"
+            )
```

## xtgeo/grid3d/_grid3d.py

 * *Ordering differences only*

```diff
@@ -1,45 +1,45 @@
-# -*- coding: utf-8 -*-
-"""Private baseclass for Grid and GridProperties, not to be used by itself."""
-
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class _Grid3D(object):
-    """Abstract base class for Grid3D."""
-
-    def __init__(self, ncol=4, nrow=3, nlay=5):
-        self._ncol = ncol
-        self._nrow = nrow
-        self._nlay = nlay
-
-    @property
-    def ncol(self) -> int:
-        """Returns the NCOL (NX or Ncolumns) number of cells."""
-        return self._ncol
-
-    @property
-    def nrow(self) -> int:
-        """Returns the NROW (NY or Nrows) number of cells."""
-        return self._nrow
-
-    @property
-    def nlay(self) -> int:
-        """Returns the NLAY (NZ or Nlayers) number of cells."""
-        return self._nlay
-
-    def _evaluate_mask(self, mask) -> bool:
-        xtg.warn(
-            f"Use of keyword 'mask' in argument list is deprecated, use alternative "
-            f"specified in API instead! In: {self}"
-        )
-
-        if not isinstance(mask, bool):
-            raise ValueError('Wrong value or use of keyword "mask"')
-
-        if mask is False:
-            return False
-
-        return True
+# -*- coding: utf-8 -*-
+"""Private baseclass for Grid and GridProperties, not to be used by itself."""
+
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class _Grid3D(object):
+    """Abstract base class for Grid3D."""
+
+    def __init__(self, ncol=4, nrow=3, nlay=5):
+        self._ncol = ncol
+        self._nrow = nrow
+        self._nlay = nlay
+
+    @property
+    def ncol(self) -> int:
+        """Returns the NCOL (NX or Ncolumns) number of cells."""
+        return self._ncol
+
+    @property
+    def nrow(self) -> int:
+        """Returns the NROW (NY or Nrows) number of cells."""
+        return self._nrow
+
+    @property
+    def nlay(self) -> int:
+        """Returns the NLAY (NZ or Nlayers) number of cells."""
+        return self._nlay
+
+    def _evaluate_mask(self, mask) -> bool:
+        xtg.warn(
+            f"Use of keyword 'mask' in argument list is deprecated, use alternative "
+            f"specified in API instead! In: {self}"
+        )
+
+        if not isinstance(mask, bool):
+            raise ValueError('Wrong value or use of keyword "mask"')
+
+        if mask is False:
+            return False
+
+        return True
```

## xtgeo/grid3d/_grid3d_fence.py

 * *Ordering differences only*

```diff
@@ -1,158 +1,158 @@
-# -*- coding: utf-8 -*-
-
-"""Some grid utilities, file scanning etc."""
-import numpy as np
-
-import xtgeo
-from xtgeo.grid3d import _gridprop_lowlevel as gl
-from xtgeo.surface import _regsurf_lowlevel as rl
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def get_randomline(
-    self,
-    fencespec,
-    prop,
-    zmin=None,
-    zmax=None,
-    zincrement=1.0,
-    hincrement=None,
-    atleast=5,
-    nextend=2,
-):
-    """Extract a randomline from a 3D grid.
-
-    This is a difficult task, in particular in terms of acceptable speed.
-    """
-
-    logger.info("Enter get_randomline from Grid...")
-
-    _update_tmpvars(self)
-
-    if hincrement is None and isinstance(fencespec, xtgeo.Polygons):
-        logger.info("Estimate hincrement from Polygons instance...")
-        fencespec = _get_randomline_fence(self, fencespec, hincrement, atleast, nextend)
-        logger.info("Estimate hincrement from Polygons instance... DONE")
-
-    logger.info("Get property...")
-    if isinstance(prop, str):
-        prop = self.get_prop_by_name(prop)
-
-    xcoords = fencespec[:, 0]
-    ycoords = fencespec[:, 1]
-    hcoords = fencespec[:, 3]
-
-    if zmin is None:
-        zmin = self._tmp["topd"].values.min()
-    if zmax is None:
-        zmax = self._tmp["basd"].values.max()
-
-    nzsam = int((zmax - zmin) / float(zincrement)) + 1
-    nsamples = xcoords.shape[0] * nzsam
-
-    logger.info("Running C routine to get randomline...")
-    self._xtgformat1()
-    _ier, values = _cxtgeo.grd3d_get_randomline(
-        xcoords,
-        ycoords,
-        zmin,
-        zmax,
-        nzsam,
-        self._tmp["topd"].ncol,
-        self._tmp["topd"].nrow,
-        self._tmp["topd"].xori,
-        self._tmp["topd"].yori,
-        self._tmp["topd"].xinc,
-        self._tmp["topd"].yinc,
-        self._tmp["topd"].rotation,
-        self._tmp["topd"].yflip,
-        self._tmp["topi_carr"],
-        self._tmp["topj_carr"],
-        self._tmp["basi_carr"],
-        self._tmp["basj_carr"],
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        gl.update_carray(prop, dtype=np.float64),
-        self._tmp["onegrid"]._zcornsv,
-        self._tmp["onegrid"]._actnumsv,
-        nsamples,
-    )
-
-    logger.info("Running C routine to get randomline... DONE")
-
-    values[values > xtgeo.UNDEF_LIMIT] = np.nan
-    arr = values.reshape((xcoords.shape[0], nzsam)).T
-
-    logger.info("Getting randomline... DONE")
-    return (hcoords[0], hcoords[-1], zmin, zmax, arr)
-
-
-def _update_tmpvars(self, force=False):
-    """The self._tmp variables are needed to speed up calculations.
-
-    If they are already created, the no need to recreate
-    """
-    if "onegrid" not in self._tmp or force:
-        logger.info("Make a tmp onegrid instance...")
-        self._tmp["onegrid"] = self.copy()
-        self._tmp["onegrid"].reduce_to_one_layer()
-        one = self._tmp["onegrid"]
-        logger.info("Make a tmp onegrid instance... DONE")
-        logger.info("Make a set of tmp surfaces for I J locations + depth...")
-        self._tmp["topd"] = xtgeo.surface_from_grid3d(
-            one, where="top", mode="depth", rfactor=4
-        )
-        self._tmp["topi"] = xtgeo.surface_from_grid3d(
-            one, where="top", mode="i", rfactor=4
-        )
-        self._tmp["topj"] = xtgeo.surface_from_grid3d(
-            one, where="top", mode="j", rfactor=4
-        )
-        self._tmp["basd"] = xtgeo.surface_from_grid3d(
-            one, where="base", mode="depth", rfactor=4
-        )
-        self._tmp["basi"] = xtgeo.surface_from_grid3d(
-            one, where="base", mode="i", rfactor=4
-        )
-        self._tmp["basj"] = xtgeo.surface_from_grid3d(
-            one, where="base", mode="j", rfactor=4
-        )
-
-        self._tmp["topi"].fill()
-        self._tmp["topj"].fill()
-        self._tmp["basi"].fill()
-        self._tmp["basj"].fill()
-
-        self._tmp["topi_carr"] = rl.get_carr_double(self._tmp["topi"])
-        self._tmp["topj_carr"] = rl.get_carr_double(self._tmp["topj"])
-        self._tmp["basi_carr"] = rl.get_carr_double(self._tmp["basi"])
-        self._tmp["basj_carr"] = rl.get_carr_double(self._tmp["basj"])
-
-        logger.info("Make a set of tmp surfaces for I J locations + depth... DONE")
-    else:
-        logger.info("Re-use existing onegrid and tmp surfaces for I J")
-
-
-def _get_randomline_fence(self, fencespec, hincrement, atleast, nextend):
-    """Compute a resampled fence from a Polygons instance."""
-    if hincrement is None:
-        geom = self.get_geometrics()
-
-        avgdxdy = 0.5 * (geom[10] + geom[11])
-        distance = 0.5 * avgdxdy
-    else:
-        distance = hincrement
-
-    logger.info("Getting fence from a Polygons instance...")
-    fspec = fencespec.get_fence(
-        distance=distance, atleast=atleast, nextend=nextend, asnumpy=True
-    )
-    logger.info("Getting fence from a Polygons instance... DONE")
-    return fspec
+# -*- coding: utf-8 -*-
+
+"""Some grid utilities, file scanning etc."""
+import numpy as np
+
+import xtgeo
+from xtgeo.grid3d import _gridprop_lowlevel as gl
+from xtgeo.surface import _regsurf_lowlevel as rl
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def get_randomline(
+    self,
+    fencespec,
+    prop,
+    zmin=None,
+    zmax=None,
+    zincrement=1.0,
+    hincrement=None,
+    atleast=5,
+    nextend=2,
+):
+    """Extract a randomline from a 3D grid.
+
+    This is a difficult task, in particular in terms of acceptable speed.
+    """
+
+    logger.info("Enter get_randomline from Grid...")
+
+    _update_tmpvars(self)
+
+    if hincrement is None and isinstance(fencespec, xtgeo.Polygons):
+        logger.info("Estimate hincrement from Polygons instance...")
+        fencespec = _get_randomline_fence(self, fencespec, hincrement, atleast, nextend)
+        logger.info("Estimate hincrement from Polygons instance... DONE")
+
+    logger.info("Get property...")
+    if isinstance(prop, str):
+        prop = self.get_prop_by_name(prop)
+
+    xcoords = fencespec[:, 0]
+    ycoords = fencespec[:, 1]
+    hcoords = fencespec[:, 3]
+
+    if zmin is None:
+        zmin = self._tmp["topd"].values.min()
+    if zmax is None:
+        zmax = self._tmp["basd"].values.max()
+
+    nzsam = int((zmax - zmin) / float(zincrement)) + 1
+    nsamples = xcoords.shape[0] * nzsam
+
+    logger.info("Running C routine to get randomline...")
+    self._xtgformat1()
+    _ier, values = _cxtgeo.grd3d_get_randomline(
+        xcoords,
+        ycoords,
+        zmin,
+        zmax,
+        nzsam,
+        self._tmp["topd"].ncol,
+        self._tmp["topd"].nrow,
+        self._tmp["topd"].xori,
+        self._tmp["topd"].yori,
+        self._tmp["topd"].xinc,
+        self._tmp["topd"].yinc,
+        self._tmp["topd"].rotation,
+        self._tmp["topd"].yflip,
+        self._tmp["topi_carr"],
+        self._tmp["topj_carr"],
+        self._tmp["basi_carr"],
+        self._tmp["basj_carr"],
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        gl.update_carray(prop, dtype=np.float64),
+        self._tmp["onegrid"]._zcornsv,
+        self._tmp["onegrid"]._actnumsv,
+        nsamples,
+    )
+
+    logger.info("Running C routine to get randomline... DONE")
+
+    values[values > xtgeo.UNDEF_LIMIT] = np.nan
+    arr = values.reshape((xcoords.shape[0], nzsam)).T
+
+    logger.info("Getting randomline... DONE")
+    return (hcoords[0], hcoords[-1], zmin, zmax, arr)
+
+
+def _update_tmpvars(self, force=False):
+    """The self._tmp variables are needed to speed up calculations.
+
+    If they are already created, the no need to recreate
+    """
+    if "onegrid" not in self._tmp or force:
+        logger.info("Make a tmp onegrid instance...")
+        self._tmp["onegrid"] = self.copy()
+        self._tmp["onegrid"].reduce_to_one_layer()
+        one = self._tmp["onegrid"]
+        logger.info("Make a tmp onegrid instance... DONE")
+        logger.info("Make a set of tmp surfaces for I J locations + depth...")
+        self._tmp["topd"] = xtgeo.surface_from_grid3d(
+            one, where="top", mode="depth", rfactor=4
+        )
+        self._tmp["topi"] = xtgeo.surface_from_grid3d(
+            one, where="top", mode="i", rfactor=4
+        )
+        self._tmp["topj"] = xtgeo.surface_from_grid3d(
+            one, where="top", mode="j", rfactor=4
+        )
+        self._tmp["basd"] = xtgeo.surface_from_grid3d(
+            one, where="base", mode="depth", rfactor=4
+        )
+        self._tmp["basi"] = xtgeo.surface_from_grid3d(
+            one, where="base", mode="i", rfactor=4
+        )
+        self._tmp["basj"] = xtgeo.surface_from_grid3d(
+            one, where="base", mode="j", rfactor=4
+        )
+
+        self._tmp["topi"].fill()
+        self._tmp["topj"].fill()
+        self._tmp["basi"].fill()
+        self._tmp["basj"].fill()
+
+        self._tmp["topi_carr"] = rl.get_carr_double(self._tmp["topi"])
+        self._tmp["topj_carr"] = rl.get_carr_double(self._tmp["topj"])
+        self._tmp["basi_carr"] = rl.get_carr_double(self._tmp["basi"])
+        self._tmp["basj_carr"] = rl.get_carr_double(self._tmp["basj"])
+
+        logger.info("Make a set of tmp surfaces for I J locations + depth... DONE")
+    else:
+        logger.info("Re-use existing onegrid and tmp surfaces for I J")
+
+
+def _get_randomline_fence(self, fencespec, hincrement, atleast, nextend):
+    """Compute a resampled fence from a Polygons instance."""
+    if hincrement is None:
+        geom = self.get_geometrics()
+
+        avgdxdy = 0.5 * (geom[10] + geom[11])
+        distance = 0.5 * avgdxdy
+    else:
+        distance = hincrement
+
+    logger.info("Getting fence from a Polygons instance...")
+    fspec = fencespec.get_fence(
+        distance=distance, atleast=atleast, nextend=nextend, asnumpy=True
+    )
+    logger.info("Getting fence from a Polygons instance... DONE")
+    return fspec
```

## xtgeo/grid3d/_grid3d_utils.py

 * *Ordering differences only*

```diff
@@ -1,209 +1,209 @@
-# -*- coding: utf-8 -*-
-
-"""Some grid utilities, file scanning etc (methods with no class)"""
-import re
-
-import pandas as pd
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo import XTGeoCLibError
-from xtgeo.common.constants import MAXDATES, MAXKEYWORDS
-
-xtg = xtgeo.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def scan_keywords(
-    pfile, fformat="xecl", maxkeys=MAXKEYWORDS, dataframe=False, dates=False
-):
-    """Quick scan of keywords in Eclipse binary restart/init/... file,
-    or ROFF binary files.
-
-    Cf. grid_properties.py description
-    """
-
-    pfile.get_cfhandle()  # just to keep cfhanclecounter correct
-
-    if fformat == "xecl":
-        if dates:
-            data = _scan_ecl_keywords_w_dates(
-                pfile, maxkeys=maxkeys, dataframe=dataframe
-            )
-        else:
-            data = _scan_ecl_keywords(pfile, maxkeys=maxkeys, dataframe=dataframe)
-
-    else:
-        data = _scan_roff_keywords(pfile, maxkeys=maxkeys, dataframe=dataframe)
-
-    pfile.cfclose()
-
-    return data
-
-
-def scan_dates(pfile, maxdates=MAXDATES, dataframe=False):
-    """Quick scan dates in a simulation restart file.
-
-    Cf. grid_properties.py description
-    """
-
-    seq = _cxtgeo.new_intarray(maxdates)
-    day = _cxtgeo.new_intarray(maxdates)
-    mon = _cxtgeo.new_intarray(maxdates)
-    yer = _cxtgeo.new_intarray(maxdates)
-
-    cfhandle = pfile.get_cfhandle()
-
-    nstat = _cxtgeo.grd3d_ecl_tsteps(cfhandle, seq, day, mon, yer, maxdates)
-
-    pfile.cfclose()
-
-    sq = []
-    da = []
-    for i in range(nstat):
-        sq.append(_cxtgeo.intarray_getitem(seq, i))
-        dday = _cxtgeo.intarray_getitem(day, i)
-        dmon = _cxtgeo.intarray_getitem(mon, i)
-        dyer = _cxtgeo.intarray_getitem(yer, i)
-        date = f"{dyer:4}{dmon:02}{dday:02}"
-        da.append(int(date))
-
-    for item in [seq, day, mon, yer]:
-        _cxtgeo.delete_intarray(item)
-
-    zdates = list(zip(sq, da))  # list for PY3
-
-    if dataframe:
-        cols = ["SEQNUM", "DATE"]
-        df = pd.DataFrame.from_records(zdates, columns=cols)
-        return df
-
-    return zdates
-
-
-def _scan_ecl_keywords(pfile, maxkeys=MAXKEYWORDS, dataframe=False):
-    logger.info("Scanning ECL keywords...")
-    cfhandle = pfile.get_cfhandle()
-
-    # maxkeys*10 is used for 1D kewords; 10 => max 8 letters in eclipse +
-    # "|" + "extra buffer"
-    nkeys, keywords, rectypes, reclens, recstarts = _cxtgeo.grd3d_scan_eclbinary(
-        cfhandle, maxkeys * 10, maxkeys, maxkeys, maxkeys
-    )
-
-    pfile.cfclose()
-
-    if nkeys == -1:
-        raise XTGeoCLibError(f"scanning ecl keywords exited with error code {nkeys}")
-
-    if nkeys == -2:
-        raise XTGeoCLibError(
-            f"Number of keywords seems greater than {maxkeys} which is currently a "
-            "hard limit"
-        )
-
-    keywords = re.sub(r"\s+\|", "|", keywords)
-    keywords = keywords.split("|")
-
-    # record types translation (cf: grd3d_scan_eclbinary.c in cxtgeo)
-    rct = {
-        1: "INTE",
-        2: "REAL",
-        3: "DOUB",
-        4: "CHAR",
-        5: "LOGI",
-        6: "MESS",
-        -1: "????",
-    }
-
-    rectypes = rectypes.tolist()
-    rc = [rct[key] for nn, key in enumerate(rectypes) if nn < nkeys]
-    rl = reclens[0:nkeys].tolist()
-    rs = recstarts[0:nkeys].tolist()
-
-    result = list(zip(keywords, rc, rl, rs))
-
-    if dataframe:
-        cols = ["KEYWORD", "TYPE", "NITEMS", "BYTESTART"]
-        df = pd.DataFrame.from_records(result, columns=cols)
-        return df
-
-    return result
-
-
-def _scan_ecl_keywords_w_dates(pfile, maxkeys=MAXKEYWORDS, dataframe=False):
-    """Add a date column to the keyword"""
-
-    logger.info("Scan keywords with dates...")
-    xkeys = _scan_ecl_keywords(pfile, maxkeys=maxkeys, dataframe=False)
-
-    xdates = scan_dates(pfile, maxdates=MAXDATES, dataframe=False)
-
-    result = []
-    # now merge these two:
-    nv = -1
-    date = 0
-    for item in xkeys:
-        name, dtype, reclen, bytepos = item
-        if name == "SEQNUM":
-            nv += 1
-            date = xdates[nv][1]
-
-        entry = (name, dtype, reclen, bytepos, date)
-        result.append(entry)
-
-    if dataframe:
-        cols = ["KEYWORD", "TYPE", "NITEMS", "BYTESTART", "DATE"]
-        df = pd.DataFrame.from_records(result, columns=cols)
-        return df
-
-    return result
-
-
-def _scan_roff_keywords(pfile, maxkeys=MAXKEYWORDS, dataframe=False):
-    rectypes = _cxtgeo.new_intarray(maxkeys)
-    reclens = _cxtgeo.new_longarray(maxkeys)
-    recstarts = _cxtgeo.new_longarray(maxkeys)
-
-    cfhandle = pfile.get_cfhandle()
-
-    # maxkeys*32 is just to give sufficient allocated character space
-    nkeys, _tmp1, keywords = _cxtgeo.grd3d_scan_roffbinary(
-        cfhandle, maxkeys * 32, rectypes, reclens, recstarts, maxkeys
-    )
-
-    pfile.cfclose()
-
-    keywords = keywords.replace(" ", "")
-    keywords = keywords.split("|")
-
-    # record types translation (cf: grd3d_scan_eclbinary.c in cxtgeo)
-    rct = {
-        "1": "int",
-        "2": "float",
-        "3": "double",
-        "4": "char",
-        "5": "bool",
-        "6": "byte",
-    }
-
-    rc = []
-    rl = []
-    rs = []
-    for i in range(nkeys):
-        rc.append(rct[str(_cxtgeo.intarray_getitem(rectypes, i))])
-        rl.append(_cxtgeo.longarray_getitem(reclens, i))
-        rs.append(_cxtgeo.longarray_getitem(recstarts, i))
-
-    _cxtgeo.delete_intarray(rectypes)
-    _cxtgeo.delete_longarray(reclens)
-    _cxtgeo.delete_longarray(recstarts)
-
-    result = list(zip(keywords, rc, rl, rs))
-
-    if dataframe:
-        cols = ["KEYWORD", "TYPE", "NITEMS", "BYTESTARTDATA"]
-        df = pd.DataFrame.from_records(result, columns=cols)
-        return df
-
-    return result
+# -*- coding: utf-8 -*-
+
+"""Some grid utilities, file scanning etc (methods with no class)"""
+import re
+
+import pandas as pd
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo import XTGeoCLibError
+from xtgeo.common.constants import MAXDATES, MAXKEYWORDS
+
+xtg = xtgeo.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def scan_keywords(
+    pfile, fformat="xecl", maxkeys=MAXKEYWORDS, dataframe=False, dates=False
+):
+    """Quick scan of keywords in Eclipse binary restart/init/... file,
+    or ROFF binary files.
+
+    Cf. grid_properties.py description
+    """
+
+    pfile.get_cfhandle()  # just to keep cfhanclecounter correct
+
+    if fformat == "xecl":
+        if dates:
+            data = _scan_ecl_keywords_w_dates(
+                pfile, maxkeys=maxkeys, dataframe=dataframe
+            )
+        else:
+            data = _scan_ecl_keywords(pfile, maxkeys=maxkeys, dataframe=dataframe)
+
+    else:
+        data = _scan_roff_keywords(pfile, maxkeys=maxkeys, dataframe=dataframe)
+
+    pfile.cfclose()
+
+    return data
+
+
+def scan_dates(pfile, maxdates=MAXDATES, dataframe=False):
+    """Quick scan dates in a simulation restart file.
+
+    Cf. grid_properties.py description
+    """
+
+    seq = _cxtgeo.new_intarray(maxdates)
+    day = _cxtgeo.new_intarray(maxdates)
+    mon = _cxtgeo.new_intarray(maxdates)
+    yer = _cxtgeo.new_intarray(maxdates)
+
+    cfhandle = pfile.get_cfhandle()
+
+    nstat = _cxtgeo.grd3d_ecl_tsteps(cfhandle, seq, day, mon, yer, maxdates)
+
+    pfile.cfclose()
+
+    sq = []
+    da = []
+    for i in range(nstat):
+        sq.append(_cxtgeo.intarray_getitem(seq, i))
+        dday = _cxtgeo.intarray_getitem(day, i)
+        dmon = _cxtgeo.intarray_getitem(mon, i)
+        dyer = _cxtgeo.intarray_getitem(yer, i)
+        date = f"{dyer:4}{dmon:02}{dday:02}"
+        da.append(int(date))
+
+    for item in [seq, day, mon, yer]:
+        _cxtgeo.delete_intarray(item)
+
+    zdates = list(zip(sq, da))  # list for PY3
+
+    if dataframe:
+        cols = ["SEQNUM", "DATE"]
+        df = pd.DataFrame.from_records(zdates, columns=cols)
+        return df
+
+    return zdates
+
+
+def _scan_ecl_keywords(pfile, maxkeys=MAXKEYWORDS, dataframe=False):
+    logger.info("Scanning ECL keywords...")
+    cfhandle = pfile.get_cfhandle()
+
+    # maxkeys*10 is used for 1D kewords; 10 => max 8 letters in eclipse +
+    # "|" + "extra buffer"
+    nkeys, keywords, rectypes, reclens, recstarts = _cxtgeo.grd3d_scan_eclbinary(
+        cfhandle, maxkeys * 10, maxkeys, maxkeys, maxkeys
+    )
+
+    pfile.cfclose()
+
+    if nkeys == -1:
+        raise XTGeoCLibError(f"scanning ecl keywords exited with error code {nkeys}")
+
+    if nkeys == -2:
+        raise XTGeoCLibError(
+            f"Number of keywords seems greater than {maxkeys} which is currently a "
+            "hard limit"
+        )
+
+    keywords = re.sub(r"\s+\|", "|", keywords)
+    keywords = keywords.split("|")
+
+    # record types translation (cf: grd3d_scan_eclbinary.c in cxtgeo)
+    rct = {
+        1: "INTE",
+        2: "REAL",
+        3: "DOUB",
+        4: "CHAR",
+        5: "LOGI",
+        6: "MESS",
+        -1: "????",
+    }
+
+    rectypes = rectypes.tolist()
+    rc = [rct[key] for nn, key in enumerate(rectypes) if nn < nkeys]
+    rl = reclens[0:nkeys].tolist()
+    rs = recstarts[0:nkeys].tolist()
+
+    result = list(zip(keywords, rc, rl, rs))
+
+    if dataframe:
+        cols = ["KEYWORD", "TYPE", "NITEMS", "BYTESTART"]
+        df = pd.DataFrame.from_records(result, columns=cols)
+        return df
+
+    return result
+
+
+def _scan_ecl_keywords_w_dates(pfile, maxkeys=MAXKEYWORDS, dataframe=False):
+    """Add a date column to the keyword"""
+
+    logger.info("Scan keywords with dates...")
+    xkeys = _scan_ecl_keywords(pfile, maxkeys=maxkeys, dataframe=False)
+
+    xdates = scan_dates(pfile, maxdates=MAXDATES, dataframe=False)
+
+    result = []
+    # now merge these two:
+    nv = -1
+    date = 0
+    for item in xkeys:
+        name, dtype, reclen, bytepos = item
+        if name == "SEQNUM":
+            nv += 1
+            date = xdates[nv][1]
+
+        entry = (name, dtype, reclen, bytepos, date)
+        result.append(entry)
+
+    if dataframe:
+        cols = ["KEYWORD", "TYPE", "NITEMS", "BYTESTART", "DATE"]
+        df = pd.DataFrame.from_records(result, columns=cols)
+        return df
+
+    return result
+
+
+def _scan_roff_keywords(pfile, maxkeys=MAXKEYWORDS, dataframe=False):
+    rectypes = _cxtgeo.new_intarray(maxkeys)
+    reclens = _cxtgeo.new_longarray(maxkeys)
+    recstarts = _cxtgeo.new_longarray(maxkeys)
+
+    cfhandle = pfile.get_cfhandle()
+
+    # maxkeys*32 is just to give sufficient allocated character space
+    nkeys, _tmp1, keywords = _cxtgeo.grd3d_scan_roffbinary(
+        cfhandle, maxkeys * 32, rectypes, reclens, recstarts, maxkeys
+    )
+
+    pfile.cfclose()
+
+    keywords = keywords.replace(" ", "")
+    keywords = keywords.split("|")
+
+    # record types translation (cf: grd3d_scan_eclbinary.c in cxtgeo)
+    rct = {
+        "1": "int",
+        "2": "float",
+        "3": "double",
+        "4": "char",
+        "5": "bool",
+        "6": "byte",
+    }
+
+    rc = []
+    rl = []
+    rs = []
+    for i in range(nkeys):
+        rc.append(rct[str(_cxtgeo.intarray_getitem(rectypes, i))])
+        rl.append(_cxtgeo.longarray_getitem(reclens, i))
+        rs.append(_cxtgeo.longarray_getitem(recstarts, i))
+
+    _cxtgeo.delete_intarray(rectypes)
+    _cxtgeo.delete_longarray(reclens)
+    _cxtgeo.delete_longarray(recstarts)
+
+    result = list(zip(keywords, rc, rl, rs))
+
+    if dataframe:
+        cols = ["KEYWORD", "TYPE", "NITEMS", "BYTESTARTDATA"]
+        df = pd.DataFrame.from_records(result, columns=cols)
+        return df
+
+    return result
```

## xtgeo/grid3d/_grid_etc1.py

 * *Ordering differences only*

```diff
@@ -1,1440 +1,1440 @@
-"""Private module, Grid ETC 1 methods, info/modify/report."""
-
-from collections import OrderedDict
-from copy import deepcopy
-from packaging.version import parse as versionparse
-from math import atan2, degrees
-from typing import Tuple
-
-import numpy as np
-import numpy.ma as ma
-import pandas as pd
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.common.calc import find_flip
-from xtgeo.xyz.polygons import Polygons
-
-from . import _gridprop_lowlevel
-from ._grid3d_fence import _update_tmpvars
-from .grid_property import GridProperty
-
-xtg = XTGeoDialog()
-
-
-logger = xtg.functionlogger(__name__)
-
-
-# Note that "self" is the grid instance
-
-
-def create_box(dimension, origin, oricenter, increment, rotation, flip):
-    """Create a shoebox grid from cubi'sh spec, xtgformat=2."""
-    ncol, nrow, nlay = dimension
-    nncol = ncol + 1
-    nnrow = nrow + 1
-    nnlay = nlay + 1
-
-    coordsv = np.zeros((nncol, nnrow, 6), dtype=np.float64)
-    zcornsv = np.zeros((nncol, nnrow, nnlay, 4), dtype=np.float32)
-    actnumsv = np.zeros((ncol, nrow, nlay), dtype=np.int32)
-
-    option = 0
-    if oricenter:
-        option = 1
-
-    _cxtgeo.grdcp3d_from_cube(
-        ncol,
-        nrow,
-        nlay,
-        coordsv,
-        zcornsv,
-        actnumsv,
-        origin[0],
-        origin[1],
-        origin[2],
-        increment[0],
-        increment[1],
-        increment[2],
-        rotation,
-        flip,
-        option,
-    )
-
-    return {
-        "coordsv": coordsv,
-        "zcornsv": zcornsv,
-        "actnumsv": actnumsv,
-    }
-
-
-method_factory = {
-    "euclid": _cxtgeo.euclid_length,
-    "horizontal": _cxtgeo.horizontal_length,
-    "east west vertical": _cxtgeo.east_west_vertical_length,
-    "north south vertical": _cxtgeo.north_south_vertical_length,
-    "x projection": _cxtgeo.x_projection,
-    "y projection": _cxtgeo.y_projection,
-    "z projection": _cxtgeo.z_projection,
-}
-
-
-def get_dz(
-    self,
-    name: str = "dZ",
-    flip: bool = True,
-    asmasked: bool = True,
-    metric="z projection",
-) -> GridProperty:
-    """Get average cell height (dz) as property.
-
-    Args:
-        flip (bool): whether to flip the z direction, ie. increasing z is
-            increasing depth (defaults to True)
-        asmasked (bool): Whether to mask property by whether
-        name (str): Name of resulting grid property, defaults to "dZ".
-    """
-    self._xtgformat2()
-    nx, ny, nz = self.dimensions
-    result = np.zeros((nx * ny * nz))
-    try:
-        metric_fun = method_factory[metric]
-    except KeyError as err:
-        raise ValueError(f"Unknown metric {metric}") from err
-    _cxtgeo.grdcp3d_calc_dz(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv.ravel(),
-        self._zcornsv.ravel(),
-        result,
-        metric_fun,
-    )
-
-    if not flip:
-        result *= -1
-
-    if asmasked:
-        result = np.ma.masked_array(result, self._actnumsv == 0)
-    else:
-        result = np.ma.masked_array(result, False)
-
-    return GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=result.ravel(),
-        name=name,
-        discrete=False,
-    )
-
-
-def get_dx(self, name="dX", asmasked=False, metric="horizontal"):
-    try:
-        metric_fun = method_factory[metric]
-    except KeyError as err:
-        raise ValueError(f"Unknown metric {metric}") from err
-    self._xtgformat2()
-    nx, ny, nz = self.dimensions
-    result = np.zeros((nx * ny * nz))
-    _cxtgeo.grdcp3d_calc_dx(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv.ravel(),
-        self._zcornsv.ravel(),
-        result,
-        metric_fun,
-    )
-
-    if asmasked:
-        result = np.ma.masked_array(result, self._actnumsv == 0)
-    else:
-        result = np.ma.masked_array(result, False)
-    return GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=result.reshape((nx, ny, nz)),
-        name=name,
-        discrete=False,
-    )
-
-
-def get_dy(self, name="dX", asmasked=False, metric="horizontal"):
-    try:
-        metric_fun = method_factory[metric]
-    except KeyError as err:
-        raise ValueError(f"Unknown metric {metric}") from err
-    self._xtgformat2()
-    nx, ny, nz = self.dimensions
-    result = np.zeros((nx * ny * nz))
-    _cxtgeo.grdcp3d_calc_dy(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv.ravel(),
-        self._zcornsv.ravel(),
-        result,
-        metric_fun,
-    )
-
-    if asmasked:
-        result = np.ma.masked_array(result, self._actnumsv == 0)
-    else:
-        result = np.ma.masked_array(result, False)
-    return GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=result.reshape((nx, ny, nz)),
-        name=name,
-        discrete=False,
-    )
-
-
-def get_bulk_volume(self, name="bulkvol", asmasked=True, precision=2):
-    """Get cell bulk volume as a GridProperty() instance."""
-    self._xtgformat2()
-
-    bulk = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        name=name,
-        discrete=False,
-    )
-
-    bval = np.zeros((bulk.dimensions))
-
-    if precision not in (1, 2, 4):
-        raise ValueError("The precision key has an invalid entry, use 1, 2, or 4")
-
-    _cxtgeo.grdcp3d_cellvol(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        bval,
-        precision,
-        0 if asmasked else 1,
-    )
-
-    if asmasked:
-        bval = np.ma.masked_greater(bval, xtgeo.UNDEF_LIMIT)
-
-    bulk.values = bval
-
-    return bulk
-
-
-def get_ijk(self, names=("IX", "JY", "KZ"), asmasked=True, zerobased=False):
-    """Get I J K as properties."""
-    ashape = (self._ncol, self._nrow, self._nlay)
-
-    ix, jy, kz = np.indices(ashape)
-
-    ix = ix.ravel()
-    jy = jy.ravel()
-    kz = kz.ravel()
-
-    if asmasked:
-        actnum = self.get_actnum()
-
-        ix = ma.masked_where(actnum.values1d == 0, ix)
-        jy = ma.masked_where(actnum.values1d == 0, jy)
-        kz = ma.masked_where(actnum.values1d == 0, kz)
-
-    if not zerobased:
-        ix += 1
-        jy += 1
-        kz += 1
-
-    ix = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=ix.reshape(ashape),
-        name=names[0],
-        discrete=True,
-    )
-    jy = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=jy.reshape(ashape),
-        name=names[1],
-        discrete=True,
-    )
-    kz = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=kz.reshape(ashape),
-        name=names[2],
-        discrete=True,
-    )
-
-    # return the objects
-    return ix, jy, kz
-
-
-def get_ijk_from_points(
-    self,
-    points,
-    activeonly=True,
-    zerobased=False,
-    dataframe=True,
-    includepoints=True,
-    columnnames=("IX", "JY", "KZ"),
-    fmt="int",
-    undef=-1,
-):
-    """Get I J K indices as a list of tuples or a dataframe.
-
-    It is here tried to get fast execution. This requires a preprosessing
-    of the grid to store a onlayer version, and maps with IJ positions
-    """
-    self._xtgformat1()
-    logger.info("Getting IJK indices from Points...")
-
-    actnumoption = 1
-    if not activeonly:
-        actnumoption = 0
-
-    _update_tmpvars(self, force=True)
-
-    arrsize = points.dataframe[points.xname].values.size
-
-    useflip = 1
-    if self.ijk_handedness == "left":
-        useflip = -1
-
-    logger.info("Grid FLIP for C code is %s", useflip)
-
-    logger.info("Running C routine...")
-    _, iarr, jarr, karr = _cxtgeo.grd3d_points_ijk_cells(
-        points.dataframe[points.xname].values,
-        points.dataframe[points.yname].values,
-        points.dataframe[points.zname].values,
-        self._tmp["topd"].ncol,
-        self._tmp["topd"].nrow,
-        self._tmp["topd"].xori,
-        self._tmp["topd"].yori,
-        self._tmp["topd"].xinc,
-        self._tmp["topd"].yinc,
-        self._tmp["topd"].rotation,
-        self._tmp["topd"].yflip,
-        self._tmp["topi_carr"],
-        self._tmp["topj_carr"],
-        self._tmp["basi_carr"],
-        self._tmp["basj_carr"],
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        self._tmp["onegrid"]._zcornsv,
-        actnumoption,
-        arrsize,
-        arrsize,
-        arrsize,
-    )
-    logger.info("Running C routine... DONE")
-
-    if zerobased:
-        # zero based cell indexing
-        iarr -= 1
-        jarr -= 1
-        karr -= 1
-
-    proplist = OrderedDict()
-    if includepoints:
-        proplist["X_UTME"] = points.dataframe[points.xname].values
-        proplist["Y_UTME"] = points.dataframe[points.yname].values
-        proplist["Z_TVDSS"] = points.dataframe[points.zname].values
-
-    proplist[columnnames[0]] = iarr
-    proplist[columnnames[1]] = jarr
-    proplist[columnnames[2]] = karr
-
-    mydataframe = pd.DataFrame.from_dict(proplist)
-    mydataframe.replace(xtgeo.UNDEF_INT, -1, inplace=True)
-
-    if fmt == "float":
-        mydataframe[columnnames[0]] = mydataframe[columnnames[0]].astype("float")
-        mydataframe[columnnames[1]] = mydataframe[columnnames[1]].astype("float")
-        mydataframe[columnnames[2]] = mydataframe[columnnames[2]].astype("float")
-
-    if undef != -1:
-        mydataframe[columnnames[0]].replace(-1, undef, inplace=True)
-        mydataframe[columnnames[1]].replace(-1, undef, inplace=True)
-        mydataframe[columnnames[2]].replace(-1, undef, inplace=True)
-
-    result = mydataframe
-    if not dataframe:
-        result = list(mydataframe.itertuples(index=False, name=None))
-
-    return result
-
-
-def get_xyz(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS"), asmasked=True):
-    """Get X Y Z as properties."""
-    # TODO: May be issues with asmasked vs activeonly here?
-    self._xtgformat2()
-
-    option: int = 0
-    if asmasked:
-        option = 1
-
-    xv, yv, zv = _cxtgeo.grdcp3d_calc_xyz(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv.ravel(),
-        self._zcornsv.ravel(),
-        self._actnumsv.ravel(),
-        option,
-        self.ntotal,  # sizes of xv, yv, zv
-        self.ntotal,
-        self.ntotal,
-    )
-
-    xv = np.ma.masked_greater(xv, xtgeo.UNDEF_LIMIT)
-    yv = np.ma.masked_greater(yv, xtgeo.UNDEF_LIMIT)
-    zv = np.ma.masked_greater(zv, xtgeo.UNDEF_LIMIT)
-
-    xo = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=xv,
-        name=names[0],
-        discrete=False,
-    )
-
-    yo = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=yv,
-        name=names[1],
-        discrete=False,
-    )
-
-    zo = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=zv,
-        name=names[2],
-        discrete=False,
-    )
-
-    return xo, yo, zo
-
-
-def get_xyz_cell_corners(self, ijk=(1, 1, 1), activeonly=True, zerobased=False):
-    """Get X Y Z cell corners for one cell."""
-    self._xtgformat1()
-
-    i, j, k = ijk
-
-    shift = 0
-    if zerobased:
-        shift = 1
-
-    if activeonly:
-        actnum = self.get_actnum()
-        iact = actnum.values3d[i - 1 + shift, j - 1 + shift, k - 1 + shift]
-        if iact == 0:
-            return None
-
-    pcorners = _cxtgeo.new_doublearray(24)
-
-    if self._xtgformat == 1:
-        logger.info("Use xtgformat 1...")
-        _cxtgeo.grd3d_corners(
-            i + shift,
-            j + shift,
-            k + shift,
-            self.ncol,
-            self.nrow,
-            self.nlay,
-            self._coordsv,
-            self._zcornsv,
-            pcorners,
-        )
-    else:
-        logger.info("Use xtgformat 2...")
-        _cxtgeo.grdcp3d_corners(
-            i + shift - 1,
-            j + shift - 1,
-            k + shift - 1,
-            self.ncol,
-            self.nrow,
-            self.nlay,
-            self._coordsv,
-            self._zcornsv,
-            pcorners,
-        )
-
-    cornerlist = []
-    for i in range(24):
-        cornerlist.append(_cxtgeo.doublearray_getitem(pcorners, i))
-
-    clist = tuple(cornerlist)
-    return clist
-
-
-def get_xyz_corners(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS")):
-    """Get X Y Z cell corners for all cells (as 24 GridProperty objects)."""
-    self._xtgformat1()
-
-    ntot = (self._ncol, self._nrow, self._nlay)
-
-    grid_props = []
-
-    for i in range(0, 8):
-        xname = names[0] + str(i)
-        yname = names[1] + str(i)
-        zname = names[2] + str(i)
-        x = GridProperty(
-            ncol=self._ncol,
-            nrow=self._nrow,
-            nlay=self._nlay,
-            values=np.zeros(ntot, dtype=np.float64),
-            name=xname,
-            discrete=False,
-        )
-
-        y = GridProperty(
-            ncol=self._ncol,
-            nrow=self._nrow,
-            nlay=self._nlay,
-            values=np.zeros(ntot, dtype=np.float64),
-            name=yname,
-            discrete=False,
-        )
-
-        z = GridProperty(
-            ncol=self._ncol,
-            nrow=self._nrow,
-            nlay=self._nlay,
-            values=np.zeros(ntot, dtype=np.float64),
-            name=zname,
-            discrete=False,
-        )
-
-        grid_props.append(x)
-        grid_props.append(y)
-        grid_props.append(z)
-
-    ptr_coord = []
-    for i in range(24):
-        some = _cxtgeo.new_doublearray(self.ntotal)
-        ptr_coord.append(some)
-
-    for i, va in enumerate(ptr_coord):
-        logger.debug("SWIG object %s   %s", i, va)
-
-    option = 0
-
-    # note, fool the argument list to unpack ptr_coord with * ...
-    _cxtgeo.grd3d_get_all_corners(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        *(ptr_coord + [option]),
-    )
-
-    for i in range(0, 24, 3):
-        _gridprop_lowlevel.update_values_from_carray(
-            grid_props[i], ptr_coord[i], np.float64, delete=True
-        )
-
-        _gridprop_lowlevel.update_values_from_carray(
-            grid_props[i + 1], ptr_coord[i + 1], np.float64, delete=True
-        )
-
-        _gridprop_lowlevel.update_values_from_carray(
-            grid_props[i + 2], ptr_coord[i + 2], np.float64, delete=True
-        )
-
-    # return the 24 objects (x1, y1, z1, ... x8, y8, z8)
-    return tuple(grid_props)
-
-
-def get_vtk_esg_geometry_data(
-    self,
-) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
-    """Get geometry data consisting of vertices and cell connectivities suitable for
-    use with VTK's vtkExplicitStructuredGrid.
-
-    Returned tuple contains:
-    - numpy array with dimensions in terms of points (not cells)
-    - vertex array, numpy array with vertex coordinates
-    - connectivity array for all the cells, numpy array with integer indices
-    - inactive cell indices, numpy array with integer indices
-    """
-
-    self._xtgformat2()
-
-    # Number of elements to allocate in the vertex and connectivity arrays
-    num_cells = self.ncol * self.nrow * self.nlay
-    n_vertex_arr = 3 * 8 * num_cells
-    n_conn_arr = 8 * num_cells
-
-    # Note first value in return tuple which is the actual number of vertices that
-    # was written into vertex_arr and which we'll use to shrink the array.
-    vertex_count, vertex_arr, conn_arr = _cxtgeo.grdcp3d_get_vtk_esg_geometry_data(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        n_vertex_arr,
-        n_conn_arr,
-    )
-
-    # Need to shrink the vertex array
-    vertex_arr = np.resize(vertex_arr, 3 * vertex_count)
-    vertex_arr = vertex_arr.reshape(-1, 3)
-
-    point_dims = np.asarray((self.ncol, self.nrow, self.nlay)) + 1
-    inact_indices = self.get_actnum_indices(order="F", inverse=True)
-
-    return point_dims, vertex_arr, conn_arr, inact_indices
-
-
-def get_vtk_geometries(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
-    """Return actnum, corners and dims arrays for VTK ExplicitStructuredGrid usage."""
-    self._xtgformat2()
-
-    narr = 8 * self.ncol * self.nrow * self.nlay
-    xarr, yarr, zarr = _cxtgeo.grdcp3d_get_vtk_grid_arrays(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        narr,
-        narr,
-        narr,
-    )
-    corners = np.stack((xarr, yarr, zarr))
-    corners = corners.transpose()
-
-    dims = np.asarray((self.ncol, self.nrow, self.nlay)) + 1
-
-    actindices = self.get_actnum_indices(order="F", inverse=True)
-
-    return dims, corners, actindices
-
-
-def get_cell_volume(self, ijk=(1, 1, 1), activeonly=True, zerobased=False, precision=2):
-    """Get bulk cell volume for one cell."""
-    self._xtgformat1()
-
-    i, j, k = ijk
-
-    if precision not in (1, 2, 4):
-        raise ValueError("The precision key has an invalid entry, use 1, 2, or 4")
-
-    shift = 0
-    if zerobased:
-        shift = 1
-
-    if activeonly:
-        actnum = self.get_actnum()
-        iact = actnum.values3d[i - 1 + shift, j - 1 + shift, k - 1 + shift]
-        if iact == 0:
-            return None
-
-    pcorners = _cxtgeo.new_doublearray(24)
-
-    if self._xtgformat == 1:
-        logger.info("Use xtgformat 1...")
-        _cxtgeo.grd3d_corners(
-            i + shift,
-            j + shift,
-            k + shift,
-            self.ncol,
-            self.nrow,
-            self.nlay,
-            self._coordsv,
-            self._zcornsv,
-            pcorners,
-        )
-    else:
-        logger.info("Use xtgformat 2...")
-        _cxtgeo.grdcp3d_corners(
-            i + shift - 1,
-            j + shift - 1,
-            k + shift - 1,
-            self.ncol,
-            self.nrow,
-            self.nlay,
-            self._coordsv,
-            self._zcornsv,
-            pcorners,
-        )
-
-    cellvol = _cxtgeo.x_hexahedron_volume(pcorners, 24, precision)
-    return cellvol
-
-
-def get_layer_slice(self, layer, top=True, activeonly=True):
-    """Get X Y cell corners (XY per cell; 5 per cell) as array."""
-    self._xtgformat1()
-    ntot = self._ncol * self._nrow * self._nlay
-
-    opt1 = 0
-    if not top:
-        opt1 = 1
-
-    opt2 = 1
-    if not activeonly:
-        opt2 = 0
-
-    icn, lay_array, ic_array = _cxtgeo.grd3d_get_lay_slice(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        layer,
-        opt1,
-        opt2,
-        10 * ntot,
-        ntot,
-    )
-
-    lay_array = lay_array[: 10 * icn]
-    ic_array = ic_array[:icn]
-
-    lay_array = lay_array.reshape((icn, 5, 2))
-
-    return lay_array, ic_array
-
-
-def get_geometrics(self, allcells=False, cellcenter=True, return_dict=False, _ver=1):
-    """Getting cell geometrics."""
-    self._xtgformat1()
-
-    if _ver == 1:
-        res = _get_geometrics_v1(
-            self, allcells=allcells, cellcenter=cellcenter, return_dict=return_dict
-        )
-    else:
-        res = _get_geometrics_v2(
-            self, allcells=allcells, cellcenter=cellcenter, return_dict=return_dict
-        )
-    return res
-
-
-def _get_geometrics_v1(self, allcells=False, cellcenter=True, return_dict=False):
-    ptr_x = []
-    for i in range(13):
-        ptr_x.append(_cxtgeo.new_doublepointer())
-
-    option1 = 1
-    if allcells:
-        option1 = 0
-
-    option2 = 1
-    if not cellcenter:
-        option2 = 0
-
-    quality = _cxtgeo.grd3d_geometrics(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        ptr_x[0],
-        ptr_x[1],
-        ptr_x[2],
-        ptr_x[3],
-        ptr_x[4],
-        ptr_x[5],
-        ptr_x[6],
-        ptr_x[7],
-        ptr_x[8],
-        ptr_x[9],
-        ptr_x[10],
-        ptr_x[11],
-        ptr_x[12],
-        option1,
-        option2,
-    )
-
-    glist = []
-    for i in range(13):
-        glist.append(_cxtgeo.doublepointer_value(ptr_x[i]))
-
-    glist.append(quality)
-
-    logger.info("Cell geometrics done")
-
-    if return_dict:
-        gdict = {}
-        gkeys = [
-            "xori",
-            "yori",
-            "zori",
-            "xmin",
-            "xmax",
-            "ymin",
-            "ymax",
-            "zmin",
-            "zmax",
-            "avg_rotation",
-            "avg_dx",
-            "avg_dy",
-            "avg_dz",
-            "grid_regularity_flag",
-        ]
-
-        for i, key in enumerate(gkeys):
-            gdict[key] = glist[i]
-
-        return gdict
-
-    return tuple(glist)
-
-
-def _get_geometrics_v2(self, allcells=False, cellcenter=True, return_dict=False):
-    # Currently a workaround as there seems to be bugs in v1
-    # Will only work with allcells False and cellcenter True
-
-    glist = []
-    if cellcenter and allcells:
-        xcor, ycor, zcor = self.get_xyz(asmasked=False)
-        glist.append(xcor.values[0, 0, 0])
-        glist.append(ycor.values[0, 0, 0])
-        glist.append(zcor.values[0, 0, 0])
-        glist.append(xcor.values.min())
-        glist.append(xcor.values.max())
-        glist.append(ycor.values.min())
-        glist.append(ycor.values.max())
-        glist.append(zcor.values.min())
-        glist.append(zcor.values.max())
-
-        # rotation (approx) for mid column
-        midcol = int(self.nrow / 2)
-        midlay = int(self.nlay / 2)
-        x0 = xcor.values[0, midcol, midlay]
-        y0 = ycor.values[0, midcol, midlay]
-        x1 = xcor.values[self.ncol - 1, midcol, midlay]
-        y1 = ycor.values[self.ncol - 1, midcol, midlay]
-        glist.append(degrees(atan2(y1 - y0, x1 - x0)))
-
-        dx, dy = self.get_dxdy(asmasked=False)
-        dz = self.get_dz(asmasked=False)
-        glist.append(dx.values.mean())
-        glist.append(dy.values.mean())
-        glist.append(dz.values.mean())
-        glist.append(1)
-
-    if return_dict:
-        gdict = {}
-        gkeys = [
-            "xori",
-            "yori",
-            "zori",
-            "xmin",
-            "xmax",
-            "ymin",
-            "ymax",
-            "zmin",
-            "zmax",
-            "avg_rotation",
-            "avg_dx",
-            "avg_dy",
-            "avg_dz",
-            "grid_regularity_flag",
-        ]
-
-        for i, key in enumerate(gkeys):
-            gdict[key] = glist[i]
-
-        return gdict
-
-    return tuple(glist)
-
-
-def inactivate_by_dz(self, threshold: float, flip: bool = True):
-    """Set cell to inactive if dz does not exceed threshold.
-    Args:
-        threshold (float): The threshold for which the absolute value
-            of dz should exceed.
-        flip (bool): Whether the z-direction should be flipped.
-
-    """
-    self._xtgformat2()
-    self._actnumsv[
-        self.get_dz(asmasked=False, flip=flip).values.reshape(self._actnumsv.shape)
-        < threshold
-    ] = 0
-
-
-def make_zconsistent(self, zsep):
-    """Make consistent in z."""
-    self._xtgformat1()
-
-    if isinstance(zsep, int):
-        zsep = float(zsep)
-
-    if not isinstance(zsep, float):
-        raise ValueError('The "zsep" is not a float or int')
-
-    _cxtgeo.grd3d_make_z_consistent(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._zcornsv,
-        zsep,
-    )
-
-
-def inactivate_inside(self, poly, layer_range=None, inside=True, force_close=False):
-    """Inactivate inside a polygon (or outside)."""
-    self._xtgformat1()
-
-    if not isinstance(poly, Polygons):
-        raise ValueError("Input polygon not a XTGeo Polygons instance")
-
-    if layer_range is not None:
-        k1, k2 = layer_range
-    else:
-        k1 = 1
-        k2 = self.nlay
-
-    method = 0
-    if not inside:
-        method = 1
-
-    iforce = 0
-    if force_close:
-        iforce = 1
-
-    # get dataframe where each polygon is ended by a 999 value
-    dfxyz = poly.get_xyz_dataframe()
-
-    xc = dfxyz["X_UTME"].values.copy()
-    yc = dfxyz["Y_UTMN"].values.copy()
-
-    ier = _cxtgeo.grd3d_inact_outside_pol(
-        xc,
-        yc,
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,  # is modified!
-        k1,
-        k2,
-        iforce,
-        method,
-    )
-
-    if ier == 1:
-        raise RuntimeError("Problems with one or more polygons. " "Not closed?")
-
-
-def collapse_inactive_cells(self):
-    """Collapse inactive cells."""
-    self._xtgformat1()
-
-    _cxtgeo.grd3d_collapse_inact(
-        self.ncol, self.nrow, self.nlay, self._zcornsv, self._actnumsv
-    )
-
-
-def copy(self):
-    """Copy a grid instance (C pointers) and other props.
-
-    Returns:
-        A new instance (attached grid properties will also be unique)
-    """
-    self._xtgformat2()
-
-    filesrc = None
-    if self._filesrc is not None and "(copy)" not in self._filesrc:
-        filesrc = self._filesrc + " (copy)"
-    elif self._filesrc is not None:
-        filesrc = self._filesrc
-
-    return self.__class__(
-        coordsv=self._coordsv.copy(),
-        zcornsv=self._zcornsv.copy(),
-        actnumsv=self._actnumsv.copy(),
-        subgrids=deepcopy(self.subgrids),
-        dualporo=self.dualporo,
-        dualperm=self.dualperm,
-        name=self.name + " (copy)" if self.name else None,
-        roxgrid=self.roxgrid,
-        roxindexer=self.roxindexer,
-        props=self._props.copy() if self._props else None,
-        filesrc=filesrc,
-    )
-
-
-def crop(self, spec, props=None):  # pylint: disable=too-many-locals
-    """Do cropping of geometry (and properties).
-
-    If props is 'all' then all properties assosiated (linked) to then
-    grid are also cropped, and the instances are updated.
-
-    Args:
-        spec (tuple): A nested tuple on the form ((i1, i2), (j1, j2), (k1, k2))
-            where 1 represents start number, and 2 reperesent end. The range
-            is inclusive for both ends, and the number start index is 1 based.
-        props (list or str): None is default, while properties can be listed.
-            If 'all', then all GridProperty objects which are linked to the
-            Grid instance are updated.
-
-    Returns:
-        The instance is updated (cropped)
-    """
-    self._xtgformat1()
-
-    (ic1, ic2), (jc1, jc2), (kc1, kc2) = spec
-
-    if (
-        ic1 < 1
-        or ic2 > self.ncol
-        or jc1 < 1
-        or jc2 > self.nrow
-        or kc1 < 1
-        or kc2 > self.nlay
-    ):
-        raise ValueError("Boundary for tuples not matching grid" "NCOL, NROW, NLAY")
-
-    oldnlay = self._nlay
-
-    # compute size of new cropped grid
-    nncol = ic2 - ic1 + 1
-    nnrow = jc2 - jc1 + 1
-    nnlay = kc2 - kc1 + 1
-
-    ntot = nncol * nnrow * nnlay
-    ncoord = (nncol + 1) * (nnrow + 1) * 2 * 3
-    nzcorn = nncol * nnrow * (nnlay + 1) * 4
-
-    new_num_act = _cxtgeo.new_intpointer()
-    new_coordsv = np.zeros(ncoord, dtype=np.float64)
-    new_zcornsv = np.zeros(nzcorn, dtype=np.float64)
-    new_actnumsv = np.zeros(ntot, dtype=np.int32)
-
-    _cxtgeo.grd3d_crop_geometry(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        new_coordsv,
-        new_zcornsv,
-        new_actnumsv,
-        ic1,
-        ic2,
-        jc1,
-        jc2,
-        kc1,
-        kc2,
-        new_num_act,
-        0,
-    )
-
-    self._coordsv = new_coordsv
-    self._zcornsv = new_zcornsv
-    self._actnumsv = new_actnumsv
-
-    self._ncol = nncol
-    self._nrow = nnrow
-    self._nlay = nnlay
-
-    if isinstance(self.subgrids, dict):
-        newsub = OrderedDict()
-        # easier to work with numpies than lists
-        newarr = np.array(range(1, oldnlay + 1))
-        newarr[newarr < kc1] = 0
-        newarr[newarr > kc2] = 0
-        newaxx = newarr.copy() - kc1 + 1
-        for sub, arr in self.subgrids.items():
-            arrx = np.array(arr)
-            arrxmap = newaxx[arrx[0] - 1 : arrx[-1]]
-            arrxmap = arrxmap[arrxmap > 0]
-            if arrxmap.size > 0:
-                newsub[sub] = arrxmap.astype(np.int32).tolist()
-
-        self.subgrids = newsub
-
-    # crop properties
-    if props is not None:
-        if props == "all":
-            props = self.props
-
-        for prop in props:
-            logger.info("Crop %s", prop.name)
-            prop.crop(spec)
-
-
-def reduce_to_one_layer(self):
-    """Reduce the grid to one single layer.
-
-    This can be useful for algorithms that need to test if a point is within
-    the full grid.
-
-    Example::
-
-        >>> import xtgeo
-        >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-        >>> grid.nlay
-        14
-        >>> grid.reduce_to_one_layer()
-        >>> grid.nlay
-        1
-
-    """
-    # need new pointers in C (not for coord)
-    # Note this could probably be done with pure numpy operations
-    self._xtgformat1()
-
-    ptr_new_num_act = _cxtgeo.new_intpointer()
-
-    nnum = (1 + 1) * 4
-
-    new_zcorn = np.zeros(self.ncol * self.nrow * nnum, dtype=np.float64)
-    new_actnum = np.zeros(self.ncol * self.nrow * 1, dtype=np.int32)
-
-    _cxtgeo.grd3d_reduce_onelayer(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._zcornsv,
-        new_zcorn,
-        self._actnumsv,
-        new_actnum,
-        ptr_new_num_act,
-        0,
-    )
-
-    self._nlay = 1
-    self._zcornsv = new_zcorn
-    self._actnumsv = new_actnum
-    self._props = None
-    self._subgrids = None
-
-
-def translate_coordinates(self, translate=(0, 0, 0), flip=(1, 1, 1)):
-    """Translate grid coordinates."""
-    self._xtgformat1()
-
-    tx, ty, tz = translate
-    fx, fy, fz = flip
-
-    ier = _cxtgeo.grd3d_translate(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        fx,
-        fy,
-        fz,
-        tx,
-        ty,
-        tz,
-        self._coordsv,
-        self._zcornsv,
-    )
-    if ier != 0:
-        raise RuntimeError(f"Something went wrong in translate, code: {ier}")
-
-    logger.info("Translation of coords done")
-
-
-def reverse_row_axis(self, ijk_handedness=None):
-    """Reverse rows (aka flip) for geometry and assosiated properties."""
-    self._xtgformat1()
-
-    if ijk_handedness == self.ijk_handedness:
-        return
-
-    ier = _cxtgeo.grd3d_reverse_jrows(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-    )
-
-    if ier != 0:
-        raise RuntimeError(f"Something went wrong in jswapping, code: {ier}")
-
-    if self._props is None:
-        return
-
-    # do it for properties
-    if self._props.props:
-        for prp in self._props.props:
-            prp.values = prp.values[:, ::-1, :]
-
-    logger.info("Reversing of rows done")
-
-
-def get_adjacent_cells(self, prop, val1, val2, activeonly=True):
-    """Get adjacents cells."""
-    self._xtgformat1()
-
-    if not isinstance(prop, GridProperty):
-        raise ValueError("The argument prop is not a xtgeo.GridPropery")
-
-    if prop.isdiscrete is False:
-        raise ValueError("The argument prop is not a discrete property")
-
-    result = GridProperty(
-        ncol=self._ncol,
-        nrow=self._nrow,
-        nlay=self._nlay,
-        values=np.zeros(self.ntotal, dtype=np.int32),
-        name="ADJ_CELLS",
-        discrete=True,
-    )
-
-    p_prop1 = _gridprop_lowlevel.update_carray(prop)
-    p_prop2 = _cxtgeo.new_intarray(self.ntotal)
-
-    iflag1 = 1
-    if activeonly:
-        iflag1 = 0
-
-    iflag2 = 1
-
-    _cxtgeo.grd3d_adj_cells(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        p_prop1,
-        self.ntotal,
-        val1,
-        val2,
-        p_prop2,
-        self.ntotal,
-        iflag1,
-        iflag2,
-    )
-
-    _gridprop_lowlevel.update_values_from_carray(result, p_prop2, np.int32, delete=True)
-    # return the property object
-    return result
-
-
-def estimate_design(self, nsubname):
-    """Estimate (guess) (sub)grid design by examing DZ in median thickness column."""
-    actv = self.get_actnum().values
-
-    dzv = self.get_dz(asmasked=False).values
-
-    # treat inactive thicknesses as zero
-    dzv[actv == 0] = 0.0
-
-    if nsubname is None:
-        vrange = np.array(range(self.nlay))
-    else:
-        vrange = np.array(list(self.subgrids[nsubname])) - 1
-
-    # find the dz for the actual subzone
-    dzv = dzv[:, :, vrange]
-
-    # find cumulative thickness as a 2D array
-    dzcum = np.sum(dzv, axis=2, keepdims=False)
-
-    # find the average thickness for nonzero thicknesses
-    dzcum2 = dzcum.copy()
-    dzcum2[dzcum == 0.0] = np.nan
-    dzavg = np.nanmean(dzcum2) / dzv.shape[2]
-
-    # find the I J indices for the median value
-    if versionparse(np.__version__) < versionparse("1.22"):
-        argmed = np.stack(
-            np.nonzero(dzcum == np.percentile(dzcum, 50, interpolation="nearest")),
-            axis=1,
-        )
-    else:
-        argmed = np.stack(
-            np.nonzero(dzcum == np.percentile(dzcum, 50, method="nearest")), axis=1
-        )
-
-    im, jm = argmed[0]
-    # find the dz stack of the median
-    dzmedian = dzv[im, jm, :]
-    logger.info("DZ median column is %s", dzmedian)
-
-    # to compare thicknesses with (divide on 2 to assure)
-    target = dzcum[im, jm] / (dzmedian.shape[0] * 2)
-    eps = target / 100.0
-
-    logger.info("Target and EPS values are %s, %s", target, eps)
-
-    status = "X"  # unknown or cannot determine
-
-    if dzmedian[0] > target and dzmedian[-1] <= eps:
-        status = "T"
-        dzavg = dzmedian[0]
-    elif dzmedian[0] < eps and dzmedian[-1] > target:
-        status = "B"
-        dzavg = dzmedian[-1]
-    elif dzmedian[0] > target and dzmedian[-1] > target:
-        ratio = dzmedian[0] / dzmedian[-1]
-        if 0.5 < ratio < 1.5:
-            status = "P"
-    elif dzmedian[0] < eps and dzmedian[-1] < eps:
-        status = "M"
-        middleindex = int(dzmedian.shape[0] / 2)
-        dzavg = dzmedian[middleindex]
-
-    return {"design": status, "dzsimbox": dzavg}
-
-
-def estimate_flip(self):
-    """Estimate if grid is left or right handed."""
-    corners = self.get_xyz_cell_corners(activeonly=False)  # for cell 1, 1, 1
-
-    v1 = (corners[3] - corners[0], corners[4] - corners[1], 0.0)
-    v2 = (corners[6] - corners[0], corners[7] - corners[1], 0.0)
-
-    flipvalue = find_flip(v1, v2)
-
-    return flipvalue
-
-
-def _convert_xtgformat2to1(self):
-    """Convert arrays from new structure xtgformat=2 to legacy xtgformat=1."""
-    if self._xtgformat == 1:
-        logger.info("No conversion, format is already xtgformat == 1 or unset")
-        return
-
-    logger.info("Convert grid from new xtgformat to legacy format...")
-
-    newcoordsv = np.zeros(((self._ncol + 1) * (self._nrow + 1) * 6), dtype=np.float64)
-    newzcornsv = np.zeros(
-        (self._ncol * self._nrow * (self._nlay + 1) * 4), dtype=np.float64
-    )
-    newactnumsv = np.zeros((self._ncol * self._nrow * self._nlay), dtype=np.int32)
-
-    _cxtgeo.grd3cp3d_xtgformat2to1_geom(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        newcoordsv,
-        self._coordsv,
-        newzcornsv,
-        self._zcornsv,
-        newactnumsv,
-        self._actnumsv,
-    )
-
-    self._coordsv = newcoordsv
-    self._zcornsv = newzcornsv
-    self._actnumsv = newactnumsv
-    self._xtgformat = 1
-
-    logger.info("Convert grid from new xtgformat to legacy format... done")
-
-
-def _convert_xtgformat1to2(self):
-    """Convert arrays from old structure xtgformat=1 to new xtgformat=2."""
-    if self._xtgformat == 2 or self._coordsv is None:
-        logger.info("No conversion, format is already xtgformat == 2 or unset")
-        return
-
-    logger.info("Convert grid from legacy xtgformat to new format...")
-
-    newcoordsv = np.zeros((self._ncol + 1, self._nrow + 1, 6), dtype=np.float64)
-    newzcornsv = np.zeros(
-        (self._ncol + 1, self._nrow + 1, self._nlay + 1, 4), dtype=np.float32
-    )
-    newactnumsv = np.zeros((self._ncol, self._nrow, self._nlay), dtype=np.int32)
-
-    _cxtgeo.grd3cp3d_xtgformat1to2_geom(
-        self._ncol,
-        self._nrow,
-        self._nlay,
-        self._coordsv,
-        newcoordsv,
-        self._zcornsv,
-        newzcornsv,
-        self._actnumsv,
-        newactnumsv,
-    )
-
-    self._coordsv = newcoordsv
-    self._zcornsv = newzcornsv
-    self._actnumsv = newactnumsv
-    self._xtgformat = 2
-
-    logger.info("Convert grid from new xtgformat to legacy format... done")
-
-
-def get_gridquality_properties(self):
-    """Get the grid quality properties."""
-    self._xtgformat2()
-
-    qcnames = {
-        0: "minangle_topbase",
-        1: "maxangle_topbase",
-        2: "minangle_topbase_proj",
-        3: "maxangle_topbase_proj",
-        4: "minangle_sides",
-        5: "maxangle_sides",
-        6: "collapsed",
-        7: "faulted",
-        8: "negative_thickness",
-        9: "concave_proj",
-    }
-
-    # some of the properties shall be discrete:
-    qcdiscrete = [6, 7, 8, 9]
-
-    fresults = np.ones(
-        (len(qcnames), self.ncol * self.nrow * self.nlay), dtype=np.float32
-    )
-
-    _cxtgeo.grdcp3d_quality_indicators(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        fresults,
-    )
-
-    grdprops = xtgeo.GridProperties()
-
-    for num, name in qcnames.items():
-        prop = xtgeo.GridProperty(self, name=name)
-        dtype = np.float32
-        if num in qcdiscrete:
-            dtype = np.int32
-            prop.isdiscrete = True
-            prop.codes = {0: "None", 1: name}
-        prop.values = fresults[num, :].astype(dtype)
-        grdprops.append_props([prop])
-
-    return grdprops
+"""Private module, Grid ETC 1 methods, info/modify/report."""
+
+from collections import OrderedDict
+from copy import deepcopy
+from packaging.version import parse as versionparse
+from math import atan2, degrees
+from typing import Tuple
+
+import numpy as np
+import numpy.ma as ma
+import pandas as pd
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.common.calc import find_flip
+from xtgeo.xyz.polygons import Polygons
+
+from . import _gridprop_lowlevel
+from ._grid3d_fence import _update_tmpvars
+from .grid_property import GridProperty
+
+xtg = XTGeoDialog()
+
+
+logger = xtg.functionlogger(__name__)
+
+
+# Note that "self" is the grid instance
+
+
+def create_box(dimension, origin, oricenter, increment, rotation, flip):
+    """Create a shoebox grid from cubi'sh spec, xtgformat=2."""
+    ncol, nrow, nlay = dimension
+    nncol = ncol + 1
+    nnrow = nrow + 1
+    nnlay = nlay + 1
+
+    coordsv = np.zeros((nncol, nnrow, 6), dtype=np.float64)
+    zcornsv = np.zeros((nncol, nnrow, nnlay, 4), dtype=np.float32)
+    actnumsv = np.zeros((ncol, nrow, nlay), dtype=np.int32)
+
+    option = 0
+    if oricenter:
+        option = 1
+
+    _cxtgeo.grdcp3d_from_cube(
+        ncol,
+        nrow,
+        nlay,
+        coordsv,
+        zcornsv,
+        actnumsv,
+        origin[0],
+        origin[1],
+        origin[2],
+        increment[0],
+        increment[1],
+        increment[2],
+        rotation,
+        flip,
+        option,
+    )
+
+    return {
+        "coordsv": coordsv,
+        "zcornsv": zcornsv,
+        "actnumsv": actnumsv,
+    }
+
+
+method_factory = {
+    "euclid": _cxtgeo.euclid_length,
+    "horizontal": _cxtgeo.horizontal_length,
+    "east west vertical": _cxtgeo.east_west_vertical_length,
+    "north south vertical": _cxtgeo.north_south_vertical_length,
+    "x projection": _cxtgeo.x_projection,
+    "y projection": _cxtgeo.y_projection,
+    "z projection": _cxtgeo.z_projection,
+}
+
+
+def get_dz(
+    self,
+    name: str = "dZ",
+    flip: bool = True,
+    asmasked: bool = True,
+    metric="z projection",
+) -> GridProperty:
+    """Get average cell height (dz) as property.
+
+    Args:
+        flip (bool): whether to flip the z direction, ie. increasing z is
+            increasing depth (defaults to True)
+        asmasked (bool): Whether to mask property by whether
+        name (str): Name of resulting grid property, defaults to "dZ".
+    """
+    self._xtgformat2()
+    nx, ny, nz = self.dimensions
+    result = np.zeros((nx * ny * nz))
+    try:
+        metric_fun = method_factory[metric]
+    except KeyError as err:
+        raise ValueError(f"Unknown metric {metric}") from err
+    _cxtgeo.grdcp3d_calc_dz(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv.ravel(),
+        self._zcornsv.ravel(),
+        result,
+        metric_fun,
+    )
+
+    if not flip:
+        result *= -1
+
+    if asmasked:
+        result = np.ma.masked_array(result, self._actnumsv == 0)
+    else:
+        result = np.ma.masked_array(result, False)
+
+    return GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=result.ravel(),
+        name=name,
+        discrete=False,
+    )
+
+
+def get_dx(self, name="dX", asmasked=False, metric="horizontal"):
+    try:
+        metric_fun = method_factory[metric]
+    except KeyError as err:
+        raise ValueError(f"Unknown metric {metric}") from err
+    self._xtgformat2()
+    nx, ny, nz = self.dimensions
+    result = np.zeros((nx * ny * nz))
+    _cxtgeo.grdcp3d_calc_dx(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv.ravel(),
+        self._zcornsv.ravel(),
+        result,
+        metric_fun,
+    )
+
+    if asmasked:
+        result = np.ma.masked_array(result, self._actnumsv == 0)
+    else:
+        result = np.ma.masked_array(result, False)
+    return GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=result.reshape((nx, ny, nz)),
+        name=name,
+        discrete=False,
+    )
+
+
+def get_dy(self, name="dX", asmasked=False, metric="horizontal"):
+    try:
+        metric_fun = method_factory[metric]
+    except KeyError as err:
+        raise ValueError(f"Unknown metric {metric}") from err
+    self._xtgformat2()
+    nx, ny, nz = self.dimensions
+    result = np.zeros((nx * ny * nz))
+    _cxtgeo.grdcp3d_calc_dy(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv.ravel(),
+        self._zcornsv.ravel(),
+        result,
+        metric_fun,
+    )
+
+    if asmasked:
+        result = np.ma.masked_array(result, self._actnumsv == 0)
+    else:
+        result = np.ma.masked_array(result, False)
+    return GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=result.reshape((nx, ny, nz)),
+        name=name,
+        discrete=False,
+    )
+
+
+def get_bulk_volume(self, name="bulkvol", asmasked=True, precision=2):
+    """Get cell bulk volume as a GridProperty() instance."""
+    self._xtgformat2()
+
+    bulk = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        name=name,
+        discrete=False,
+    )
+
+    bval = np.zeros((bulk.dimensions))
+
+    if precision not in (1, 2, 4):
+        raise ValueError("The precision key has an invalid entry, use 1, 2, or 4")
+
+    _cxtgeo.grdcp3d_cellvol(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        bval,
+        precision,
+        0 if asmasked else 1,
+    )
+
+    if asmasked:
+        bval = np.ma.masked_greater(bval, xtgeo.UNDEF_LIMIT)
+
+    bulk.values = bval
+
+    return bulk
+
+
+def get_ijk(self, names=("IX", "JY", "KZ"), asmasked=True, zerobased=False):
+    """Get I J K as properties."""
+    ashape = (self._ncol, self._nrow, self._nlay)
+
+    ix, jy, kz = np.indices(ashape)
+
+    ix = ix.ravel()
+    jy = jy.ravel()
+    kz = kz.ravel()
+
+    if asmasked:
+        actnum = self.get_actnum()
+
+        ix = ma.masked_where(actnum.values1d == 0, ix)
+        jy = ma.masked_where(actnum.values1d == 0, jy)
+        kz = ma.masked_where(actnum.values1d == 0, kz)
+
+    if not zerobased:
+        ix += 1
+        jy += 1
+        kz += 1
+
+    ix = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=ix.reshape(ashape),
+        name=names[0],
+        discrete=True,
+    )
+    jy = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=jy.reshape(ashape),
+        name=names[1],
+        discrete=True,
+    )
+    kz = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=kz.reshape(ashape),
+        name=names[2],
+        discrete=True,
+    )
+
+    # return the objects
+    return ix, jy, kz
+
+
+def get_ijk_from_points(
+    self,
+    points,
+    activeonly=True,
+    zerobased=False,
+    dataframe=True,
+    includepoints=True,
+    columnnames=("IX", "JY", "KZ"),
+    fmt="int",
+    undef=-1,
+):
+    """Get I J K indices as a list of tuples or a dataframe.
+
+    It is here tried to get fast execution. This requires a preprosessing
+    of the grid to store a onlayer version, and maps with IJ positions
+    """
+    self._xtgformat1()
+    logger.info("Getting IJK indices from Points...")
+
+    actnumoption = 1
+    if not activeonly:
+        actnumoption = 0
+
+    _update_tmpvars(self, force=True)
+
+    arrsize = points.dataframe[points.xname].values.size
+
+    useflip = 1
+    if self.ijk_handedness == "left":
+        useflip = -1
+
+    logger.info("Grid FLIP for C code is %s", useflip)
+
+    logger.info("Running C routine...")
+    _, iarr, jarr, karr = _cxtgeo.grd3d_points_ijk_cells(
+        points.dataframe[points.xname].values,
+        points.dataframe[points.yname].values,
+        points.dataframe[points.zname].values,
+        self._tmp["topd"].ncol,
+        self._tmp["topd"].nrow,
+        self._tmp["topd"].xori,
+        self._tmp["topd"].yori,
+        self._tmp["topd"].xinc,
+        self._tmp["topd"].yinc,
+        self._tmp["topd"].rotation,
+        self._tmp["topd"].yflip,
+        self._tmp["topi_carr"],
+        self._tmp["topj_carr"],
+        self._tmp["basi_carr"],
+        self._tmp["basj_carr"],
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        self._tmp["onegrid"]._zcornsv,
+        actnumoption,
+        arrsize,
+        arrsize,
+        arrsize,
+    )
+    logger.info("Running C routine... DONE")
+
+    if zerobased:
+        # zero based cell indexing
+        iarr -= 1
+        jarr -= 1
+        karr -= 1
+
+    proplist = OrderedDict()
+    if includepoints:
+        proplist["X_UTME"] = points.dataframe[points.xname].values
+        proplist["Y_UTME"] = points.dataframe[points.yname].values
+        proplist["Z_TVDSS"] = points.dataframe[points.zname].values
+
+    proplist[columnnames[0]] = iarr
+    proplist[columnnames[1]] = jarr
+    proplist[columnnames[2]] = karr
+
+    mydataframe = pd.DataFrame.from_dict(proplist)
+    mydataframe.replace(xtgeo.UNDEF_INT, -1, inplace=True)
+
+    if fmt == "float":
+        mydataframe[columnnames[0]] = mydataframe[columnnames[0]].astype("float")
+        mydataframe[columnnames[1]] = mydataframe[columnnames[1]].astype("float")
+        mydataframe[columnnames[2]] = mydataframe[columnnames[2]].astype("float")
+
+    if undef != -1:
+        mydataframe[columnnames[0]].replace(-1, undef, inplace=True)
+        mydataframe[columnnames[1]].replace(-1, undef, inplace=True)
+        mydataframe[columnnames[2]].replace(-1, undef, inplace=True)
+
+    result = mydataframe
+    if not dataframe:
+        result = list(mydataframe.itertuples(index=False, name=None))
+
+    return result
+
+
+def get_xyz(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS"), asmasked=True):
+    """Get X Y Z as properties."""
+    # TODO: May be issues with asmasked vs activeonly here?
+    self._xtgformat2()
+
+    option: int = 0
+    if asmasked:
+        option = 1
+
+    xv, yv, zv = _cxtgeo.grdcp3d_calc_xyz(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv.ravel(),
+        self._zcornsv.ravel(),
+        self._actnumsv.ravel(),
+        option,
+        self.ntotal,  # sizes of xv, yv, zv
+        self.ntotal,
+        self.ntotal,
+    )
+
+    xv = np.ma.masked_greater(xv, xtgeo.UNDEF_LIMIT)
+    yv = np.ma.masked_greater(yv, xtgeo.UNDEF_LIMIT)
+    zv = np.ma.masked_greater(zv, xtgeo.UNDEF_LIMIT)
+
+    xo = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=xv,
+        name=names[0],
+        discrete=False,
+    )
+
+    yo = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=yv,
+        name=names[1],
+        discrete=False,
+    )
+
+    zo = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=zv,
+        name=names[2],
+        discrete=False,
+    )
+
+    return xo, yo, zo
+
+
+def get_xyz_cell_corners(self, ijk=(1, 1, 1), activeonly=True, zerobased=False):
+    """Get X Y Z cell corners for one cell."""
+    self._xtgformat1()
+
+    i, j, k = ijk
+
+    shift = 0
+    if zerobased:
+        shift = 1
+
+    if activeonly:
+        actnum = self.get_actnum()
+        iact = actnum.values3d[i - 1 + shift, j - 1 + shift, k - 1 + shift]
+        if iact == 0:
+            return None
+
+    pcorners = _cxtgeo.new_doublearray(24)
+
+    if self._xtgformat == 1:
+        logger.info("Use xtgformat 1...")
+        _cxtgeo.grd3d_corners(
+            i + shift,
+            j + shift,
+            k + shift,
+            self.ncol,
+            self.nrow,
+            self.nlay,
+            self._coordsv,
+            self._zcornsv,
+            pcorners,
+        )
+    else:
+        logger.info("Use xtgformat 2...")
+        _cxtgeo.grdcp3d_corners(
+            i + shift - 1,
+            j + shift - 1,
+            k + shift - 1,
+            self.ncol,
+            self.nrow,
+            self.nlay,
+            self._coordsv,
+            self._zcornsv,
+            pcorners,
+        )
+
+    cornerlist = []
+    for i in range(24):
+        cornerlist.append(_cxtgeo.doublearray_getitem(pcorners, i))
+
+    clist = tuple(cornerlist)
+    return clist
+
+
+def get_xyz_corners(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS")):
+    """Get X Y Z cell corners for all cells (as 24 GridProperty objects)."""
+    self._xtgformat1()
+
+    ntot = (self._ncol, self._nrow, self._nlay)
+
+    grid_props = []
+
+    for i in range(0, 8):
+        xname = names[0] + str(i)
+        yname = names[1] + str(i)
+        zname = names[2] + str(i)
+        x = GridProperty(
+            ncol=self._ncol,
+            nrow=self._nrow,
+            nlay=self._nlay,
+            values=np.zeros(ntot, dtype=np.float64),
+            name=xname,
+            discrete=False,
+        )
+
+        y = GridProperty(
+            ncol=self._ncol,
+            nrow=self._nrow,
+            nlay=self._nlay,
+            values=np.zeros(ntot, dtype=np.float64),
+            name=yname,
+            discrete=False,
+        )
+
+        z = GridProperty(
+            ncol=self._ncol,
+            nrow=self._nrow,
+            nlay=self._nlay,
+            values=np.zeros(ntot, dtype=np.float64),
+            name=zname,
+            discrete=False,
+        )
+
+        grid_props.append(x)
+        grid_props.append(y)
+        grid_props.append(z)
+
+    ptr_coord = []
+    for i in range(24):
+        some = _cxtgeo.new_doublearray(self.ntotal)
+        ptr_coord.append(some)
+
+    for i, va in enumerate(ptr_coord):
+        logger.debug("SWIG object %s   %s", i, va)
+
+    option = 0
+
+    # note, fool the argument list to unpack ptr_coord with * ...
+    _cxtgeo.grd3d_get_all_corners(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        *(ptr_coord + [option]),
+    )
+
+    for i in range(0, 24, 3):
+        _gridprop_lowlevel.update_values_from_carray(
+            grid_props[i], ptr_coord[i], np.float64, delete=True
+        )
+
+        _gridprop_lowlevel.update_values_from_carray(
+            grid_props[i + 1], ptr_coord[i + 1], np.float64, delete=True
+        )
+
+        _gridprop_lowlevel.update_values_from_carray(
+            grid_props[i + 2], ptr_coord[i + 2], np.float64, delete=True
+        )
+
+    # return the 24 objects (x1, y1, z1, ... x8, y8, z8)
+    return tuple(grid_props)
+
+
+def get_vtk_esg_geometry_data(
+    self,
+) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
+    """Get geometry data consisting of vertices and cell connectivities suitable for
+    use with VTK's vtkExplicitStructuredGrid.
+
+    Returned tuple contains:
+    - numpy array with dimensions in terms of points (not cells)
+    - vertex array, numpy array with vertex coordinates
+    - connectivity array for all the cells, numpy array with integer indices
+    - inactive cell indices, numpy array with integer indices
+    """
+
+    self._xtgformat2()
+
+    # Number of elements to allocate in the vertex and connectivity arrays
+    num_cells = self.ncol * self.nrow * self.nlay
+    n_vertex_arr = 3 * 8 * num_cells
+    n_conn_arr = 8 * num_cells
+
+    # Note first value in return tuple which is the actual number of vertices that
+    # was written into vertex_arr and which we'll use to shrink the array.
+    vertex_count, vertex_arr, conn_arr = _cxtgeo.grdcp3d_get_vtk_esg_geometry_data(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        n_vertex_arr,
+        n_conn_arr,
+    )
+
+    # Need to shrink the vertex array
+    vertex_arr = np.resize(vertex_arr, 3 * vertex_count)
+    vertex_arr = vertex_arr.reshape(-1, 3)
+
+    point_dims = np.asarray((self.ncol, self.nrow, self.nlay)) + 1
+    inact_indices = self.get_actnum_indices(order="F", inverse=True)
+
+    return point_dims, vertex_arr, conn_arr, inact_indices
+
+
+def get_vtk_geometries(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """Return actnum, corners and dims arrays for VTK ExplicitStructuredGrid usage."""
+    self._xtgformat2()
+
+    narr = 8 * self.ncol * self.nrow * self.nlay
+    xarr, yarr, zarr = _cxtgeo.grdcp3d_get_vtk_grid_arrays(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        narr,
+        narr,
+        narr,
+    )
+    corners = np.stack((xarr, yarr, zarr))
+    corners = corners.transpose()
+
+    dims = np.asarray((self.ncol, self.nrow, self.nlay)) + 1
+
+    actindices = self.get_actnum_indices(order="F", inverse=True)
+
+    return dims, corners, actindices
+
+
+def get_cell_volume(self, ijk=(1, 1, 1), activeonly=True, zerobased=False, precision=2):
+    """Get bulk cell volume for one cell."""
+    self._xtgformat1()
+
+    i, j, k = ijk
+
+    if precision not in (1, 2, 4):
+        raise ValueError("The precision key has an invalid entry, use 1, 2, or 4")
+
+    shift = 0
+    if zerobased:
+        shift = 1
+
+    if activeonly:
+        actnum = self.get_actnum()
+        iact = actnum.values3d[i - 1 + shift, j - 1 + shift, k - 1 + shift]
+        if iact == 0:
+            return None
+
+    pcorners = _cxtgeo.new_doublearray(24)
+
+    if self._xtgformat == 1:
+        logger.info("Use xtgformat 1...")
+        _cxtgeo.grd3d_corners(
+            i + shift,
+            j + shift,
+            k + shift,
+            self.ncol,
+            self.nrow,
+            self.nlay,
+            self._coordsv,
+            self._zcornsv,
+            pcorners,
+        )
+    else:
+        logger.info("Use xtgformat 2...")
+        _cxtgeo.grdcp3d_corners(
+            i + shift - 1,
+            j + shift - 1,
+            k + shift - 1,
+            self.ncol,
+            self.nrow,
+            self.nlay,
+            self._coordsv,
+            self._zcornsv,
+            pcorners,
+        )
+
+    cellvol = _cxtgeo.x_hexahedron_volume(pcorners, 24, precision)
+    return cellvol
+
+
+def get_layer_slice(self, layer, top=True, activeonly=True):
+    """Get X Y cell corners (XY per cell; 5 per cell) as array."""
+    self._xtgformat1()
+    ntot = self._ncol * self._nrow * self._nlay
+
+    opt1 = 0
+    if not top:
+        opt1 = 1
+
+    opt2 = 1
+    if not activeonly:
+        opt2 = 0
+
+    icn, lay_array, ic_array = _cxtgeo.grd3d_get_lay_slice(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        layer,
+        opt1,
+        opt2,
+        10 * ntot,
+        ntot,
+    )
+
+    lay_array = lay_array[: 10 * icn]
+    ic_array = ic_array[:icn]
+
+    lay_array = lay_array.reshape((icn, 5, 2))
+
+    return lay_array, ic_array
+
+
+def get_geometrics(self, allcells=False, cellcenter=True, return_dict=False, _ver=1):
+    """Getting cell geometrics."""
+    self._xtgformat1()
+
+    if _ver == 1:
+        res = _get_geometrics_v1(
+            self, allcells=allcells, cellcenter=cellcenter, return_dict=return_dict
+        )
+    else:
+        res = _get_geometrics_v2(
+            self, allcells=allcells, cellcenter=cellcenter, return_dict=return_dict
+        )
+    return res
+
+
+def _get_geometrics_v1(self, allcells=False, cellcenter=True, return_dict=False):
+    ptr_x = []
+    for i in range(13):
+        ptr_x.append(_cxtgeo.new_doublepointer())
+
+    option1 = 1
+    if allcells:
+        option1 = 0
+
+    option2 = 1
+    if not cellcenter:
+        option2 = 0
+
+    quality = _cxtgeo.grd3d_geometrics(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        ptr_x[0],
+        ptr_x[1],
+        ptr_x[2],
+        ptr_x[3],
+        ptr_x[4],
+        ptr_x[5],
+        ptr_x[6],
+        ptr_x[7],
+        ptr_x[8],
+        ptr_x[9],
+        ptr_x[10],
+        ptr_x[11],
+        ptr_x[12],
+        option1,
+        option2,
+    )
+
+    glist = []
+    for i in range(13):
+        glist.append(_cxtgeo.doublepointer_value(ptr_x[i]))
+
+    glist.append(quality)
+
+    logger.info("Cell geometrics done")
+
+    if return_dict:
+        gdict = {}
+        gkeys = [
+            "xori",
+            "yori",
+            "zori",
+            "xmin",
+            "xmax",
+            "ymin",
+            "ymax",
+            "zmin",
+            "zmax",
+            "avg_rotation",
+            "avg_dx",
+            "avg_dy",
+            "avg_dz",
+            "grid_regularity_flag",
+        ]
+
+        for i, key in enumerate(gkeys):
+            gdict[key] = glist[i]
+
+        return gdict
+
+    return tuple(glist)
+
+
+def _get_geometrics_v2(self, allcells=False, cellcenter=True, return_dict=False):
+    # Currently a workaround as there seems to be bugs in v1
+    # Will only work with allcells False and cellcenter True
+
+    glist = []
+    if cellcenter and allcells:
+        xcor, ycor, zcor = self.get_xyz(asmasked=False)
+        glist.append(xcor.values[0, 0, 0])
+        glist.append(ycor.values[0, 0, 0])
+        glist.append(zcor.values[0, 0, 0])
+        glist.append(xcor.values.min())
+        glist.append(xcor.values.max())
+        glist.append(ycor.values.min())
+        glist.append(ycor.values.max())
+        glist.append(zcor.values.min())
+        glist.append(zcor.values.max())
+
+        # rotation (approx) for mid column
+        midcol = int(self.nrow / 2)
+        midlay = int(self.nlay / 2)
+        x0 = xcor.values[0, midcol, midlay]
+        y0 = ycor.values[0, midcol, midlay]
+        x1 = xcor.values[self.ncol - 1, midcol, midlay]
+        y1 = ycor.values[self.ncol - 1, midcol, midlay]
+        glist.append(degrees(atan2(y1 - y0, x1 - x0)))
+
+        dx, dy = self.get_dxdy(asmasked=False)
+        dz = self.get_dz(asmasked=False)
+        glist.append(dx.values.mean())
+        glist.append(dy.values.mean())
+        glist.append(dz.values.mean())
+        glist.append(1)
+
+    if return_dict:
+        gdict = {}
+        gkeys = [
+            "xori",
+            "yori",
+            "zori",
+            "xmin",
+            "xmax",
+            "ymin",
+            "ymax",
+            "zmin",
+            "zmax",
+            "avg_rotation",
+            "avg_dx",
+            "avg_dy",
+            "avg_dz",
+            "grid_regularity_flag",
+        ]
+
+        for i, key in enumerate(gkeys):
+            gdict[key] = glist[i]
+
+        return gdict
+
+    return tuple(glist)
+
+
+def inactivate_by_dz(self, threshold: float, flip: bool = True):
+    """Set cell to inactive if dz does not exceed threshold.
+    Args:
+        threshold (float): The threshold for which the absolute value
+            of dz should exceed.
+        flip (bool): Whether the z-direction should be flipped.
+
+    """
+    self._xtgformat2()
+    self._actnumsv[
+        self.get_dz(asmasked=False, flip=flip).values.reshape(self._actnumsv.shape)
+        < threshold
+    ] = 0
+
+
+def make_zconsistent(self, zsep):
+    """Make consistent in z."""
+    self._xtgformat1()
+
+    if isinstance(zsep, int):
+        zsep = float(zsep)
+
+    if not isinstance(zsep, float):
+        raise ValueError('The "zsep" is not a float or int')
+
+    _cxtgeo.grd3d_make_z_consistent(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._zcornsv,
+        zsep,
+    )
+
+
+def inactivate_inside(self, poly, layer_range=None, inside=True, force_close=False):
+    """Inactivate inside a polygon (or outside)."""
+    self._xtgformat1()
+
+    if not isinstance(poly, Polygons):
+        raise ValueError("Input polygon not a XTGeo Polygons instance")
+
+    if layer_range is not None:
+        k1, k2 = layer_range
+    else:
+        k1 = 1
+        k2 = self.nlay
+
+    method = 0
+    if not inside:
+        method = 1
+
+    iforce = 0
+    if force_close:
+        iforce = 1
+
+    # get dataframe where each polygon is ended by a 999 value
+    dfxyz = poly.get_xyz_dataframe()
+
+    xc = dfxyz["X_UTME"].values.copy()
+    yc = dfxyz["Y_UTMN"].values.copy()
+
+    ier = _cxtgeo.grd3d_inact_outside_pol(
+        xc,
+        yc,
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,  # is modified!
+        k1,
+        k2,
+        iforce,
+        method,
+    )
+
+    if ier == 1:
+        raise RuntimeError("Problems with one or more polygons. " "Not closed?")
+
+
+def collapse_inactive_cells(self):
+    """Collapse inactive cells."""
+    self._xtgformat1()
+
+    _cxtgeo.grd3d_collapse_inact(
+        self.ncol, self.nrow, self.nlay, self._zcornsv, self._actnumsv
+    )
+
+
+def copy(self):
+    """Copy a grid instance (C pointers) and other props.
+
+    Returns:
+        A new instance (attached grid properties will also be unique)
+    """
+    self._xtgformat2()
+
+    filesrc = None
+    if self._filesrc is not None and "(copy)" not in self._filesrc:
+        filesrc = self._filesrc + " (copy)"
+    elif self._filesrc is not None:
+        filesrc = self._filesrc
+
+    return self.__class__(
+        coordsv=self._coordsv.copy(),
+        zcornsv=self._zcornsv.copy(),
+        actnumsv=self._actnumsv.copy(),
+        subgrids=deepcopy(self.subgrids),
+        dualporo=self.dualporo,
+        dualperm=self.dualperm,
+        name=self.name + " (copy)" if self.name else None,
+        roxgrid=self.roxgrid,
+        roxindexer=self.roxindexer,
+        props=self._props.copy() if self._props else None,
+        filesrc=filesrc,
+    )
+
+
+def crop(self, spec, props=None):  # pylint: disable=too-many-locals
+    """Do cropping of geometry (and properties).
+
+    If props is 'all' then all properties assosiated (linked) to then
+    grid are also cropped, and the instances are updated.
+
+    Args:
+        spec (tuple): A nested tuple on the form ((i1, i2), (j1, j2), (k1, k2))
+            where 1 represents start number, and 2 reperesent end. The range
+            is inclusive for both ends, and the number start index is 1 based.
+        props (list or str): None is default, while properties can be listed.
+            If 'all', then all GridProperty objects which are linked to the
+            Grid instance are updated.
+
+    Returns:
+        The instance is updated (cropped)
+    """
+    self._xtgformat1()
+
+    (ic1, ic2), (jc1, jc2), (kc1, kc2) = spec
+
+    if (
+        ic1 < 1
+        or ic2 > self.ncol
+        or jc1 < 1
+        or jc2 > self.nrow
+        or kc1 < 1
+        or kc2 > self.nlay
+    ):
+        raise ValueError("Boundary for tuples not matching grid" "NCOL, NROW, NLAY")
+
+    oldnlay = self._nlay
+
+    # compute size of new cropped grid
+    nncol = ic2 - ic1 + 1
+    nnrow = jc2 - jc1 + 1
+    nnlay = kc2 - kc1 + 1
+
+    ntot = nncol * nnrow * nnlay
+    ncoord = (nncol + 1) * (nnrow + 1) * 2 * 3
+    nzcorn = nncol * nnrow * (nnlay + 1) * 4
+
+    new_num_act = _cxtgeo.new_intpointer()
+    new_coordsv = np.zeros(ncoord, dtype=np.float64)
+    new_zcornsv = np.zeros(nzcorn, dtype=np.float64)
+    new_actnumsv = np.zeros(ntot, dtype=np.int32)
+
+    _cxtgeo.grd3d_crop_geometry(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        new_coordsv,
+        new_zcornsv,
+        new_actnumsv,
+        ic1,
+        ic2,
+        jc1,
+        jc2,
+        kc1,
+        kc2,
+        new_num_act,
+        0,
+    )
+
+    self._coordsv = new_coordsv
+    self._zcornsv = new_zcornsv
+    self._actnumsv = new_actnumsv
+
+    self._ncol = nncol
+    self._nrow = nnrow
+    self._nlay = nnlay
+
+    if isinstance(self.subgrids, dict):
+        newsub = OrderedDict()
+        # easier to work with numpies than lists
+        newarr = np.array(range(1, oldnlay + 1))
+        newarr[newarr < kc1] = 0
+        newarr[newarr > kc2] = 0
+        newaxx = newarr.copy() - kc1 + 1
+        for sub, arr in self.subgrids.items():
+            arrx = np.array(arr)
+            arrxmap = newaxx[arrx[0] - 1 : arrx[-1]]
+            arrxmap = arrxmap[arrxmap > 0]
+            if arrxmap.size > 0:
+                newsub[sub] = arrxmap.astype(np.int32).tolist()
+
+        self.subgrids = newsub
+
+    # crop properties
+    if props is not None:
+        if props == "all":
+            props = self.props
+
+        for prop in props:
+            logger.info("Crop %s", prop.name)
+            prop.crop(spec)
+
+
+def reduce_to_one_layer(self):
+    """Reduce the grid to one single layer.
+
+    This can be useful for algorithms that need to test if a point is within
+    the full grid.
+
+    Example::
+
+        >>> import xtgeo
+        >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+        >>> grid.nlay
+        14
+        >>> grid.reduce_to_one_layer()
+        >>> grid.nlay
+        1
+
+    """
+    # need new pointers in C (not for coord)
+    # Note this could probably be done with pure numpy operations
+    self._xtgformat1()
+
+    ptr_new_num_act = _cxtgeo.new_intpointer()
+
+    nnum = (1 + 1) * 4
+
+    new_zcorn = np.zeros(self.ncol * self.nrow * nnum, dtype=np.float64)
+    new_actnum = np.zeros(self.ncol * self.nrow * 1, dtype=np.int32)
+
+    _cxtgeo.grd3d_reduce_onelayer(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._zcornsv,
+        new_zcorn,
+        self._actnumsv,
+        new_actnum,
+        ptr_new_num_act,
+        0,
+    )
+
+    self._nlay = 1
+    self._zcornsv = new_zcorn
+    self._actnumsv = new_actnum
+    self._props = None
+    self._subgrids = None
+
+
+def translate_coordinates(self, translate=(0, 0, 0), flip=(1, 1, 1)):
+    """Translate grid coordinates."""
+    self._xtgformat1()
+
+    tx, ty, tz = translate
+    fx, fy, fz = flip
+
+    ier = _cxtgeo.grd3d_translate(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        fx,
+        fy,
+        fz,
+        tx,
+        ty,
+        tz,
+        self._coordsv,
+        self._zcornsv,
+    )
+    if ier != 0:
+        raise RuntimeError(f"Something went wrong in translate, code: {ier}")
+
+    logger.info("Translation of coords done")
+
+
+def reverse_row_axis(self, ijk_handedness=None):
+    """Reverse rows (aka flip) for geometry and assosiated properties."""
+    self._xtgformat1()
+
+    if ijk_handedness == self.ijk_handedness:
+        return
+
+    ier = _cxtgeo.grd3d_reverse_jrows(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+    )
+
+    if ier != 0:
+        raise RuntimeError(f"Something went wrong in jswapping, code: {ier}")
+
+    if self._props is None:
+        return
+
+    # do it for properties
+    if self._props.props:
+        for prp in self._props.props:
+            prp.values = prp.values[:, ::-1, :]
+
+    logger.info("Reversing of rows done")
+
+
+def get_adjacent_cells(self, prop, val1, val2, activeonly=True):
+    """Get adjacents cells."""
+    self._xtgformat1()
+
+    if not isinstance(prop, GridProperty):
+        raise ValueError("The argument prop is not a xtgeo.GridPropery")
+
+    if prop.isdiscrete is False:
+        raise ValueError("The argument prop is not a discrete property")
+
+    result = GridProperty(
+        ncol=self._ncol,
+        nrow=self._nrow,
+        nlay=self._nlay,
+        values=np.zeros(self.ntotal, dtype=np.int32),
+        name="ADJ_CELLS",
+        discrete=True,
+    )
+
+    p_prop1 = _gridprop_lowlevel.update_carray(prop)
+    p_prop2 = _cxtgeo.new_intarray(self.ntotal)
+
+    iflag1 = 1
+    if activeonly:
+        iflag1 = 0
+
+    iflag2 = 1
+
+    _cxtgeo.grd3d_adj_cells(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        p_prop1,
+        self.ntotal,
+        val1,
+        val2,
+        p_prop2,
+        self.ntotal,
+        iflag1,
+        iflag2,
+    )
+
+    _gridprop_lowlevel.update_values_from_carray(result, p_prop2, np.int32, delete=True)
+    # return the property object
+    return result
+
+
+def estimate_design(self, nsubname):
+    """Estimate (guess) (sub)grid design by examing DZ in median thickness column."""
+    actv = self.get_actnum().values
+
+    dzv = self.get_dz(asmasked=False).values
+
+    # treat inactive thicknesses as zero
+    dzv[actv == 0] = 0.0
+
+    if nsubname is None:
+        vrange = np.array(range(self.nlay))
+    else:
+        vrange = np.array(list(self.subgrids[nsubname])) - 1
+
+    # find the dz for the actual subzone
+    dzv = dzv[:, :, vrange]
+
+    # find cumulative thickness as a 2D array
+    dzcum = np.sum(dzv, axis=2, keepdims=False)
+
+    # find the average thickness for nonzero thicknesses
+    dzcum2 = dzcum.copy()
+    dzcum2[dzcum == 0.0] = np.nan
+    dzavg = np.nanmean(dzcum2) / dzv.shape[2]
+
+    # find the I J indices for the median value
+    if versionparse(np.__version__) < versionparse("1.22"):
+        argmed = np.stack(
+            np.nonzero(dzcum == np.percentile(dzcum, 50, interpolation="nearest")),
+            axis=1,
+        )
+    else:
+        argmed = np.stack(
+            np.nonzero(dzcum == np.percentile(dzcum, 50, method="nearest")), axis=1
+        )
+
+    im, jm = argmed[0]
+    # find the dz stack of the median
+    dzmedian = dzv[im, jm, :]
+    logger.info("DZ median column is %s", dzmedian)
+
+    # to compare thicknesses with (divide on 2 to assure)
+    target = dzcum[im, jm] / (dzmedian.shape[0] * 2)
+    eps = target / 100.0
+
+    logger.info("Target and EPS values are %s, %s", target, eps)
+
+    status = "X"  # unknown or cannot determine
+
+    if dzmedian[0] > target and dzmedian[-1] <= eps:
+        status = "T"
+        dzavg = dzmedian[0]
+    elif dzmedian[0] < eps and dzmedian[-1] > target:
+        status = "B"
+        dzavg = dzmedian[-1]
+    elif dzmedian[0] > target and dzmedian[-1] > target:
+        ratio = dzmedian[0] / dzmedian[-1]
+        if 0.5 < ratio < 1.5:
+            status = "P"
+    elif dzmedian[0] < eps and dzmedian[-1] < eps:
+        status = "M"
+        middleindex = int(dzmedian.shape[0] / 2)
+        dzavg = dzmedian[middleindex]
+
+    return {"design": status, "dzsimbox": dzavg}
+
+
+def estimate_flip(self):
+    """Estimate if grid is left or right handed."""
+    corners = self.get_xyz_cell_corners(activeonly=False)  # for cell 1, 1, 1
+
+    v1 = (corners[3] - corners[0], corners[4] - corners[1], 0.0)
+    v2 = (corners[6] - corners[0], corners[7] - corners[1], 0.0)
+
+    flipvalue = find_flip(v1, v2)
+
+    return flipvalue
+
+
+def _convert_xtgformat2to1(self):
+    """Convert arrays from new structure xtgformat=2 to legacy xtgformat=1."""
+    if self._xtgformat == 1:
+        logger.info("No conversion, format is already xtgformat == 1 or unset")
+        return
+
+    logger.info("Convert grid from new xtgformat to legacy format...")
+
+    newcoordsv = np.zeros(((self._ncol + 1) * (self._nrow + 1) * 6), dtype=np.float64)
+    newzcornsv = np.zeros(
+        (self._ncol * self._nrow * (self._nlay + 1) * 4), dtype=np.float64
+    )
+    newactnumsv = np.zeros((self._ncol * self._nrow * self._nlay), dtype=np.int32)
+
+    _cxtgeo.grd3cp3d_xtgformat2to1_geom(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        newcoordsv,
+        self._coordsv,
+        newzcornsv,
+        self._zcornsv,
+        newactnumsv,
+        self._actnumsv,
+    )
+
+    self._coordsv = newcoordsv
+    self._zcornsv = newzcornsv
+    self._actnumsv = newactnumsv
+    self._xtgformat = 1
+
+    logger.info("Convert grid from new xtgformat to legacy format... done")
+
+
+def _convert_xtgformat1to2(self):
+    """Convert arrays from old structure xtgformat=1 to new xtgformat=2."""
+    if self._xtgformat == 2 or self._coordsv is None:
+        logger.info("No conversion, format is already xtgformat == 2 or unset")
+        return
+
+    logger.info("Convert grid from legacy xtgformat to new format...")
+
+    newcoordsv = np.zeros((self._ncol + 1, self._nrow + 1, 6), dtype=np.float64)
+    newzcornsv = np.zeros(
+        (self._ncol + 1, self._nrow + 1, self._nlay + 1, 4), dtype=np.float32
+    )
+    newactnumsv = np.zeros((self._ncol, self._nrow, self._nlay), dtype=np.int32)
+
+    _cxtgeo.grd3cp3d_xtgformat1to2_geom(
+        self._ncol,
+        self._nrow,
+        self._nlay,
+        self._coordsv,
+        newcoordsv,
+        self._zcornsv,
+        newzcornsv,
+        self._actnumsv,
+        newactnumsv,
+    )
+
+    self._coordsv = newcoordsv
+    self._zcornsv = newzcornsv
+    self._actnumsv = newactnumsv
+    self._xtgformat = 2
+
+    logger.info("Convert grid from new xtgformat to legacy format... done")
+
+
+def get_gridquality_properties(self):
+    """Get the grid quality properties."""
+    self._xtgformat2()
+
+    qcnames = {
+        0: "minangle_topbase",
+        1: "maxangle_topbase",
+        2: "minangle_topbase_proj",
+        3: "maxangle_topbase_proj",
+        4: "minangle_sides",
+        5: "maxangle_sides",
+        6: "collapsed",
+        7: "faulted",
+        8: "negative_thickness",
+        9: "concave_proj",
+    }
+
+    # some of the properties shall be discrete:
+    qcdiscrete = [6, 7, 8, 9]
+
+    fresults = np.ones(
+        (len(qcnames), self.ncol * self.nrow * self.nlay), dtype=np.float32
+    )
+
+    _cxtgeo.grdcp3d_quality_indicators(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        fresults,
+    )
+
+    grdprops = xtgeo.GridProperties()
+
+    for num, name in qcnames.items():
+        prop = xtgeo.GridProperty(self, name=name)
+        dtype = np.float32
+        if num in qcdiscrete:
+            dtype = np.int32
+            prop.isdiscrete = True
+            prop.codes = {0: "None", 1: name}
+        prop.values = fresults[num, :].astype(dtype)
+        grdprops.append_props([prop])
+
+    return grdprops
```

## xtgeo/grid3d/_grid_export.py

 * *Ordering differences only*

```diff
@@ -1,203 +1,203 @@
-# -*- coding: utf-8 -*-
-import json
-import struct
-from copy import deepcopy
-
-import h5py
-import hdf5plugin
-import roffio
-
-from xtgeo.common import XTGeoDialog
-from xtgeo.grid3d._egrid import EGrid
-
-from ._grdecl_grid import GrdeclGrid
-from ._roff_grid import RoffGrid
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-# valid byte settings for xtgeo/hdf5 based export e.g. 441 means
-# 4byte float for coord, 4byte float for zcorn, and 1 byte Int for actnums.
-# Native XTGeo is 844
-VALIDXTGFMT = (221, 421, 441, 444, 841, 844, 881, 884)
-
-
-def export_roff(self, gfile, roff_format="binary"):
-    """Export grid to ROFF format."""
-    if roff_format == "binary":
-        RoffGrid.from_xtgeo_grid(self).to_file(gfile, roff_format=roffio.Format.BINARY)
-    elif roff_format == "ascii":
-        RoffGrid.from_xtgeo_grid(self).to_file(gfile, roff_format=roffio.Format.ASCII)
-    else:
-        raise ValueError(
-            "Incorrect format specifier in export_roff,"
-            f" expected 'binary' or 'ascii, got {roff_format}"
-        )
-
-
-def export_grdecl(self, gfile, mode):
-    """Export grid to Eclipse GRDECL format (ascii, mode=1) or binary (mode=0)."""
-    fileformat = "grdecl" if mode == 1 else "bgrdecl"
-    GrdeclGrid.from_xtgeo_grid(self).to_file(gfile, fileformat=fileformat)
-
-
-def export_egrid(self, gfile):
-    """Export grid to Eclipse EGRID format, binary."""
-    EGrid.from_xtgeo_grid(self).to_file(gfile, fileformat="egrid")
-
-
-def export_fegrid(self, gfile):
-    """Export grid to Eclipse FEGRID format, ascii."""
-    EGrid.from_xtgeo_grid(self).to_file(gfile, fileformat="fegrid")
-
-
-def export_xtgcpgeom(self, gfile, subformat=844):
-    """Export grid to binary XTGeo xtgcpgeom format, in prep. and experimental."""
-    logger.info("Export to native binary xtgeo...")
-
-    self._xtgformat2()
-    logger.info("Export to native binary xtgeo...(2)")
-
-    self.metadata.required = self
-    meta = self.metadata.get_metadata()
-
-    # subformat processing, indicating number of bytes per datatype
-    # here, 844 is native XTGeo (float64, float32, int32)
-    dtypecoo, dtypezco, dtypeact, meta = _define_dtypes(self, subformat, meta)
-
-    coordsv, zcornsv, actnumsv = _transform_vectors(
-        self, meta, dtypecoo, dtypezco, dtypeact
-    )
-
-    prevalues = (1, 1301, int(subformat), self.ncol, self.nrow, self.nlay)
-    mystruct = struct.Struct("= i i i q q q")
-    hdr = mystruct.pack(*prevalues)
-
-    with open(gfile.file, "wb") as fout:
-        fout.write(hdr)
-
-    with open(gfile.file, "ab") as fout:
-        coordsv.tofile(fout)
-        zcornsv.tofile(fout)
-        actnumsv.tofile(fout)
-
-    with open(gfile.file, "ab") as fout:
-        fout.write("\nXTGMETA.v01\n".encode())
-
-    with open(gfile.file, "ab") as fout:
-        fout.write(json.dumps(meta).encode())
-
-    logger.info("Export to native binary xtgeo... done!")
-
-
-def export_hdf5_cpgeom(self, gfile, compression=None, chunks=False, subformat=844):
-    """Export grid to h5/hdf5, in prep. and experimental."""
-    self._xtgformat2()
-
-    logger.info("Export to hdf5 xtgeo layout...")
-
-    self.metadata.required = self
-    meta = self.metadata.get_metadata()
-
-    if compression and compression == "blosc":
-        compression = hdf5plugin.Blosc(
-            cname="blosclz", clevel=9, shuffle=hdf5plugin.Blosc.SHUFFLE
-        )
-
-    if not chunks:
-        chunks = None
-
-    dtypecoo, dtypezco, dtypeact, meta = _define_dtypes(self, subformat, meta)
-
-    coordsv, zcornsv, actnumsv = _transform_vectors(
-        self, meta, dtypecoo, dtypezco, dtypeact
-    )
-
-    jmeta = json.dumps(meta).encode()
-
-    with h5py.File(gfile.name, "w") as fh5:
-        grp = fh5.create_group("CornerPointGeometry")
-        grp.create_dataset(
-            "coord",
-            data=coordsv,
-            compression=compression,
-            chunks=chunks,
-        )
-        grp.create_dataset(
-            "zcorn",
-            data=zcornsv,
-            compression=compression,
-            chunks=chunks,
-        )
-        grp.create_dataset(
-            "actnum",
-            data=actnumsv,
-            compression=compression,
-            chunks=chunks,
-        )
-
-        grp.attrs["metadata"] = jmeta
-        grp.attrs["provider"] = "xtgeo"
-        grp.attrs["format-idcode"] = 1301
-
-    logger.info("Export to hdf5 xtgeo layout... done!")
-
-
-def _define_dtypes(self, fmt, meta):
-    # subformat processing, indicating number of bytes per datatype
-    # here, 844 is native XTGeo (float64, float32, int32)
-    logger.debug("Define dtypes...")
-
-    newmeta = deepcopy(meta)
-
-    if int(fmt) not in VALIDXTGFMT:
-        raise ValueError(f"The subformat value is not valid, must be in: {VALIDXTGFMT}")
-
-    if fmt <= 444:
-        # the float() is for JSON serialization which cannot handle float32
-        newmeta["_required_"]["xshift"] = float(self._coordsv[:, :, 0::3].mean())
-        newmeta["_required_"]["yshift"] = float(self._coordsv[:, :, 1::3].mean())
-        newmeta["_required_"]["zshift"] = float(self._zcornsv.mean())
-
-    nbytecoord, nbytezcorn, nbyteactnum = [int(nbyte) for nbyte in str(fmt)]
-
-    dtype_coord = "float" + str(nbytecoord * 8)
-    dtype_zcorn = "float" + str(nbytezcorn * 8)
-    dtype_actnum = "int" + str(nbyteactnum * 8)
-
-    logger.debug("Define dtypes... done!")
-    return dtype_coord, dtype_zcorn, dtype_actnum, newmeta
-
-
-def _transform_vectors(self, meta, dtypecoo, dtypezco, dtypeact):
-    logger.debug("Transform vectors...")
-
-    xshift = meta["_required_"]["xshift"]
-    yshift = meta["_required_"]["yshift"]
-    zshift = meta["_required_"]["zshift"]
-
-    if xshift != 0.0 or yshift != 0.0 or zshift != 0.0:
-        coordsv = self._coordsv.copy()
-        zcornsv = self._zcornsv.copy()
-        actnumsv = self._actnumsv.copy()
-        coordsv[:, :, 0::3] -= xshift
-        coordsv[:, :, 1::3] -= yshift
-        coordsv[:, :, 2::3] -= zshift
-        zcornsv -= zshift
-    else:
-        coordsv = self._coordsv
-        zcornsv = self._zcornsv
-        actnumsv = self._actnumsv
-
-    if dtypecoo != "float64":
-        coordsv = coordsv.astype(dtypecoo)
-
-    if dtypecoo != "float32":
-        zcornsv = zcornsv.astype(dtypezco)
-
-    if dtypecoo != "int32":
-        actnumsv = actnumsv.astype(dtypeact)
-
-    logger.debug("Transform vectors... done!")
-    return coordsv, zcornsv, actnumsv
+# -*- coding: utf-8 -*-
+import json
+import struct
+from copy import deepcopy
+
+import h5py
+import hdf5plugin
+import roffio
+
+from xtgeo.common import XTGeoDialog
+from xtgeo.grid3d._egrid import EGrid
+
+from ._grdecl_grid import GrdeclGrid
+from ._roff_grid import RoffGrid
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+# valid byte settings for xtgeo/hdf5 based export e.g. 441 means
+# 4byte float for coord, 4byte float for zcorn, and 1 byte Int for actnums.
+# Native XTGeo is 844
+VALIDXTGFMT = (221, 421, 441, 444, 841, 844, 881, 884)
+
+
+def export_roff(self, gfile, roff_format="binary"):
+    """Export grid to ROFF format."""
+    if roff_format == "binary":
+        RoffGrid.from_xtgeo_grid(self).to_file(gfile, roff_format=roffio.Format.BINARY)
+    elif roff_format == "ascii":
+        RoffGrid.from_xtgeo_grid(self).to_file(gfile, roff_format=roffio.Format.ASCII)
+    else:
+        raise ValueError(
+            "Incorrect format specifier in export_roff,"
+            f" expected 'binary' or 'ascii, got {roff_format}"
+        )
+
+
+def export_grdecl(self, gfile, mode):
+    """Export grid to Eclipse GRDECL format (ascii, mode=1) or binary (mode=0)."""
+    fileformat = "grdecl" if mode == 1 else "bgrdecl"
+    GrdeclGrid.from_xtgeo_grid(self).to_file(gfile, fileformat=fileformat)
+
+
+def export_egrid(self, gfile):
+    """Export grid to Eclipse EGRID format, binary."""
+    EGrid.from_xtgeo_grid(self).to_file(gfile, fileformat="egrid")
+
+
+def export_fegrid(self, gfile):
+    """Export grid to Eclipse FEGRID format, ascii."""
+    EGrid.from_xtgeo_grid(self).to_file(gfile, fileformat="fegrid")
+
+
+def export_xtgcpgeom(self, gfile, subformat=844):
+    """Export grid to binary XTGeo xtgcpgeom format, in prep. and experimental."""
+    logger.info("Export to native binary xtgeo...")
+
+    self._xtgformat2()
+    logger.info("Export to native binary xtgeo...(2)")
+
+    self.metadata.required = self
+    meta = self.metadata.get_metadata()
+
+    # subformat processing, indicating number of bytes per datatype
+    # here, 844 is native XTGeo (float64, float32, int32)
+    dtypecoo, dtypezco, dtypeact, meta = _define_dtypes(self, subformat, meta)
+
+    coordsv, zcornsv, actnumsv = _transform_vectors(
+        self, meta, dtypecoo, dtypezco, dtypeact
+    )
+
+    prevalues = (1, 1301, int(subformat), self.ncol, self.nrow, self.nlay)
+    mystruct = struct.Struct("= i i i q q q")
+    hdr = mystruct.pack(*prevalues)
+
+    with open(gfile.file, "wb") as fout:
+        fout.write(hdr)
+
+    with open(gfile.file, "ab") as fout:
+        coordsv.tofile(fout)
+        zcornsv.tofile(fout)
+        actnumsv.tofile(fout)
+
+    with open(gfile.file, "ab") as fout:
+        fout.write("\nXTGMETA.v01\n".encode())
+
+    with open(gfile.file, "ab") as fout:
+        fout.write(json.dumps(meta).encode())
+
+    logger.info("Export to native binary xtgeo... done!")
+
+
+def export_hdf5_cpgeom(self, gfile, compression=None, chunks=False, subformat=844):
+    """Export grid to h5/hdf5, in prep. and experimental."""
+    self._xtgformat2()
+
+    logger.info("Export to hdf5 xtgeo layout...")
+
+    self.metadata.required = self
+    meta = self.metadata.get_metadata()
+
+    if compression and compression == "blosc":
+        compression = hdf5plugin.Blosc(
+            cname="blosclz", clevel=9, shuffle=hdf5plugin.Blosc.SHUFFLE
+        )
+
+    if not chunks:
+        chunks = None
+
+    dtypecoo, dtypezco, dtypeact, meta = _define_dtypes(self, subformat, meta)
+
+    coordsv, zcornsv, actnumsv = _transform_vectors(
+        self, meta, dtypecoo, dtypezco, dtypeact
+    )
+
+    jmeta = json.dumps(meta).encode()
+
+    with h5py.File(gfile.name, "w") as fh5:
+        grp = fh5.create_group("CornerPointGeometry")
+        grp.create_dataset(
+            "coord",
+            data=coordsv,
+            compression=compression,
+            chunks=chunks,
+        )
+        grp.create_dataset(
+            "zcorn",
+            data=zcornsv,
+            compression=compression,
+            chunks=chunks,
+        )
+        grp.create_dataset(
+            "actnum",
+            data=actnumsv,
+            compression=compression,
+            chunks=chunks,
+        )
+
+        grp.attrs["metadata"] = jmeta
+        grp.attrs["provider"] = "xtgeo"
+        grp.attrs["format-idcode"] = 1301
+
+    logger.info("Export to hdf5 xtgeo layout... done!")
+
+
+def _define_dtypes(self, fmt, meta):
+    # subformat processing, indicating number of bytes per datatype
+    # here, 844 is native XTGeo (float64, float32, int32)
+    logger.debug("Define dtypes...")
+
+    newmeta = deepcopy(meta)
+
+    if int(fmt) not in VALIDXTGFMT:
+        raise ValueError(f"The subformat value is not valid, must be in: {VALIDXTGFMT}")
+
+    if fmt <= 444:
+        # the float() is for JSON serialization which cannot handle float32
+        newmeta["_required_"]["xshift"] = float(self._coordsv[:, :, 0::3].mean())
+        newmeta["_required_"]["yshift"] = float(self._coordsv[:, :, 1::3].mean())
+        newmeta["_required_"]["zshift"] = float(self._zcornsv.mean())
+
+    nbytecoord, nbytezcorn, nbyteactnum = [int(nbyte) for nbyte in str(fmt)]
+
+    dtype_coord = "float" + str(nbytecoord * 8)
+    dtype_zcorn = "float" + str(nbytezcorn * 8)
+    dtype_actnum = "int" + str(nbyteactnum * 8)
+
+    logger.debug("Define dtypes... done!")
+    return dtype_coord, dtype_zcorn, dtype_actnum, newmeta
+
+
+def _transform_vectors(self, meta, dtypecoo, dtypezco, dtypeact):
+    logger.debug("Transform vectors...")
+
+    xshift = meta["_required_"]["xshift"]
+    yshift = meta["_required_"]["yshift"]
+    zshift = meta["_required_"]["zshift"]
+
+    if xshift != 0.0 or yshift != 0.0 or zshift != 0.0:
+        coordsv = self._coordsv.copy()
+        zcornsv = self._zcornsv.copy()
+        actnumsv = self._actnumsv.copy()
+        coordsv[:, :, 0::3] -= xshift
+        coordsv[:, :, 1::3] -= yshift
+        coordsv[:, :, 2::3] -= zshift
+        zcornsv -= zshift
+    else:
+        coordsv = self._coordsv
+        zcornsv = self._zcornsv
+        actnumsv = self._actnumsv
+
+    if dtypecoo != "float64":
+        coordsv = coordsv.astype(dtypecoo)
+
+    if dtypecoo != "float32":
+        zcornsv = zcornsv.astype(dtypezco)
+
+    if dtypecoo != "int32":
+        actnumsv = actnumsv.astype(dtypeact)
+
+    logger.debug("Transform vectors... done!")
+    return coordsv, zcornsv, actnumsv
```

## xtgeo/grid3d/_grid_hybrid.py

 * *Ordering differences only*

```diff
@@ -1,61 +1,61 @@
-import numpy as np
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-
-xtg = xtgeo.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def make_hybridgrid(
-    self,
-    nhdiv=10,
-    toplevel=1000.0,
-    bottomlevel=1100.0,
-    region=None,
-    region_number=None,
-):
-    """Make hybrid grid."""
-    self._xtgformat1()
-
-    newnlay = self.nlay * 2 + nhdiv
-    newnzcorn = self.ncol * self.nrow * (newnlay + 1) * 4
-    newnactnum = self.ncol * self.nrow * newnlay
-
-    # initialize
-    hyb_zcornsv = np.zeros(newnzcorn, dtype=np.float64)
-    hyb_actnumsv = np.zeros(newnactnum, dtype=np.int32)
-
-    if region is None:
-        region_number = -1
-        rvalues = np.ones(1, dtype=np.int32)
-    else:
-        rvalues = np.ma.filled(region.values, fill_value=xtgeo.UNDEF_INT)
-        rvalues = rvalues.ravel()
-
-    _cxtgeo.grd3d_convert_hybrid(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        self._zcornsv,
-        self._actnumsv,
-        newnlay,
-        hyb_zcornsv,
-        hyb_actnumsv,
-        toplevel,
-        bottomlevel,
-        nhdiv,
-        rvalues,
-        region_number,
-    )
-
-    del rvalues
-
-    # when a hybridgrid is made, the current subrid settings lose relevance, hence
-    # it is forced set to None
-    self.subgrids = None
-
-    self._nlay = newnlay
-    self._zcornsv = hyb_zcornsv
-    self._actnumsv = hyb_actnumsv
+import numpy as np
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+
+xtg = xtgeo.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def make_hybridgrid(
+    self,
+    nhdiv=10,
+    toplevel=1000.0,
+    bottomlevel=1100.0,
+    region=None,
+    region_number=None,
+):
+    """Make hybrid grid."""
+    self._xtgformat1()
+
+    newnlay = self.nlay * 2 + nhdiv
+    newnzcorn = self.ncol * self.nrow * (newnlay + 1) * 4
+    newnactnum = self.ncol * self.nrow * newnlay
+
+    # initialize
+    hyb_zcornsv = np.zeros(newnzcorn, dtype=np.float64)
+    hyb_actnumsv = np.zeros(newnactnum, dtype=np.int32)
+
+    if region is None:
+        region_number = -1
+        rvalues = np.ones(1, dtype=np.int32)
+    else:
+        rvalues = np.ma.filled(region.values, fill_value=xtgeo.UNDEF_INT)
+        rvalues = rvalues.ravel()
+
+    _cxtgeo.grd3d_convert_hybrid(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        self._zcornsv,
+        self._actnumsv,
+        newnlay,
+        hyb_zcornsv,
+        hyb_actnumsv,
+        toplevel,
+        bottomlevel,
+        nhdiv,
+        rvalues,
+        region_number,
+    )
+
+    del rvalues
+
+    # when a hybridgrid is made, the current subrid settings lose relevance, hence
+    # it is forced set to None
+    self.subgrids = None
+
+    self._nlay = newnlay
+    self._zcornsv = hyb_zcornsv
+    self._actnumsv = hyb_actnumsv
```

## xtgeo/grid3d/_grid_import.py

 * *Ordering differences only*

```diff
@@ -1,57 +1,57 @@
-# -*- coding: utf-8 -*-
-"""Grid import functions for various formats."""
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.grid3d import _grid_import_ecl, _grid_import_roff
-
-from . import _grid_import_xtgcpgeom
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def from_file(gfile, fformat=None, **kwargs):  # pylint: disable=too-many-branches
-    """Import grid geometry from file, and makes an instance of this class.
-
-    Returns:
-    dictionary of keyword arguments to be used in Grid constructor.
-    """
-    if not isinstance(gfile, xtgeo._XTGeoFile):
-        raise RuntimeError("Error gfile must be a _XTGeoFile instance")
-
-    result = {}
-
-    result["filesrc"] = gfile.name
-
-    if fformat is None or fformat == "guess":
-        fformat = gfile.detect_fformat()
-    else:
-        fformat = gfile.generic_format_by_proposal(fformat)  # default
-
-    gfile.check_file(raiseerror=IOError, raisetext=f"Cannot access file {gfile.name}")
-
-    if fformat in ["roff_binary", "roff_ascii"]:
-        result.update(_grid_import_roff.import_roff(gfile, **kwargs))
-    elif fformat in ["egrid", "fegrid"]:
-        result.update(
-            _grid_import_ecl.import_ecl_egrid(gfile, fileformat=fformat, **kwargs)
-        )
-    elif fformat == "grdecl":
-        result.update(_grid_import_ecl.import_ecl_grdecl(gfile, **kwargs))
-    elif fformat == "bgrdecl":
-        result.update(_grid_import_ecl.import_ecl_bgrdecl(gfile, **kwargs))
-    elif fformat == "xtg":
-        result.update(_grid_import_xtgcpgeom.import_xtgcpgeom(gfile, **kwargs))
-    elif fformat == "hdf":
-        result.update(_grid_import_xtgcpgeom.import_hdf5_cpgeom(gfile, **kwargs))
-    else:
-        raise ValueError(f"Invalid file format: {fformat}")
-
-    if gfile.memstream:
-        result["name"] = "unknown"
-    else:
-        result["name"] = gfile.file.stem
-
-    return result
+# -*- coding: utf-8 -*-
+"""Grid import functions for various formats."""
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.grid3d import _grid_import_ecl, _grid_import_roff
+
+from . import _grid_import_xtgcpgeom
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def from_file(gfile, fformat=None, **kwargs):  # pylint: disable=too-many-branches
+    """Import grid geometry from file, and makes an instance of this class.
+
+    Returns:
+    dictionary of keyword arguments to be used in Grid constructor.
+    """
+    if not isinstance(gfile, xtgeo._XTGeoFile):
+        raise RuntimeError("Error gfile must be a _XTGeoFile instance")
+
+    result = {}
+
+    result["filesrc"] = gfile.name
+
+    if fformat is None or fformat == "guess":
+        fformat = gfile.detect_fformat()
+    else:
+        fformat = gfile.generic_format_by_proposal(fformat)  # default
+
+    gfile.check_file(raiseerror=IOError, raisetext=f"Cannot access file {gfile.name}")
+
+    if fformat in ["roff_binary", "roff_ascii"]:
+        result.update(_grid_import_roff.import_roff(gfile, **kwargs))
+    elif fformat in ["egrid", "fegrid"]:
+        result.update(
+            _grid_import_ecl.import_ecl_egrid(gfile, fileformat=fformat, **kwargs)
+        )
+    elif fformat == "grdecl":
+        result.update(_grid_import_ecl.import_ecl_grdecl(gfile, **kwargs))
+    elif fformat == "bgrdecl":
+        result.update(_grid_import_ecl.import_ecl_bgrdecl(gfile, **kwargs))
+    elif fformat == "xtg":
+        result.update(_grid_import_xtgcpgeom.import_xtgcpgeom(gfile, **kwargs))
+    elif fformat == "hdf":
+        result.update(_grid_import_xtgcpgeom.import_hdf5_cpgeom(gfile, **kwargs))
+    else:
+        raise ValueError(f"Invalid file format: {fformat}")
+
+    if gfile.memstream:
+        result["name"] = "unknown"
+    else:
+        result["name"] = gfile.file.stem
+
+    return result
```

## xtgeo/grid3d/_grid_import_ecl.py

 * *Ordering differences only*

```diff
@@ -1,95 +1,95 @@
-# -*- coding: utf-8 -*-
-
-"""Grid import functions for Eclipse, new approach (i.e. version 2)."""
-
-
-import xtgeo
-from xtgeo.grid3d._egrid import EGrid, RockModel
-from xtgeo.grid3d._grdecl_grid import GrdeclGrid, GridRelative
-
-xtg = xtgeo.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_ecl_egrid(
-    gfile,
-    relative_to=GridRelative.MAP,
-    fileformat="egrid",
-):
-    egrid = EGrid.from_file(gfile._file, fileformat=fileformat)
-
-    result = grid_from_ecl_grid(egrid, relative_to=relative_to)
-
-    if egrid.egrid_head.file_head.rock_model == RockModel.DUAL_POROSITY:
-        result["dualporo"] = True
-        result["dualperm"] = False
-    elif egrid.egrid_head.file_head.rock_model == RockModel.DUAL_PERMEABILITY:
-        result["dualporo"] = True
-        result["dualperm"] = True
-
-    return result
-
-
-# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-# Import eclipse run suite: EGRID + properties from INIT and UNRST
-# For the INIT and UNRST, props dates shall be selected
-# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-def import_ecl_run(
-    groot, ecl_grid, initprops=None, restartprops=None, restartdates=None
-):
-    """Import combo ECL runs."""
-    ecl_init = groot + ".INIT"
-    ecl_rsta = groot + ".UNRST"
-
-    ecl_init = xtgeo._XTGeoFile(ecl_init)
-    ecl_rsta = xtgeo._XTGeoFile(ecl_rsta)
-
-    grdprops = xtgeo.grid3d.GridProperties()
-
-    # import the init properties unless list is empty
-    if initprops:
-        initprops = xtgeo.gridproperties_from_file(
-            ecl_init.name, names=initprops, fformat="init", dates=None, grid=ecl_grid
-        )
-        grdprops.append_props(initprops.props)
-
-    # import the restart properties for dates unless lists are empty
-    if restartprops and restartdates:
-        restartprops = xtgeo.gridproperties_from_file(
-            ecl_rsta.name,
-            names=restartprops,
-            fformat="unrst",
-            dates=restartdates,
-            grid=ecl_grid,
-        )
-        grdprops.append_props(restartprops.props)
-
-    ecl_grid.gridprops = grdprops
-
-
-def import_ecl_grdecl(gfile, relative_to=GridRelative.MAP):
-    """Import grdecl format."""
-
-    grdecl_grid = GrdeclGrid.from_file(gfile._file, fileformat="grdecl")
-    return grid_from_ecl_grid(grdecl_grid, relative_to=relative_to)
-
-
-def import_ecl_bgrdecl(gfile, relative_to=GridRelative.MAP):
-    """Import binary files with GRDECL layout."""
-
-    grdecl_grid = GrdeclGrid.from_file(gfile._file, fileformat="bgrdecl")
-    return grid_from_ecl_grid(grdecl_grid, relative_to=relative_to)
-
-
-def grid_from_ecl_grid(ecl_grid, relative_to=GridRelative.MAP):
-    result = dict()
-    result["coordsv"] = ecl_grid.xtgeo_coord(relative_to=relative_to)
-    result["zcornsv"] = ecl_grid.xtgeo_zcorn(relative_to=relative_to)
-    result["actnumsv"] = ecl_grid.xtgeo_actnum()
-    if relative_to == GridRelative.MAP and ecl_grid.map_axis_units is not None:
-        result["units"] = ecl_grid.map_axis_units
-    else:
-        result["units"] = ecl_grid.grid_units
-
-    return result
+# -*- coding: utf-8 -*-
+
+"""Grid import functions for Eclipse, new approach (i.e. version 2)."""
+
+
+import xtgeo
+from xtgeo.grid3d._egrid import EGrid, RockModel
+from xtgeo.grid3d._grdecl_grid import GrdeclGrid, GridRelative
+
+xtg = xtgeo.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_ecl_egrid(
+    gfile,
+    relative_to=GridRelative.MAP,
+    fileformat="egrid",
+):
+    egrid = EGrid.from_file(gfile._file, fileformat=fileformat)
+
+    result = grid_from_ecl_grid(egrid, relative_to=relative_to)
+
+    if egrid.egrid_head.file_head.rock_model == RockModel.DUAL_POROSITY:
+        result["dualporo"] = True
+        result["dualperm"] = False
+    elif egrid.egrid_head.file_head.rock_model == RockModel.DUAL_PERMEABILITY:
+        result["dualporo"] = True
+        result["dualperm"] = True
+
+    return result
+
+
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+# Import eclipse run suite: EGRID + properties from INIT and UNRST
+# For the INIT and UNRST, props dates shall be selected
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+def import_ecl_run(
+    groot, ecl_grid, initprops=None, restartprops=None, restartdates=None
+):
+    """Import combo ECL runs."""
+    ecl_init = groot + ".INIT"
+    ecl_rsta = groot + ".UNRST"
+
+    ecl_init = xtgeo._XTGeoFile(ecl_init)
+    ecl_rsta = xtgeo._XTGeoFile(ecl_rsta)
+
+    grdprops = xtgeo.grid3d.GridProperties()
+
+    # import the init properties unless list is empty
+    if initprops:
+        initprops = xtgeo.gridproperties_from_file(
+            ecl_init.name, names=initprops, fformat="init", dates=None, grid=ecl_grid
+        )
+        grdprops.append_props(initprops.props)
+
+    # import the restart properties for dates unless lists are empty
+    if restartprops and restartdates:
+        restartprops = xtgeo.gridproperties_from_file(
+            ecl_rsta.name,
+            names=restartprops,
+            fformat="unrst",
+            dates=restartdates,
+            grid=ecl_grid,
+        )
+        grdprops.append_props(restartprops.props)
+
+    ecl_grid.gridprops = grdprops
+
+
+def import_ecl_grdecl(gfile, relative_to=GridRelative.MAP):
+    """Import grdecl format."""
+
+    grdecl_grid = GrdeclGrid.from_file(gfile._file, fileformat="grdecl")
+    return grid_from_ecl_grid(grdecl_grid, relative_to=relative_to)
+
+
+def import_ecl_bgrdecl(gfile, relative_to=GridRelative.MAP):
+    """Import binary files with GRDECL layout."""
+
+    grdecl_grid = GrdeclGrid.from_file(gfile._file, fileformat="bgrdecl")
+    return grid_from_ecl_grid(grdecl_grid, relative_to=relative_to)
+
+
+def grid_from_ecl_grid(ecl_grid, relative_to=GridRelative.MAP):
+    result = dict()
+    result["coordsv"] = ecl_grid.xtgeo_coord(relative_to=relative_to)
+    result["zcornsv"] = ecl_grid.xtgeo_zcorn(relative_to=relative_to)
+    result["actnumsv"] = ecl_grid.xtgeo_actnum()
+    if relative_to == GridRelative.MAP and ecl_grid.map_axis_units is not None:
+        result["units"] = ecl_grid.map_axis_units
+    else:
+        result["units"] = ecl_grid.grid_units
+
+    return result
```

## xtgeo/grid3d/_grid_import_roff.py

 * *Ordering differences only*

```diff
@@ -1,117 +1,117 @@
-# coding: utf-8
-"""Private module, Grid Import private functions for ROFF format."""
-
-import pathlib
-import tempfile
-import warnings
-from contextlib import contextmanager
-
-import xtgeo
-
-from ._roff_grid import RoffGrid
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def match_xtgeo_214_header(header: bytes) -> bool:
-    """
-    Check whether the start of a binary file matches
-    the problematic xtgeo version 2.14 contents.
-
-    Params:
-      header: the start of a file
-
-    """
-    first_part_match = header.startswith(
-        b"roff-bin\0"
-        b"#ROFF file#\0"
-        b"#Creator: CXTGeo subsystem of XTGeo by JCR#\0"
-        b"tag\0filedata\0"
-        b"int\0byteswaptest\0"
-    )
-    problem_area_match = header[99:].startswith(
-        b"char\0filetype\0grid\0char\0creationDate\0UNKNOWNendtag"
-    )
-    return first_part_match and problem_area_match
-
-
-def replace_xtgeo_214_header(header):
-    """
-    Given that match_xtgeo_214_header(header), inserts
-    a \0 in the correct place.
-    """
-    return header[:143] + b"\0" + header[143:]
-
-
-@contextmanager
-def handle_deprecated_xtgeo_roff_file(filelike):
-    """
-    A contextmanager that inplace fixes grid roff files
-    that were written by XTGeo prior to version 2.15. This
-    backwards compatibility should eventually be deprecated
-
-    Example::
-
-       with handle_deprecated_xtgeo_roff_file("grid.roff") as converted_grid:
-          roff_grid = RoffGrid.from_file(converted_file)
-
-
-    Before version 2.15, grids with _xtgformat=1 would be written with missing
-    '\\0' after the creationDate. However, it would also silently read that
-    file without any issues in the final product. roffio is less leanient when
-    it comes to the format it will accept and so does not recover. Luckily, the
-    creationDate was set to 'UNKNOWN' so we can be fairly certain we replace
-    correctly.
-
-    """
-    header = None
-    inhandle = filelike
-    close = False
-    if isinstance(filelike, (str, pathlib.Path)):
-        inhandle = open(filelike, "rb")
-        close = True
-    goback = inhandle.tell()
-    header = inhandle.read(200)
-
-    if match_xtgeo_214_header(header):
-        name = "buffer"
-        if hasattr(filelike, "name"):
-            name = filelike.name
-        warnings.warn(
-            f"The roff file {name} contains nonstandard but harmless roff"
-            " format detail written by XTGeo version <=2.14. Reading of such files"
-            " is deprecated, consider re-exporting the file with XTGeo version >=2.15.3"
-        )
-        new_header = replace_xtgeo_214_header(header)
-
-        with tempfile.NamedTemporaryFile() as outhandle:
-            outhandle.write(new_header)
-            inhandle.seek(200)
-            outhandle.write(inhandle.read())
-            outhandle.seek(0)
-            yield outhandle
-            if close:
-                inhandle.close()
-            else:
-                inhandle.seek(goback)
-
-    else:
-        if close:
-            inhandle.close()
-        else:
-            inhandle.seek(goback)
-
-        yield filelike
-
-
-def import_roff(gfile):
-    with handle_deprecated_xtgeo_roff_file(gfile._file) as converted_file:
-        roff_grid = RoffGrid.from_file(converted_file)
-    return {
-        "actnumsv": roff_grid.xtgeo_actnum(),
-        "coordsv": roff_grid.xtgeo_coord(),
-        "zcornsv": roff_grid.xtgeo_zcorn(),
-        "subgrids": roff_grid.xtgeo_subgrids(),
-    }
+# coding: utf-8
+"""Private module, Grid Import private functions for ROFF format."""
+
+import pathlib
+import tempfile
+import warnings
+from contextlib import contextmanager
+
+import xtgeo
+
+from ._roff_grid import RoffGrid
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def match_xtgeo_214_header(header: bytes) -> bool:
+    """
+    Check whether the start of a binary file matches
+    the problematic xtgeo version 2.14 contents.
+
+    Params:
+      header: the start of a file
+
+    """
+    first_part_match = header.startswith(
+        b"roff-bin\0"
+        b"#ROFF file#\0"
+        b"#Creator: CXTGeo subsystem of XTGeo by JCR#\0"
+        b"tag\0filedata\0"
+        b"int\0byteswaptest\0"
+    )
+    problem_area_match = header[99:].startswith(
+        b"char\0filetype\0grid\0char\0creationDate\0UNKNOWNendtag"
+    )
+    return first_part_match and problem_area_match
+
+
+def replace_xtgeo_214_header(header):
+    """
+    Given that match_xtgeo_214_header(header), inserts
+    a \0 in the correct place.
+    """
+    return header[:143] + b"\0" + header[143:]
+
+
+@contextmanager
+def handle_deprecated_xtgeo_roff_file(filelike):
+    """
+    A contextmanager that inplace fixes grid roff files
+    that were written by XTGeo prior to version 2.15. This
+    backwards compatibility should eventually be deprecated
+
+    Example::
+
+       with handle_deprecated_xtgeo_roff_file("grid.roff") as converted_grid:
+          roff_grid = RoffGrid.from_file(converted_file)
+
+
+    Before version 2.15, grids with _xtgformat=1 would be written with missing
+    '\\0' after the creationDate. However, it would also silently read that
+    file without any issues in the final product. roffio is less leanient when
+    it comes to the format it will accept and so does not recover. Luckily, the
+    creationDate was set to 'UNKNOWN' so we can be fairly certain we replace
+    correctly.
+
+    """
+    header = None
+    inhandle = filelike
+    close = False
+    if isinstance(filelike, (str, pathlib.Path)):
+        inhandle = open(filelike, "rb")
+        close = True
+    goback = inhandle.tell()
+    header = inhandle.read(200)
+
+    if match_xtgeo_214_header(header):
+        name = "buffer"
+        if hasattr(filelike, "name"):
+            name = filelike.name
+        warnings.warn(
+            f"The roff file {name} contains nonstandard but harmless roff"
+            " format detail written by XTGeo version <=2.14. Reading of such files"
+            " is deprecated, consider re-exporting the file with XTGeo version >=2.15.3"
+        )
+        new_header = replace_xtgeo_214_header(header)
+
+        with tempfile.NamedTemporaryFile() as outhandle:
+            outhandle.write(new_header)
+            inhandle.seek(200)
+            outhandle.write(inhandle.read())
+            outhandle.seek(0)
+            yield outhandle
+            if close:
+                inhandle.close()
+            else:
+                inhandle.seek(goback)
+
+    else:
+        if close:
+            inhandle.close()
+        else:
+            inhandle.seek(goback)
+
+        yield filelike
+
+
+def import_roff(gfile):
+    with handle_deprecated_xtgeo_roff_file(gfile._file) as converted_file:
+        roff_grid = RoffGrid.from_file(converted_file)
+    return {
+        "actnumsv": roff_grid.xtgeo_actnum(),
+        "coordsv": roff_grid.xtgeo_coord(),
+        "zcornsv": roff_grid.xtgeo_zcorn(),
+        "subgrids": roff_grid.xtgeo_subgrids(),
+    }
```

## xtgeo/grid3d/_grid_import_xtgcpgeom.py

 * *Ordering differences only*

```diff
@@ -1,290 +1,290 @@
-# coding: utf-8
-"""Private module, Grid Import private functions for xtgeo based formats."""
-
-import json
-from collections import OrderedDict
-from struct import unpack
-
-import h5py
-import numpy as np
-
-import xtgeo
-import xtgeo.common.sys as xsys
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def convert_subgrids(sdict):
-    """Convert a simplified ordered dictionary to
-        subgrid required by Grid.
-
-    The simplified dictionary is on the form
-    {"name1": 3, "name2": 5}
-
-    Note that the input must be an OrderedDict!
-
-    """
-    if sdict is None:
-        return None
-
-    if not isinstance(sdict, OrderedDict):
-        raise ValueError("Input sdict is not an OrderedDict")
-
-    newsub = OrderedDict()
-
-    inn1 = 1
-    for name, nsub in sdict.items():
-        inn2 = inn1 + nsub
-        newsub[name] = range(inn1, inn2)
-        inn1 = inn2
-
-    return newsub
-
-
-def handle_metadata(result, meta, ncol, nrow, nlay):
-    # meta _optional_ *may* contain xshift, xscale etc which in case must be taken
-    # into account
-    coordsv = result["coordsv"]
-    zcornsv = result["zcornsv"]
-    actnumsv = result["actnumsv"]
-    nncol = ncol + 1
-    nnrow = nrow + 1
-    nnlay = nlay + 1
-    req = meta["_required_"]
-    shi = req["xshift"]
-    coordsv[0::3] = np.where(shi != 0, coordsv[0::3] + shi, coordsv[0::3])
-    sca = req["xscale"]
-    coordsv[0::3] = np.where(sca != 1, coordsv[0::3] * sca, coordsv[0::3])
-    shi = req["yshift"]
-    coordsv[1::3] = np.where(shi != 0, coordsv[1::3] + shi, coordsv[1::3])
-    sca = req["yscale"]
-    coordsv[1::3] = np.where(sca != 1, coordsv[1::3] * sca, coordsv[1::3])
-    shi = req["zshift"]
-    coordsv[2::3] = np.where(shi != 0, coordsv[2::3] + shi, coordsv[2::3])
-    sca = req["zscale"]
-    coordsv[2::3] = np.where(sca != 1, coordsv[2::3] * sca, coordsv[2::3])
-
-    result["coordsv"] = coordsv.reshape((nncol, nnrow, 6)).astype(np.float64)
-    result["zcornsv"] = zcornsv.reshape((nncol, nnrow, nnlay, 4)).astype(np.float32)
-    result["actnumsv"] = actnumsv.reshape((ncol, nrow, nlay)).astype(np.int32)
-    if "subgrids" in req:
-        result["subgrids"] = convert_subgrids(req["subgrids"])
-
-
-def import_xtgcpgeom(
-    mfile, mmap=False
-):  # pylint: disable=too-many-locals, too-many-statements
-    """Using pure python for experimental grid geometry import."""
-    #
-    offset = 36
-    with open(mfile.file, "rb") as fhandle:
-        buf = fhandle.read(offset)
-
-    # unpack header
-    swap, magic, nformat, ncol, nrow, nlay = unpack("= i i i q q q", buf)
-    nncol = ncol + 1
-    nnrow = nrow + 1
-    nnlay = nlay + 1
-
-    if swap != 1 or magic != 1301:
-        raise ValueError(f"Error, swap magic are {swap} {magic}, expected is 1 1301")
-
-    # subformat processing, indicating number of bytes per datatype
-    # here, 844 is native XTGeo (float64, float32, int32)
-    if nformat not in (444, 844, 841, 881, 884):
-        raise ValueError(f"The subformat value {nformat} is not valid")
-
-    coordfmt, zcornfmt, actnumfmt = [int(nbyte) for nbyte in str(nformat)]
-
-    dtype_coordsv = "float" + str(coordfmt * 8)
-    dtype_zcornsv = "float" + str(zcornfmt * 8)
-    dtype_actnumv = "int" + str(actnumfmt * 8)
-
-    ncoord = nncol * nnrow * 6
-    nzcorn = nncol * nnrow * nnlay * 4
-    nactnum = ncol * nrow * nlay
-    result = dict()
-    # read numpy arrays from file
-    result["coordsv"] = xsys.npfromfile(
-        mfile.file, dtype=dtype_coordsv, count=ncoord, offset=offset, mmap=mmap
-    )
-    newoffset = offset + ncoord * coordfmt
-    result["zcornsv"] = xsys.npfromfile(
-        mfile.file, dtype=dtype_zcornsv, count=nzcorn, offset=newoffset, mmap=mmap
-    )
-    newoffset += nzcorn * zcornfmt
-    result["actnumsv"] = xsys.npfromfile(
-        mfile.file, dtype=dtype_actnumv, count=nactnum, offset=newoffset, mmap=mmap
-    )
-    newoffset += nactnum * actnumfmt
-
-    # read metadata which will be at position offet + nfloat*narr +13
-    pos = newoffset + 13
-    with open(mfile.file, "rb") as fhandle:
-        fhandle.seek(pos)
-        jmeta = fhandle.read().decode()
-
-    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-
-    handle_metadata(result, meta, ncol, nrow, nlay)
-    return result
-
-
-def import_hdf5_cpgeom(mfile, ijkrange=None, zerobased=False):
-    """Experimental grid geometry import using hdf5."""
-    #
-    with h5py.File(mfile.name, "r") as h5h:
-        grp = h5h["CornerPointGeometry"]
-
-        idcode = grp.attrs["format-idcode"]
-        provider = grp.attrs["provider"]
-        if idcode != 1301:
-            raise ValueError(f"Wrong id code: {idcode}")
-        logger.info("Provider is %s", provider)
-
-        jmeta = grp.attrs["metadata"]
-        meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-
-        req = meta["_required_"]
-        ncol = req["ncol"]
-        nrow = req["nrow"]
-        nlay = req["nlay"]
-
-        if ijkrange is not None:
-            incoord, inzcorn, inactnum, ncol, nrow, nlay = _partial_read(
-                h5h, req, ijkrange, zerobased
-            )
-            if (
-                "subgrids" in meta["_required_"]
-                and meta["_required_"]["subgrids"] is not None
-            ):
-                meta["_required_"]["subgrids"] = filter_subgrids_partial(
-                    meta["_required_"]["subgrids"], *ijkrange[-2:], nlay, zerobased
-                )
-        else:
-            incoord = grp["coord"][:, :, :]
-            inzcorn = grp["zcorn"][:, :, :, :]
-            inactnum = grp["actnum"][:, :, :]
-
-    result = {}
-    result["coordsv"] = incoord.astype("float64")
-    result["zcornsv"] = inzcorn.astype("float32")
-    result["actnumsv"] = inactnum.astype("float32")
-
-    handle_metadata(result, meta, ncol, nrow, nlay)
-    return result
-
-
-def filter_subgrids_partial(subgrids, k1, k2, nlay, zerobased):
-    """
-    Filters and truncates the subgrids of the global grid so that they
-    refer to the filtered grid.
-
-    >>> filter_subgrids_partial(
-    ...   {"subgrid1": 4, "subgrid2": 1, "subgrid3":9},
-    ...   4,
-    ...   5,
-    ...   12,
-    ...   True
-    ... )
-    OrderedDict([('subgrid2', 1), ('subgrid3', 1)])
-
-    Args:
-        subgrids: The OrderedDict of subgrids.
-        k1: Start of subgrid layers (can be "min" to mean 0 or 1 dependent on zerobased)
-        k2: End of subgrid layers (cna be "max" to mean nlay or nlay -1
-            dependent on zerobased.
-        nlay: Original number of layers
-        zerobased: Whether indexing starts with 0 or 1.
-
-    Returns:
-        New Orderedict which is the subgrids dictionary with out of range
-        subgrids removed or truncated.
-
-    """
-    if k1 == "min":
-        k1 = 0 if zerobased else 1
-
-    if k2 == "max":
-        k2 = nlay - 1 if zerobased else nlay
-
-    # convert k1 and k2 to zero based
-    if not zerobased:
-        k1 -= 1
-        k2 -= 1
-
-    partial_subgrid = OrderedDict()
-    start = 0
-    for key, value in subgrids.items():
-        end = value + start
-        partial_start = max(start, k1)
-        partial_end = min(end, k2 + 1)
-        if partial_end - partial_start > 0:
-            partial_subgrid[key] = partial_end - partial_start
-        start = end
-    return partial_subgrid
-
-
-def _partial_read(h5h, req, ijkrange, zerobased):
-    """Read a partial IJ range."""
-    ncol = req["ncol"]
-    nrow = req["nrow"]
-    nlay = req["nlay"]
-
-    if len(ijkrange) != 6:
-        raise ValueError("The ijkrange list must have 6 elements")
-
-    i1, i2, j1, j2, k1, k2 = ijkrange
-
-    if i1 == "min":
-        i1 = 0 if zerobased else 1
-    if j1 == "min":
-        j1 = 0 if zerobased else 1
-    if k1 == "min":
-        k1 = 0 if zerobased else 1
-
-    if i2 == "max":
-        i2 = ncol - 1 if zerobased else ncol
-    if j2 == "max":
-        j2 = nrow - 1 if zerobased else nrow
-    if k2 == "max":
-        k2 = nlay - 1 if zerobased else nlay
-
-    if not zerobased:
-        i1 -= 1
-        i2 -= 1
-        j1 -= 1
-        j2 -= 1
-        k1 -= 1
-        k2 -= 1
-
-    ncol2 = i2 - i1 + 1
-    nrow2 = j2 - j1 + 1
-    nlay2 = k2 - k1 + 1
-
-    if (
-        ncol2 < 1
-        or ncol2 > ncol
-        or nrow2 < 1
-        or nrow2 > nrow
-        or nlay2 < 1
-        or nlay2 > nlay
-    ):
-        raise ValueError("The ijkrange spesification exceeds boundaries.")
-
-    nncol2 = ncol2 + 1
-    nnrow2 = nrow2 + 1
-    nnlay2 = nlay2 + 1
-
-    dset = h5h["CornerPointGeometry/coord"]
-    cv = dset[i1 : i1 + nncol2, j1 : j1 + nnrow2, :]
-
-    dset = h5h["CornerPointGeometry/zcorn"]
-    zv = dset[i1 : i1 + nncol2, j1 : j1 + nnrow2, k1 : k1 + nnlay2, :]
-
-    dset = h5h["CornerPointGeometry/actnum"]
-    av = dset[i1 : i1 + ncol2, j1 : j1 + nrow2, k1 : k1 + nlay2]
-
-    return cv, zv, av, ncol2, nrow2, nlay2
+# coding: utf-8
+"""Private module, Grid Import private functions for xtgeo based formats."""
+
+import json
+from collections import OrderedDict
+from struct import unpack
+
+import h5py
+import numpy as np
+
+import xtgeo
+import xtgeo.common.sys as xsys
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def convert_subgrids(sdict):
+    """Convert a simplified ordered dictionary to
+        subgrid required by Grid.
+
+    The simplified dictionary is on the form
+    {"name1": 3, "name2": 5}
+
+    Note that the input must be an OrderedDict!
+
+    """
+    if sdict is None:
+        return None
+
+    if not isinstance(sdict, OrderedDict):
+        raise ValueError("Input sdict is not an OrderedDict")
+
+    newsub = OrderedDict()
+
+    inn1 = 1
+    for name, nsub in sdict.items():
+        inn2 = inn1 + nsub
+        newsub[name] = range(inn1, inn2)
+        inn1 = inn2
+
+    return newsub
+
+
+def handle_metadata(result, meta, ncol, nrow, nlay):
+    # meta _optional_ *may* contain xshift, xscale etc which in case must be taken
+    # into account
+    coordsv = result["coordsv"]
+    zcornsv = result["zcornsv"]
+    actnumsv = result["actnumsv"]
+    nncol = ncol + 1
+    nnrow = nrow + 1
+    nnlay = nlay + 1
+    req = meta["_required_"]
+    shi = req["xshift"]
+    coordsv[0::3] = np.where(shi != 0, coordsv[0::3] + shi, coordsv[0::3])
+    sca = req["xscale"]
+    coordsv[0::3] = np.where(sca != 1, coordsv[0::3] * sca, coordsv[0::3])
+    shi = req["yshift"]
+    coordsv[1::3] = np.where(shi != 0, coordsv[1::3] + shi, coordsv[1::3])
+    sca = req["yscale"]
+    coordsv[1::3] = np.where(sca != 1, coordsv[1::3] * sca, coordsv[1::3])
+    shi = req["zshift"]
+    coordsv[2::3] = np.where(shi != 0, coordsv[2::3] + shi, coordsv[2::3])
+    sca = req["zscale"]
+    coordsv[2::3] = np.where(sca != 1, coordsv[2::3] * sca, coordsv[2::3])
+
+    result["coordsv"] = coordsv.reshape((nncol, nnrow, 6)).astype(np.float64)
+    result["zcornsv"] = zcornsv.reshape((nncol, nnrow, nnlay, 4)).astype(np.float32)
+    result["actnumsv"] = actnumsv.reshape((ncol, nrow, nlay)).astype(np.int32)
+    if "subgrids" in req:
+        result["subgrids"] = convert_subgrids(req["subgrids"])
+
+
+def import_xtgcpgeom(
+    mfile, mmap=False
+):  # pylint: disable=too-many-locals, too-many-statements
+    """Using pure python for experimental grid geometry import."""
+    #
+    offset = 36
+    with open(mfile.file, "rb") as fhandle:
+        buf = fhandle.read(offset)
+
+    # unpack header
+    swap, magic, nformat, ncol, nrow, nlay = unpack("= i i i q q q", buf)
+    nncol = ncol + 1
+    nnrow = nrow + 1
+    nnlay = nlay + 1
+
+    if swap != 1 or magic != 1301:
+        raise ValueError(f"Error, swap magic are {swap} {magic}, expected is 1 1301")
+
+    # subformat processing, indicating number of bytes per datatype
+    # here, 844 is native XTGeo (float64, float32, int32)
+    if nformat not in (444, 844, 841, 881, 884):
+        raise ValueError(f"The subformat value {nformat} is not valid")
+
+    coordfmt, zcornfmt, actnumfmt = [int(nbyte) for nbyte in str(nformat)]
+
+    dtype_coordsv = "float" + str(coordfmt * 8)
+    dtype_zcornsv = "float" + str(zcornfmt * 8)
+    dtype_actnumv = "int" + str(actnumfmt * 8)
+
+    ncoord = nncol * nnrow * 6
+    nzcorn = nncol * nnrow * nnlay * 4
+    nactnum = ncol * nrow * nlay
+    result = dict()
+    # read numpy arrays from file
+    result["coordsv"] = xsys.npfromfile(
+        mfile.file, dtype=dtype_coordsv, count=ncoord, offset=offset, mmap=mmap
+    )
+    newoffset = offset + ncoord * coordfmt
+    result["zcornsv"] = xsys.npfromfile(
+        mfile.file, dtype=dtype_zcornsv, count=nzcorn, offset=newoffset, mmap=mmap
+    )
+    newoffset += nzcorn * zcornfmt
+    result["actnumsv"] = xsys.npfromfile(
+        mfile.file, dtype=dtype_actnumv, count=nactnum, offset=newoffset, mmap=mmap
+    )
+    newoffset += nactnum * actnumfmt
+
+    # read metadata which will be at position offet + nfloat*narr +13
+    pos = newoffset + 13
+    with open(mfile.file, "rb") as fhandle:
+        fhandle.seek(pos)
+        jmeta = fhandle.read().decode()
+
+    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+
+    handle_metadata(result, meta, ncol, nrow, nlay)
+    return result
+
+
+def import_hdf5_cpgeom(mfile, ijkrange=None, zerobased=False):
+    """Experimental grid geometry import using hdf5."""
+    #
+    with h5py.File(mfile.name, "r") as h5h:
+        grp = h5h["CornerPointGeometry"]
+
+        idcode = grp.attrs["format-idcode"]
+        provider = grp.attrs["provider"]
+        if idcode != 1301:
+            raise ValueError(f"Wrong id code: {idcode}")
+        logger.info("Provider is %s", provider)
+
+        jmeta = grp.attrs["metadata"]
+        meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+
+        req = meta["_required_"]
+        ncol = req["ncol"]
+        nrow = req["nrow"]
+        nlay = req["nlay"]
+
+        if ijkrange is not None:
+            incoord, inzcorn, inactnum, ncol, nrow, nlay = _partial_read(
+                h5h, req, ijkrange, zerobased
+            )
+            if (
+                "subgrids" in meta["_required_"]
+                and meta["_required_"]["subgrids"] is not None
+            ):
+                meta["_required_"]["subgrids"] = filter_subgrids_partial(
+                    meta["_required_"]["subgrids"], *ijkrange[-2:], nlay, zerobased
+                )
+        else:
+            incoord = grp["coord"][:, :, :]
+            inzcorn = grp["zcorn"][:, :, :, :]
+            inactnum = grp["actnum"][:, :, :]
+
+    result = {}
+    result["coordsv"] = incoord.astype("float64")
+    result["zcornsv"] = inzcorn.astype("float32")
+    result["actnumsv"] = inactnum.astype("float32")
+
+    handle_metadata(result, meta, ncol, nrow, nlay)
+    return result
+
+
+def filter_subgrids_partial(subgrids, k1, k2, nlay, zerobased):
+    """
+    Filters and truncates the subgrids of the global grid so that they
+    refer to the filtered grid.
+
+    >>> filter_subgrids_partial(
+    ...   {"subgrid1": 4, "subgrid2": 1, "subgrid3":9},
+    ...   4,
+    ...   5,
+    ...   12,
+    ...   True
+    ... )
+    OrderedDict([('subgrid2', 1), ('subgrid3', 1)])
+
+    Args:
+        subgrids: The OrderedDict of subgrids.
+        k1: Start of subgrid layers (can be "min" to mean 0 or 1 dependent on zerobased)
+        k2: End of subgrid layers (cna be "max" to mean nlay or nlay -1
+            dependent on zerobased.
+        nlay: Original number of layers
+        zerobased: Whether indexing starts with 0 or 1.
+
+    Returns:
+        New Orderedict which is the subgrids dictionary with out of range
+        subgrids removed or truncated.
+
+    """
+    if k1 == "min":
+        k1 = 0 if zerobased else 1
+
+    if k2 == "max":
+        k2 = nlay - 1 if zerobased else nlay
+
+    # convert k1 and k2 to zero based
+    if not zerobased:
+        k1 -= 1
+        k2 -= 1
+
+    partial_subgrid = OrderedDict()
+    start = 0
+    for key, value in subgrids.items():
+        end = value + start
+        partial_start = max(start, k1)
+        partial_end = min(end, k2 + 1)
+        if partial_end - partial_start > 0:
+            partial_subgrid[key] = partial_end - partial_start
+        start = end
+    return partial_subgrid
+
+
+def _partial_read(h5h, req, ijkrange, zerobased):
+    """Read a partial IJ range."""
+    ncol = req["ncol"]
+    nrow = req["nrow"]
+    nlay = req["nlay"]
+
+    if len(ijkrange) != 6:
+        raise ValueError("The ijkrange list must have 6 elements")
+
+    i1, i2, j1, j2, k1, k2 = ijkrange
+
+    if i1 == "min":
+        i1 = 0 if zerobased else 1
+    if j1 == "min":
+        j1 = 0 if zerobased else 1
+    if k1 == "min":
+        k1 = 0 if zerobased else 1
+
+    if i2 == "max":
+        i2 = ncol - 1 if zerobased else ncol
+    if j2 == "max":
+        j2 = nrow - 1 if zerobased else nrow
+    if k2 == "max":
+        k2 = nlay - 1 if zerobased else nlay
+
+    if not zerobased:
+        i1 -= 1
+        i2 -= 1
+        j1 -= 1
+        j2 -= 1
+        k1 -= 1
+        k2 -= 1
+
+    ncol2 = i2 - i1 + 1
+    nrow2 = j2 - j1 + 1
+    nlay2 = k2 - k1 + 1
+
+    if (
+        ncol2 < 1
+        or ncol2 > ncol
+        or nrow2 < 1
+        or nrow2 > nrow
+        or nlay2 < 1
+        or nlay2 > nlay
+    ):
+        raise ValueError("The ijkrange spesification exceeds boundaries.")
+
+    nncol2 = ncol2 + 1
+    nnrow2 = nrow2 + 1
+    nnlay2 = nlay2 + 1
+
+    dset = h5h["CornerPointGeometry/coord"]
+    cv = dset[i1 : i1 + nncol2, j1 : j1 + nnrow2, :]
+
+    dset = h5h["CornerPointGeometry/zcorn"]
+    zv = dset[i1 : i1 + nncol2, j1 : j1 + nnrow2, k1 : k1 + nnlay2, :]
+
+    dset = h5h["CornerPointGeometry/actnum"]
+    av = dset[i1 : i1 + ncol2, j1 : j1 + nrow2, k1 : k1 + nlay2]
+
+    return cv, zv, av, ncol2, nrow2, nlay2
```

## xtgeo/grid3d/_grid_refine.py

 * *Ordering differences only*

```diff
@@ -1,120 +1,120 @@
-# -*- coding: utf-8 -*-
-"""Private module for refinement of a grid."""
-from collections import OrderedDict
-import numpy as np
-
-from xtgeo.common import XTGeoDialog
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-# pylint: disable=too-many-branches
-# pylint: disable=too-many-statements
-
-
-def refine_vertically(self, rfactor, zoneprop=None):
-    """Refine vertically, proportionally.
-
-    See details in caller.
-    """
-    self._xtgformat1()
-
-    rfactord = OrderedDict()
-
-    # case 1 rfactor as scalar value.
-    if isinstance(rfactor, int):
-        if self.subgrids:
-            subgrids = self.get_subgrids()
-            for i, _ in enumerate(self.subgrids.keys()):
-                rfactord[i + 1] = rfactor
-        else:
-            rfactord[0] = rfactor
-            subgrids = OrderedDict()
-            subgrids[1] = self.nlay
-
-    # case 2 rfactor is a dict
-    else:
-        rfactord = OrderedDict(sorted(rfactor.items()))  # redefined to ordered
-        # 2a: zoneprop is present
-        if zoneprop is not None:
-            oldsubgrids = None
-            if self.subgrids:
-                oldsubgrids = self.get_subgrids()
-
-            subgrids = self.subgrids_from_zoneprop(zoneprop)
-
-            if oldsubgrids:
-                if subgrids.values() != oldsubgrids.values():
-                    xtg.warn("ISSUES!!!")
-
-        # 2b: zoneprop is not present
-        elif zoneprop is None and self.subgrids:
-            subgrids = self.get_subgrids()
-
-        elif zoneprop is None and not self.subgrids:
-            raise ValueError(
-                "You gave in a dict, but no zoneprops and "
-                "subgrids are not preesent in the grid"
-            )
-        else:
-            raise ValueError("Some major unexpected issue in routine...")
-
-    if len(subgrids) != len(rfactord):
-        raise RuntimeError("Subgrids and refinements: different definition!")
-
-    self.set_subgrids(subgrids)
-
-    # Now, based on dict, give a value per subgrid for key, val in rfactor
-    newsubgrids = OrderedDict()
-    newnlay = 0
-    for (_x, rfi), (snam, sran) in zip(rfactord.items(), subgrids.items()):
-        newsubgrids[snam] = sran * rfi
-        newnlay += newsubgrids[snam]
-
-    logger.debug("New layers: %s", newnlay)
-
-    # refinefactors is an array with length nlay; has N refinements per single K layer
-    refinefactors = _cxtgeo.new_intarray(self.nlay)
-
-    totvector = []
-
-    for (_, rfi), (_, arr) in zip(rfactord.items(), self.subgrids.items()):
-        for _ in range(len(arr)):
-            totvector.append(rfi)
-
-    for inn, rfi in enumerate(totvector):
-        _cxtgeo.intarray_setitem(refinefactors, inn, rfi)
-
-    ref_zcornsv = np.zeros(self.ncol * self.nrow * (newnlay + 1) * 4, dtype=np.float64)
-    ref_actnumsv = np.zeros(self.ncol * self.nrow * newnlay, dtype=np.int32)
-
-    ier = _cxtgeo.grd3d_refine_vert(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._zcornsv,
-        self._actnumsv,
-        newnlay,
-        ref_zcornsv,
-        ref_actnumsv,
-        refinefactors,
-    )
-
-    if ier != 0:
-        raise RuntimeError(
-            f"An error occured in the C routine grd3d_refine_vert, code {ier}"
-        )
-
-    # update instance:
-    self._nlay = newnlay
-    self._zcornsv = ref_zcornsv
-    self._actnumsv = ref_actnumsv
-
-    if self.subgrids is None or len(self.subgrids) <= 1:
-        self.subgrids = None
-    else:
-        self.set_subgrids(newsubgrids)
-
-    return self
+# -*- coding: utf-8 -*-
+"""Private module for refinement of a grid."""
+from collections import OrderedDict
+import numpy as np
+
+from xtgeo.common import XTGeoDialog
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+# pylint: disable=too-many-branches
+# pylint: disable=too-many-statements
+
+
+def refine_vertically(self, rfactor, zoneprop=None):
+    """Refine vertically, proportionally.
+
+    See details in caller.
+    """
+    self._xtgformat1()
+
+    rfactord = OrderedDict()
+
+    # case 1 rfactor as scalar value.
+    if isinstance(rfactor, int):
+        if self.subgrids:
+            subgrids = self.get_subgrids()
+            for i, _ in enumerate(self.subgrids.keys()):
+                rfactord[i + 1] = rfactor
+        else:
+            rfactord[0] = rfactor
+            subgrids = OrderedDict()
+            subgrids[1] = self.nlay
+
+    # case 2 rfactor is a dict
+    else:
+        rfactord = OrderedDict(sorted(rfactor.items()))  # redefined to ordered
+        # 2a: zoneprop is present
+        if zoneprop is not None:
+            oldsubgrids = None
+            if self.subgrids:
+                oldsubgrids = self.get_subgrids()
+
+            subgrids = self.subgrids_from_zoneprop(zoneprop)
+
+            if oldsubgrids:
+                if subgrids.values() != oldsubgrids.values():
+                    xtg.warn("ISSUES!!!")
+
+        # 2b: zoneprop is not present
+        elif zoneprop is None and self.subgrids:
+            subgrids = self.get_subgrids()
+
+        elif zoneprop is None and not self.subgrids:
+            raise ValueError(
+                "You gave in a dict, but no zoneprops and "
+                "subgrids are not preesent in the grid"
+            )
+        else:
+            raise ValueError("Some major unexpected issue in routine...")
+
+    if len(subgrids) != len(rfactord):
+        raise RuntimeError("Subgrids and refinements: different definition!")
+
+    self.set_subgrids(subgrids)
+
+    # Now, based on dict, give a value per subgrid for key, val in rfactor
+    newsubgrids = OrderedDict()
+    newnlay = 0
+    for (_x, rfi), (snam, sran) in zip(rfactord.items(), subgrids.items()):
+        newsubgrids[snam] = sran * rfi
+        newnlay += newsubgrids[snam]
+
+    logger.debug("New layers: %s", newnlay)
+
+    # refinefactors is an array with length nlay; has N refinements per single K layer
+    refinefactors = _cxtgeo.new_intarray(self.nlay)
+
+    totvector = []
+
+    for (_, rfi), (_, arr) in zip(rfactord.items(), self.subgrids.items()):
+        for _ in range(len(arr)):
+            totvector.append(rfi)
+
+    for inn, rfi in enumerate(totvector):
+        _cxtgeo.intarray_setitem(refinefactors, inn, rfi)
+
+    ref_zcornsv = np.zeros(self.ncol * self.nrow * (newnlay + 1) * 4, dtype=np.float64)
+    ref_actnumsv = np.zeros(self.ncol * self.nrow * newnlay, dtype=np.int32)
+
+    ier = _cxtgeo.grd3d_refine_vert(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._zcornsv,
+        self._actnumsv,
+        newnlay,
+        ref_zcornsv,
+        ref_actnumsv,
+        refinefactors,
+    )
+
+    if ier != 0:
+        raise RuntimeError(
+            f"An error occured in the C routine grd3d_refine_vert, code {ier}"
+        )
+
+    # update instance:
+    self._nlay = newnlay
+    self._zcornsv = ref_zcornsv
+    self._actnumsv = ref_actnumsv
+
+    if self.subgrids is None or len(self.subgrids) <= 1:
+        self.subgrids = None
+    else:
+        self.set_subgrids(newsubgrids)
+
+    return self
```

## xtgeo/grid3d/_grid_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,514 +1,514 @@
-# -*- coding: utf-8 -*-
-"""Roxar API functions for XTGeo Grid Geometry."""
-import os
-import tempfile
-from collections import OrderedDict
-
-import numpy as np
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo import RoxUtils
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-# logger.info(roxmsg)
-
-# self is Grid() instance
-
-
-# ======================================================================================
-# Load/import
-# ======================================================================================
-
-
-def import_grid_roxapi(
-    projectname, gname, realisation, dimonly, info
-):  # pragma: no cover
-    """Import a Grid via ROXAR API spec.
-
-    Returns:
-        dictionary of parameters to be used in the Grid constructor function.
-
-    """
-    rox = RoxUtils(projectname, readonly=True)
-    if dimonly:
-        return _import_grid_roxapi_v1(rox, gname, realisation, dimonly, info)
-    else:
-        return _import_grid_roxapi_v2(rox, gname, realisation, info)
-
-
-def _import_grid_roxapi_v1(rox, gname, realisation, dimonly, info):  # pragma: no cover
-    """Import a Grid via ROXAR API spec."""
-
-    proj = rox.project
-
-    logger.info("Loading grid with realisation %s...", realisation)
-    try:
-        if gname not in proj.grid_models:
-            raise KeyError(f"No such gridmodel: {gname}")
-
-        logger.info("Get roxgrid...")
-        roxgrid = proj.grid_models[gname].get_grid(realisation=realisation)
-
-        if dimonly:
-            corners = None
-        else:
-            logger.info("Get corners...")
-            corners = roxgrid.get_cell_corners_by_index()
-            logger.info("Get corners... done")
-
-        if info:
-            _display_roxapi_grid_info(rox, roxgrid)
-
-        result = _convert_to_xtgeo_grid_v1(rox, roxgrid, corners, gname)
-
-    except KeyError as keyerror:
-        raise RuntimeError(keyerror)
-
-    if rox._roxexternal:
-        rox.safe_close()
-
-    return result
-
-
-def _display_roxapi_grid_info(rox, roxgrid):  # pragma: no cover
-    """Push info to screen (mostly for debugging), experimental."""
-
-    indexer = roxgrid.grid_indexer
-    ncol, nrow, _ = indexer.dimensions
-
-    xtg.say("ROXAPI with support for CornerPointGeometry")
-    geom = roxgrid.get_geometry()
-    defined_cells = geom.get_defined_cells()
-    xtg.say(f"Defined cells \n{defined_cells}")
-
-    xtg.say(f"IJK handedness: {geom.ijk_handedness}")
-    for ipi in range(ncol + 1):
-        for jpi in range(nrow + 1):
-            tpi, bpi, zco = geom.get_pillar_data(ipi, jpi)
-            xtg.say(f"For pillar {ipi}, {jpi}\n")
-            xtg.say(f"Tops\n{tpi}")
-            xtg.say(f"Bots\n{bpi}")
-            xtg.say(f"Depths\n{zco}")
-
-
-def _convert_to_xtgeo_grid_v1(rox, roxgrid, corners, gname):  # pragma: no cover
-    """Convert from RMS API to XTGeo API."""
-    # pylint: disable=too-many-statements
-
-    logger.info("Converting to XTGeo internals...")
-    logger.info("Call the ROXAPI grid indexer")
-    indexer = roxgrid.grid_indexer
-
-    ncol, nrow, nlay = indexer.dimensions
-    ntot = ncol * nrow * nlay
-
-    result = {}
-    result["name"] = gname
-
-    if corners is None:
-        logger.info("Asked for dimensions_only: No geometry read!")
-        return
-
-    logger.info("Get active cells")
-    mybuffer = np.ndarray(indexer.dimensions, dtype=np.int32)
-
-    mybuffer.fill(0)
-
-    logger.info("Get cell numbers")
-    cellno = indexer.get_cell_numbers_in_range((0, 0, 0), indexer.dimensions)
-
-    logger.info("Reorder...")
-    ijk = indexer.get_indices(cellno)
-
-    iind = ijk[:, 0]
-    jind = ijk[:, 1]
-    kind = ijk[:, 2]
-
-    pvalues = np.ones(len(cellno))
-    pvalues[cellno] = 1
-    mybuffer[iind, jind, kind] = pvalues[cellno]
-
-    actnum = mybuffer
-
-    logger.info("Handedness (new) %s", indexer.ijk_handedness)
-
-    corners = corners.ravel()
-    actnum = actnum.ravel()
-
-    ntot = ncol * nrow * nlay
-    ncoord = (ncol + 1) * (nrow + 1) * 2 * 3
-    nzcorn = ncol * nrow * (nlay + 1) * 4
-
-    coordsv = np.zeros(ncoord, dtype=np.float64)
-    zcornsv = np.zeros(nzcorn, dtype=np.float64)
-    actnumsv = np.zeros(ntot, dtype=np.int32)
-
-    # next task is to convert geometry to cxtgeo internal format
-    logger.info("Run XTGeo C code...")
-    _cxtgeo.grd3d_conv_roxapi_grid(
-        ncol,
-        nrow,
-        nlay,
-        ntot,
-        actnum,
-        corners,
-        coordsv,
-        zcornsv,
-        actnumsv,
-    )
-
-    # convert to xtgformat=2
-    newcoordsv = np.zeros((ncol + 1, nrow + 1, 6), dtype=np.float64)
-    newzcornsv = np.zeros((ncol + 1, nrow + 1, nlay + 1, 4), dtype=np.float32)
-    newactnumsv = np.zeros((ncol, nrow, nlay), dtype=np.int32)
-
-    _cxtgeo.grd3cp3d_xtgformat1to2_geom(
-        ncol,
-        nrow,
-        nlay,
-        coordsv,
-        newcoordsv,
-        zcornsv,
-        newzcornsv,
-        actnumsv,
-        newactnumsv,
-    )
-
-    result["coordsv"] = newcoordsv
-    result["zcornsv"] = newzcornsv
-    result["actnumsv"] = newactnumsv
-    logger.info("Run XTGeo C code... done")
-    logger.info("Converting to XTGeo internals... done")
-
-    del corners
-    del actnum
-
-    # subgrids
-    if len(indexer.zonation) > 1:
-        logger.debug("Zonation length (N subzones) is %s", len(indexer.zonation))
-        subz = OrderedDict()
-        for inum, zrange in indexer.zonation.items():
-            logger.debug("inum: %s, zrange: %s", inum, zrange)
-            zname = roxgrid.zone_names[inum]
-            logger.debug("zname is: %s", zname)
-            zra = [nn + 1 for ira in zrange for nn in ira]  # nested lists
-            subz[zname] = zra
-
-        result["subgrids"] = subz
-
-    result["roxgrid"] = roxgrid
-    result["roxindexer"] = indexer
-
-    return result
-
-
-def _import_grid_roxapi_v2(rox, gname, realisation, info):  # pragma: no cover
-    """Import a Grid via ROXAR API spec."""
-    proj = rox.project
-
-    logger.info("Loading grid with realisation %s...", realisation)
-    try:
-        if gname not in proj.grid_models:
-            raise KeyError(f"No such gridmodel: {gname}")
-
-        logger.info("Get roxgrid...")
-        roxgrid = proj.grid_models[gname].get_grid(realisation=realisation)
-
-        if roxgrid.has_dual_index_system:
-            xtg.warnuser(
-                f"The roxar grid {gname} has dual index system.\n"
-                "XTGeo does not implement extraction of simbox grid\n"
-                "and only considers physical index."
-            )
-
-        if info:
-            _display_roxapi_grid_info(rox, roxgrid)
-
-        result = _convert_to_xtgeo_grid_v2(roxgrid, gname)
-
-    except KeyError as keyerror:
-        raise RuntimeError(keyerror)
-
-    if rox._roxexternal:
-        rox.safe_close()
-
-    return result
-
-
-def _convert_to_xtgeo_grid_v2(roxgrid, gname):
-    """Convert from roxar CornerPointGeometry to xtgeo, version 2 using _xtgformat=2."""
-    indexer = roxgrid.grid_indexer
-
-    ncol, nrow, nlay = indexer.dimensions
-
-    # update other attributes
-    result = {}
-
-    nncol = ncol + 1
-    nnrow = nrow + 1
-    nnlay = nlay + 1
-
-    result["name"] = gname
-
-    geom = roxgrid.get_geometry()
-
-    coordsv = np.zeros((nncol, nnrow, 6), dtype=np.float64)
-    zcornsv = np.zeros((nncol, nnrow, nnlay, 4), dtype=np.float32)
-    actnumsv = np.zeros((ncol, nrow, nlay), dtype=np.int32)
-
-    for icol in range(nncol):
-        for jrow in range(nnrow):
-            topc, basc, zcorn = geom.get_pillar_data(icol, jrow)
-            coordsv[icol, jrow, 0:3] = topc
-            coordsv[icol, jrow, 3:6] = basc
-
-            zcorn = np.ma.filled(zcorn, fill_value=0.0)
-
-            zcornsv[icol, jrow, :, :] = zcorn.T
-
-    _cxtgeo.grdcp3d_process_edges(ncol, nrow, nlay, zcornsv)
-    result["coordsv"] = coordsv
-    result["zcornsv"] = zcornsv
-
-    actnumsv[geom.get_defined_cells()] = 1
-    result["actnumsv"] = actnumsv
-
-    # subgrids
-    if len(indexer.zonation) > 1:
-        logger.debug("Zonation length (N subzones) is %s", len(indexer.zonation))
-        subz = OrderedDict()
-        for inum, zrange in indexer.zonation.items():
-            logger.debug("inum: %s, zrange: %s", inum, zrange)
-            zname = roxgrid.zone_names[inum]
-            logger.debug("zname is: %s", zname)
-            zra = [nn + 1 for ira in zrange for nn in ira]  # nested lists
-            subz[zname] = zra
-
-        result["subgrids"] = subz
-
-    result["roxgrid"] = roxgrid
-    result["roxindexer"] = indexer
-
-    return result
-
-
-# ======================================================================================
-# Save/export
-# ======================================================================================
-
-
-def export_grid_roxapi(
-    self, projectname, gname, realisation, info=False, method="cpg"
-):  # pragma: no cover
-    """Export (i.e. store in RMS) via ROXAR API spec.
-
-    Using method 'cpg' means that the CPG method is applied (from ROXAPI 1.3).
-    This is possible from version ROXAPI ver 1.3, where the CornerPointGeometry
-    class is defined.
-
-    An alternative is to use simple roff import (via some /tmp area),
-    can be used from version 1.2
-
-    """
-    rox = RoxUtils(projectname, readonly=False)
-
-    if method == "cpg":
-        if self._xtgformat == 1:
-            _export_grid_cornerpoint_roxapi_v1(self, rox, gname, realisation, info)
-        else:
-            _export_grid_cornerpoint_roxapi_v2(self, rox, gname, realisation, info)
-
-    else:
-        _export_grid_viaroff_roxapi(self, rox, gname, realisation)
-
-    if rox._roxexternal:
-        rox.project.save()
-
-    rox.safe_close()
-
-
-def _export_grid_cornerpoint_roxapi_v1(
-    self, rox, gname, realisation, info
-):  # pragma: no cover
-    """Convert xtgeo geometry to pillar spec in ROXAPI and store."""
-    try:
-        from roxar.grids import CornerPointGridGeometry as CPG
-    except ImportError:
-        raise RuntimeError("Cannot load Roxar module")
-
-    logger.info("Load grid via CornerPointGridGeometry...")
-
-    grid_model = rox.project.grid_models.create(gname)
-    grid_model.set_empty(realisation)
-    grid = grid_model.get_grid(realisation)
-
-    geom = CPG.create(self.dimensions)
-
-    logger.info(geom)
-    scopy = self.copy()
-    scopy.make_zconsistent()
-    scopy._xtgformat1()
-    self._xtgformat1()
-
-    npill = (self.ncol + 1) * (self.nrow + 1) * 3
-    nzcrn = (self.ncol + 1) * (self.nrow + 1) * 4 * (self.nlay + 1)
-
-    _ier, tpi, bpi, zco = _cxtgeo.grd3d_conv_grid_roxapi(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        scopy._zcornsv,
-        self._actnumsv,
-        npill,
-        npill,
-        nzcrn,
-    )
-
-    tpi = tpi.reshape(self.ncol + 1, self.nrow + 1, 3)
-    bpi = bpi.reshape(self.ncol + 1, self.nrow + 1, 3)
-
-    zco = np.ma.masked_greater(zco, xtgeo.UNDEF_LIMIT)
-    zco = zco.reshape((self.ncol + 1, self.nrow + 1, 4, self.nlay + 1))
-
-    for ipi in range(self.ncol + 1):
-        for jpi in range(self.nrow + 1):
-            zzco = zco[ipi, jpi].reshape((self.nlay + 1), 4).T
-            geom.set_pillar_data(
-                ipi,
-                jpi,
-                top_point=tpi[ipi, jpi],
-                bottom_point=bpi[ipi, jpi],
-                depth_values=zzco,
-            )
-            if info and ipi < 5 and jpi < 5:
-                if ipi == 0 and jpi == 0:
-                    xtg.say("Showing info for i<5 and j<5 only!")
-                xtg.say(f"XTGeo pillar {ipi}, {jpi}\n")
-                xtg.say(f"XTGeo Tops\n{tpi[ipi, jpi]}")
-                xtg.say(f"XTGeo Bots\n{bpi[ipi, jpi]}")
-                xtg.say(f"XTGeo Depths\n{zzco}")
-
-    geom.set_defined_cells(self.get_actnum().values.astype(np.bool))
-    grid.set_geometry(geom)
-
-    _set_subgrids(self, rox, grid)
-
-
-def _export_grid_cornerpoint_roxapi_v2(
-    self, rox, gname, realisation, info
-):  # pragma: no cover
-    """Convert xtgeo geometry to pillar spec in ROXAPI and store _xtgformat=2."""
-    try:
-        from roxar.grids import CornerPointGridGeometry as CPG
-    except ImportError:
-        raise RuntimeError("Cannot load Roxar module")
-
-    grid_model = rox.project.grid_models.create(gname)
-    grid_model.set_empty(realisation)
-    grid = grid_model.get_grid(realisation)
-
-    geom = CPG.create(self.dimensions)
-
-    scopy = self.copy()
-    scopy.make_zconsistent()
-    scopy._xtgformat2()
-
-    npill = (self.ncol + 1) * (self.nrow + 1) * 3
-    nzcrn = (self.ncol + 1) * (self.nrow + 1) * (self.nlay + 1) * 4
-
-    _ier, tpi, bpi, zco = _cxtgeo.grdcp3d_conv_grid_roxapi(
-        self.ncol,
-        self.nrow,
-        self.nlay,
-        self._coordsv,
-        scopy._zcornsv,
-        npill,
-        npill,
-        nzcrn,
-    )
-
-    tpi = tpi.reshape(self.ncol + 1, self.nrow + 1, 3)
-    bpi = bpi.reshape(self.ncol + 1, self.nrow + 1, 3)
-
-    zco = np.ma.masked_greater(zco, xtgeo.UNDEF_LIMIT)
-    zco = zco.reshape((self.ncol + 1, self.nrow + 1, 4, self.nlay + 1))
-
-    for ipi in range(self.ncol + 1):
-        for jpi in range(self.nrow + 1):
-            zzco = zco[ipi, jpi].reshape((self.nlay + 1), 4).T
-            geom.set_pillar_data(
-                ipi,
-                jpi,
-                top_point=tpi[ipi, jpi],
-                bottom_point=bpi[ipi, jpi],
-                depth_values=zzco,
-            )
-            if info and ipi < 5 and jpi < 5:
-                if ipi == 0 and jpi == 0:
-                    xtg.say("Showing info for i<5 and j<5 only!")
-                xtg.say(f"XTGeo pillar {ipi}, {jpi}\n")
-                xtg.say(f"XTGeo Tops\n{tpi[ipi, jpi]}")
-                xtg.say(f"XTGeo Bots\n{bpi[ipi, jpi]}")
-                xtg.say(f"XTGeo Depths\n{zzco}")
-
-    geom.set_defined_cells(self._actnumsv.astype(np.bool))
-    grid.set_geometry(geom)
-    _set_subgrids(self, rox, grid)
-
-    del scopy
-
-
-def _set_subgrids(self, rox, grid):
-    """Export the subgrid index (zones) to Roxar API.
-
-    From roxar API:
-        set_zonation(zone_dict)
-
-            zone_dict A dictionary with start-layer (zero based) and name for each zone.
-
-    """
-
-    if not self.subgrids:
-        return
-
-    if rox.version_required("1.6"):
-        subs = self.subgrids
-        roxar_subs = {}
-        for name, zrange in subs.items():
-            roxar_subs[int(zrange[0] - 1)] = name
-
-        grid.set_zonation(roxar_subs)
-
-    else:
-        xtg.warnuser(
-            "Implementation of subgrids is lacking in Roxar API for this "
-            "RMS version. Will continue to store in RMS but without subgrid index."
-        )
-
-
-def _export_grid_viaroff_roxapi(self, rox, gname, realisation):  # pragma: no cover
-    """Convert xtgeo geometry to internal RMS via i/o ROFF tricks."""
-    logger.info("Realisation is %s", realisation)
-
-    # make a temporary folder and work within the with.. block
-    with tempfile.TemporaryDirectory() as tmpdir:
-        logger.info("Made a tmp folder: %s", tmpdir)
-
-        fname = os.path.join(tmpdir, gname)
-        self.to_file(fname)
-
-        grd = rox.project.grid_models
-
-        try:
-            del grd[gname]
-            logger.info("Overwriting existing grid in RMS")
-        except KeyError:
-            logger.info("Grid in RMS is new")
-
-        grd.load(fname)
+# -*- coding: utf-8 -*-
+"""Roxar API functions for XTGeo Grid Geometry."""
+import os
+import tempfile
+from collections import OrderedDict
+
+import numpy as np
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo import RoxUtils
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+# logger.info(roxmsg)
+
+# self is Grid() instance
+
+
+# ======================================================================================
+# Load/import
+# ======================================================================================
+
+
+def import_grid_roxapi(
+    projectname, gname, realisation, dimonly, info
+):  # pragma: no cover
+    """Import a Grid via ROXAR API spec.
+
+    Returns:
+        dictionary of parameters to be used in the Grid constructor function.
+
+    """
+    rox = RoxUtils(projectname, readonly=True)
+    if dimonly:
+        return _import_grid_roxapi_v1(rox, gname, realisation, dimonly, info)
+    else:
+        return _import_grid_roxapi_v2(rox, gname, realisation, info)
+
+
+def _import_grid_roxapi_v1(rox, gname, realisation, dimonly, info):  # pragma: no cover
+    """Import a Grid via ROXAR API spec."""
+
+    proj = rox.project
+
+    logger.info("Loading grid with realisation %s...", realisation)
+    try:
+        if gname not in proj.grid_models:
+            raise KeyError(f"No such gridmodel: {gname}")
+
+        logger.info("Get roxgrid...")
+        roxgrid = proj.grid_models[gname].get_grid(realisation=realisation)
+
+        if dimonly:
+            corners = None
+        else:
+            logger.info("Get corners...")
+            corners = roxgrid.get_cell_corners_by_index()
+            logger.info("Get corners... done")
+
+        if info:
+            _display_roxapi_grid_info(rox, roxgrid)
+
+        result = _convert_to_xtgeo_grid_v1(rox, roxgrid, corners, gname)
+
+    except KeyError as keyerror:
+        raise RuntimeError(keyerror)
+
+    if rox._roxexternal:
+        rox.safe_close()
+
+    return result
+
+
+def _display_roxapi_grid_info(rox, roxgrid):  # pragma: no cover
+    """Push info to screen (mostly for debugging), experimental."""
+
+    indexer = roxgrid.grid_indexer
+    ncol, nrow, _ = indexer.dimensions
+
+    xtg.say("ROXAPI with support for CornerPointGeometry")
+    geom = roxgrid.get_geometry()
+    defined_cells = geom.get_defined_cells()
+    xtg.say(f"Defined cells \n{defined_cells}")
+
+    xtg.say(f"IJK handedness: {geom.ijk_handedness}")
+    for ipi in range(ncol + 1):
+        for jpi in range(nrow + 1):
+            tpi, bpi, zco = geom.get_pillar_data(ipi, jpi)
+            xtg.say(f"For pillar {ipi}, {jpi}\n")
+            xtg.say(f"Tops\n{tpi}")
+            xtg.say(f"Bots\n{bpi}")
+            xtg.say(f"Depths\n{zco}")
+
+
+def _convert_to_xtgeo_grid_v1(rox, roxgrid, corners, gname):  # pragma: no cover
+    """Convert from RMS API to XTGeo API."""
+    # pylint: disable=too-many-statements
+
+    logger.info("Converting to XTGeo internals...")
+    logger.info("Call the ROXAPI grid indexer")
+    indexer = roxgrid.grid_indexer
+
+    ncol, nrow, nlay = indexer.dimensions
+    ntot = ncol * nrow * nlay
+
+    result = {}
+    result["name"] = gname
+
+    if corners is None:
+        logger.info("Asked for dimensions_only: No geometry read!")
+        return
+
+    logger.info("Get active cells")
+    mybuffer = np.ndarray(indexer.dimensions, dtype=np.int32)
+
+    mybuffer.fill(0)
+
+    logger.info("Get cell numbers")
+    cellno = indexer.get_cell_numbers_in_range((0, 0, 0), indexer.dimensions)
+
+    logger.info("Reorder...")
+    ijk = indexer.get_indices(cellno)
+
+    iind = ijk[:, 0]
+    jind = ijk[:, 1]
+    kind = ijk[:, 2]
+
+    pvalues = np.ones(len(cellno))
+    pvalues[cellno] = 1
+    mybuffer[iind, jind, kind] = pvalues[cellno]
+
+    actnum = mybuffer
+
+    logger.info("Handedness (new) %s", indexer.ijk_handedness)
+
+    corners = corners.ravel()
+    actnum = actnum.ravel()
+
+    ntot = ncol * nrow * nlay
+    ncoord = (ncol + 1) * (nrow + 1) * 2 * 3
+    nzcorn = ncol * nrow * (nlay + 1) * 4
+
+    coordsv = np.zeros(ncoord, dtype=np.float64)
+    zcornsv = np.zeros(nzcorn, dtype=np.float64)
+    actnumsv = np.zeros(ntot, dtype=np.int32)
+
+    # next task is to convert geometry to cxtgeo internal format
+    logger.info("Run XTGeo C code...")
+    _cxtgeo.grd3d_conv_roxapi_grid(
+        ncol,
+        nrow,
+        nlay,
+        ntot,
+        actnum,
+        corners,
+        coordsv,
+        zcornsv,
+        actnumsv,
+    )
+
+    # convert to xtgformat=2
+    newcoordsv = np.zeros((ncol + 1, nrow + 1, 6), dtype=np.float64)
+    newzcornsv = np.zeros((ncol + 1, nrow + 1, nlay + 1, 4), dtype=np.float32)
+    newactnumsv = np.zeros((ncol, nrow, nlay), dtype=np.int32)
+
+    _cxtgeo.grd3cp3d_xtgformat1to2_geom(
+        ncol,
+        nrow,
+        nlay,
+        coordsv,
+        newcoordsv,
+        zcornsv,
+        newzcornsv,
+        actnumsv,
+        newactnumsv,
+    )
+
+    result["coordsv"] = newcoordsv
+    result["zcornsv"] = newzcornsv
+    result["actnumsv"] = newactnumsv
+    logger.info("Run XTGeo C code... done")
+    logger.info("Converting to XTGeo internals... done")
+
+    del corners
+    del actnum
+
+    # subgrids
+    if len(indexer.zonation) > 1:
+        logger.debug("Zonation length (N subzones) is %s", len(indexer.zonation))
+        subz = OrderedDict()
+        for inum, zrange in indexer.zonation.items():
+            logger.debug("inum: %s, zrange: %s", inum, zrange)
+            zname = roxgrid.zone_names[inum]
+            logger.debug("zname is: %s", zname)
+            zra = [nn + 1 for ira in zrange for nn in ira]  # nested lists
+            subz[zname] = zra
+
+        result["subgrids"] = subz
+
+    result["roxgrid"] = roxgrid
+    result["roxindexer"] = indexer
+
+    return result
+
+
+def _import_grid_roxapi_v2(rox, gname, realisation, info):  # pragma: no cover
+    """Import a Grid via ROXAR API spec."""
+    proj = rox.project
+
+    logger.info("Loading grid with realisation %s...", realisation)
+    try:
+        if gname not in proj.grid_models:
+            raise KeyError(f"No such gridmodel: {gname}")
+
+        logger.info("Get roxgrid...")
+        roxgrid = proj.grid_models[gname].get_grid(realisation=realisation)
+
+        if roxgrid.has_dual_index_system:
+            xtg.warnuser(
+                f"The roxar grid {gname} has dual index system.\n"
+                "XTGeo does not implement extraction of simbox grid\n"
+                "and only considers physical index."
+            )
+
+        if info:
+            _display_roxapi_grid_info(rox, roxgrid)
+
+        result = _convert_to_xtgeo_grid_v2(roxgrid, gname)
+
+    except KeyError as keyerror:
+        raise RuntimeError(keyerror)
+
+    if rox._roxexternal:
+        rox.safe_close()
+
+    return result
+
+
+def _convert_to_xtgeo_grid_v2(roxgrid, gname):
+    """Convert from roxar CornerPointGeometry to xtgeo, version 2 using _xtgformat=2."""
+    indexer = roxgrid.grid_indexer
+
+    ncol, nrow, nlay = indexer.dimensions
+
+    # update other attributes
+    result = {}
+
+    nncol = ncol + 1
+    nnrow = nrow + 1
+    nnlay = nlay + 1
+
+    result["name"] = gname
+
+    geom = roxgrid.get_geometry()
+
+    coordsv = np.zeros((nncol, nnrow, 6), dtype=np.float64)
+    zcornsv = np.zeros((nncol, nnrow, nnlay, 4), dtype=np.float32)
+    actnumsv = np.zeros((ncol, nrow, nlay), dtype=np.int32)
+
+    for icol in range(nncol):
+        for jrow in range(nnrow):
+            topc, basc, zcorn = geom.get_pillar_data(icol, jrow)
+            coordsv[icol, jrow, 0:3] = topc
+            coordsv[icol, jrow, 3:6] = basc
+
+            zcorn = np.ma.filled(zcorn, fill_value=0.0)
+
+            zcornsv[icol, jrow, :, :] = zcorn.T
+
+    _cxtgeo.grdcp3d_process_edges(ncol, nrow, nlay, zcornsv)
+    result["coordsv"] = coordsv
+    result["zcornsv"] = zcornsv
+
+    actnumsv[geom.get_defined_cells()] = 1
+    result["actnumsv"] = actnumsv
+
+    # subgrids
+    if len(indexer.zonation) > 1:
+        logger.debug("Zonation length (N subzones) is %s", len(indexer.zonation))
+        subz = OrderedDict()
+        for inum, zrange in indexer.zonation.items():
+            logger.debug("inum: %s, zrange: %s", inum, zrange)
+            zname = roxgrid.zone_names[inum]
+            logger.debug("zname is: %s", zname)
+            zra = [nn + 1 for ira in zrange for nn in ira]  # nested lists
+            subz[zname] = zra
+
+        result["subgrids"] = subz
+
+    result["roxgrid"] = roxgrid
+    result["roxindexer"] = indexer
+
+    return result
+
+
+# ======================================================================================
+# Save/export
+# ======================================================================================
+
+
+def export_grid_roxapi(
+    self, projectname, gname, realisation, info=False, method="cpg"
+):  # pragma: no cover
+    """Export (i.e. store in RMS) via ROXAR API spec.
+
+    Using method 'cpg' means that the CPG method is applied (from ROXAPI 1.3).
+    This is possible from version ROXAPI ver 1.3, where the CornerPointGeometry
+    class is defined.
+
+    An alternative is to use simple roff import (via some /tmp area),
+    can be used from version 1.2
+
+    """
+    rox = RoxUtils(projectname, readonly=False)
+
+    if method == "cpg":
+        if self._xtgformat == 1:
+            _export_grid_cornerpoint_roxapi_v1(self, rox, gname, realisation, info)
+        else:
+            _export_grid_cornerpoint_roxapi_v2(self, rox, gname, realisation, info)
+
+    else:
+        _export_grid_viaroff_roxapi(self, rox, gname, realisation)
+
+    if rox._roxexternal:
+        rox.project.save()
+
+    rox.safe_close()
+
+
+def _export_grid_cornerpoint_roxapi_v1(
+    self, rox, gname, realisation, info
+):  # pragma: no cover
+    """Convert xtgeo geometry to pillar spec in ROXAPI and store."""
+    try:
+        from roxar.grids import CornerPointGridGeometry as CPG
+    except ImportError:
+        raise RuntimeError("Cannot load Roxar module")
+
+    logger.info("Load grid via CornerPointGridGeometry...")
+
+    grid_model = rox.project.grid_models.create(gname)
+    grid_model.set_empty(realisation)
+    grid = grid_model.get_grid(realisation)
+
+    geom = CPG.create(self.dimensions)
+
+    logger.info(geom)
+    scopy = self.copy()
+    scopy.make_zconsistent()
+    scopy._xtgformat1()
+    self._xtgformat1()
+
+    npill = (self.ncol + 1) * (self.nrow + 1) * 3
+    nzcrn = (self.ncol + 1) * (self.nrow + 1) * 4 * (self.nlay + 1)
+
+    _ier, tpi, bpi, zco = _cxtgeo.grd3d_conv_grid_roxapi(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        scopy._zcornsv,
+        self._actnumsv,
+        npill,
+        npill,
+        nzcrn,
+    )
+
+    tpi = tpi.reshape(self.ncol + 1, self.nrow + 1, 3)
+    bpi = bpi.reshape(self.ncol + 1, self.nrow + 1, 3)
+
+    zco = np.ma.masked_greater(zco, xtgeo.UNDEF_LIMIT)
+    zco = zco.reshape((self.ncol + 1, self.nrow + 1, 4, self.nlay + 1))
+
+    for ipi in range(self.ncol + 1):
+        for jpi in range(self.nrow + 1):
+            zzco = zco[ipi, jpi].reshape((self.nlay + 1), 4).T
+            geom.set_pillar_data(
+                ipi,
+                jpi,
+                top_point=tpi[ipi, jpi],
+                bottom_point=bpi[ipi, jpi],
+                depth_values=zzco,
+            )
+            if info and ipi < 5 and jpi < 5:
+                if ipi == 0 and jpi == 0:
+                    xtg.say("Showing info for i<5 and j<5 only!")
+                xtg.say(f"XTGeo pillar {ipi}, {jpi}\n")
+                xtg.say(f"XTGeo Tops\n{tpi[ipi, jpi]}")
+                xtg.say(f"XTGeo Bots\n{bpi[ipi, jpi]}")
+                xtg.say(f"XTGeo Depths\n{zzco}")
+
+    geom.set_defined_cells(self.get_actnum().values.astype(np.bool))
+    grid.set_geometry(geom)
+
+    _set_subgrids(self, rox, grid)
+
+
+def _export_grid_cornerpoint_roxapi_v2(
+    self, rox, gname, realisation, info
+):  # pragma: no cover
+    """Convert xtgeo geometry to pillar spec in ROXAPI and store _xtgformat=2."""
+    try:
+        from roxar.grids import CornerPointGridGeometry as CPG
+    except ImportError:
+        raise RuntimeError("Cannot load Roxar module")
+
+    grid_model = rox.project.grid_models.create(gname)
+    grid_model.set_empty(realisation)
+    grid = grid_model.get_grid(realisation)
+
+    geom = CPG.create(self.dimensions)
+
+    scopy = self.copy()
+    scopy.make_zconsistent()
+    scopy._xtgformat2()
+
+    npill = (self.ncol + 1) * (self.nrow + 1) * 3
+    nzcrn = (self.ncol + 1) * (self.nrow + 1) * (self.nlay + 1) * 4
+
+    _ier, tpi, bpi, zco = _cxtgeo.grdcp3d_conv_grid_roxapi(
+        self.ncol,
+        self.nrow,
+        self.nlay,
+        self._coordsv,
+        scopy._zcornsv,
+        npill,
+        npill,
+        nzcrn,
+    )
+
+    tpi = tpi.reshape(self.ncol + 1, self.nrow + 1, 3)
+    bpi = bpi.reshape(self.ncol + 1, self.nrow + 1, 3)
+
+    zco = np.ma.masked_greater(zco, xtgeo.UNDEF_LIMIT)
+    zco = zco.reshape((self.ncol + 1, self.nrow + 1, 4, self.nlay + 1))
+
+    for ipi in range(self.ncol + 1):
+        for jpi in range(self.nrow + 1):
+            zzco = zco[ipi, jpi].reshape((self.nlay + 1), 4).T
+            geom.set_pillar_data(
+                ipi,
+                jpi,
+                top_point=tpi[ipi, jpi],
+                bottom_point=bpi[ipi, jpi],
+                depth_values=zzco,
+            )
+            if info and ipi < 5 and jpi < 5:
+                if ipi == 0 and jpi == 0:
+                    xtg.say("Showing info for i<5 and j<5 only!")
+                xtg.say(f"XTGeo pillar {ipi}, {jpi}\n")
+                xtg.say(f"XTGeo Tops\n{tpi[ipi, jpi]}")
+                xtg.say(f"XTGeo Bots\n{bpi[ipi, jpi]}")
+                xtg.say(f"XTGeo Depths\n{zzco}")
+
+    geom.set_defined_cells(self._actnumsv.astype(np.bool))
+    grid.set_geometry(geom)
+    _set_subgrids(self, rox, grid)
+
+    del scopy
+
+
+def _set_subgrids(self, rox, grid):
+    """Export the subgrid index (zones) to Roxar API.
+
+    From roxar API:
+        set_zonation(zone_dict)
+
+            zone_dict A dictionary with start-layer (zero based) and name for each zone.
+
+    """
+
+    if not self.subgrids:
+        return
+
+    if rox.version_required("1.6"):
+        subs = self.subgrids
+        roxar_subs = {}
+        for name, zrange in subs.items():
+            roxar_subs[int(zrange[0] - 1)] = name
+
+        grid.set_zonation(roxar_subs)
+
+    else:
+        xtg.warnuser(
+            "Implementation of subgrids is lacking in Roxar API for this "
+            "RMS version. Will continue to store in RMS but without subgrid index."
+        )
+
+
+def _export_grid_viaroff_roxapi(self, rox, gname, realisation):  # pragma: no cover
+    """Convert xtgeo geometry to internal RMS via i/o ROFF tricks."""
+    logger.info("Realisation is %s", realisation)
+
+    # make a temporary folder and work within the with.. block
+    with tempfile.TemporaryDirectory() as tmpdir:
+        logger.info("Made a tmp folder: %s", tmpdir)
+
+        fname = os.path.join(tmpdir, gname)
+        self.to_file(fname)
+
+        grd = rox.project.grid_models
+
+        try:
+            del grd[gname]
+            logger.info("Overwriting existing grid in RMS")
+        except KeyError:
+            logger.info("Grid in RMS is new")
+
+        grd.load(fname)
```

## xtgeo/grid3d/_grid_wellzone.py

 * *Ordering differences only*

```diff
@@ -1,154 +1,154 @@
-"""Private module for grid vs well zonelog checks."""
-
-import numpy as np
-import xtgeo
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def report_zone_mismatch(
-    self,
-    well=None,
-    zonelogname="ZONELOG",
-    zoneprop=None,
-    onelayergrid=None,  # redundant; will be computed internally
-    zonelogrange=(0, 9999),
-    zonelogshift=0,
-    depthrange=None,
-    perflogname=None,
-    perflogrange=(1, 9999),
-    filterlogname=None,
-    filterlogrange=(1e-32, 9999.0),
-    resultformat=1,
-):  # pylint: disable=too-many-locals, too-many-branches, too-many-statements
-    """Reports well to zone mismatch; this works together with a Well object.
-
-    The idea is to sample the current zone property for the well in the grid as fast as
-    possible.
-
-    Then the sampled zonelog is compared with the actual zonelog, and the difference
-    is reported.
-
-    One can apply a perforation log as a mask, meaning that we filter zonelog
-    match in intervals with a perforation log only if requested.
-
-    This method was completely redesigned in version 2.8
-    """
-    self._xtgformat1()
-
-    if onelayergrid is not None:
-        xtg.warndeprecated("Using key 'onelayergrid' is redundant and can be skipped")
-
-    if not isinstance(well, xtgeo.Well):
-        raise ValueError("Input well is not a Well() instance")
-
-    if zonelogname not in well.dataframe.columns:
-        logger.warning("Zonelog %s is missing for well %s", zonelogname, well.name)
-        return None
-
-    if perflogname == "None" or perflogname is None:  # "None" for backwards compat
-        logger.info("No perforation log as filter")
-        perflogname = None
-
-    # get the IJK along the well as logs; use a copy of the well instance
-    wll = well.copy()
-    wll._df[zonelogname] += zonelogshift
-
-    if depthrange:
-        d1, d2 = depthrange
-        wll._df = wll._df[(d1 < wll._df.Z_TVDSS) & (wll._df.Z_TVDSS < d2)]
-
-    wll.get_gridproperties(zoneprop, self)
-    zmodel = zoneprop.name + "_model"
-
-    # from here, work with the dataframe only
-    df = wll._df
-
-    # zonelogrange
-    z1, z2 = zonelogrange
-    zmin = zmax = 0
-    try:
-        zmin = int(df[zonelogname].min())
-    except ValueError as verr:
-        if "cannot convert" in str(verr):
-            msg = f"TVD range {depthrange} is possibly to narrow? ({str(verr)})"
-            raise ValueError(msg)
-    try:
-        zmax = int(df[zonelogname].max())
-    except ValueError as verr:
-        if "cannot convert" in str(verr):
-            msg = f"TVD range {depthrange} is possibly to narrow? ({str(verr)})"
-            raise ValueError(msg)
-
-    skiprange = list(range(zmin, z1)) + list(range(z2 + 1, zmax + 1))
-
-    for zname in (zonelogname, zmodel):
-        if skiprange:  # needed check; du to a bug in pandas version 0.21 .. 0.23
-            df[zname].replace(skiprange, -888, inplace=True)
-        df[zname].fillna(-999, inplace=True)
-        if perflogname:
-            if perflogname in df.columns:
-                df[perflogname].replace(np.nan, -1, inplace=True)
-                pfr1, pfr2 = perflogrange
-                df[zname] = np.where(df[perflogname] < pfr1, -899, df[zname])
-                df[zname] = np.where(df[perflogname] > pfr2, -899, df[zname])
-            else:
-                return None
-        if filterlogname:
-            if filterlogname in df.columns:
-                df[filterlogname].replace(np.nan, -1, inplace=True)
-                ffr1, ffr2 = filterlogrange
-                df[zname] = np.where(df[filterlogname] < ffr1, -919, df[zname])
-                df[zname] = np.where(df[filterlogname] > ffr2, -919, df[zname])
-            else:
-                return None
-
-    # now there are various variotions on how to count mismatch:
-    # dfuse 1: count matches when zonelogname is valid (exclude -888)
-    # dfuse 2: count matches when zonelogname OR zmodel are valid (exclude < -888
-    # or -999)
-    # The first one is the original approach
-
-    dfuse1 = df.copy(deep=True)
-    dfuse1 = dfuse1.loc[dfuse1[zonelogname] > -888]
-
-    dfuse1["zmatch1"] = np.where(dfuse1[zmodel] == dfuse1[zonelogname], 1, 0)
-    mcount1 = dfuse1["zmatch1"].sum()
-    tcount1 = dfuse1["zmatch1"].count()
-    if not np.isnan(mcount1):
-        mcount1 = int(mcount1)
-    if not np.isnan(tcount1):
-        tcount1 = int(tcount1)
-
-    res1 = dfuse1["zmatch1"].mean() * 100
-
-    dfuse2 = df.copy(deep=True)
-    dfuse2 = dfuse2.loc[(df[zmodel] > -888) | (df[zonelogname] > -888)]
-    dfuse2["zmatch2"] = np.where(dfuse2[zmodel] == dfuse2[zonelogname], 1, 0)
-    mcount2 = dfuse2["zmatch2"].sum()
-    tcount2 = dfuse2["zmatch2"].count()
-    if not np.isnan(mcount2):
-        mcount2 = int(mcount2)
-    if not np.isnan(tcount2):
-        tcount2 = int(tcount2)
-
-    res2 = dfuse2["zmatch2"].mean() * 100
-
-    # update Well() copy (segment only)
-    wll.dataframe = dfuse2
-
-    if resultformat == 1:
-        return (res1, mcount1, tcount1)
-
-    res = {
-        "MATCH1": res1,
-        "MCOUNT1": mcount1,
-        "TCOUNT1": tcount1,
-        "MATCH2": res2,
-        "MCOUNT2": mcount2,
-        "TCOUNT2": tcount2,
-        "WELLINTV": wll,
-    }
-    return res
+"""Private module for grid vs well zonelog checks."""
+
+import numpy as np
+import xtgeo
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def report_zone_mismatch(
+    self,
+    well=None,
+    zonelogname="ZONELOG",
+    zoneprop=None,
+    onelayergrid=None,  # redundant; will be computed internally
+    zonelogrange=(0, 9999),
+    zonelogshift=0,
+    depthrange=None,
+    perflogname=None,
+    perflogrange=(1, 9999),
+    filterlogname=None,
+    filterlogrange=(1e-32, 9999.0),
+    resultformat=1,
+):  # pylint: disable=too-many-locals, too-many-branches, too-many-statements
+    """Reports well to zone mismatch; this works together with a Well object.
+
+    The idea is to sample the current zone property for the well in the grid as fast as
+    possible.
+
+    Then the sampled zonelog is compared with the actual zonelog, and the difference
+    is reported.
+
+    One can apply a perforation log as a mask, meaning that we filter zonelog
+    match in intervals with a perforation log only if requested.
+
+    This method was completely redesigned in version 2.8
+    """
+    self._xtgformat1()
+
+    if onelayergrid is not None:
+        xtg.warndeprecated("Using key 'onelayergrid' is redundant and can be skipped")
+
+    if not isinstance(well, xtgeo.Well):
+        raise ValueError("Input well is not a Well() instance")
+
+    if zonelogname not in well.dataframe.columns:
+        logger.warning("Zonelog %s is missing for well %s", zonelogname, well.name)
+        return None
+
+    if perflogname == "None" or perflogname is None:  # "None" for backwards compat
+        logger.info("No perforation log as filter")
+        perflogname = None
+
+    # get the IJK along the well as logs; use a copy of the well instance
+    wll = well.copy()
+    wll._df[zonelogname] += zonelogshift
+
+    if depthrange:
+        d1, d2 = depthrange
+        wll._df = wll._df[(d1 < wll._df.Z_TVDSS) & (wll._df.Z_TVDSS < d2)]
+
+    wll.get_gridproperties(zoneprop, self)
+    zmodel = zoneprop.name + "_model"
+
+    # from here, work with the dataframe only
+    df = wll._df
+
+    # zonelogrange
+    z1, z2 = zonelogrange
+    zmin = zmax = 0
+    try:
+        zmin = int(df[zonelogname].min())
+    except ValueError as verr:
+        if "cannot convert" in str(verr):
+            msg = f"TVD range {depthrange} is possibly to narrow? ({str(verr)})"
+            raise ValueError(msg)
+    try:
+        zmax = int(df[zonelogname].max())
+    except ValueError as verr:
+        if "cannot convert" in str(verr):
+            msg = f"TVD range {depthrange} is possibly to narrow? ({str(verr)})"
+            raise ValueError(msg)
+
+    skiprange = list(range(zmin, z1)) + list(range(z2 + 1, zmax + 1))
+
+    for zname in (zonelogname, zmodel):
+        if skiprange:  # needed check; du to a bug in pandas version 0.21 .. 0.23
+            df[zname].replace(skiprange, -888, inplace=True)
+        df[zname].fillna(-999, inplace=True)
+        if perflogname:
+            if perflogname in df.columns:
+                df[perflogname].replace(np.nan, -1, inplace=True)
+                pfr1, pfr2 = perflogrange
+                df[zname] = np.where(df[perflogname] < pfr1, -899, df[zname])
+                df[zname] = np.where(df[perflogname] > pfr2, -899, df[zname])
+            else:
+                return None
+        if filterlogname:
+            if filterlogname in df.columns:
+                df[filterlogname].replace(np.nan, -1, inplace=True)
+                ffr1, ffr2 = filterlogrange
+                df[zname] = np.where(df[filterlogname] < ffr1, -919, df[zname])
+                df[zname] = np.where(df[filterlogname] > ffr2, -919, df[zname])
+            else:
+                return None
+
+    # now there are various variotions on how to count mismatch:
+    # dfuse 1: count matches when zonelogname is valid (exclude -888)
+    # dfuse 2: count matches when zonelogname OR zmodel are valid (exclude < -888
+    # or -999)
+    # The first one is the original approach
+
+    dfuse1 = df.copy(deep=True)
+    dfuse1 = dfuse1.loc[dfuse1[zonelogname] > -888]
+
+    dfuse1["zmatch1"] = np.where(dfuse1[zmodel] == dfuse1[zonelogname], 1, 0)
+    mcount1 = dfuse1["zmatch1"].sum()
+    tcount1 = dfuse1["zmatch1"].count()
+    if not np.isnan(mcount1):
+        mcount1 = int(mcount1)
+    if not np.isnan(tcount1):
+        tcount1 = int(tcount1)
+
+    res1 = dfuse1["zmatch1"].mean() * 100
+
+    dfuse2 = df.copy(deep=True)
+    dfuse2 = dfuse2.loc[(df[zmodel] > -888) | (df[zonelogname] > -888)]
+    dfuse2["zmatch2"] = np.where(dfuse2[zmodel] == dfuse2[zonelogname], 1, 0)
+    mcount2 = dfuse2["zmatch2"].sum()
+    tcount2 = dfuse2["zmatch2"].count()
+    if not np.isnan(mcount2):
+        mcount2 = int(mcount2)
+    if not np.isnan(tcount2):
+        tcount2 = int(tcount2)
+
+    res2 = dfuse2["zmatch2"].mean() * 100
+
+    # update Well() copy (segment only)
+    wll.dataframe = dfuse2
+
+    if resultformat == 1:
+        return (res1, mcount1, tcount1)
+
+    res = {
+        "MATCH1": res1,
+        "MCOUNT1": mcount1,
+        "TCOUNT1": tcount1,
+        "MATCH2": res2,
+        "MCOUNT2": mcount2,
+        "TCOUNT2": tcount2,
+        "WELLINTV": wll,
+    }
+    return res
```

## xtgeo/grid3d/_gridprop_export.py

 * *Ordering differences only*

```diff
@@ -1,139 +1,139 @@
-"""GridProperty (not GridProperies) export functions."""
-import json
-import struct
-
-import ecl_data_io as eclio
-import numpy as np
-import roffio
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-
-from ._roff_parameter import RoffParameter
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def to_file(self, pfile, fformat="roff", name=None, append=False, dtype=None, fmt=None):
-    """Export the grid property to file."""
-    logger.info("Export property to file %s as %s", pfile, fformat)
-
-    fobj = xtgeo._XTGeoFile(pfile, mode="rb")
-    if not fobj.memstream:
-        fobj.check_folder(raiseerror=OSError)
-
-    if name is None:
-        name = self.name
-
-    if "roff" in fformat:
-        binary = True
-        if "asc" in fformat:
-            binary = False
-
-        if append:
-            logger.warning(
-                "Append is not implemented for roff format, defaulting to write."
-            )
-
-        export_roff(self, fobj.name, name, binary=binary)
-
-    elif fformat == "grdecl":
-        export_grdecl(
-            self, fobj.name, name, append=append, binary=False, dtype=dtype, fmt=fmt
-        )
-
-    elif fformat == "bgrdecl":
-        export_grdecl(self, fobj.name, name, append=append, binary=True, dtype=dtype)
-
-    elif fformat == "xtgcpprop":
-        export_xtgcpprop(self, fobj.name)
-
-    else:
-        raise ValueError(f"Cannot export, invalid fformat: {fformat}")
-
-
-# Export ascii or binary ROFF format
-
-
-def export_roff(self, pfile, name, binary=True):
-    logger.info("Export roff to %s", pfile)
-    roff_param = RoffParameter.from_xtgeo_grid_property(self)
-    roff_param.name = name
-    roff_format = roffio.Format.ASCII
-    if binary:
-        roff_format = roffio.Format.BINARY
-
-    roff_param.to_file(pfile, roff_format)
-
-
-def export_grdecl(self, pfile, name, append=False, binary=False, dtype=None, fmt=None):
-    """Export ascii or binary GRDECL"""
-    vals = self.values.ravel(order="F")
-    if np.ma.isMaskedArray(vals):
-        vals = np.ma.filled(vals, self.undef)
-
-    if binary:
-        mode = "wb"
-        if append:
-            mode = "ab"
-
-        with open(pfile, mode) as fh:
-            if dtype is not None:
-                eclio.write(fh, [(name.ljust(8), vals.astype(dtype))])
-            elif self.isdiscrete:
-                eclio.write(fh, [(name.ljust(8), vals.astype(np.int32))])
-            else:
-                eclio.write(fh, [(name.ljust(8), vals.astype(np.float32))])
-
-    else:
-        mode = "w"
-        if append:
-            mode = "a"
-        with open(pfile, mode) as fh:
-            fh.write(name)
-            fh.write("\n")
-            for i, v in enumerate(vals):
-                fh.write(" ")
-                if fmt:
-                    fh.write(fmt % v)
-                elif self.isdiscrete:
-                    fh.write(str(v))
-                else:
-                    fh.write(f"{v:3e}")
-                if i % 6 == 5:
-                    fh.write("\n")
-
-            fh.write(" /\n")
-
-
-def export_xtgcpprop(self, mfile):
-    """Export to experimental xtgcpproperty format, python version."""
-    logger.info("Export as xtgcpprop...")
-    self._metadata.required = self
-
-    magic = 1351
-    if self.isdiscrete:
-        magic = 1352
-
-    prevalues = (1, magic, 4, self.ncol, self.nrow, self.nlay)
-    mystruct = struct.Struct("= i i i q q q")
-    pre = mystruct.pack(*prevalues)
-
-    meta = self.metadata.get_metadata()
-
-    with open(mfile, "wb") as fout:
-        fout.write(pre)
-
-    with open(mfile, "ab") as fout:
-        vv = self.get_npvalues1d(fill_value=self.undef)
-        vv.astype(np.float32).tofile(fout)
-
-    with open(mfile, "ab") as fout:
-        fout.write("\nXTGMETA.v01\n".encode())
-
-    with open(mfile, "ab") as fout:
-        fout.write(json.dumps(meta).encode())
-
-    logger.info("Export as xtgcpprop... done")
+"""GridProperty (not GridProperies) export functions."""
+import json
+import struct
+
+import ecl_data_io as eclio
+import numpy as np
+import roffio
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+
+from ._roff_parameter import RoffParameter
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def to_file(self, pfile, fformat="roff", name=None, append=False, dtype=None, fmt=None):
+    """Export the grid property to file."""
+    logger.info("Export property to file %s as %s", pfile, fformat)
+
+    fobj = xtgeo._XTGeoFile(pfile, mode="rb")
+    if not fobj.memstream:
+        fobj.check_folder(raiseerror=OSError)
+
+    if name is None:
+        name = self.name
+
+    if "roff" in fformat:
+        binary = True
+        if "asc" in fformat:
+            binary = False
+
+        if append:
+            logger.warning(
+                "Append is not implemented for roff format, defaulting to write."
+            )
+
+        export_roff(self, fobj.name, name, binary=binary)
+
+    elif fformat == "grdecl":
+        export_grdecl(
+            self, fobj.name, name, append=append, binary=False, dtype=dtype, fmt=fmt
+        )
+
+    elif fformat == "bgrdecl":
+        export_grdecl(self, fobj.name, name, append=append, binary=True, dtype=dtype)
+
+    elif fformat == "xtgcpprop":
+        export_xtgcpprop(self, fobj.name)
+
+    else:
+        raise ValueError(f"Cannot export, invalid fformat: {fformat}")
+
+
+# Export ascii or binary ROFF format
+
+
+def export_roff(self, pfile, name, binary=True):
+    logger.info("Export roff to %s", pfile)
+    roff_param = RoffParameter.from_xtgeo_grid_property(self)
+    roff_param.name = name
+    roff_format = roffio.Format.ASCII
+    if binary:
+        roff_format = roffio.Format.BINARY
+
+    roff_param.to_file(pfile, roff_format)
+
+
+def export_grdecl(self, pfile, name, append=False, binary=False, dtype=None, fmt=None):
+    """Export ascii or binary GRDECL"""
+    vals = self.values.ravel(order="F")
+    if np.ma.isMaskedArray(vals):
+        vals = np.ma.filled(vals, self.undef)
+
+    if binary:
+        mode = "wb"
+        if append:
+            mode = "ab"
+
+        with open(pfile, mode) as fh:
+            if dtype is not None:
+                eclio.write(fh, [(name.ljust(8), vals.astype(dtype))])
+            elif self.isdiscrete:
+                eclio.write(fh, [(name.ljust(8), vals.astype(np.int32))])
+            else:
+                eclio.write(fh, [(name.ljust(8), vals.astype(np.float32))])
+
+    else:
+        mode = "w"
+        if append:
+            mode = "a"
+        with open(pfile, mode) as fh:
+            fh.write(name)
+            fh.write("\n")
+            for i, v in enumerate(vals):
+                fh.write(" ")
+                if fmt:
+                    fh.write(fmt % v)
+                elif self.isdiscrete:
+                    fh.write(str(v))
+                else:
+                    fh.write(f"{v:3e}")
+                if i % 6 == 5:
+                    fh.write("\n")
+
+            fh.write(" /\n")
+
+
+def export_xtgcpprop(self, mfile):
+    """Export to experimental xtgcpproperty format, python version."""
+    logger.info("Export as xtgcpprop...")
+    self._metadata.required = self
+
+    magic = 1351
+    if self.isdiscrete:
+        magic = 1352
+
+    prevalues = (1, magic, 4, self.ncol, self.nrow, self.nlay)
+    mystruct = struct.Struct("= i i i q q q")
+    pre = mystruct.pack(*prevalues)
+
+    meta = self.metadata.get_metadata()
+
+    with open(mfile, "wb") as fout:
+        fout.write(pre)
+
+    with open(mfile, "ab") as fout:
+        vv = self.get_npvalues1d(fill_value=self.undef)
+        vv.astype(np.float32).tofile(fout)
+
+    with open(mfile, "ab") as fout:
+        fout.write("\nXTGMETA.v01\n".encode())
+
+    with open(mfile, "ab") as fout:
+        fout.write(json.dumps(meta).encode())
+
+    logger.info("Export as xtgcpprop... done")
```

## xtgeo/grid3d/_gridprop_import_eclrun.py

 * *Ordering differences only*

```diff
@@ -1,144 +1,144 @@
-from typing import List, Union
-
-import ecl_data_io as eclio
-from typing_extensions import Literal
-
-from ._find_gridprop_in_eclrun import (
-    find_gridprop_from_init_file,
-    find_gridprops_from_restart_file,
-)
-
-
-def decorate_name(name, dual_porosity, fracture, date=None):
-    """Decorate a property name with date and matrix/fracture.
-
-    >>> decorate_name('PORO', True, False, 19991231)
-    'POROM_19991231'
-    """
-    decorated_name = name
-    if dual_porosity:
-        if fracture:
-            decorated_name += "F"
-        else:
-            decorated_name += "M"
-
-    if date is not None:
-        decorated_name += "_" + str(date)
-    return decorated_name
-
-
-def import_gridprop_from_init(pfile, name, grid, fracture=False):
-    """Import one parameter with the given name from an init file.
-
-    Args:
-        pfile: The init file.
-        name: The name of the parmaeter
-        grid: The grid used by the simulator to produce the init file.
-        fracture: If a dual porosity module, indicates that the fracture
-            (as apposed to the matrix) grid property should be imported.
-    Raises:
-        ValueError: If the parameter does not exist in the file.
-    Returns:
-        GridProperty parameter dictionary.
-    """
-    init_props = find_gridprop_from_init_file(pfile.file, [name], grid, fracture)
-    if len(init_props) != 1:
-        raise ValueError(f"Could not find property {name} in {pfile}")
-    init_props[0]["name"] = decorate_name(
-        init_props[0]["name"], grid.dualporo, fracture
-    )
-    return init_props[0]
-
-
-def sanitize_date(
-    date: Union[int, str, Literal["first", "last"]]
-) -> Union[List[int], Literal["first", "last"]]:
-    """
-    Converts dateformats of the form 'YYYY-MM-DD', 'YYYYMMDD' or YYYYMMDD to
-    list of integers of the form [YYYYMMDD] (ie. suitible for find_gridprops
-    functions), but lets the special literals 'first' and 'last' remain
-    unchanged.
-
-    >>> sanitize_date('first')
-    'first'
-    >>> sanitize_date('last')
-    'last'
-    >>> sanitize_date('2020-01-01')
-    [20200101]
-    >>> sanitize_date('20200101')
-    [20200101]
-    >>> sanitize_date(20200101)
-    [20200101]
-    """
-    if isinstance(date, int):
-        return [date]
-    if date not in ("first", "last"):
-        try:
-            if isinstance(date, str):
-                if len(date) == 10 and date[4] == "-" and date[7] == "-":
-                    date = date.replace("-", "")
-            return [int(date)]
-        except ValueError as err:
-            raise ValueError(
-                "valid dates are either of the "
-                "form 'YYYY-MM-DD', 'YYYYMMDD' or 'first'/'last' "
-                f"got {date}"
-            ) from err
-    return date
-
-
-def sanitize_fformat(fformat: Literal["unrst", "funrst"]) -> eclio.Format:
-    """Converts 'unrst' and 'funrst' to the corresponding eclio.Format.
-
-    >>> sanitize_fformat('unrst')
-    <Format.UNFORMATTED: 2>
-    >>> sanitize_fformat('funrst')
-    <Format.FORMATTED: 1>
-    """
-    if fformat == "unrst":
-        return eclio.Format.UNFORMATTED
-    if fformat == "funrst":
-        return eclio.Format.FORMATTED
-    raise ValueError(f"fformat must be either 'unrst' or 'funrst' got {fformat}")
-
-
-def import_gridprop_from_restart(
-    pfile,
-    name: str,
-    grid,
-    date: Union[int, str, Literal["first", "last"]],
-    fracture: bool = False,
-    fformat: Literal["unrst", "funrst"] = "unrst",
-):
-    """Import one parameter for the given name and date in a restart file.
-
-    Args:
-        pfile: The restart file.
-        name: The name of the parmaeter
-        date: xtgeo style date (e.g. int(19990101) or "YYYYMMDD"), also
-            accepts "YYYY-MM-DD".  "last" and "first" can be given for
-            last or first date in the file
-        grid: The grid used by the simulator to produce the restart file.
-        fracture: If a dual porosity module, indicates that the fracture
-            (as apposed to the matrix) grid property should be imported.
-    Raises:
-        ValueError: If the parameter does not exist in the file.
-    Returns:
-        GridProperty parameter dictionary.
-    """
-    restart_props = find_gridprops_from_restart_file(
-        pfile.file,
-        [name],
-        sanitize_date(date),
-        grid,
-        fracture,
-        sanitize_fformat(fformat),
-    )
-    if len(restart_props) == 0:
-        raise ValueError(f"Could not find property {name} for {date} in {pfile.file}")
-    if len(restart_props) > 1:
-        raise ValueError(f"Ambiguous property {name} for {date} in {pfile.file}")
-    restart_props[0]["name"] = decorate_name(
-        restart_props[0]["name"], grid.dualporo, fracture, restart_props[0]["date"]
-    )
-    return restart_props[0]
+from typing import List, Union
+
+import ecl_data_io as eclio
+from typing_extensions import Literal
+
+from ._find_gridprop_in_eclrun import (
+    find_gridprop_from_init_file,
+    find_gridprops_from_restart_file,
+)
+
+
+def decorate_name(name, dual_porosity, fracture, date=None):
+    """Decorate a property name with date and matrix/fracture.
+
+    >>> decorate_name('PORO', True, False, 19991231)
+    'POROM_19991231'
+    """
+    decorated_name = name
+    if dual_porosity:
+        if fracture:
+            decorated_name += "F"
+        else:
+            decorated_name += "M"
+
+    if date is not None:
+        decorated_name += "_" + str(date)
+    return decorated_name
+
+
+def import_gridprop_from_init(pfile, name, grid, fracture=False):
+    """Import one parameter with the given name from an init file.
+
+    Args:
+        pfile: The init file.
+        name: The name of the parmaeter
+        grid: The grid used by the simulator to produce the init file.
+        fracture: If a dual porosity module, indicates that the fracture
+            (as apposed to the matrix) grid property should be imported.
+    Raises:
+        ValueError: If the parameter does not exist in the file.
+    Returns:
+        GridProperty parameter dictionary.
+    """
+    init_props = find_gridprop_from_init_file(pfile.file, [name], grid, fracture)
+    if len(init_props) != 1:
+        raise ValueError(f"Could not find property {name} in {pfile}")
+    init_props[0]["name"] = decorate_name(
+        init_props[0]["name"], grid.dualporo, fracture
+    )
+    return init_props[0]
+
+
+def sanitize_date(
+    date: Union[int, str, Literal["first", "last"]]
+) -> Union[List[int], Literal["first", "last"]]:
+    """
+    Converts dateformats of the form 'YYYY-MM-DD', 'YYYYMMDD' or YYYYMMDD to
+    list of integers of the form [YYYYMMDD] (ie. suitible for find_gridprops
+    functions), but lets the special literals 'first' and 'last' remain
+    unchanged.
+
+    >>> sanitize_date('first')
+    'first'
+    >>> sanitize_date('last')
+    'last'
+    >>> sanitize_date('2020-01-01')
+    [20200101]
+    >>> sanitize_date('20200101')
+    [20200101]
+    >>> sanitize_date(20200101)
+    [20200101]
+    """
+    if isinstance(date, int):
+        return [date]
+    if date not in ("first", "last"):
+        try:
+            if isinstance(date, str):
+                if len(date) == 10 and date[4] == "-" and date[7] == "-":
+                    date = date.replace("-", "")
+            return [int(date)]
+        except ValueError as err:
+            raise ValueError(
+                "valid dates are either of the "
+                "form 'YYYY-MM-DD', 'YYYYMMDD' or 'first'/'last' "
+                f"got {date}"
+            ) from err
+    return date
+
+
+def sanitize_fformat(fformat: Literal["unrst", "funrst"]) -> eclio.Format:
+    """Converts 'unrst' and 'funrst' to the corresponding eclio.Format.
+
+    >>> sanitize_fformat('unrst')
+    <Format.UNFORMATTED: 2>
+    >>> sanitize_fformat('funrst')
+    <Format.FORMATTED: 1>
+    """
+    if fformat == "unrst":
+        return eclio.Format.UNFORMATTED
+    if fformat == "funrst":
+        return eclio.Format.FORMATTED
+    raise ValueError(f"fformat must be either 'unrst' or 'funrst' got {fformat}")
+
+
+def import_gridprop_from_restart(
+    pfile,
+    name: str,
+    grid,
+    date: Union[int, str, Literal["first", "last"]],
+    fracture: bool = False,
+    fformat: Literal["unrst", "funrst"] = "unrst",
+):
+    """Import one parameter for the given name and date in a restart file.
+
+    Args:
+        pfile: The restart file.
+        name: The name of the parmaeter
+        date: xtgeo style date (e.g. int(19990101) or "YYYYMMDD"), also
+            accepts "YYYY-MM-DD".  "last" and "first" can be given for
+            last or first date in the file
+        grid: The grid used by the simulator to produce the restart file.
+        fracture: If a dual porosity module, indicates that the fracture
+            (as apposed to the matrix) grid property should be imported.
+    Raises:
+        ValueError: If the parameter does not exist in the file.
+    Returns:
+        GridProperty parameter dictionary.
+    """
+    restart_props = find_gridprops_from_restart_file(
+        pfile.file,
+        [name],
+        sanitize_date(date),
+        grid,
+        fracture,
+        sanitize_fformat(fformat),
+    )
+    if len(restart_props) == 0:
+        raise ValueError(f"Could not find property {name} for {date} in {pfile.file}")
+    if len(restart_props) > 1:
+        raise ValueError(f"Ambiguous property {name} for {date} in {pfile.file}")
+    restart_props[0]["name"] = decorate_name(
+        restart_props[0]["name"], grid.dualporo, fracture, restart_props[0]["date"]
+    )
+    return restart_props[0]
```

## xtgeo/grid3d/_gridprop_import_grdecl.py

 * *Ordering differences only*

```diff
@@ -1,106 +1,106 @@
-"""Importing grid props from GRDECL, ascii or binary"""
-
-
-import ecl_data_io as eclio
-import numpy as np
-import numpy.ma as ma
-
-import xtgeo
-
-from ._grdecl_format import match_keyword, open_grdecl
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_bgrdecl_prop(pfile, name, grid):
-    """Import prop for binary files with GRDECL layout.
-
-    Args:
-        pfile (_XTgeoCFile): xtgeo file instance
-        name (str): Name of parameter.
-        grid (Grid()): XTGeo Grid instance.
-
-    Raises:
-        xtgeo.KeywordNotFoundError: Cannot find property...
-    """
-    result = dict()
-    result["ncol"] = grid.ncol
-    result["nrow"] = grid.nrow
-    result["nlay"] = grid.nlay
-    result["name"] = name
-    result["filesrc"] = pfile
-
-    for entry in eclio.lazy_read(pfile.file):
-        if match_keyword(entry.read_keyword(), name):
-            values = entry.read_array()
-            result["discrete"] = np.issubdtype(values.dtype, np.integer)
-            if result["discrete"]:
-                uniq = np.unique(values).tolist()
-                codes = dict(zip(uniq, uniq))
-                codes = {key: str(val) for key, val in codes.items()}  # val: strings
-                result["codes"] = codes
-                values = values.astype(np.int32)
-                result["roxar_dtype"] = np.uint16
-            else:
-                values = values.astype(np.float64)
-                result["codes"] = {}
-                result["roxar_dtype"] = np.float32
-            result["values"] = ma.masked_where(
-                grid.get_actnum().values < 1, values.reshape(grid.dimensions, order="F")
-            )
-            return result
-
-    raise xtgeo.KeywordNotFoundError(
-        f"Cannot find property name {name} in file {pfile.name}"
-    )
-
-
-def read_grdecl_3d_property(filename, keyword, dimensions, dtype=float):
-    """
-    Read a 3d grid property from a grdecl file, see open_grdecl for description
-    of format.
-
-    Args:
-        filename (pathlib.Path or str): File in grdecl format.
-        keyword (str): The keyword of the property in the file
-        dimensions ((int,int,int)): Triple of the size of grid.
-        dtype (function): The datatype to be read, ie., float.
-
-    Raises:
-        xtgeo.KeywordNotFoundError: If keyword is not found in the file.
-
-    Returns:
-        numpy array with given dimensions and data type read
-        from the grdecl file.
-    """
-    result = None
-
-    with open_grdecl(filename, keywords=[], simple_keywords=(keyword,)) as kw_generator:
-        try:
-            _, result = next(kw_generator)
-        except StopIteration as si:
-            raise xtgeo.KeywordNotFoundError(
-                f"Cannot import {keyword}, not present in file {filename}?"
-            ) from si
-
-    # The values are stored in F order in the grdecl file
-    f_order_values = np.array([dtype(v) for v in result])
-    return np.ascontiguousarray(f_order_values.reshape(dimensions, order="F"))
-
-
-def import_grdecl_prop(pfile, name, grid):
-    """Read a GRDECL ASCII property record"""
-    result = dict()
-    result["ncol"] = grid.ncol
-    result["nrow"] = grid.nrow
-    result["nlay"] = grid.nlay
-    result["name"] = name
-    result["filesrc"] = pfile
-    actnumv = grid.get_actnum().values
-
-    result["values"] = ma.masked_where(
-        actnumv == 0, read_grdecl_3d_property(pfile.file, name, grid.dimensions, float)
-    )
-    return result
+"""Importing grid props from GRDECL, ascii or binary"""
+
+
+import ecl_data_io as eclio
+import numpy as np
+import numpy.ma as ma
+
+import xtgeo
+
+from ._grdecl_format import match_keyword, open_grdecl
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_bgrdecl_prop(pfile, name, grid):
+    """Import prop for binary files with GRDECL layout.
+
+    Args:
+        pfile (_XTgeoCFile): xtgeo file instance
+        name (str): Name of parameter.
+        grid (Grid()): XTGeo Grid instance.
+
+    Raises:
+        xtgeo.KeywordNotFoundError: Cannot find property...
+    """
+    result = dict()
+    result["ncol"] = grid.ncol
+    result["nrow"] = grid.nrow
+    result["nlay"] = grid.nlay
+    result["name"] = name
+    result["filesrc"] = pfile
+
+    for entry in eclio.lazy_read(pfile.file):
+        if match_keyword(entry.read_keyword(), name):
+            values = entry.read_array()
+            result["discrete"] = np.issubdtype(values.dtype, np.integer)
+            if result["discrete"]:
+                uniq = np.unique(values).tolist()
+                codes = dict(zip(uniq, uniq))
+                codes = {key: str(val) for key, val in codes.items()}  # val: strings
+                result["codes"] = codes
+                values = values.astype(np.int32)
+                result["roxar_dtype"] = np.uint16
+            else:
+                values = values.astype(np.float64)
+                result["codes"] = {}
+                result["roxar_dtype"] = np.float32
+            result["values"] = ma.masked_where(
+                grid.get_actnum().values < 1, values.reshape(grid.dimensions, order="F")
+            )
+            return result
+
+    raise xtgeo.KeywordNotFoundError(
+        f"Cannot find property name {name} in file {pfile.name}"
+    )
+
+
+def read_grdecl_3d_property(filename, keyword, dimensions, dtype=float):
+    """
+    Read a 3d grid property from a grdecl file, see open_grdecl for description
+    of format.
+
+    Args:
+        filename (pathlib.Path or str): File in grdecl format.
+        keyword (str): The keyword of the property in the file
+        dimensions ((int,int,int)): Triple of the size of grid.
+        dtype (function): The datatype to be read, ie., float.
+
+    Raises:
+        xtgeo.KeywordNotFoundError: If keyword is not found in the file.
+
+    Returns:
+        numpy array with given dimensions and data type read
+        from the grdecl file.
+    """
+    result = None
+
+    with open_grdecl(filename, keywords=[], simple_keywords=(keyword,)) as kw_generator:
+        try:
+            _, result = next(kw_generator)
+        except StopIteration as si:
+            raise xtgeo.KeywordNotFoundError(
+                f"Cannot import {keyword}, not present in file {filename}?"
+            ) from si
+
+    # The values are stored in F order in the grdecl file
+    f_order_values = np.array([dtype(v) for v in result])
+    return np.ascontiguousarray(f_order_values.reshape(dimensions, order="F"))
+
+
+def import_grdecl_prop(pfile, name, grid):
+    """Read a GRDECL ASCII property record"""
+    result = dict()
+    result["ncol"] = grid.ncol
+    result["nrow"] = grid.nrow
+    result["nlay"] = grid.nlay
+    result["name"] = name
+    result["filesrc"] = pfile
+    actnumv = grid.get_actnum().values
+
+    result["values"] = ma.masked_where(
+        actnumv == 0, read_grdecl_3d_property(pfile.file, name, grid.dimensions, float)
+    )
+    return result
```

## xtgeo/grid3d/_gridprop_import_roff.py

 * *Ordering differences only*

```diff
@@ -1,52 +1,52 @@
-"""Importing grid props from ROFF, binary"""
-
-
-import warnings
-
-import numpy as np
-import xtgeo
-
-from ._roff_parameter import RoffParameter
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_roff(pfile, name=None, grid=None):
-    """Import ROFF format"""
-
-    if name == "unknown":
-        warnings.warn(
-            "Using name='unknown' to select first property in roff file"
-            " is deprecated, use name=None instead",
-            DeprecationWarning,
-        )
-        name = None
-
-    result = dict()
-    roff_param = RoffParameter.from_file(pfile._file, name)
-    result["codes"] = roff_param.xtgeo_codes()
-    result["name"] = roff_param.name
-    result["ncol"] = int(roff_param.nx)
-    result["nrow"] = int(roff_param.ny)
-    result["nlay"] = int(roff_param.nz)
-    result["discrete"] = roff_param.is_discrete
-    result["values"] = roff_param.xtgeo_values()
-    if grid is not None:
-        result["values"] = np.ma.masked_where(
-            grid.get_actnum().values < 1,
-            result["values"],
-        )
-
-    roff_val = roff_param.values
-    if isinstance(roff_val, bytes) or np.issubdtype(roff_val.dtype, np.uint8):
-        result["roxar_dtype"] = np.uint8
-    elif np.issubdtype(roff_val.dtype, np.integer):
-        result["roxar_dtype"] = np.uint16
-    elif np.issubdtype(roff_val.dtype, np.floating):
-        result["roxar_dtype"] = np.float32
-    else:
-        raise ValueError(f"Could not deduce roxar type of {roff_val.dtype}")
-
-    return result
+"""Importing grid props from ROFF, binary"""
+
+
+import warnings
+
+import numpy as np
+import xtgeo
+
+from ._roff_parameter import RoffParameter
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_roff(pfile, name=None, grid=None):
+    """Import ROFF format"""
+
+    if name == "unknown":
+        warnings.warn(
+            "Using name='unknown' to select first property in roff file"
+            " is deprecated, use name=None instead",
+            DeprecationWarning,
+        )
+        name = None
+
+    result = dict()
+    roff_param = RoffParameter.from_file(pfile._file, name)
+    result["codes"] = roff_param.xtgeo_codes()
+    result["name"] = roff_param.name
+    result["ncol"] = int(roff_param.nx)
+    result["nrow"] = int(roff_param.ny)
+    result["nlay"] = int(roff_param.nz)
+    result["discrete"] = roff_param.is_discrete
+    result["values"] = roff_param.xtgeo_values()
+    if grid is not None:
+        result["values"] = np.ma.masked_where(
+            grid.get_actnum().values < 1,
+            result["values"],
+        )
+
+    roff_val = roff_param.values
+    if isinstance(roff_val, bytes) or np.issubdtype(roff_val.dtype, np.uint8):
+        result["roxar_dtype"] = np.uint8
+    elif np.issubdtype(roff_val.dtype, np.integer):
+        result["roxar_dtype"] = np.uint16
+    elif np.issubdtype(roff_val.dtype, np.floating):
+        result["roxar_dtype"] = np.float32
+    else:
+        raise ValueError(f"Could not deduce roxar type of {roff_val.dtype}")
+
+    return result
```

## xtgeo/grid3d/_gridprop_import_xtgcpprop.py

 * *Ordering differences only*

```diff
@@ -1,107 +1,107 @@
-"""GridProperty import function of xtgcpprop format."""
-
-import json
-from collections import OrderedDict
-from struct import unpack
-
-import numpy as np
-
-import xtgeo
-import xtgeo.common.sys as xsys
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_xtgcpprop(mfile, ijrange=None, zerobased=False):
-    """Using pure python for experimental xtgcpprop import.
-
-    Args:
-        mfile (_XTGeoFile): Input file reference
-        ijrange (list-like): List or tuple with 4 members [i_from, i_to, j_from, j_to]
-            where cell indices are zero based (starts with 0)
-        zerobased (bool): If ijrange basis is zero or one.
-
-    """
-    offset = 36
-    with open(mfile.file, "rb") as fhandle:
-        buf = fhandle.read(offset)
-
-    # unpack header
-    swap, magic, nbyte, ncol, nrow, nlay = unpack("= i i i q q q", buf)
-
-    if swap != 1 or magic not in (1351, 1352):
-        raise ValueError("Invalid file format (wrong swap id or magic number).")
-
-    if magic == 1351:
-        dtype = np.float32 if nbyte == 4 else np.float64
-    else:
-        dtype = "int" + str(nbyte * 8)
-
-    vals = None
-    narr = ncol * nrow * nlay
-
-    ncolnew = nrownew = 0
-
-    if ijrange:
-        vals, ncolnew, nrownew = _import_xtgcpprop_partial(
-            mfile, nbyte, dtype, offset, ijrange, zerobased, ncol, nrow, nlay
-        )
-
-    else:
-        vals = xsys.npfromfile(mfile.file, dtype=dtype, count=narr, offset=offset)
-
-    # read metadata which will be at position offet + nfloat*narr +13
-    pos = offset + nbyte * narr + 13
-
-    with open(mfile.file, "rb") as fhandle:
-        fhandle.seek(pos)
-        jmeta = fhandle.read().decode()
-
-    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-    req = meta["_required_"]
-
-    reqattrs = xtgeo.MetaDataCPProperty.REQUIRED
-
-    result = dict()
-    for myattr in reqattrs:
-        result[myattr] = req[myattr]
-
-    if ijrange:
-        result["ncol"] = ncolnew
-        result["nrow"] = nrownew
-
-    result["values"] = np.ma.masked_equal(
-        vals.reshape((result["ncol"], result["nrow"], result["nlay"])),
-        xtgeo.UNDEF_INT if result["discrete"] else xtgeo.UNDEF,
-    )
-    return result
-
-
-def _import_xtgcpprop_partial(
-    mfile, nbyte, dtype, offset, ijrange, zerobased, ncol, nrow, nlay
-):
-    """Partial import of a property."""
-    i1, i2, j1, j2 = ijrange
-    if not zerobased:
-        i1 -= 1
-        i2 -= 1
-        j1 -= 1
-        j2 -= 1
-
-    ncolnew = i2 - i1 + 1
-    nrownew = j2 - j1 + 1
-
-    if ncolnew < 1 or ncolnew > ncol or nrownew < 1 or nrownew > nrow:
-        raise ValueError("The ijrange spesification is invalid.")
-
-    vals = np.zeros(ncolnew * nrownew * nlay, dtype=dtype)
-
-    for newnum, inum in enumerate(range(i1, i2 + 1)):
-        newpos = offset + (inum * nrow * nlay + j1 * nlay) * nbyte
-        ncount = nrownew * nlay
-        xvals = xsys.npfromfile(mfile.file, dtype=dtype, count=ncount, offset=newpos)
-        vals[newnum * ncount : newnum * ncount + ncount] = xvals
-
-    return vals, ncolnew, nrownew
+"""GridProperty import function of xtgcpprop format."""
+
+import json
+from collections import OrderedDict
+from struct import unpack
+
+import numpy as np
+
+import xtgeo
+import xtgeo.common.sys as xsys
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_xtgcpprop(mfile, ijrange=None, zerobased=False):
+    """Using pure python for experimental xtgcpprop import.
+
+    Args:
+        mfile (_XTGeoFile): Input file reference
+        ijrange (list-like): List or tuple with 4 members [i_from, i_to, j_from, j_to]
+            where cell indices are zero based (starts with 0)
+        zerobased (bool): If ijrange basis is zero or one.
+
+    """
+    offset = 36
+    with open(mfile.file, "rb") as fhandle:
+        buf = fhandle.read(offset)
+
+    # unpack header
+    swap, magic, nbyte, ncol, nrow, nlay = unpack("= i i i q q q", buf)
+
+    if swap != 1 or magic not in (1351, 1352):
+        raise ValueError("Invalid file format (wrong swap id or magic number).")
+
+    if magic == 1351:
+        dtype = np.float32 if nbyte == 4 else np.float64
+    else:
+        dtype = "int" + str(nbyte * 8)
+
+    vals = None
+    narr = ncol * nrow * nlay
+
+    ncolnew = nrownew = 0
+
+    if ijrange:
+        vals, ncolnew, nrownew = _import_xtgcpprop_partial(
+            mfile, nbyte, dtype, offset, ijrange, zerobased, ncol, nrow, nlay
+        )
+
+    else:
+        vals = xsys.npfromfile(mfile.file, dtype=dtype, count=narr, offset=offset)
+
+    # read metadata which will be at position offet + nfloat*narr +13
+    pos = offset + nbyte * narr + 13
+
+    with open(mfile.file, "rb") as fhandle:
+        fhandle.seek(pos)
+        jmeta = fhandle.read().decode()
+
+    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+    req = meta["_required_"]
+
+    reqattrs = xtgeo.MetaDataCPProperty.REQUIRED
+
+    result = dict()
+    for myattr in reqattrs:
+        result[myattr] = req[myattr]
+
+    if ijrange:
+        result["ncol"] = ncolnew
+        result["nrow"] = nrownew
+
+    result["values"] = np.ma.masked_equal(
+        vals.reshape((result["ncol"], result["nrow"], result["nlay"])),
+        xtgeo.UNDEF_INT if result["discrete"] else xtgeo.UNDEF,
+    )
+    return result
+
+
+def _import_xtgcpprop_partial(
+    mfile, nbyte, dtype, offset, ijrange, zerobased, ncol, nrow, nlay
+):
+    """Partial import of a property."""
+    i1, i2, j1, j2 = ijrange
+    if not zerobased:
+        i1 -= 1
+        i2 -= 1
+        j1 -= 1
+        j2 -= 1
+
+    ncolnew = i2 - i1 + 1
+    nrownew = j2 - j1 + 1
+
+    if ncolnew < 1 or ncolnew > ncol or nrownew < 1 or nrownew > nrow:
+        raise ValueError("The ijrange spesification is invalid.")
+
+    vals = np.zeros(ncolnew * nrownew * nlay, dtype=dtype)
+
+    for newnum, inum in enumerate(range(i1, i2 + 1)):
+        newpos = offset + (inum * nrow * nlay + j1 * nlay) * nbyte
+        ncount = nrownew * nlay
+        xvals = xsys.npfromfile(mfile.file, dtype=dtype, count=ncount, offset=newpos)
+        vals[newnum * ncount : newnum * ncount + ncount] = xvals
+
+    return vals, ncolnew, nrownew
```

## xtgeo/grid3d/_gridprop_lowlevel.py

 * *Ordering differences only*

```diff
@@ -1,160 +1,160 @@
-"""GridProperty (not GridProperies) low level functions"""
-
-
-import numpy as np
-import numpy.ma as ma
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def f2c_order(obj, values1d):
-    """Convert values1d from Fortran to C order, obj can be a Grid() or GridProperty()
-    instance
-    """
-    val = np.reshape(values1d, (obj.ncol, obj.nrow, obj.nlay), order="F")
-    val = np.asanyarray(val, order="C")
-    val = val.ravel()
-    return val
-
-
-def c2f_order(obj, values1d):
-    """Convert values1d from C to F order, obj can be a Grid() or GridProperty()
-    instance
-    """
-    val = np.reshape(values1d, (obj.ncol, obj.nrow, obj.nlay), order="C")
-    val = np.asanyarray(val, order="F")
-    val = val.ravel(order="F")
-    return val
-
-
-def update_values_from_carray(self, carray, dtype, delete=False):
-    """Transfer values from SWIG 1D carray to numpy, 3D array"""
-
-    logger.debug("Update numpy from C array values")
-
-    nv = self.ntotal
-
-    self._isdiscrete = False
-
-    if dtype == np.float64:
-        logger.info("Entering conversion to numpy (float64) ...")
-        values1d = _cxtgeo.swig_carr_to_numpy_1d(nv, carray)
-    else:
-        logger.info("Entering conversion to numpy (int32) ...")
-        values1d = _cxtgeo.swig_carr_to_numpy_i1d(nv, carray)
-        self._isdiscrete = True
-
-    values = np.reshape(values1d, (self._ncol, self._nrow, self._nlay), order="F")
-
-    # make into C order as this is standard Python order...
-    values = np.asanyarray(values, order="C")
-
-    # make it float64 or whatever(?) and mask it
-    self._values = values
-    self.mask_undef()
-
-    # optionally delete the C array if needed
-    if delete:
-        carray = delete_carray(self, carray)
-
-
-def update_carray(self, undef=None, discrete=None, dtype=None, order="F"):
-    """Copy (update) values from numpy to SWIG, 1D array, returns a pointer
-    to SWIG C array. If discrete is defined as True or False, force
-    the SWIG array to be of that kind.
-
-    Note that dtype will "override" current datatype if set. The resulting
-    carray will be in Fortran order, unless order is specified as 'C'
-    """
-
-    dstatus = self._isdiscrete
-    if discrete is not None:
-        dstatus = bool(discrete)
-
-    if undef is None:
-        undef = xtgeo.UNDEF
-        if dstatus:
-            undef = xtgeo.UNDEF_INT
-
-    logger.debug("Entering conversion from numpy to C array ...")
-
-    values = self._values.copy()
-
-    if not dtype:
-        if dstatus:
-            values = values.astype(np.int32)
-        else:
-            values = values.astype(np.float64)
-    else:
-        values = values.astype(dtype)
-
-    values = ma.filled(values, undef)
-
-    if order == "F":
-        values = np.asfortranarray(values)
-        values1d = np.ravel(values, order="F")
-
-    if values1d.dtype == "float64" and dstatus and not dtype:
-        values1d = values1d.astype("int32")
-        logger.debug("Casting has been done")
-
-    if values1d.dtype == "float64":
-        logger.debug("Convert to carray (double)")
-        carray = _cxtgeo.new_doublearray(self.ntotal)
-        _cxtgeo.swig_numpy_to_carr_1d(values1d, carray)
-    elif values1d.dtype == "float32":
-        logger.debug("Convert to carray (float)")
-        carray = _cxtgeo.new_floatarray(self.ntotal)
-        _cxtgeo.swig_numpy_to_carr_f1d(values1d, carray)
-    elif values1d.dtype == "int32":
-        logger.debug("Convert to carray (int32)")
-        carray = _cxtgeo.new_intarray(self.ntotal)
-        _cxtgeo.swig_numpy_to_carr_i1d(values1d, carray)
-    else:
-        raise RuntimeError(f"Unsupported dtype, probable bug in {__name__}")
-    return carray
-
-
-def delete_carray(self, carray):
-    """Delete carray SWIG C pointer, return carray as None"""
-
-    logger.debug("Enter delete carray values method for %d", id(self))
-    if carray is None:
-        return None
-
-    if "int" in str(carray):
-        _cxtgeo.delete_intarray(carray)
-        carray = None
-    elif "float" in str(carray):
-        _cxtgeo.delete_floatarray(carray)
-        carray = None
-    elif "double" in str(carray):
-        _cxtgeo.delete_doublearray(carray)
-        carray = None
-    else:
-        raise RuntimeError("BUG?")
-
-    return carray
-
-
-def check_shape_ok(self, values):
-    """Check if chape of values is OK"""
-    (ncol, nrow, nlay) = values.shape
-    if ncol != self._ncol or nrow != self._nrow or nlay != self._nlay:
-        logger.error(
-            "Wrong shape: Dimens of values %s %s %s" "vs %s %s %s",
-            ncol,
-            nrow,
-            nlay,
-            self._ncol,
-            self._nrow,
-            self._nlay,
-        )
-        return False
-    return True
+"""GridProperty (not GridProperies) low level functions"""
+
+
+import numpy as np
+import numpy.ma as ma
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def f2c_order(obj, values1d):
+    """Convert values1d from Fortran to C order, obj can be a Grid() or GridProperty()
+    instance
+    """
+    val = np.reshape(values1d, (obj.ncol, obj.nrow, obj.nlay), order="F")
+    val = np.asanyarray(val, order="C")
+    val = val.ravel()
+    return val
+
+
+def c2f_order(obj, values1d):
+    """Convert values1d from C to F order, obj can be a Grid() or GridProperty()
+    instance
+    """
+    val = np.reshape(values1d, (obj.ncol, obj.nrow, obj.nlay), order="C")
+    val = np.asanyarray(val, order="F")
+    val = val.ravel(order="F")
+    return val
+
+
+def update_values_from_carray(self, carray, dtype, delete=False):
+    """Transfer values from SWIG 1D carray to numpy, 3D array"""
+
+    logger.debug("Update numpy from C array values")
+
+    nv = self.ntotal
+
+    self._isdiscrete = False
+
+    if dtype == np.float64:
+        logger.info("Entering conversion to numpy (float64) ...")
+        values1d = _cxtgeo.swig_carr_to_numpy_1d(nv, carray)
+    else:
+        logger.info("Entering conversion to numpy (int32) ...")
+        values1d = _cxtgeo.swig_carr_to_numpy_i1d(nv, carray)
+        self._isdiscrete = True
+
+    values = np.reshape(values1d, (self._ncol, self._nrow, self._nlay), order="F")
+
+    # make into C order as this is standard Python order...
+    values = np.asanyarray(values, order="C")
+
+    # make it float64 or whatever(?) and mask it
+    self._values = values
+    self.mask_undef()
+
+    # optionally delete the C array if needed
+    if delete:
+        carray = delete_carray(self, carray)
+
+
+def update_carray(self, undef=None, discrete=None, dtype=None, order="F"):
+    """Copy (update) values from numpy to SWIG, 1D array, returns a pointer
+    to SWIG C array. If discrete is defined as True or False, force
+    the SWIG array to be of that kind.
+
+    Note that dtype will "override" current datatype if set. The resulting
+    carray will be in Fortran order, unless order is specified as 'C'
+    """
+
+    dstatus = self._isdiscrete
+    if discrete is not None:
+        dstatus = bool(discrete)
+
+    if undef is None:
+        undef = xtgeo.UNDEF
+        if dstatus:
+            undef = xtgeo.UNDEF_INT
+
+    logger.debug("Entering conversion from numpy to C array ...")
+
+    values = self._values.copy()
+
+    if not dtype:
+        if dstatus:
+            values = values.astype(np.int32)
+        else:
+            values = values.astype(np.float64)
+    else:
+        values = values.astype(dtype)
+
+    values = ma.filled(values, undef)
+
+    if order == "F":
+        values = np.asfortranarray(values)
+        values1d = np.ravel(values, order="F")
+
+    if values1d.dtype == "float64" and dstatus and not dtype:
+        values1d = values1d.astype("int32")
+        logger.debug("Casting has been done")
+
+    if values1d.dtype == "float64":
+        logger.debug("Convert to carray (double)")
+        carray = _cxtgeo.new_doublearray(self.ntotal)
+        _cxtgeo.swig_numpy_to_carr_1d(values1d, carray)
+    elif values1d.dtype == "float32":
+        logger.debug("Convert to carray (float)")
+        carray = _cxtgeo.new_floatarray(self.ntotal)
+        _cxtgeo.swig_numpy_to_carr_f1d(values1d, carray)
+    elif values1d.dtype == "int32":
+        logger.debug("Convert to carray (int32)")
+        carray = _cxtgeo.new_intarray(self.ntotal)
+        _cxtgeo.swig_numpy_to_carr_i1d(values1d, carray)
+    else:
+        raise RuntimeError(f"Unsupported dtype, probable bug in {__name__}")
+    return carray
+
+
+def delete_carray(self, carray):
+    """Delete carray SWIG C pointer, return carray as None"""
+
+    logger.debug("Enter delete carray values method for %d", id(self))
+    if carray is None:
+        return None
+
+    if "int" in str(carray):
+        _cxtgeo.delete_intarray(carray)
+        carray = None
+    elif "float" in str(carray):
+        _cxtgeo.delete_floatarray(carray)
+        carray = None
+    elif "double" in str(carray):
+        _cxtgeo.delete_doublearray(carray)
+        carray = None
+    else:
+        raise RuntimeError("BUG?")
+
+    return carray
+
+
+def check_shape_ok(self, values):
+    """Check if chape of values is OK"""
+    (ncol, nrow, nlay) = values.shape
+    if ncol != self._ncol or nrow != self._nrow or nlay != self._nlay:
+        logger.error(
+            "Wrong shape: Dimens of values %s %s %s" "vs %s %s %s",
+            ncol,
+            nrow,
+            nlay,
+            self._ncol,
+            self._nrow,
+            self._nlay,
+        )
+        return False
+    return True
```

## xtgeo/grid3d/_gridprop_op1.py

 * *Ordering differences only*

```diff
@@ -1,160 +1,160 @@
-"""Various grid property operations"""
-
-
-import numpy as np
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.grid3d import _gridprop_lowlevel as gl
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-# pylint: disable=protected-access
-
-
-def get_xy_value_lists(self, **kwargs):
-    """Get values for webportal format
-
-    Two cells:
-    [[[(x1,y1), (x2,y2), (x3,y3), (x4,y4)],
-    [(x5,y5), (x6,y6), (x7,y7), (x8,y8)]]]
-
-    If mask is True then inactive cells are ommited from the lists,
-    else the active cells corners will be present while the property
-    will have a -999 value.
-
-    """
-
-    grid = kwargs.get("grid", None)
-
-    mask = kwargs.get("mask", True)
-
-    if grid is None:
-        raise RuntimeError("Missing grid object")
-
-    if not isinstance(grid, xtgeo.grid3d.Grid):
-        raise RuntimeError("The input grid is not a XTGeo Grid instance")
-
-    if not isinstance(self, xtgeo.grid3d.GridProperty):
-        raise RuntimeError("The property is not a XTGeo GridProperty instance")
-
-    clist = grid.get_xyz_corners()
-    actnum = grid.get_actnum()
-
-    # set value 0 if actnum is 0 to facilitate later operations
-    if mask:
-        for cli in clist:
-            cli.values[actnum.values == 0] = 0
-
-    # now some numpy operations (coffee?, any?)
-    xy0 = np.column_stack((clist[0].values1d, clist[1].values1d))
-    xy1 = np.column_stack((clist[3].values1d, clist[4].values1d))
-    xy2 = np.column_stack((clist[6].values1d, clist[7].values1d))
-    xy3 = np.column_stack((clist[9].values1d, clist[10].values1d))
-
-    xyc = np.column_stack((xy0, xy1, xy2, xy3))
-    xyc = xyc.reshape(grid.nlay, grid.ncol * grid.nrow, 4, 2)
-
-    coordlist = xyc.tolist()
-
-    # remove cells that are undefined ("marked" as coordinate [0, 0] if mask)
-    coordlist = [
-        [[tuple(xy) for xy in cell if xy[0] > 0] for cell in lay] for lay in coordlist
-    ]
-
-    coordlist = [[cell for cell in lay if len(cell) > 1] for lay in coordlist]
-
-    pval = self.values1d.reshape((grid.nlay, grid.ncol * grid.nrow))
-    valuelist = pval.tolist(fill_value=-999.0)
-    if mask:
-        valuelist = [[val for val in lay if val != -999.0] for lay in valuelist]
-
-    return coordlist, valuelist
-
-
-def operation_polygons(self, poly, value, opname="add", inside=True):
-    """A generic function for doing operations restricted to inside
-    or outside polygon(s).
-    """
-
-    grid = self.geometry
-    grid._xtgformat1()
-
-    if not isinstance(poly, xtgeo.xyz.Polygons):
-        raise ValueError("The poly input is not a Polygons instance")
-
-    # make a copy of the array which is used a "filter" or "proxy"
-    # value will be 1 inside polygons, 0 outside. Undef cells are kept as is
-    dtype = self.dtype
-
-    proxy = self.copy()
-    proxy.discrete_to_continuous()
-
-    proxy.values *= 0.0
-    cvals = gl.update_carray(proxy)
-
-    idgroups = poly.dataframe.groupby(poly.pname)
-
-    for id_, grp in idgroups:
-        xcor = grp[poly.xname].values
-        ycor = grp[poly.yname].values
-
-        ier = _cxtgeo.grd3d_setval_poly(
-            xcor,
-            ycor,
-            self.ncol,
-            self.nrow,
-            self.nlay,
-            grid._coordsv,
-            grid._zcornsv,
-            grid._actnumsv,
-            cvals,
-            1,
-        )
-        if ier == -9:
-            print(f"## Polygon no {id_ + 1} is not closed")
-
-    gl.update_values_from_carray(proxy, cvals, np.float64, delete=True)
-
-    proxyv = proxy.values.astype(np.int8)
-
-    proxytarget = 1
-    if not inside:
-        proxytarget = 0
-
-    if opname == "add":
-        tmp = self.values.copy() + value
-    elif opname == "sub":
-        tmp = self.values.copy() - value
-    elif opname == "mul":
-        tmp = self.values.copy() * value
-    elif opname == "div":
-        # Dividing a map of zero is always a hazzle; try to obtain 0.0
-        # as result in these cases
-        if 0.0 in value:
-            xtg.warn(
-                "Dividing a surface with value or surface with zero "
-                "elements; may get unexpected results, try to "
-                "achieve zero values as result!"
-            )
-        with np.errstate(divide="ignore", invalid="ignore"):
-            this = np.ma.filled(self.values, fill_value=1.0)
-            that = np.ma.filled(value, fill_value=1.0)
-            mask = np.ma.getmaskarray(self.values)
-            tmp = np.true_divide(this, that)
-            tmp = np.where(np.isinf(tmp), 0, tmp)
-            tmp = np.nan_to_num(tmp)
-            tmp = np.ma.array(tmp, mask=mask)
-
-    elif opname == "set":
-        tmp = self.values.copy() * 0 + value
-
-    # convert tmp back to correct dtype
-    tmp = tmp.astype(dtype)
-
-    self.values[proxyv == proxytarget] = tmp[proxyv == proxytarget]
-    del tmp
+"""Various grid property operations"""
+
+
+import numpy as np
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.grid3d import _gridprop_lowlevel as gl
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+# pylint: disable=protected-access
+
+
+def get_xy_value_lists(self, **kwargs):
+    """Get values for webportal format
+
+    Two cells:
+    [[[(x1,y1), (x2,y2), (x3,y3), (x4,y4)],
+    [(x5,y5), (x6,y6), (x7,y7), (x8,y8)]]]
+
+    If mask is True then inactive cells are ommited from the lists,
+    else the active cells corners will be present while the property
+    will have a -999 value.
+
+    """
+
+    grid = kwargs.get("grid", None)
+
+    mask = kwargs.get("mask", True)
+
+    if grid is None:
+        raise RuntimeError("Missing grid object")
+
+    if not isinstance(grid, xtgeo.grid3d.Grid):
+        raise RuntimeError("The input grid is not a XTGeo Grid instance")
+
+    if not isinstance(self, xtgeo.grid3d.GridProperty):
+        raise RuntimeError("The property is not a XTGeo GridProperty instance")
+
+    clist = grid.get_xyz_corners()
+    actnum = grid.get_actnum()
+
+    # set value 0 if actnum is 0 to facilitate later operations
+    if mask:
+        for cli in clist:
+            cli.values[actnum.values == 0] = 0
+
+    # now some numpy operations (coffee?, any?)
+    xy0 = np.column_stack((clist[0].values1d, clist[1].values1d))
+    xy1 = np.column_stack((clist[3].values1d, clist[4].values1d))
+    xy2 = np.column_stack((clist[6].values1d, clist[7].values1d))
+    xy3 = np.column_stack((clist[9].values1d, clist[10].values1d))
+
+    xyc = np.column_stack((xy0, xy1, xy2, xy3))
+    xyc = xyc.reshape(grid.nlay, grid.ncol * grid.nrow, 4, 2)
+
+    coordlist = xyc.tolist()
+
+    # remove cells that are undefined ("marked" as coordinate [0, 0] if mask)
+    coordlist = [
+        [[tuple(xy) for xy in cell if xy[0] > 0] for cell in lay] for lay in coordlist
+    ]
+
+    coordlist = [[cell for cell in lay if len(cell) > 1] for lay in coordlist]
+
+    pval = self.values1d.reshape((grid.nlay, grid.ncol * grid.nrow))
+    valuelist = pval.tolist(fill_value=-999.0)
+    if mask:
+        valuelist = [[val for val in lay if val != -999.0] for lay in valuelist]
+
+    return coordlist, valuelist
+
+
+def operation_polygons(self, poly, value, opname="add", inside=True):
+    """A generic function for doing operations restricted to inside
+    or outside polygon(s).
+    """
+
+    grid = self.geometry
+    grid._xtgformat1()
+
+    if not isinstance(poly, xtgeo.xyz.Polygons):
+        raise ValueError("The poly input is not a Polygons instance")
+
+    # make a copy of the array which is used a "filter" or "proxy"
+    # value will be 1 inside polygons, 0 outside. Undef cells are kept as is
+    dtype = self.dtype
+
+    proxy = self.copy()
+    proxy.discrete_to_continuous()
+
+    proxy.values *= 0.0
+    cvals = gl.update_carray(proxy)
+
+    idgroups = poly.dataframe.groupby(poly.pname)
+
+    for id_, grp in idgroups:
+        xcor = grp[poly.xname].values
+        ycor = grp[poly.yname].values
+
+        ier = _cxtgeo.grd3d_setval_poly(
+            xcor,
+            ycor,
+            self.ncol,
+            self.nrow,
+            self.nlay,
+            grid._coordsv,
+            grid._zcornsv,
+            grid._actnumsv,
+            cvals,
+            1,
+        )
+        if ier == -9:
+            print(f"## Polygon no {id_ + 1} is not closed")
+
+    gl.update_values_from_carray(proxy, cvals, np.float64, delete=True)
+
+    proxyv = proxy.values.astype(np.int8)
+
+    proxytarget = 1
+    if not inside:
+        proxytarget = 0
+
+    if opname == "add":
+        tmp = self.values.copy() + value
+    elif opname == "sub":
+        tmp = self.values.copy() - value
+    elif opname == "mul":
+        tmp = self.values.copy() * value
+    elif opname == "div":
+        # Dividing a map of zero is always a hazzle; try to obtain 0.0
+        # as result in these cases
+        if 0.0 in value:
+            xtg.warn(
+                "Dividing a surface with value or surface with zero "
+                "elements; may get unexpected results, try to "
+                "achieve zero values as result!"
+            )
+        with np.errstate(divide="ignore", invalid="ignore"):
+            this = np.ma.filled(self.values, fill_value=1.0)
+            that = np.ma.filled(value, fill_value=1.0)
+            mask = np.ma.getmaskarray(self.values)
+            tmp = np.true_divide(this, that)
+            tmp = np.where(np.isinf(tmp), 0, tmp)
+            tmp = np.nan_to_num(tmp)
+            tmp = np.ma.array(tmp, mask=mask)
+
+    elif opname == "set":
+        tmp = self.values.copy() * 0 + value
+
+    # convert tmp back to correct dtype
+    tmp = tmp.astype(dtype)
+
+    self.values[proxyv == proxytarget] = tmp[proxyv == proxytarget]
+    del tmp
```

## xtgeo/grid3d/_gridprop_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,227 +1,227 @@
-# coding: utf-8
-"""Roxar API functions for XTGeo Grid Property."""
-
-import numpy as np
-import numpy.ma as ma
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-
-try:
-    import roxar  # type: ignore
-except ImportError:
-    pass
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-# self is the XTGeo GridProperty instance
-# pragma: no cover
-
-VALID_ROXAR_DTYPES = [np.uint8, np.uint16, np.float32]
-
-
-def import_prop_roxapi(
-    project, gname, pname, realisation, faciescodes
-):  # pragma: no cover
-    """Import a Property via ROXAR API spec."""
-    logger.info("Opening RMS project ...")
-
-    rox = xtgeo.RoxUtils(project, readonly=True)
-
-    result = _get_gridprop_data(rox, gname, pname, realisation, faciescodes)
-
-    rox.safe_close()
-    return result
-
-
-def _get_gridprop_data(rox, gname, pname, realisation, faciescodes):  # pragma: no cover
-    # inside a RMS project
-
-    if gname not in rox.project.grid_models:
-        raise ValueError(f"No gridmodel with name {gname}")
-    if pname not in rox.project.grid_models[gname].properties:
-        raise ValueError(f"No property in {gname} with name {pname}")
-
-    try:
-        return _convert_to_xtgeo_prop(rox, gname, pname, realisation, faciescodes)
-    except KeyError as keyerror:
-        raise RuntimeError(keyerror) from keyerror
-
-
-def _convert_to_xtgeo_prop(
-    rox, gname, pname, realisation, faciescodes
-):  # pragma: no cover
-    result = dict()
-    roxgrid = rox.project.grid_models[gname]
-    roxprop = roxgrid.properties[pname]
-
-    if str(roxprop.type) in ("discrete", "body_facies"):
-        result["discrete"] = True
-    else:
-        result["discrete"] = False
-
-    result["roxorigin"] = True
-    indexer = roxgrid.get_grid(realisation=realisation).grid_indexer
-    result["ncol"], result["nrow"], result["nlay"] = indexer.dimensions
-
-    logger.info(indexer.ijk_handedness)
-
-    pvalues = roxprop.get_values(realisation=realisation)
-
-    if str(roxprop.type) == "body_facies" and faciescodes:
-        fmap = roxprop.get_facies_map(realisation=realisation)
-        pvalues = fmap[pvalues]  # numpy magics
-
-    result["roxar_dtype"] = pvalues.dtype
-
-    if result["discrete"]:
-        mybuffer = np.ndarray(indexer.dimensions, dtype=np.int32)
-        mybuffer.fill(xtgeo.UNDEF_INT)
-    else:
-        mybuffer = np.ndarray(indexer.dimensions, dtype=np.float64)
-        mybuffer.fill(xtgeo.UNDEF)
-
-    cellno = indexer.get_cell_numbers_in_range((0, 0, 0), indexer.dimensions)
-
-    ijk = indexer.get_indices(cellno)
-
-    iind = ijk[:, 0]
-    jind = ijk[:, 1]
-    kind = ijk[:, 2]
-
-    mybuffer[iind, jind, kind] = pvalues[cellno]
-
-    if result["discrete"]:
-        mybuffer = ma.masked_greater(mybuffer, xtgeo.UNDEF_INT_LIMIT)
-    else:
-        mybuffer = ma.masked_greater(mybuffer, xtgeo.UNDEF_LIMIT)
-
-    result["values"] = mybuffer
-    result["name"] = pname
-
-    if result["discrete"]:
-        result["codes"] = _fix_codes(
-            result["values"].reshape(-1).compressed(), roxprop.code_names
-        )
-        logger.info("Fixed codes: %s", result["codes"])
-    return result
-
-
-def export_prop_roxapi(
-    self, project, gname, pname, realisation=0, casting="unsafe"
-):  # pragma: no cover
-    """Export (i.e. store or save) to a Property icon in RMS via ROXAR API spec."""
-    rox = xtgeo.RoxUtils(project, readonly=False)
-
-    try:
-        roxgrid = rox.project.grid_models[gname]
-        _store_in_roxar(self, pname, roxgrid, realisation, casting)
-
-    except KeyError as keyerror:
-        raise RuntimeError(keyerror)
-
-    if rox._roxexternal:
-        rox.project.save()
-
-    rox.safe_close()
-
-
-def _store_in_roxar(self, pname, roxgrid, realisation, casting):  # pragma: no cover
-    """Store property in RMS."""
-    indexer = roxgrid.get_grid(realisation=realisation).grid_indexer
-
-    logger.info("Store in RMS...")
-
-    val3d = self.values.copy()
-
-    cellno = indexer.get_cell_numbers_in_range((0, 0, 0), indexer.dimensions)
-
-    ijk = indexer.get_indices(cellno)
-
-    iind = ijk[:, 0]
-    jind = ijk[:, 1]
-    kind = ijk[:, 2]
-
-    dtype = self._roxar_dtype
-    logger.info("DTYPE is %s for %s", dtype, pname)
-
-    # casting will secure correct types
-    if dtype not in VALID_ROXAR_DTYPES:
-        raise TypeError(
-            f"Roxar dtype is not valid: {dtype} must be in {VALID_ROXAR_DTYPES}"
-        )
-
-    if self.isdiscrete:
-        pvalues = roxgrid.get_grid(realisation=realisation).generate_values(
-            data_type=dtype
-        )
-        roxar_property_type = roxar.GridPropertyType.discrete
-
-    else:
-        pvalues = roxgrid.get_grid(realisation=realisation).generate_values(
-            data_type=dtype
-        )
-        roxar_property_type = roxar.GridPropertyType.continuous
-
-    pvalues[cellno] = val3d[iind, jind, kind]
-
-    properties = roxgrid.properties
-
-    if pname not in properties:
-        rprop = properties.create(
-            pname, property_type=roxar_property_type, data_type=dtype
-        )
-    else:
-        rprop = properties[pname]
-        dtype = rprop.data_type
-
-    rprop.set_values(pvalues.astype(dtype, casting=casting), realisation=realisation)
-
-    if self.isdiscrete:
-        rprop.code_names = _rox_compatible_codes(self.codes)
-
-
-def _fix_codes(active_values, codes):  # pragma: no cover
-    """Roxar may provide a code list with empty strings values, fix this issue here.
-
-    Roxar may also interpolate code values which are actually not present in the
-    property. Here, the presence of actual codes is also checked.
-    """
-    newcodes = {}
-    codes_data = {val: str(val) for val in np.unique(active_values)}
-
-    for code, name in codes.items():
-        if not isinstance(code, int):
-            code = int(code)
-
-        if not name:
-            name = str(code)
-
-        if code not in codes_data.keys():
-            continue
-
-        newcodes[code] = name
-
-    return newcodes
-
-
-def _rox_compatible_codes(codes: dict) -> dict:  # pragma: no cover
-    """Ensure that keys in codes are int's prior to storage in RMS."""
-
-    newcodes = {}
-    for code, name in codes.items():
-        if code is None:
-            continue  # skip codes of type None; assumed to be spurious
-        if not isinstance(code, int):
-            try:
-                code = int(code)
-            except ValueError:
-                raise ValueError(
-                    "The keys in codes must be an integer prior to RMS "
-                    f"storage. Actual key found here is '{code}' of type {type(code)}"
-                )
-
-        newcodes[code] = name
-    return newcodes
+# coding: utf-8
+"""Roxar API functions for XTGeo Grid Property."""
+
+import numpy as np
+import numpy.ma as ma
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+
+try:
+    import roxar  # type: ignore
+except ImportError:
+    pass
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+# self is the XTGeo GridProperty instance
+# pragma: no cover
+
+VALID_ROXAR_DTYPES = [np.uint8, np.uint16, np.float32]
+
+
+def import_prop_roxapi(
+    project, gname, pname, realisation, faciescodes
+):  # pragma: no cover
+    """Import a Property via ROXAR API spec."""
+    logger.info("Opening RMS project ...")
+
+    rox = xtgeo.RoxUtils(project, readonly=True)
+
+    result = _get_gridprop_data(rox, gname, pname, realisation, faciescodes)
+
+    rox.safe_close()
+    return result
+
+
+def _get_gridprop_data(rox, gname, pname, realisation, faciescodes):  # pragma: no cover
+    # inside a RMS project
+
+    if gname not in rox.project.grid_models:
+        raise ValueError(f"No gridmodel with name {gname}")
+    if pname not in rox.project.grid_models[gname].properties:
+        raise ValueError(f"No property in {gname} with name {pname}")
+
+    try:
+        return _convert_to_xtgeo_prop(rox, gname, pname, realisation, faciescodes)
+    except KeyError as keyerror:
+        raise RuntimeError(keyerror) from keyerror
+
+
+def _convert_to_xtgeo_prop(
+    rox, gname, pname, realisation, faciescodes
+):  # pragma: no cover
+    result = dict()
+    roxgrid = rox.project.grid_models[gname]
+    roxprop = roxgrid.properties[pname]
+
+    if str(roxprop.type) in ("discrete", "body_facies"):
+        result["discrete"] = True
+    else:
+        result["discrete"] = False
+
+    result["roxorigin"] = True
+    indexer = roxgrid.get_grid(realisation=realisation).grid_indexer
+    result["ncol"], result["nrow"], result["nlay"] = indexer.dimensions
+
+    logger.info(indexer.ijk_handedness)
+
+    pvalues = roxprop.get_values(realisation=realisation)
+
+    if str(roxprop.type) == "body_facies" and faciescodes:
+        fmap = roxprop.get_facies_map(realisation=realisation)
+        pvalues = fmap[pvalues]  # numpy magics
+
+    result["roxar_dtype"] = pvalues.dtype
+
+    if result["discrete"]:
+        mybuffer = np.ndarray(indexer.dimensions, dtype=np.int32)
+        mybuffer.fill(xtgeo.UNDEF_INT)
+    else:
+        mybuffer = np.ndarray(indexer.dimensions, dtype=np.float64)
+        mybuffer.fill(xtgeo.UNDEF)
+
+    cellno = indexer.get_cell_numbers_in_range((0, 0, 0), indexer.dimensions)
+
+    ijk = indexer.get_indices(cellno)
+
+    iind = ijk[:, 0]
+    jind = ijk[:, 1]
+    kind = ijk[:, 2]
+
+    mybuffer[iind, jind, kind] = pvalues[cellno]
+
+    if result["discrete"]:
+        mybuffer = ma.masked_greater(mybuffer, xtgeo.UNDEF_INT_LIMIT)
+    else:
+        mybuffer = ma.masked_greater(mybuffer, xtgeo.UNDEF_LIMIT)
+
+    result["values"] = mybuffer
+    result["name"] = pname
+
+    if result["discrete"]:
+        result["codes"] = _fix_codes(
+            result["values"].reshape(-1).compressed(), roxprop.code_names
+        )
+        logger.info("Fixed codes: %s", result["codes"])
+    return result
+
+
+def export_prop_roxapi(
+    self, project, gname, pname, realisation=0, casting="unsafe"
+):  # pragma: no cover
+    """Export (i.e. store or save) to a Property icon in RMS via ROXAR API spec."""
+    rox = xtgeo.RoxUtils(project, readonly=False)
+
+    try:
+        roxgrid = rox.project.grid_models[gname]
+        _store_in_roxar(self, pname, roxgrid, realisation, casting)
+
+    except KeyError as keyerror:
+        raise RuntimeError(keyerror)
+
+    if rox._roxexternal:
+        rox.project.save()
+
+    rox.safe_close()
+
+
+def _store_in_roxar(self, pname, roxgrid, realisation, casting):  # pragma: no cover
+    """Store property in RMS."""
+    indexer = roxgrid.get_grid(realisation=realisation).grid_indexer
+
+    logger.info("Store in RMS...")
+
+    val3d = self.values.copy()
+
+    cellno = indexer.get_cell_numbers_in_range((0, 0, 0), indexer.dimensions)
+
+    ijk = indexer.get_indices(cellno)
+
+    iind = ijk[:, 0]
+    jind = ijk[:, 1]
+    kind = ijk[:, 2]
+
+    dtype = self._roxar_dtype
+    logger.info("DTYPE is %s for %s", dtype, pname)
+
+    # casting will secure correct types
+    if dtype not in VALID_ROXAR_DTYPES:
+        raise TypeError(
+            f"Roxar dtype is not valid: {dtype} must be in {VALID_ROXAR_DTYPES}"
+        )
+
+    if self.isdiscrete:
+        pvalues = roxgrid.get_grid(realisation=realisation).generate_values(
+            data_type=dtype
+        )
+        roxar_property_type = roxar.GridPropertyType.discrete
+
+    else:
+        pvalues = roxgrid.get_grid(realisation=realisation).generate_values(
+            data_type=dtype
+        )
+        roxar_property_type = roxar.GridPropertyType.continuous
+
+    pvalues[cellno] = val3d[iind, jind, kind]
+
+    properties = roxgrid.properties
+
+    if pname not in properties:
+        rprop = properties.create(
+            pname, property_type=roxar_property_type, data_type=dtype
+        )
+    else:
+        rprop = properties[pname]
+        dtype = rprop.data_type
+
+    rprop.set_values(pvalues.astype(dtype, casting=casting), realisation=realisation)
+
+    if self.isdiscrete:
+        rprop.code_names = _rox_compatible_codes(self.codes)
+
+
+def _fix_codes(active_values, codes):  # pragma: no cover
+    """Roxar may provide a code list with empty strings values, fix this issue here.
+
+    Roxar may also interpolate code values which are actually not present in the
+    property. Here, the presence of actual codes is also checked.
+    """
+    newcodes = {}
+    codes_data = {val: str(val) for val in np.unique(active_values)}
+
+    for code, name in codes.items():
+        if not isinstance(code, int):
+            code = int(code)
+
+        if not name:
+            name = str(code)
+
+        if code not in codes_data.keys():
+            continue
+
+        newcodes[code] = name
+
+    return newcodes
+
+
+def _rox_compatible_codes(codes: dict) -> dict:  # pragma: no cover
+    """Ensure that keys in codes are int's prior to storage in RMS."""
+
+    newcodes = {}
+    for code, name in codes.items():
+        if code is None:
+            continue  # skip codes of type None; assumed to be spurious
+        if not isinstance(code, int):
+            try:
+                code = int(code)
+            except ValueError:
+                raise ValueError(
+                    "The keys in codes must be an integer prior to RMS "
+                    f"storage. Actual key found here is '{code}' of type {type(code)}"
+                )
+
+        newcodes[code] = name
+    return newcodes
```

## xtgeo/grid3d/_gridprop_value_init.py

 * *Ordering differences only*

```diff
@@ -1,101 +1,101 @@
-"""GridProperty (not GridProperies) some etc functions"""
-
-# from ._gridprop_import_eclrun import import_eclbinary
-#  --> INIT, UNRST
-#
-# from ._gridprop_import_grdecl import import_grdecl_prop, import_bgrdecl_prop
-#  --> ASCII and BINARY GRDECL format
-# from ._gridprop_import_roff import import_roff
-#  --> BINARY ROFF format
-
-
-import numpy as np
-
-import xtgeo
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def gridproperty_non_dummy_values(gridlike, dimensions, values, isdiscrete):
-    """Gives the initial values array of an gridprop.
-
-    Param:
-      gridlike: Either Grid or GridProperty, giving the mask to replicate.
-      dimensions: The dimensions of the gridprop
-      values: The values parameter given to init.
-      isdiscrete: The discrete parameter given to init.
-
-    Returns:
-      The array to be set to GridProp._values.
-    """
-    if values is None:
-        values = initial_gridprop_values_zero(dimensions, isdiscrete)
-    elif np.isscalar(values):
-        values = initial_gridprop_values_from_scalar(dimensions, values, isdiscrete)
-    elif isinstance(values, np.ndarray):
-        values = initial_gridprop_values_from_array(dimensions, values, isdiscrete)
-
-    if gridlike is not None:
-        if isinstance(gridlike, xtgeo.grid3d.Grid):
-            act = gridlike.get_actnum(asmasked=True)
-            values = np.ma.array(values, mask=np.ma.getmaskarray(act.values))
-        else:
-            values = np.ma.array(values, mask=np.ma.getmaskarray(gridlike.values))
-
-    return values
-
-
-def gridproperty_dummy_values(isdiscrete):
-    """Given no parameters to init, these dummy values should be set for backwards
-    compatability."""
-    if isdiscrete:
-        values = np.ma.MaskedArray(np.full((4, 3, 5), 99), dtype=np.int32)
-    else:
-        values = np.ma.MaskedArray(np.full((4, 3, 5), 99.0))
-    values[0:4, 0, 0:2] = np.ma.masked
-    return values
-
-
-def initial_gridprop_values_zero(dimensions, isdiscrete):
-    """Initial values for an GridProperty with zeros.
-
-    Given that the user supplies at least some parameters, but not a values array,
-    values should be initialized to zero.
-    Param:
-      dimensions: The dimensions of the gridproperty.
-
-
-    Returns:
-        zero initialized values array
-    """
-    if isdiscrete:
-        return np.ma.zeros(dimensions, dtype=np.int32)
-    return np.ma.zeros(dimensions)
-
-
-def initial_gridprop_values_from_scalar(dimensions, value, isdiscrete):
-    """Initial gridproperties values from scalar.
-
-    Given scalar values, the gridproperties value array should be
-    filled with that value, with possible conversion depending
-    on the isdiscrete parameter.
-
-    Returns:
-        filled array with given scalar value.
-    """
-    if isinstance(value, (float, int)):
-        dtype = np.float64
-        if isdiscrete:
-            dtype = np.int32
-        return np.ma.zeros(dimensions, dtype=dtype) + value
-    raise ValueError("Scalar input values of invalid type")
-
-
-def initial_gridprop_values_from_array(dimensions, values, isdiscrete):
-    """Initial gridproperties values from numpy array"""
-    values = np.ma.MaskedArray(values.reshape(dimensions))
-    if isdiscrete:
-        return values.astype(np.int32)
-    return values.astype(np.float64)
+"""GridProperty (not GridProperies) some etc functions"""
+
+# from ._gridprop_import_eclrun import import_eclbinary
+#  --> INIT, UNRST
+#
+# from ._gridprop_import_grdecl import import_grdecl_prop, import_bgrdecl_prop
+#  --> ASCII and BINARY GRDECL format
+# from ._gridprop_import_roff import import_roff
+#  --> BINARY ROFF format
+
+
+import numpy as np
+
+import xtgeo
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def gridproperty_non_dummy_values(gridlike, dimensions, values, isdiscrete):
+    """Gives the initial values array of an gridprop.
+
+    Param:
+      gridlike: Either Grid or GridProperty, giving the mask to replicate.
+      dimensions: The dimensions of the gridprop
+      values: The values parameter given to init.
+      isdiscrete: The discrete parameter given to init.
+
+    Returns:
+      The array to be set to GridProp._values.
+    """
+    if values is None:
+        values = initial_gridprop_values_zero(dimensions, isdiscrete)
+    elif np.isscalar(values):
+        values = initial_gridprop_values_from_scalar(dimensions, values, isdiscrete)
+    elif isinstance(values, np.ndarray):
+        values = initial_gridprop_values_from_array(dimensions, values, isdiscrete)
+
+    if gridlike is not None:
+        if isinstance(gridlike, xtgeo.grid3d.Grid):
+            act = gridlike.get_actnum(asmasked=True)
+            values = np.ma.array(values, mask=np.ma.getmaskarray(act.values))
+        else:
+            values = np.ma.array(values, mask=np.ma.getmaskarray(gridlike.values))
+
+    return values
+
+
+def gridproperty_dummy_values(isdiscrete):
+    """Given no parameters to init, these dummy values should be set for backwards
+    compatability."""
+    if isdiscrete:
+        values = np.ma.MaskedArray(np.full((4, 3, 5), 99), dtype=np.int32)
+    else:
+        values = np.ma.MaskedArray(np.full((4, 3, 5), 99.0))
+    values[0:4, 0, 0:2] = np.ma.masked
+    return values
+
+
+def initial_gridprop_values_zero(dimensions, isdiscrete):
+    """Initial values for an GridProperty with zeros.
+
+    Given that the user supplies at least some parameters, but not a values array,
+    values should be initialized to zero.
+    Param:
+      dimensions: The dimensions of the gridproperty.
+
+
+    Returns:
+        zero initialized values array
+    """
+    if isdiscrete:
+        return np.ma.zeros(dimensions, dtype=np.int32)
+    return np.ma.zeros(dimensions)
+
+
+def initial_gridprop_values_from_scalar(dimensions, value, isdiscrete):
+    """Initial gridproperties values from scalar.
+
+    Given scalar values, the gridproperties value array should be
+    filled with that value, with possible conversion depending
+    on the isdiscrete parameter.
+
+    Returns:
+        filled array with given scalar value.
+    """
+    if isinstance(value, (float, int)):
+        dtype = np.float64
+        if isdiscrete:
+            dtype = np.int32
+        return np.ma.zeros(dimensions, dtype=dtype) + value
+    raise ValueError("Scalar input values of invalid type")
+
+
+def initial_gridprop_values_from_array(dimensions, values, isdiscrete):
+    """Initial gridproperties values from numpy array"""
+    values = np.ma.MaskedArray(values.reshape(dimensions))
+    if isdiscrete:
+        return values.astype(np.int32)
+    return values.astype(np.float64)
```

## xtgeo/grid3d/_gridprops_import_eclrun.py

 * *Ordering differences only*

```diff
@@ -1,306 +1,306 @@
-from copy import deepcopy
-from typing import List, Tuple, Union
-
-import xtgeo
-from typing_extensions import Literal
-from xtgeo.common.constants import MAXKEYWORDS
-
-from . import _grid3d_utils as utils
-from ._find_gridprop_in_eclrun import (
-    find_gridprop_from_init_file,
-    find_gridprops_from_restart_file,
-    valid_gridprop_lengths,
-)
-from ._gridprop_import_eclrun import decorate_name
-from .grid_property import GridProperty
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def sanitize_date_list(
-    dates: Union[List[int], List[str], Literal["first", "all", "last"]]
-) -> Union[List[int], Literal["first", "all", "last"]]:
-    """
-    Converts dateformats of the form 'YYYY-MM-DD', 'YYYYMMDD' or YYYYMMDD to
-    list of integers of the form [YYYYMMDD] (ie. suitible for find_gridprops
-    functions), but lets the special literals 'first' and 'last' remain
-    unchanged.
-
-    >>> sanitize_date_list('first')
-    'first'
-    >>> sanitize_date_list('last')
-    'last'
-    >>> sanitize_date_list('all')
-    'all'
-    >>> sanitize_date_list(['2020-01-01'])
-    [20200101]
-    >>> sanitize_date_list(['20200101'])
-    [20200101]
-    >>> sanitize_date_list([20200101])
-    [20200101]
-    """
-    if dates in ("first", "last", "all"):
-        return dates
-    new_dates = []
-    for date in dates:
-        if isinstance(date, int):
-            new_dates.append(date)
-        else:
-            try:
-                if (
-                    isinstance(date, str)
-                    and len(date) == 10
-                    and date[4] == "-"
-                    and date[7] == "-"
-                ):
-                    date = date.replace("-", "")
-                new_dates.append(int(date))
-            except ValueError as err:
-                raise ValueError(
-                    "valid dates are either 'first'/'all'/'last', "
-                    "list ints of the form YYYYMMDD or list of strings of "
-                    f"'YYYY-MM-DD'/'YYYYMMDD' got {dates}"
-                ) from err
-    return new_dates
-
-
-def import_ecl_init_gridproperties(
-    pfile,
-    names: Union[List[str], Literal["all"]],
-    grid,
-    strict=True,
-    maxkeys: int = MAXKEYWORDS,
-) -> List[GridProperty]:
-    """Imports list of properties from an init file.
-
-    Note, the method does not determine whether a given keyword in the file
-    is a grid property, only that it has the correct data type and length
-    to be considered as a grid property.
-
-    Args:
-        pfile: Path to the ecl restart file
-        names: List of names to fetch, can also be "all" to fetch all properties.
-        grid: The grid used by the simulator to produce the restart file.
-        strict: If strict=True, will raise error if key is not found.
-        maxkeys: Maximum number of keywords allocated
-    Returns:
-        List of GridProperty objects fetched from the init file.
-    """
-    if not isinstance(pfile, xtgeo._XTGeoFile):
-        pfile = xtgeo._XTGeoFile(pfile)
-
-    if not grid:
-        raise ValueError("Grid Geometry object is missing")
-
-    if not names:
-        raise ValueError("Name list cannot be empty (None)")
-
-    # scan valid keywords
-    kwlist = utils.scan_keywords(
-        pfile,
-        fformat="xecl",
-        dataframe=True,
-        dates=True,
-        maxkeys=maxkeys,
-    )
-
-    validnames = list()
-
-    valid_lengths = valid_gridprop_lengths(grid)
-
-    # get a list of valid property names
-    for kw in list(kwlist.itertuples(index=False, name=None)):
-        kwname, _, nlen, _, _ = kw
-        if nlen in valid_lengths and kwname not in validnames:
-            validnames.append(kwname)
-
-    if names == "all":
-        usenames = deepcopy(validnames)
-    else:
-        usenames = list(names)
-
-    for name in usenames:
-        if name not in validnames:
-            if strict:
-                raise ValueError(
-                    f"Requested keyword {name} is not in INIT file,"
-                    f"valid entries are {validnames}, set strict=False to warn instead."
-                )
-            else:
-                logger.warning(
-                    "Requested keyword %s is not in INIT file."
-                    "Entry will not be read, set strict=True to raise Error instead.",
-                    name,
-                )
-
-    results = find_gridprop_from_init_file(
-        pfile.file,
-        names=names,
-        grid=grid,
-    )
-    properties_list = []
-    for result in results:
-        result["name"] = decorate_name(result["name"], grid.dualporo, fracture=False)
-        properties_list.append(GridProperty(**result))
-
-    return properties_list
-
-
-def import_ecl_restart_gridproperties(
-    pfile,
-    names: Union[List[str], Literal["all"]],
-    dates: Union[List[int], List[str], Literal["all", "last", "first"]],
-    grid,
-    strict: Tuple[bool, bool],
-    namestyle: Literal[0, 1],
-    maxkeys: int = MAXKEYWORDS,
-) -> List[GridProperty]:
-    """Imports list of gridproperties from a restart file.
-
-    Note, the method does not determine whether a given keyword in the file
-    is a grid property, only that it has the correct data type and length
-    to be considered as a grid property.
-
-    Args:
-        pfile: Path to the ecl restart file
-        names: List of names to fetch, can also be "all" to fetch all properties.
-        dates: List of xtgeo style dates (e.g. int(19990101) or "YYYYMMDD"),
-            also Also accepts "YYYY-MM-DD".  "all", "last" and "first" can be
-            given for all, last or first date(s) in the file. Dates=None means
-            look for properties in the init file.
-        grid: The grid used by the simulator to produce the restart file.
-        strict: strict (False, False) means that if keyname,
-            optionally with date is not found is will just warn and continue to
-            next. If (True, True) it will warn but TRY to import anyway, which
-            in turn may raise a KeywordNotError or DateNotFoundError.  The
-            (True, False) will be strict on keywords, but sloppy on dates,
-            meaning that missing dates will be skipped. However, if all dates
-            are missing an exception will be raised
-        namestyle : 0 (default) for style SWAT_20110223,
-            1 for SWAT--2011_02_23 (applies to restart only)
-        maxkeys: Maximum number of keywords allocated
-    Returns:
-        List of GridProperty objects fetched from the restart file.
-    """
-
-    strictkeycomb, strictdate = strict
-    if not grid:
-        raise ValueError("Grid Geometry object is missing")
-
-    if not names:
-        raise ValueError("Name list cannot be empty (None)")
-
-    dates = sanitize_date_list(dates)
-
-    # scan valid keywords with dates
-    kwlist = utils.scan_keywords(
-        pfile,
-        fformat="xecl",
-        dataframe=True,
-        dates=True,
-        maxkeys=maxkeys,
-    )
-
-    validnamedatepairs, validdates = _process_valid_namesdates(kwlist, grid)
-
-    # allow sloppy dates, i.e. remove invalid date entries
-    if isinstance(dates, list) and strictdate is False:
-        dates = _process_sloppydates(dates, validdates)
-
-    usenamedatepairs = list()
-    if names == "all" and dates == "all":
-        usenamedatepairs = deepcopy(validnamedatepairs)
-        usedates = dates
-    else:
-        if names == "all" and dates != "all":
-            usenames = [namedate[0] for namedate in validnamedatepairs]
-            usedates = dates
-        elif names != "all" and dates == "all":
-            usedates = [namedate[1] for namedate in validnamedatepairs]
-            usenames = names
-        else:
-            usedates = dates
-            usenames = names
-
-        for name in usenames:
-            for date in usedates:
-                usenamedatepairs.append((name, date))
-
-    # Do the actual import
-    for namedate in usenamedatepairs:
-        name, date = namedate
-
-        if name not in ("SGAS", "SOIL", "SWAT") and namedate not in validnamedatepairs:
-            # saturation keywords are a mess in Eclipse and friends; check later
-            if strictkeycomb:
-                raise ValueError(
-                    f"Keyword data combo {name} {date} is not in RESTART file."
-                    f"Possible entries are: {validnamedatepairs}"
-                )
-            else:
-                logger.warning(
-                    "Keyword data combo %s %s is not in RESTART file."
-                    "Possible entries are: %s"
-                    "Value will not be imported",
-                    name,
-                    date,
-                    validnamedatepairs,
-                )
-
-    results = find_gridprops_from_restart_file(pfile.file, names, dates, grid=grid)
-    properties_list = []
-    for result in results:
-        if namestyle == 1:
-            sdate = str(result["date"])
-            result["name"] += "--" + sdate[0:4] + "_" + sdate[4:6] + "_" + sdate[6:8]
-        else:
-            result["name"] = decorate_name(
-                result["name"], grid.dualporo, fracture=False, date=result["date"]
-            )
-
-        properties_list.append(GridProperty(**result))
-
-    return properties_list
-
-
-def _process_valid_namesdates(kwlist, grid):
-    """Return lists with valid pairs, dates scanned from RESTART"""
-    validnamedatepairs = list()
-    validdates = list()
-    valid_lengths = valid_gridprop_lengths(grid)
-    for kw in list(kwlist.itertuples(index=False, name=None)):
-        kwname, kwtyp, nlen, _, date = kw
-        if (
-            kwtyp != "CHAR"
-            and nlen in valid_lengths
-            and (kwname, date) not in validnamedatepairs
-        ):
-            validnamedatepairs.append((kwname, date))
-        if kwtyp != "CHAR" and nlen in valid_lengths and date not in validdates:
-            validdates.append(date)
-
-    return validnamedatepairs, validdates
-
-
-def _process_sloppydates(dates, validdates):
-    """Allow "sloppy dates", which removes invalid dates from the list"""
-
-    usedates = []
-    skipdates = []
-    for date in dates:
-        if date not in validdates:
-            skipdates.append(date)
-        else:
-            usedates.append(date)
-    if not usedates:
-        msg = f"No valid dates given (dates: {dates} vs {validdates})"
-        xtg.error(msg)
-        raise ValueError(msg)
-
-    if skipdates:
-        msg = f"Some dates not found: {skipdates}; will continue with dates: {usedates}"
-        xtg.warn(msg)
-
-    return usedates
+from copy import deepcopy
+from typing import List, Tuple, Union
+
+import xtgeo
+from typing_extensions import Literal
+from xtgeo.common.constants import MAXKEYWORDS
+
+from . import _grid3d_utils as utils
+from ._find_gridprop_in_eclrun import (
+    find_gridprop_from_init_file,
+    find_gridprops_from_restart_file,
+    valid_gridprop_lengths,
+)
+from ._gridprop_import_eclrun import decorate_name
+from .grid_property import GridProperty
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def sanitize_date_list(
+    dates: Union[List[int], List[str], Literal["first", "all", "last"]]
+) -> Union[List[int], Literal["first", "all", "last"]]:
+    """
+    Converts dateformats of the form 'YYYY-MM-DD', 'YYYYMMDD' or YYYYMMDD to
+    list of integers of the form [YYYYMMDD] (ie. suitible for find_gridprops
+    functions), but lets the special literals 'first' and 'last' remain
+    unchanged.
+
+    >>> sanitize_date_list('first')
+    'first'
+    >>> sanitize_date_list('last')
+    'last'
+    >>> sanitize_date_list('all')
+    'all'
+    >>> sanitize_date_list(['2020-01-01'])
+    [20200101]
+    >>> sanitize_date_list(['20200101'])
+    [20200101]
+    >>> sanitize_date_list([20200101])
+    [20200101]
+    """
+    if dates in ("first", "last", "all"):
+        return dates
+    new_dates = []
+    for date in dates:
+        if isinstance(date, int):
+            new_dates.append(date)
+        else:
+            try:
+                if (
+                    isinstance(date, str)
+                    and len(date) == 10
+                    and date[4] == "-"
+                    and date[7] == "-"
+                ):
+                    date = date.replace("-", "")
+                new_dates.append(int(date))
+            except ValueError as err:
+                raise ValueError(
+                    "valid dates are either 'first'/'all'/'last', "
+                    "list ints of the form YYYYMMDD or list of strings of "
+                    f"'YYYY-MM-DD'/'YYYYMMDD' got {dates}"
+                ) from err
+    return new_dates
+
+
+def import_ecl_init_gridproperties(
+    pfile,
+    names: Union[List[str], Literal["all"]],
+    grid,
+    strict=True,
+    maxkeys: int = MAXKEYWORDS,
+) -> List[GridProperty]:
+    """Imports list of properties from an init file.
+
+    Note, the method does not determine whether a given keyword in the file
+    is a grid property, only that it has the correct data type and length
+    to be considered as a grid property.
+
+    Args:
+        pfile: Path to the ecl restart file
+        names: List of names to fetch, can also be "all" to fetch all properties.
+        grid: The grid used by the simulator to produce the restart file.
+        strict: If strict=True, will raise error if key is not found.
+        maxkeys: Maximum number of keywords allocated
+    Returns:
+        List of GridProperty objects fetched from the init file.
+    """
+    if not isinstance(pfile, xtgeo._XTGeoFile):
+        pfile = xtgeo._XTGeoFile(pfile)
+
+    if not grid:
+        raise ValueError("Grid Geometry object is missing")
+
+    if not names:
+        raise ValueError("Name list cannot be empty (None)")
+
+    # scan valid keywords
+    kwlist = utils.scan_keywords(
+        pfile,
+        fformat="xecl",
+        dataframe=True,
+        dates=True,
+        maxkeys=maxkeys,
+    )
+
+    validnames = list()
+
+    valid_lengths = valid_gridprop_lengths(grid)
+
+    # get a list of valid property names
+    for kw in list(kwlist.itertuples(index=False, name=None)):
+        kwname, _, nlen, _, _ = kw
+        if nlen in valid_lengths and kwname not in validnames:
+            validnames.append(kwname)
+
+    if names == "all":
+        usenames = deepcopy(validnames)
+    else:
+        usenames = list(names)
+
+    for name in usenames:
+        if name not in validnames:
+            if strict:
+                raise ValueError(
+                    f"Requested keyword {name} is not in INIT file,"
+                    f"valid entries are {validnames}, set strict=False to warn instead."
+                )
+            else:
+                logger.warning(
+                    "Requested keyword %s is not in INIT file."
+                    "Entry will not be read, set strict=True to raise Error instead.",
+                    name,
+                )
+
+    results = find_gridprop_from_init_file(
+        pfile.file,
+        names=names,
+        grid=grid,
+    )
+    properties_list = []
+    for result in results:
+        result["name"] = decorate_name(result["name"], grid.dualporo, fracture=False)
+        properties_list.append(GridProperty(**result))
+
+    return properties_list
+
+
+def import_ecl_restart_gridproperties(
+    pfile,
+    names: Union[List[str], Literal["all"]],
+    dates: Union[List[int], List[str], Literal["all", "last", "first"]],
+    grid,
+    strict: Tuple[bool, bool],
+    namestyle: Literal[0, 1],
+    maxkeys: int = MAXKEYWORDS,
+) -> List[GridProperty]:
+    """Imports list of gridproperties from a restart file.
+
+    Note, the method does not determine whether a given keyword in the file
+    is a grid property, only that it has the correct data type and length
+    to be considered as a grid property.
+
+    Args:
+        pfile: Path to the ecl restart file
+        names: List of names to fetch, can also be "all" to fetch all properties.
+        dates: List of xtgeo style dates (e.g. int(19990101) or "YYYYMMDD"),
+            also Also accepts "YYYY-MM-DD".  "all", "last" and "first" can be
+            given for all, last or first date(s) in the file. Dates=None means
+            look for properties in the init file.
+        grid: The grid used by the simulator to produce the restart file.
+        strict: strict (False, False) means that if keyname,
+            optionally with date is not found is will just warn and continue to
+            next. If (True, True) it will warn but TRY to import anyway, which
+            in turn may raise a KeywordNotError or DateNotFoundError.  The
+            (True, False) will be strict on keywords, but sloppy on dates,
+            meaning that missing dates will be skipped. However, if all dates
+            are missing an exception will be raised
+        namestyle : 0 (default) for style SWAT_20110223,
+            1 for SWAT--2011_02_23 (applies to restart only)
+        maxkeys: Maximum number of keywords allocated
+    Returns:
+        List of GridProperty objects fetched from the restart file.
+    """
+
+    strictkeycomb, strictdate = strict
+    if not grid:
+        raise ValueError("Grid Geometry object is missing")
+
+    if not names:
+        raise ValueError("Name list cannot be empty (None)")
+
+    dates = sanitize_date_list(dates)
+
+    # scan valid keywords with dates
+    kwlist = utils.scan_keywords(
+        pfile,
+        fformat="xecl",
+        dataframe=True,
+        dates=True,
+        maxkeys=maxkeys,
+    )
+
+    validnamedatepairs, validdates = _process_valid_namesdates(kwlist, grid)
+
+    # allow sloppy dates, i.e. remove invalid date entries
+    if isinstance(dates, list) and strictdate is False:
+        dates = _process_sloppydates(dates, validdates)
+
+    usenamedatepairs = list()
+    if names == "all" and dates == "all":
+        usenamedatepairs = deepcopy(validnamedatepairs)
+        usedates = dates
+    else:
+        if names == "all" and dates != "all":
+            usenames = [namedate[0] for namedate in validnamedatepairs]
+            usedates = dates
+        elif names != "all" and dates == "all":
+            usedates = [namedate[1] for namedate in validnamedatepairs]
+            usenames = names
+        else:
+            usedates = dates
+            usenames = names
+
+        for name in usenames:
+            for date in usedates:
+                usenamedatepairs.append((name, date))
+
+    # Do the actual import
+    for namedate in usenamedatepairs:
+        name, date = namedate
+
+        if name not in ("SGAS", "SOIL", "SWAT") and namedate not in validnamedatepairs:
+            # saturation keywords are a mess in Eclipse and friends; check later
+            if strictkeycomb:
+                raise ValueError(
+                    f"Keyword data combo {name} {date} is not in RESTART file."
+                    f"Possible entries are: {validnamedatepairs}"
+                )
+            else:
+                logger.warning(
+                    "Keyword data combo %s %s is not in RESTART file."
+                    "Possible entries are: %s"
+                    "Value will not be imported",
+                    name,
+                    date,
+                    validnamedatepairs,
+                )
+
+    results = find_gridprops_from_restart_file(pfile.file, names, dates, grid=grid)
+    properties_list = []
+    for result in results:
+        if namestyle == 1:
+            sdate = str(result["date"])
+            result["name"] += "--" + sdate[0:4] + "_" + sdate[4:6] + "_" + sdate[6:8]
+        else:
+            result["name"] = decorate_name(
+                result["name"], grid.dualporo, fracture=False, date=result["date"]
+            )
+
+        properties_list.append(GridProperty(**result))
+
+    return properties_list
+
+
+def _process_valid_namesdates(kwlist, grid):
+    """Return lists with valid pairs, dates scanned from RESTART"""
+    validnamedatepairs = list()
+    validdates = list()
+    valid_lengths = valid_gridprop_lengths(grid)
+    for kw in list(kwlist.itertuples(index=False, name=None)):
+        kwname, kwtyp, nlen, _, date = kw
+        if (
+            kwtyp != "CHAR"
+            and nlen in valid_lengths
+            and (kwname, date) not in validnamedatepairs
+        ):
+            validnamedatepairs.append((kwname, date))
+        if kwtyp != "CHAR" and nlen in valid_lengths and date not in validdates:
+            validdates.append(date)
+
+    return validnamedatepairs, validdates
+
+
+def _process_sloppydates(dates, validdates):
+    """Allow "sloppy dates", which removes invalid dates from the list"""
+
+    usedates = []
+    skipdates = []
+    for date in dates:
+        if date not in validdates:
+            skipdates.append(date)
+        else:
+            usedates.append(date)
+    if not usedates:
+        msg = f"No valid dates given (dates: {dates} vs {validdates})"
+        xtg.error(msg)
+        raise ValueError(msg)
+
+    if skipdates:
+        msg = f"Some dates not found: {skipdates}; will continue with dates: {usedates}"
+        xtg.warn(msg)
+
+    return usedates
```

## xtgeo/grid3d/_gridprops_import_roff.py

 * *Ordering differences only*

```diff
@@ -1,65 +1,65 @@
-from typing import List, Union
-
-from typing_extensions import Literal
-
-import xtgeo
-from xtgeo.common.sys import _XTGeoFile
-
-from . import _grid3d_utils as utils
-from .grid_property import GridProperty
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_roff_gridproperties(
-    pfile: _XTGeoFile,
-    names: Union[List[str], Literal["all"]],
-    strict: bool = True,
-) -> List[GridProperty]:
-    """Imports list of properties from a roff file.
-
-    Args:
-        pfile: Reference to the file
-        names: List of names to fetch, can also be "all" to fetch all properties.
-        strict: If strict=True, will raise error if key is not found.
-    Returns:
-        List of GridProperty objects fetched from the ROFF file.
-    """
-    print(pfile)
-    validnames = []
-
-    collectdata = utils.scan_keywords(pfile, fformat="roff")
-    for item in collectdata:
-        keyname = item[0]
-        if keyname.startswith("parameter!name!"):
-            # format is 'parameter!name!FIPNUM'
-            validnames.append(keyname.split("!").pop())
-
-    usenames = []
-    if names == "all":
-        usenames = validnames
-    else:
-        for name in names:
-            if name not in validnames:
-                if strict:
-                    raise ValueError(
-                        f"Requested keyword {name} is not in ROFF file, valid "
-                        f"entries are {validnames}, set strict=False to warn instead."
-                    )
-                else:
-                    logger.warning(
-                        "Requested keyword %s is not in ROFF file. Entry will"
-                        "not be read, set strict=True to raise Error instead.",
-                        name,
-                    )
-            else:
-                usenames.append(name)
-
-    props = [
-        xtgeo.gridproperty_from_file(pfile.file, fformat="roff", name=name)
-        for name in usenames
-    ]
-
-    return props
+from typing import List, Union
+
+from typing_extensions import Literal
+
+import xtgeo
+from xtgeo.common.sys import _XTGeoFile
+
+from . import _grid3d_utils as utils
+from .grid_property import GridProperty
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_roff_gridproperties(
+    pfile: _XTGeoFile,
+    names: Union[List[str], Literal["all"]],
+    strict: bool = True,
+) -> List[GridProperty]:
+    """Imports list of properties from a roff file.
+
+    Args:
+        pfile: Reference to the file
+        names: List of names to fetch, can also be "all" to fetch all properties.
+        strict: If strict=True, will raise error if key is not found.
+    Returns:
+        List of GridProperty objects fetched from the ROFF file.
+    """
+    print(pfile)
+    validnames = []
+
+    collectdata = utils.scan_keywords(pfile, fformat="roff")
+    for item in collectdata:
+        keyname = item[0]
+        if keyname.startswith("parameter!name!"):
+            # format is 'parameter!name!FIPNUM'
+            validnames.append(keyname.split("!").pop())
+
+    usenames = []
+    if names == "all":
+        usenames = validnames
+    else:
+        for name in names:
+            if name not in validnames:
+                if strict:
+                    raise ValueError(
+                        f"Requested keyword {name} is not in ROFF file, valid "
+                        f"entries are {validnames}, set strict=False to warn instead."
+                    )
+                else:
+                    logger.warning(
+                        "Requested keyword %s is not in ROFF file. Entry will"
+                        "not be read, set strict=True to raise Error instead.",
+                        name,
+                    )
+            else:
+                usenames.append(name)
+
+    props = [
+        xtgeo.gridproperty_from_file(pfile.file, fformat="roff", name=name)
+        for name in usenames
+    ]
+
+    return props
```

## xtgeo/grid3d/_roff_grid.py

 * *Ordering differences only*

```diff
@@ -1,444 +1,444 @@
-from collections import OrderedDict, defaultdict
-from dataclasses import dataclass
-from typing import Optional
-
-import numpy as np
-import roffio
-
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-
-
-@dataclass
-class RoffGrid:
-    """
-    A RoffGrid contains a grid as represented in a roff file.  The
-    grid layout is corner point geometry with increasing x,y, and z
-    values. The coordinate values are stored in an local coordinate
-    system, defined by the x,y,z offset and scale values which
-    converts to utm/tvd values by the usual formula.
-
-    .. code-block:: python
-
-        x_utm = (x_local + xoffset) * xscale
-
-    The corner lines are stored in the corner_lines array and define
-    the edges of the cell. They go from the bottom layer to the
-    top layer in a straight line such that the north-west  to south-west
-    line of cell at index i,j,k is stored at
-
-    .. code-block:: python
-
-        node_index = i * (ny + 1)  + j
-        bottom_x = corner_lines[node_index]
-        bottom_y = corner_lines[node_index+1]
-        bottom_z = corner_lines[node_index+2]
-        top_x = corner_lines[node_index+3]
-        top_y = corner_lines[node_index+4]
-        top_z = corner_lines[node_index+5]
-
-    The z values of the corner of a cell is stored in zvals. For any
-    given corner there are 8 adjacent cells. These are usually given
-    the directions
-
-    below_nw, below_ne, below_sw, below_se,
-    above_nw, above_ne, above_sw and above_se.
-
-    where below means lower k value, above means higher k value,
-    south means lower j value, north means high j value, and
-    west means lower i value, east means higher i value.
-
-    All of these cells might have different z values along the line, meaning
-    there is a 'split' in the layers. How many different values there are is
-    dependent on the split_enz values which for any given corner can be 1,2,4
-    or 8.  1 means all cells have the same z value, 2 means the below cells
-    have different z value than the above cells in that corner. 4 means split
-    in north/south and east/west directions.  8 means split in all directions.
-
-    .. code-block:: python
-
-        node_index = i * (ny + 1) * (nz + 1) + j * (nz + 1) + k
-        split_number = roff_grid.split_enz[node_index]
-
-    The z-values are listed in c-order of i,j,k coordines, with the number
-    of z-values depends on the split number. for instance, if split_enz[0]=2
-    Then zvals[0] is the below z value for the corner at i=0,j=0,k=0 and
-    zvals[1] is the above z value for the same corner. zvals[2] is then
-    a z value for the i=0,j=0,k=1 corner. The z values are listed in
-    the order of below before above, south before north, and west before east.
-
-    The grid can optionally be divided into subgrids by layers. This is
-    defined by the subgrids array which for each subgrid contains the
-    number of layers to each subgrid in decreasing k-value.
-
-    Args:
-        nx (int): The number of cells in x direction.
-        ny (int): The number of cells in y direction.
-        nz (int): The number of cells in z direction.
-        subgrids (numpy.array of numpy.int32): The number of layers to each
-            subgrid by decreasing k value.
-        corner_lines (numpy.array of numpy.float32): The coordinates of the top
-            and bottom node for each corner line.
-        split_enz (bytes): The split number for any given node.
-        zvals (numpy.array of numpy.float32): Z values for each cell.
-        active (numpy.array of bool): Whether a given cell is active.
-        xoffset (float): translation value for utm coordinates.
-        yoffset (float): translation value for utm coordinates.
-        zoffset (float): translation value for tvd coordinates.
-        xscale (float): scaling value for utm coordinates.
-        yscale (float): scaling value for utm coordinates.
-        zscale (float): scaling value for tvd coordinates.
-
-    """
-
-    nx: int
-    ny: int
-    nz: int
-    corner_lines: np.ndarray
-    zvals: np.ndarray
-
-    split_enz: Optional[bytes] = None
-    active: Optional[np.ndarray] = None
-    subgrids: Optional[np.ndarray] = None
-    xoffset: float = 0.0
-    yoffset: float = 0.0
-    zoffset: float = 0.0
-    xscale: float = 1.0
-    yscale: float = 1.0
-    zscale: float = -1.0
-
-    def __post_init__(self):
-        if self.active is None:
-            self.active = np.ones(self.nx * self.ny * self.nz, dtype=np.bool_)
-        if self.split_enz is None:
-            self.split_enz = np.ones(
-                self.nx * self.ny * self.nz, dtype=np.uint8
-            ).tobytes()
-
-    def __eq__(self, other):
-        if not isinstance(other, RoffGrid):
-            return False
-        return (
-            self.nx == other.nx
-            and self.ny == other.ny
-            and self.nz == other.nz
-            and self.xoffset == other.xoffset
-            and self.yoffset == other.yoffset
-            and self.zoffset == other.zoffset
-            and self.xscale == other.xscale
-            and self.yscale == other.yscale
-            and self.zscale == other.zscale
-            and np.array_equal(self.subgrids, other.subgrids)
-            and np.array_equal(self.split_enz, other.split_enz)
-            and np.array_equal(self.zvals, other.zvals)
-            and np.array_equal(self.corner_lines, other.corner_lines)
-            and np.array_equal(self.active, other.active)
-        )
-
-    @property
-    def num_nodes(self):
-        """
-        The number of nodes in the grid, ie. the size of split_enz.
-        """
-        return (self.nx + 1) * (self.ny + 1) * (self.nz + 1)
-
-    def _create_lookup(self):
-        if not hasattr(self, "_lookup"):
-            n = self.num_nodes
-            self._lookup = np.zeros(n + 1, dtype=np.int32)
-            for i in range(n):
-                if self.split_enz is not None:
-                    self._lookup[i + 1] = self.split_enz[i] + self._lookup[i]
-                else:
-                    self._lookup[i + 1] = 1 + self._lookup[i]
-
-    def z_value(self, node):
-        """
-        Gives the 8 z values for any given node for
-        adjacent cells in the order:
-
-        * below_sw
-        * below_se
-        * below_nw
-        * below_ne
-        * above_sw
-        * above_se
-        * above_nw
-        * above_ne
-
-        Args:
-            node (tuple of i,j,k index): The index of the node.
-
-        Raises:
-            ValueError if the split array contains unsupported
-            split types. (must be 1,2,4 or 8)
-
-        Returns:
-            numpy array of float32 with z values for adjacent
-            corners in the order given above.
-        """
-        i, j, k = node
-        self._create_lookup()
-
-        node_number = i * (self.ny + 1) * (self.nz + 1) + j * (self.nz + 1) + k
-        pos = self._lookup[node_number]
-        split = self._lookup[node_number + 1] - self._lookup[node_number]
-
-        if split == 1:
-            return np.array([self.zvals[pos]] * 8)
-        elif split == 2:
-            return np.array([self.zvals[pos]] * 4 + [self.zvals[pos + 1]] * 4)
-        elif split == 4:
-            return np.array(
-                [
-                    self.zvals[pos],
-                    self.zvals[pos + 1],
-                    self.zvals[pos + 2],
-                    self.zvals[pos + 3],
-                ]
-                * 2
-            )
-        elif split == 8:
-            return np.array(
-                [
-                    self.zvals[pos],
-                    self.zvals[pos + 1],
-                    self.zvals[pos + 2],
-                    self.zvals[pos + 3],
-                    self.zvals[pos + 4],
-                    self.zvals[pos + 5],
-                    self.zvals[pos + 6],
-                    self.zvals[pos + 7],
-                ]
-            )
-        else:
-            raise ValueError("Only split types 1, 2, 4 and 8 are supported!")
-
-    def xtgeo_coord(self):
-        """
-        Returns:
-            The coordinates of nodes in the format of xtgeo.Grid.coordsv
-        """
-        offset = (self.xoffset, self.yoffset, self.zoffset)
-        scale = (self.xscale, self.yscale, self.zscale)
-        coordsv = self.corner_lines.reshape((self.nx + 1, self.ny + 1, 2, 3))
-        coordsv = np.flip(coordsv, -2)
-        coordsv = coordsv + offset
-        coordsv *= scale
-        return coordsv.reshape((self.nx + 1, self.ny + 1, 6)).astype(np.float64)
-
-    def xtgeo_actnum(self):
-        """
-        Returns:
-            The active field in the format of xtgeo.Grid.actnumsv
-        """
-        actnum = self.active.reshape((self.nx, self.ny, self.nz))
-        actnum = np.flip(actnum, -1)
-        return actnum.astype(np.int32)
-
-    def xtgeo_zcorn(self):
-        """
-        Returns:
-            The z values for nodes in the format of xtgeo.Grid.zcornsv
-        """
-        zcornsv = np.zeros(
-            (self.nx + 1) * (self.ny + 1) * (self.nz + 1) * 4, dtype=np.float32
-        )
-        retval = _cxtgeo.grd3d_roff2xtgeo_splitenz(
-            int(self.nz + 1),
-            float(self.zoffset),
-            float(self.zscale),
-            self.split_enz,
-            self.zvals,
-            zcornsv,
-        )
-        if retval == 0:
-            return zcornsv.reshape((self.nx + 1, self.ny + 1, self.nz + 1, 4))
-        elif retval == -1:
-            raise ValueError("Unsupported split type in split_enz")
-        elif retval == -2:
-            expected_size = (self.nx + 1) * (self.ny + 1) * (self.nz + 1)
-            raise ValueError(
-                "Incorrect size of splitenz,"
-                f" expected {expected_size} got {len(self.split_enz)}"
-            )
-        elif retval == -3:
-            expected_size = sum(self.split_enz)
-            raise ValueError(
-                "Incorrect size of zdata,"
-                f" expected {expected_size} got {len(self.zvals)}"
-            )
-        elif retval == -4:
-            raise ValueError(
-                "Incorrect size of zcorn,"
-                f" found {zcornsv.shape} should be multiple of {4 * self.nz}"
-            )
-        else:
-            raise ValueError(f"Unknown error {retval} occurred")
-
-    def xtgeo_subgrids(self):
-        """
-        Returns:
-            The z values for nodes in the format of xtgeo.Grid.zcornsv
-        """
-        if self.subgrids is None:
-            return None
-        result = OrderedDict()
-        next_ind = 1
-        for i, current in enumerate(self.subgrids):
-            result[f"subgrid_{i}"] = range(next_ind, current + next_ind)
-            next_ind += current
-        return result
-
-    @staticmethod
-    def _from_xtgeo_subgrids(xtgeo_subgrids):
-        """
-        Args:
-            A xtgeo.Grid._subgrids dictionary
-        Returns:
-            The corresponding RoffGrid.subgrids
-        """
-        if xtgeo_subgrids is None:
-            return None
-        subgrids = []
-        for key, value in xtgeo_subgrids.items():
-            if isinstance(value, range):
-                subgrids.append(value.stop - value.start)
-            elif value != list(range(value[0], value[-1] + 1)):
-                raise ValueError(
-                    "Cannot convert non-consecutive subgrids to roff format."
-                )
-            else:
-                subgrids.append(value[-1] + 1 - value[0])
-        return np.array(subgrids, dtype=np.int32)
-
-    @staticmethod
-    def from_xtgeo_grid(xtgeo_grid):
-        """
-        Args:
-            An xtgeo.Grid
-        Returns:
-            That grid geometry converted to a RoffGrid.
-        """
-        xtgeo_grid._xtgformat2()
-        nx, ny, nz = xtgeo_grid.dimensions
-        active = xtgeo_grid._actnumsv.reshape((nx, ny, nz))
-        active = np.flip(active, -1).ravel().astype(np.bool_)
-        corner_lines = xtgeo_grid._coordsv.reshape((nx + 1, ny + 1, 2, 3)) * np.array(
-            [1, 1, -1]
-        )
-        corner_lines = np.flip(corner_lines, -2).ravel().astype(np.float32)
-        zvals = xtgeo_grid._zcornsv.reshape((nx + 1, ny + 1, nz + 1, 4))
-        zvals = np.flip(zvals, 2).ravel().view(np.float32) * -1
-        split_enz = np.repeat(b"\x04", (nx + 1) * (ny + 1) * (nz + 1)).tobytes()
-        subgrids = RoffGrid._from_xtgeo_subgrids(xtgeo_grid._subgrids)
-
-        return RoffGrid(nx, ny, nz, corner_lines, zvals, split_enz, active, subgrids)
-
-    def to_file(self, filelike, roff_format=roffio.Format.BINARY):
-        """
-        Writes the RoffGrid to a roff file
-        Args:
-            filelike (str or byte stream): The file to write to.
-        """
-        data = {
-            "filedata": {"filetype": "grid"},
-            "dimensions": {"nX": self.nx, "nY": self.ny, "nZ": self.nz},
-            "translate": {
-                "xoffset": np.float32(self.xoffset),
-                "yoffset": np.float32(self.yoffset),
-                "zoffset": np.float32(self.zoffset),
-            },
-            "scale": {
-                "xscale": np.float32(self.xscale),
-                "yscale": np.float32(self.yscale),
-                "zscale": np.float32(self.zscale),
-            },
-            "cornerLines": {"data": self.corner_lines},
-            "zvalues": {"data": self.zvals},
-            "active": {"data": self.active},
-        }
-        if self.subgrids is not None:
-            data["subgrids"] = {"nLayers": self.subgrids}
-        if self.split_enz is not None:
-            data["zvalues"]["splitEnz"] = self.split_enz
-        roffio.write(filelike, data, roff_format=roff_format)
-
-    @staticmethod
-    def from_file(filelike):
-        """
-        Read a RoffGrid from a roff file
-        Args:
-            filelike (str or byte stream): The file to read from.
-        Returns:
-            The RoffGrid in the roff file.
-        """
-        translate_kws = {
-            "dimensions": {"nX": "nx", "nY": "ny", "nZ": "nz"},
-            "translate": {
-                "xoffset": "xoffset",
-                "yoffset": "yoffset",
-                "zoffset": "zoffset",
-            },
-            "scale": {
-                "xscale": "xscale",
-                "yscale": "yscale",
-                "zscale": "zscale",
-            },
-            "cornerLines": {"data": "corner_lines"},
-            "zvalues": {"splitEnz": "split_enz", "data": "zvals"},
-            "active": {"data": "active"},
-            "subgrids": {"nLayers": "subgrids"},
-        }
-        optional_keywords = defaultdict(
-            list,
-            {
-                "translate": ["xoffset", "yoffset", "zoffset"],
-                "scale": ["xscale", "yscale", "zscale"],
-                "subgrids": ["nLayers"],
-                "active": ["data"],
-            },
-        )
-        # The found dictionary contains all tags/tagkeys which we are
-        # interested in with None as the initial value. We go through the
-        # tag/tagkeys in the file and replace as they are found.
-        found = {
-            tag_name: {key_name: None for key_name in tag_keys.keys()}
-            for tag_name, tag_keys in translate_kws.items()
-        }
-        found["filedata"] = {"filetype": None}
-        with roffio.lazy_read(filelike) as tag_generator:
-            for tag, keys in tag_generator:
-                if tag in found:
-                    # We do not destruct keys yet as this fetches the value too early.
-                    # key is not a tuple but an object that fetches the value when
-                    # __getitem__ is called.
-                    for key in keys:
-                        if key[0] in found[tag]:
-                            if found[tag][key[0]] is not None:
-                                raise ValueError(
-                                    f"Multiple tag, tagkey pair {tag}, {key[0]}"
-                                    " in {filelike}"
-                                )
-                            found[tag][key[0]] = key[1]
-
-        for tag_name, keys in found.items():
-            for key_name, value in keys.items():
-                if value is None and key_name not in optional_keywords[tag_name]:
-                    raise ValueError(
-                        f"Missing non-optional keyword {tag_name}:{key_name}"
-                    )
-
-        filetype = found["filedata"]["filetype"]
-        if filetype != "grid":
-            raise ValueError(
-                f"File {filelike} did not have filetype set to grid, found {filetype}"
-            )
-
-        return RoffGrid(
-            **{
-                translated: found[tag][key]
-                for tag, tag_keys in translate_kws.items()
-                for key, translated in tag_keys.items()
-                if found[tag][key] is not None
-            }
-        )
+from collections import OrderedDict, defaultdict
+from dataclasses import dataclass
+from typing import Optional
+
+import numpy as np
+import roffio
+
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+
+
+@dataclass
+class RoffGrid:
+    """
+    A RoffGrid contains a grid as represented in a roff file.  The
+    grid layout is corner point geometry with increasing x,y, and z
+    values. The coordinate values are stored in an local coordinate
+    system, defined by the x,y,z offset and scale values which
+    converts to utm/tvd values by the usual formula.
+
+    .. code-block:: python
+
+        x_utm = (x_local + xoffset) * xscale
+
+    The corner lines are stored in the corner_lines array and define
+    the edges of the cell. They go from the bottom layer to the
+    top layer in a straight line such that the north-west  to south-west
+    line of cell at index i,j,k is stored at
+
+    .. code-block:: python
+
+        node_index = i * (ny + 1)  + j
+        bottom_x = corner_lines[node_index]
+        bottom_y = corner_lines[node_index+1]
+        bottom_z = corner_lines[node_index+2]
+        top_x = corner_lines[node_index+3]
+        top_y = corner_lines[node_index+4]
+        top_z = corner_lines[node_index+5]
+
+    The z values of the corner of a cell is stored in zvals. For any
+    given corner there are 8 adjacent cells. These are usually given
+    the directions
+
+    below_nw, below_ne, below_sw, below_se,
+    above_nw, above_ne, above_sw and above_se.
+
+    where below means lower k value, above means higher k value,
+    south means lower j value, north means high j value, and
+    west means lower i value, east means higher i value.
+
+    All of these cells might have different z values along the line, meaning
+    there is a 'split' in the layers. How many different values there are is
+    dependent on the split_enz values which for any given corner can be 1,2,4
+    or 8.  1 means all cells have the same z value, 2 means the below cells
+    have different z value than the above cells in that corner. 4 means split
+    in north/south and east/west directions.  8 means split in all directions.
+
+    .. code-block:: python
+
+        node_index = i * (ny + 1) * (nz + 1) + j * (nz + 1) + k
+        split_number = roff_grid.split_enz[node_index]
+
+    The z-values are listed in c-order of i,j,k coordines, with the number
+    of z-values depends on the split number. for instance, if split_enz[0]=2
+    Then zvals[0] is the below z value for the corner at i=0,j=0,k=0 and
+    zvals[1] is the above z value for the same corner. zvals[2] is then
+    a z value for the i=0,j=0,k=1 corner. The z values are listed in
+    the order of below before above, south before north, and west before east.
+
+    The grid can optionally be divided into subgrids by layers. This is
+    defined by the subgrids array which for each subgrid contains the
+    number of layers to each subgrid in decreasing k-value.
+
+    Args:
+        nx (int): The number of cells in x direction.
+        ny (int): The number of cells in y direction.
+        nz (int): The number of cells in z direction.
+        subgrids (numpy.array of numpy.int32): The number of layers to each
+            subgrid by decreasing k value.
+        corner_lines (numpy.array of numpy.float32): The coordinates of the top
+            and bottom node for each corner line.
+        split_enz (bytes): The split number for any given node.
+        zvals (numpy.array of numpy.float32): Z values for each cell.
+        active (numpy.array of bool): Whether a given cell is active.
+        xoffset (float): translation value for utm coordinates.
+        yoffset (float): translation value for utm coordinates.
+        zoffset (float): translation value for tvd coordinates.
+        xscale (float): scaling value for utm coordinates.
+        yscale (float): scaling value for utm coordinates.
+        zscale (float): scaling value for tvd coordinates.
+
+    """
+
+    nx: int
+    ny: int
+    nz: int
+    corner_lines: np.ndarray
+    zvals: np.ndarray
+
+    split_enz: Optional[bytes] = None
+    active: Optional[np.ndarray] = None
+    subgrids: Optional[np.ndarray] = None
+    xoffset: float = 0.0
+    yoffset: float = 0.0
+    zoffset: float = 0.0
+    xscale: float = 1.0
+    yscale: float = 1.0
+    zscale: float = -1.0
+
+    def __post_init__(self):
+        if self.active is None:
+            self.active = np.ones(self.nx * self.ny * self.nz, dtype=np.bool_)
+        if self.split_enz is None:
+            self.split_enz = np.ones(
+                self.nx * self.ny * self.nz, dtype=np.uint8
+            ).tobytes()
+
+    def __eq__(self, other):
+        if not isinstance(other, RoffGrid):
+            return False
+        return (
+            self.nx == other.nx
+            and self.ny == other.ny
+            and self.nz == other.nz
+            and self.xoffset == other.xoffset
+            and self.yoffset == other.yoffset
+            and self.zoffset == other.zoffset
+            and self.xscale == other.xscale
+            and self.yscale == other.yscale
+            and self.zscale == other.zscale
+            and np.array_equal(self.subgrids, other.subgrids)
+            and np.array_equal(self.split_enz, other.split_enz)
+            and np.array_equal(self.zvals, other.zvals)
+            and np.array_equal(self.corner_lines, other.corner_lines)
+            and np.array_equal(self.active, other.active)
+        )
+
+    @property
+    def num_nodes(self):
+        """
+        The number of nodes in the grid, ie. the size of split_enz.
+        """
+        return (self.nx + 1) * (self.ny + 1) * (self.nz + 1)
+
+    def _create_lookup(self):
+        if not hasattr(self, "_lookup"):
+            n = self.num_nodes
+            self._lookup = np.zeros(n + 1, dtype=np.int32)
+            for i in range(n):
+                if self.split_enz is not None:
+                    self._lookup[i + 1] = self.split_enz[i] + self._lookup[i]
+                else:
+                    self._lookup[i + 1] = 1 + self._lookup[i]
+
+    def z_value(self, node):
+        """
+        Gives the 8 z values for any given node for
+        adjacent cells in the order:
+
+        * below_sw
+        * below_se
+        * below_nw
+        * below_ne
+        * above_sw
+        * above_se
+        * above_nw
+        * above_ne
+
+        Args:
+            node (tuple of i,j,k index): The index of the node.
+
+        Raises:
+            ValueError if the split array contains unsupported
+            split types. (must be 1,2,4 or 8)
+
+        Returns:
+            numpy array of float32 with z values for adjacent
+            corners in the order given above.
+        """
+        i, j, k = node
+        self._create_lookup()
+
+        node_number = i * (self.ny + 1) * (self.nz + 1) + j * (self.nz + 1) + k
+        pos = self._lookup[node_number]
+        split = self._lookup[node_number + 1] - self._lookup[node_number]
+
+        if split == 1:
+            return np.array([self.zvals[pos]] * 8)
+        elif split == 2:
+            return np.array([self.zvals[pos]] * 4 + [self.zvals[pos + 1]] * 4)
+        elif split == 4:
+            return np.array(
+                [
+                    self.zvals[pos],
+                    self.zvals[pos + 1],
+                    self.zvals[pos + 2],
+                    self.zvals[pos + 3],
+                ]
+                * 2
+            )
+        elif split == 8:
+            return np.array(
+                [
+                    self.zvals[pos],
+                    self.zvals[pos + 1],
+                    self.zvals[pos + 2],
+                    self.zvals[pos + 3],
+                    self.zvals[pos + 4],
+                    self.zvals[pos + 5],
+                    self.zvals[pos + 6],
+                    self.zvals[pos + 7],
+                ]
+            )
+        else:
+            raise ValueError("Only split types 1, 2, 4 and 8 are supported!")
+
+    def xtgeo_coord(self):
+        """
+        Returns:
+            The coordinates of nodes in the format of xtgeo.Grid.coordsv
+        """
+        offset = (self.xoffset, self.yoffset, self.zoffset)
+        scale = (self.xscale, self.yscale, self.zscale)
+        coordsv = self.corner_lines.reshape((self.nx + 1, self.ny + 1, 2, 3))
+        coordsv = np.flip(coordsv, -2)
+        coordsv = coordsv + offset
+        coordsv *= scale
+        return coordsv.reshape((self.nx + 1, self.ny + 1, 6)).astype(np.float64)
+
+    def xtgeo_actnum(self):
+        """
+        Returns:
+            The active field in the format of xtgeo.Grid.actnumsv
+        """
+        actnum = self.active.reshape((self.nx, self.ny, self.nz))
+        actnum = np.flip(actnum, -1)
+        return actnum.astype(np.int32)
+
+    def xtgeo_zcorn(self):
+        """
+        Returns:
+            The z values for nodes in the format of xtgeo.Grid.zcornsv
+        """
+        zcornsv = np.zeros(
+            (self.nx + 1) * (self.ny + 1) * (self.nz + 1) * 4, dtype=np.float32
+        )
+        retval = _cxtgeo.grd3d_roff2xtgeo_splitenz(
+            int(self.nz + 1),
+            float(self.zoffset),
+            float(self.zscale),
+            self.split_enz,
+            self.zvals,
+            zcornsv,
+        )
+        if retval == 0:
+            return zcornsv.reshape((self.nx + 1, self.ny + 1, self.nz + 1, 4))
+        elif retval == -1:
+            raise ValueError("Unsupported split type in split_enz")
+        elif retval == -2:
+            expected_size = (self.nx + 1) * (self.ny + 1) * (self.nz + 1)
+            raise ValueError(
+                "Incorrect size of splitenz,"
+                f" expected {expected_size} got {len(self.split_enz)}"
+            )
+        elif retval == -3:
+            expected_size = sum(self.split_enz)
+            raise ValueError(
+                "Incorrect size of zdata,"
+                f" expected {expected_size} got {len(self.zvals)}"
+            )
+        elif retval == -4:
+            raise ValueError(
+                "Incorrect size of zcorn,"
+                f" found {zcornsv.shape} should be multiple of {4 * self.nz}"
+            )
+        else:
+            raise ValueError(f"Unknown error {retval} occurred")
+
+    def xtgeo_subgrids(self):
+        """
+        Returns:
+            The z values for nodes in the format of xtgeo.Grid.zcornsv
+        """
+        if self.subgrids is None:
+            return None
+        result = OrderedDict()
+        next_ind = 1
+        for i, current in enumerate(self.subgrids):
+            result[f"subgrid_{i}"] = range(next_ind, current + next_ind)
+            next_ind += current
+        return result
+
+    @staticmethod
+    def _from_xtgeo_subgrids(xtgeo_subgrids):
+        """
+        Args:
+            A xtgeo.Grid._subgrids dictionary
+        Returns:
+            The corresponding RoffGrid.subgrids
+        """
+        if xtgeo_subgrids is None:
+            return None
+        subgrids = []
+        for key, value in xtgeo_subgrids.items():
+            if isinstance(value, range):
+                subgrids.append(value.stop - value.start)
+            elif value != list(range(value[0], value[-1] + 1)):
+                raise ValueError(
+                    "Cannot convert non-consecutive subgrids to roff format."
+                )
+            else:
+                subgrids.append(value[-1] + 1 - value[0])
+        return np.array(subgrids, dtype=np.int32)
+
+    @staticmethod
+    def from_xtgeo_grid(xtgeo_grid):
+        """
+        Args:
+            An xtgeo.Grid
+        Returns:
+            That grid geometry converted to a RoffGrid.
+        """
+        xtgeo_grid._xtgformat2()
+        nx, ny, nz = xtgeo_grid.dimensions
+        active = xtgeo_grid._actnumsv.reshape((nx, ny, nz))
+        active = np.flip(active, -1).ravel().astype(np.bool_)
+        corner_lines = xtgeo_grid._coordsv.reshape((nx + 1, ny + 1, 2, 3)) * np.array(
+            [1, 1, -1]
+        )
+        corner_lines = np.flip(corner_lines, -2).ravel().astype(np.float32)
+        zvals = xtgeo_grid._zcornsv.reshape((nx + 1, ny + 1, nz + 1, 4))
+        zvals = np.flip(zvals, 2).ravel().view(np.float32) * -1
+        split_enz = np.repeat(b"\x04", (nx + 1) * (ny + 1) * (nz + 1)).tobytes()
+        subgrids = RoffGrid._from_xtgeo_subgrids(xtgeo_grid._subgrids)
+
+        return RoffGrid(nx, ny, nz, corner_lines, zvals, split_enz, active, subgrids)
+
+    def to_file(self, filelike, roff_format=roffio.Format.BINARY):
+        """
+        Writes the RoffGrid to a roff file
+        Args:
+            filelike (str or byte stream): The file to write to.
+        """
+        data = {
+            "filedata": {"filetype": "grid"},
+            "dimensions": {"nX": self.nx, "nY": self.ny, "nZ": self.nz},
+            "translate": {
+                "xoffset": np.float32(self.xoffset),
+                "yoffset": np.float32(self.yoffset),
+                "zoffset": np.float32(self.zoffset),
+            },
+            "scale": {
+                "xscale": np.float32(self.xscale),
+                "yscale": np.float32(self.yscale),
+                "zscale": np.float32(self.zscale),
+            },
+            "cornerLines": {"data": self.corner_lines},
+            "zvalues": {"data": self.zvals},
+            "active": {"data": self.active},
+        }
+        if self.subgrids is not None:
+            data["subgrids"] = {"nLayers": self.subgrids}
+        if self.split_enz is not None:
+            data["zvalues"]["splitEnz"] = self.split_enz
+        roffio.write(filelike, data, roff_format=roff_format)
+
+    @staticmethod
+    def from_file(filelike):
+        """
+        Read a RoffGrid from a roff file
+        Args:
+            filelike (str or byte stream): The file to read from.
+        Returns:
+            The RoffGrid in the roff file.
+        """
+        translate_kws = {
+            "dimensions": {"nX": "nx", "nY": "ny", "nZ": "nz"},
+            "translate": {
+                "xoffset": "xoffset",
+                "yoffset": "yoffset",
+                "zoffset": "zoffset",
+            },
+            "scale": {
+                "xscale": "xscale",
+                "yscale": "yscale",
+                "zscale": "zscale",
+            },
+            "cornerLines": {"data": "corner_lines"},
+            "zvalues": {"splitEnz": "split_enz", "data": "zvals"},
+            "active": {"data": "active"},
+            "subgrids": {"nLayers": "subgrids"},
+        }
+        optional_keywords = defaultdict(
+            list,
+            {
+                "translate": ["xoffset", "yoffset", "zoffset"],
+                "scale": ["xscale", "yscale", "zscale"],
+                "subgrids": ["nLayers"],
+                "active": ["data"],
+            },
+        )
+        # The found dictionary contains all tags/tagkeys which we are
+        # interested in with None as the initial value. We go through the
+        # tag/tagkeys in the file and replace as they are found.
+        found = {
+            tag_name: {key_name: None for key_name in tag_keys.keys()}
+            for tag_name, tag_keys in translate_kws.items()
+        }
+        found["filedata"] = {"filetype": None}
+        with roffio.lazy_read(filelike) as tag_generator:
+            for tag, keys in tag_generator:
+                if tag in found:
+                    # We do not destruct keys yet as this fetches the value too early.
+                    # key is not a tuple but an object that fetches the value when
+                    # __getitem__ is called.
+                    for key in keys:
+                        if key[0] in found[tag]:
+                            if found[tag][key[0]] is not None:
+                                raise ValueError(
+                                    f"Multiple tag, tagkey pair {tag}, {key[0]}"
+                                    " in {filelike}"
+                                )
+                            found[tag][key[0]] = key[1]
+
+        for tag_name, keys in found.items():
+            for key_name, value in keys.items():
+                if value is None and key_name not in optional_keywords[tag_name]:
+                    raise ValueError(
+                        f"Missing non-optional keyword {tag_name}:{key_name}"
+                    )
+
+        filetype = found["filedata"]["filetype"]
+        if filetype != "grid":
+            raise ValueError(
+                f"File {filelike} did not have filetype set to grid, found {filetype}"
+            )
+
+        return RoffGrid(
+            **{
+                translated: found[tag][key]
+                for tag, tag_keys in translate_kws.items()
+                for key, translated in tag_keys.items()
+                if found[tag][key] is not None
+            }
+        )
```

## xtgeo/grid3d/_roff_parameter.py

 * *Ordering differences only*

```diff
@@ -1,289 +1,289 @@
-import warnings
-from collections import OrderedDict, defaultdict
-from dataclasses import dataclass
-from typing import List, Optional, Union
-
-import numpy as np
-import roffio
-
-from xtgeo.common.constants import UNDEF_INT_LIMIT, UNDEF_LIMIT
-
-
-@dataclass
-class RoffParameter:
-    """
-    Roff parameter contains a parameter (1 value per grid cell) in a roff file.
-
-    Parameters are either discrete (int) or floating point. Discrete values can
-    come with code names giving the meaning of the values. The list code_names
-    gives the name for each value in code_values.
-
-    The value -999 for discrete parameters (255 for byte valued discrete
-    parameters) and -999.0 for floating point parameters are used undefined
-    value.
-
-    Args:
-        nx (int): The number of cells in x direction.
-        ny (int): The number of cells in y direction.
-        nz (int): The number of cells in z direction.
-        names (str): The name of the parameter
-        values (array of int32 or float, or bytes): One value per cell in c
-            order.
-        code_names (List of str): The name of the coded value.
-        code_values (array of int32): The code values.
-    """
-
-    nx: int
-    ny: int
-    nz: int
-
-    name: str
-    values: Union[np.ndarray, bytes]
-
-    code_names: Optional[List[str]] = None
-    code_values: Optional[np.ndarray] = None
-
-    def __eq__(self, other):
-        if not isinstance(other, RoffParameter):
-            return False
-        return (
-            self.nx == other.nx
-            and self.ny == other.ny
-            and self.nz == other.nz
-            and self.name == other.name
-            and np.array_equal(self.values, other.values)
-            and self.same_codes(other)
-        )
-
-    def same_codes(self, other):
-        """
-        Args:
-            other (RoffParameter): Any roff parameter
-        Returns:
-            True if the roff parameters have the same coded values.
-        """
-        if self.code_names is None:
-            return other.code_names is None
-        if self.code_values is None:
-            return other.code_values is None
-
-        if other.code_names is None:
-            return self.code_names is None
-        if other.code_values is None:
-            return self.code_values is None
-
-        return dict(zip(self.code_values, self.code_names)) == dict(
-            zip(other.code_values, other.code_names)
-        )
-
-    @property
-    def undefined_value(self):
-        """
-        Returns:
-            The undefined value for the type of values in the
-            roff parameter (either 255, -999, or -999.0)
-        """
-        if isinstance(self.values, bytes) or np.issubdtype(self.values.dtype, np.uint8):
-            return 255
-        if np.issubdtype(self.values.dtype, np.integer):
-            return -999
-        if np.issubdtype(self.values.dtype, np.floating):
-            return -999.0
-
-    @property
-    def is_discrete(self):
-        """
-        Returns:
-            True if the RoffParameter is a discrete type
-        """
-        return isinstance(self.values, bytes) or np.issubdtype(
-            self.values.dtype, np.integer
-        )
-
-    def xtgeo_codes(self):
-        """
-        Returns:
-            The discrete codes of the parameter in the format of
-            xtgeo.GridProperty.
-        """
-        if self.code_names is not None and self.code_values is not None:
-            return dict(zip(self.code_values, self.code_names))
-        else:
-            return dict()
-
-    def xtgeo_values(self):
-        """
-        Args:
-            The value to use for undefined. Defaults to that defined by
-            roff.
-        Returns:
-            The values in the format of xtgeo grid property
-        """
-        vals = self.values
-        if isinstance(vals, bytes):
-            vals = np.ndarray(len(vals), np.uint8, vals)
-        vals = vals.copy()
-        vals = np.flip(vals.reshape((self.nx, self.ny, self.nz)), -1)
-
-        if self.is_discrete:
-            vals = vals.astype(np.int32)
-        else:
-            vals = vals.astype(np.float64)
-
-        return np.ma.masked_values(vals, self.undefined_value)
-
-    @staticmethod
-    def from_xtgeo_grid_property(xtgeo_grid_property):
-        """
-        Args:
-            xtgeo_grid_property (xtgeo.GridProperty): Any xtgeo.GridProperty
-        Returns:
-            That grid property as a RoffParameter
-        """
-        code_names = None
-        code_values = None
-        if xtgeo_grid_property.isdiscrete:
-            code_names = list(xtgeo_grid_property.codes.values())
-            code_values = np.array(
-                list(xtgeo_grid_property.codes.keys()), dtype=np.int32
-            )
-
-        values = xtgeo_grid_property.values
-        if not np.ma.isMaskedArray(values):
-            if xtgeo_grid_property.isdiscrete:
-                values = np.ma.masked_greater(values, UNDEF_INT_LIMIT)
-            else:
-                values = np.ma.masked_greater(values, UNDEF_LIMIT)
-
-        if xtgeo_grid_property.isdiscrete:
-            values = values.astype(np.int32).filled(-999)
-        else:
-            # Although the roff format can contain double,
-            # double typed parameters are not read by RMS so we
-            # need to convert to float32 here
-            values = values.astype(np.float32).filled(-999.0)
-
-        return RoffParameter(
-            *xtgeo_grid_property.dimensions,
-            name=xtgeo_grid_property.name,
-            values=np.asarray(np.flip(values, -1).ravel()),
-            code_names=code_names,
-            code_values=code_values,
-        )
-
-    def to_file(self, filelike, roff_format=roffio.Format.BINARY):
-        """
-        Writes the RoffParameter to a roff file
-        Args:
-            filelike (str or byte stream): The file to write to.
-            roff_format (roffio.Format): The format to write the file in.
-        """
-        data = OrderedDict(
-            {
-                "filedata": {"filetype": "parameter"},
-                "dimensions": {"nX": self.nx, "nY": self.ny, "nZ": self.nz},
-                "parameter": {"name": self.name},
-            }
-        )
-        if self.code_names is not None:
-            data["parameter"]["codeNames"] = list(self.code_names)
-        if self.code_values is not None:
-            data["parameter"]["codeValues"] = self.code_values
-        data["parameter"]["data"] = self.values
-        with warnings.catch_warnings():
-            warnings.filterwarnings("ignore", r"casting array")
-            roffio.write(filelike, data, roff_format=roff_format)
-
-    @staticmethod
-    def from_file(filelike, name=None):
-        """
-        Read a RoffParameter from a roff file
-        Args:
-            filelike (str or byte stream): The file to read from.
-            name(str or None): The name of the parameter to get from the file,
-                if name=None, returns the first parameter read.
-        Returns:
-            The RoffGrid in the roff file.
-        """
-
-        def should_skip_parameter(tag, key):
-            if tag == "parameter" and key[0] == "name":
-                if name is None or key[1] == name:
-                    return False
-                return True
-            return False
-
-        translate_kws = {
-            "dimensions": {"nX": "nx", "nY": "ny", "nZ": "nz"},
-            "parameter": {
-                "name": "name",
-                "data": "values",
-                "codeValues": "code_values",
-                "codeNames": "code_names",
-            },
-        }
-        optional_keywords = defaultdict(
-            list,
-            {"parameter": ["codeValues", "codeNames"]},
-        )
-        # The found dictionary contains all tags/tagkeys which we are
-        # interested in with None as the initial value. We go through the
-        # tag/tagkeys in the file and replace as they are found.
-        found = {
-            tag_name: {key_name: None for key_name in tag_keys.keys()}
-            for tag_name, tag_keys in translate_kws.items()
-        }
-        found["filedata"] = {"filetype": None}
-        with roffio.lazy_read(filelike) as tag_generator:
-            for tag, keys in tag_generator:
-                if tag in found:
-                    if tag == "parameter" and found["parameter"]["data"] is not None:
-                        # We have already found the right parameter so skip
-                        # reading and potentially overwriting
-                        continue
-                    # We do not destruct keys yet as this fetches the value too early.
-                    # key is not a tuple but an object that fetches the value when
-                    # __getitem__ is called.
-                    for key in keys:
-                        if should_skip_parameter(tag, key):
-                            # Found a parameter, but not the one we are looking for
-                            # reset and look on
-                            for key_name in found["parameter"].keys():
-                                found["parameter"][key_name] = None
-                            break
-                        if key[0] in found[tag]:
-                            if found[tag][key[0]] is not None:
-                                raise ValueError(
-                                    f"Multiple tag, tagkey pair {tag}, {key[0]}"
-                                    f" in {filelike}"
-                                )
-                            found[tag][key[0]] = key[1]
-
-        if name is not None and (found["parameter"]["name"] != name):
-            raise ValueError(
-                "Did not find parameter"
-                f" {name} in roff file, got {found['parameter']['name']}"
-            )
-
-        for tag_name, keys in found.items():
-            for key_name, value in keys.items():
-                if value is None and key_name not in optional_keywords[tag_name]:
-                    raise ValueError(
-                        f"Missing non-optional keyword {tag_name}:{key_name}"
-                    )
-
-        filetype = found["filedata"]["filetype"]
-        if filetype not in ["grid", "parameter"]:
-            raise ValueError(
-                f"File {filelike} did not"
-                f" have filetype parameter or grid, found {filetype}"
-            )
-
-        return RoffParameter(
-            **{
-                translated: found[tag][key]
-                for tag, tag_keys in translate_kws.items()
-                for key, translated in tag_keys.items()
-                if found[tag][key] is not None
-            }
-        )
+import warnings
+from collections import OrderedDict, defaultdict
+from dataclasses import dataclass
+from typing import List, Optional, Union
+
+import numpy as np
+import roffio
+
+from xtgeo.common.constants import UNDEF_INT_LIMIT, UNDEF_LIMIT
+
+
+@dataclass
+class RoffParameter:
+    """
+    Roff parameter contains a parameter (1 value per grid cell) in a roff file.
+
+    Parameters are either discrete (int) or floating point. Discrete values can
+    come with code names giving the meaning of the values. The list code_names
+    gives the name for each value in code_values.
+
+    The value -999 for discrete parameters (255 for byte valued discrete
+    parameters) and -999.0 for floating point parameters are used undefined
+    value.
+
+    Args:
+        nx (int): The number of cells in x direction.
+        ny (int): The number of cells in y direction.
+        nz (int): The number of cells in z direction.
+        names (str): The name of the parameter
+        values (array of int32 or float, or bytes): One value per cell in c
+            order.
+        code_names (List of str): The name of the coded value.
+        code_values (array of int32): The code values.
+    """
+
+    nx: int
+    ny: int
+    nz: int
+
+    name: str
+    values: Union[np.ndarray, bytes]
+
+    code_names: Optional[List[str]] = None
+    code_values: Optional[np.ndarray] = None
+
+    def __eq__(self, other):
+        if not isinstance(other, RoffParameter):
+            return False
+        return (
+            self.nx == other.nx
+            and self.ny == other.ny
+            and self.nz == other.nz
+            and self.name == other.name
+            and np.array_equal(self.values, other.values)
+            and self.same_codes(other)
+        )
+
+    def same_codes(self, other):
+        """
+        Args:
+            other (RoffParameter): Any roff parameter
+        Returns:
+            True if the roff parameters have the same coded values.
+        """
+        if self.code_names is None:
+            return other.code_names is None
+        if self.code_values is None:
+            return other.code_values is None
+
+        if other.code_names is None:
+            return self.code_names is None
+        if other.code_values is None:
+            return self.code_values is None
+
+        return dict(zip(self.code_values, self.code_names)) == dict(
+            zip(other.code_values, other.code_names)
+        )
+
+    @property
+    def undefined_value(self):
+        """
+        Returns:
+            The undefined value for the type of values in the
+            roff parameter (either 255, -999, or -999.0)
+        """
+        if isinstance(self.values, bytes) or np.issubdtype(self.values.dtype, np.uint8):
+            return 255
+        if np.issubdtype(self.values.dtype, np.integer):
+            return -999
+        if np.issubdtype(self.values.dtype, np.floating):
+            return -999.0
+
+    @property
+    def is_discrete(self):
+        """
+        Returns:
+            True if the RoffParameter is a discrete type
+        """
+        return isinstance(self.values, bytes) or np.issubdtype(
+            self.values.dtype, np.integer
+        )
+
+    def xtgeo_codes(self):
+        """
+        Returns:
+            The discrete codes of the parameter in the format of
+            xtgeo.GridProperty.
+        """
+        if self.code_names is not None and self.code_values is not None:
+            return dict(zip(self.code_values, self.code_names))
+        else:
+            return dict()
+
+    def xtgeo_values(self):
+        """
+        Args:
+            The value to use for undefined. Defaults to that defined by
+            roff.
+        Returns:
+            The values in the format of xtgeo grid property
+        """
+        vals = self.values
+        if isinstance(vals, bytes):
+            vals = np.ndarray(len(vals), np.uint8, vals)
+        vals = vals.copy()
+        vals = np.flip(vals.reshape((self.nx, self.ny, self.nz)), -1)
+
+        if self.is_discrete:
+            vals = vals.astype(np.int32)
+        else:
+            vals = vals.astype(np.float64)
+
+        return np.ma.masked_values(vals, self.undefined_value)
+
+    @staticmethod
+    def from_xtgeo_grid_property(xtgeo_grid_property):
+        """
+        Args:
+            xtgeo_grid_property (xtgeo.GridProperty): Any xtgeo.GridProperty
+        Returns:
+            That grid property as a RoffParameter
+        """
+        code_names = None
+        code_values = None
+        if xtgeo_grid_property.isdiscrete:
+            code_names = list(xtgeo_grid_property.codes.values())
+            code_values = np.array(
+                list(xtgeo_grid_property.codes.keys()), dtype=np.int32
+            )
+
+        values = xtgeo_grid_property.values
+        if not np.ma.isMaskedArray(values):
+            if xtgeo_grid_property.isdiscrete:
+                values = np.ma.masked_greater(values, UNDEF_INT_LIMIT)
+            else:
+                values = np.ma.masked_greater(values, UNDEF_LIMIT)
+
+        if xtgeo_grid_property.isdiscrete:
+            values = values.astype(np.int32).filled(-999)
+        else:
+            # Although the roff format can contain double,
+            # double typed parameters are not read by RMS so we
+            # need to convert to float32 here
+            values = values.astype(np.float32).filled(-999.0)
+
+        return RoffParameter(
+            *xtgeo_grid_property.dimensions,
+            name=xtgeo_grid_property.name,
+            values=np.asarray(np.flip(values, -1).ravel()),
+            code_names=code_names,
+            code_values=code_values,
+        )
+
+    def to_file(self, filelike, roff_format=roffio.Format.BINARY):
+        """
+        Writes the RoffParameter to a roff file
+        Args:
+            filelike (str or byte stream): The file to write to.
+            roff_format (roffio.Format): The format to write the file in.
+        """
+        data = OrderedDict(
+            {
+                "filedata": {"filetype": "parameter"},
+                "dimensions": {"nX": self.nx, "nY": self.ny, "nZ": self.nz},
+                "parameter": {"name": self.name},
+            }
+        )
+        if self.code_names is not None:
+            data["parameter"]["codeNames"] = list(self.code_names)
+        if self.code_values is not None:
+            data["parameter"]["codeValues"] = self.code_values
+        data["parameter"]["data"] = self.values
+        with warnings.catch_warnings():
+            warnings.filterwarnings("ignore", r"casting array")
+            roffio.write(filelike, data, roff_format=roff_format)
+
+    @staticmethod
+    def from_file(filelike, name=None):
+        """
+        Read a RoffParameter from a roff file
+        Args:
+            filelike (str or byte stream): The file to read from.
+            name(str or None): The name of the parameter to get from the file,
+                if name=None, returns the first parameter read.
+        Returns:
+            The RoffGrid in the roff file.
+        """
+
+        def should_skip_parameter(tag, key):
+            if tag == "parameter" and key[0] == "name":
+                if name is None or key[1] == name:
+                    return False
+                return True
+            return False
+
+        translate_kws = {
+            "dimensions": {"nX": "nx", "nY": "ny", "nZ": "nz"},
+            "parameter": {
+                "name": "name",
+                "data": "values",
+                "codeValues": "code_values",
+                "codeNames": "code_names",
+            },
+        }
+        optional_keywords = defaultdict(
+            list,
+            {"parameter": ["codeValues", "codeNames"]},
+        )
+        # The found dictionary contains all tags/tagkeys which we are
+        # interested in with None as the initial value. We go through the
+        # tag/tagkeys in the file and replace as they are found.
+        found = {
+            tag_name: {key_name: None for key_name in tag_keys.keys()}
+            for tag_name, tag_keys in translate_kws.items()
+        }
+        found["filedata"] = {"filetype": None}
+        with roffio.lazy_read(filelike) as tag_generator:
+            for tag, keys in tag_generator:
+                if tag in found:
+                    if tag == "parameter" and found["parameter"]["data"] is not None:
+                        # We have already found the right parameter so skip
+                        # reading and potentially overwriting
+                        continue
+                    # We do not destruct keys yet as this fetches the value too early.
+                    # key is not a tuple but an object that fetches the value when
+                    # __getitem__ is called.
+                    for key in keys:
+                        if should_skip_parameter(tag, key):
+                            # Found a parameter, but not the one we are looking for
+                            # reset and look on
+                            for key_name in found["parameter"].keys():
+                                found["parameter"][key_name] = None
+                            break
+                        if key[0] in found[tag]:
+                            if found[tag][key[0]] is not None:
+                                raise ValueError(
+                                    f"Multiple tag, tagkey pair {tag}, {key[0]}"
+                                    f" in {filelike}"
+                                )
+                            found[tag][key[0]] = key[1]
+
+        if name is not None and (found["parameter"]["name"] != name):
+            raise ValueError(
+                "Did not find parameter"
+                f" {name} in roff file, got {found['parameter']['name']}"
+            )
+
+        for tag_name, keys in found.items():
+            for key_name, value in keys.items():
+                if value is None and key_name not in optional_keywords[tag_name]:
+                    raise ValueError(
+                        f"Missing non-optional keyword {tag_name}:{key_name}"
+                    )
+
+        filetype = found["filedata"]["filetype"]
+        if filetype not in ["grid", "parameter"]:
+            raise ValueError(
+                f"File {filelike} did not"
+                f" have filetype parameter or grid, found {filetype}"
+            )
+
+        return RoffParameter(
+            **{
+                translated: found[tag][key]
+                for tag, tag_keys in translate_kws.items()
+                for key, translated in tag_keys.items()
+                if found[tag][key] is not None
+            }
+        )
```

## xtgeo/grid3d/grid.py

 * *Ordering differences only*

```diff
@@ -1,2576 +1,2576 @@
-# -*- coding: utf-8 -*-
-"""Module/class for 3D grids (corner point geometry) with XTGeo."""
-
-import functools
-import json
-import warnings
-from collections import OrderedDict
-from pathlib import Path
-from typing import Optional, Tuple, Union
-
-import deprecation
-import numpy as np
-import numpy.ma as ma
-
-import xtgeo
-from xtgeo.common import XTGDescription, _XTGeoFile
-
-from . import (
-    _grid3d_fence,
-    _grid_etc1,
-    _grid_export,
-    _grid_hybrid,
-    _grid_import,
-    _grid_import_ecl,
-    _grid_import_xtgcpgeom,
-    _grid_refine,
-    _grid_roxapi,
-    _grid_wellzone,
-    _gridprop_lowlevel,
-)
-from ._ecl_grid import Units
-from ._grid3d import _Grid3D
-from .grid_properties import GridProperties
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-# --------------------------------------------------------------------------------------
-# Comment on "asmasked" vs "activeonly:
-#
-# "asmasked"=True will return a np.ma array, while "asmasked" = False will
-# return a np.ndarray
-#
-# The "activeonly" will filter out masked entries, or use None or np.nan
-# if "activeonly" is False.
-#
-# Use word "zerobased" for a bool regarding if startcell basis is 1 or 0
-#
-# For functions with mask=... ,they should be replaced with asmasked=...
-# --------------------------------------------------------------------------------------
-
-
-# METHODS as wrappers to class init + import
-def _handle_import(grid_constructor, gfile, fformat=None, **kwargs):
-    """Handles the import given a constructor.
-
-    For backwards compatability we need to call different constructors
-    with grid __init__ parameters (As returned by _grid_import.from_file).
-    These are generally either the _reset method of an instance or Grid().
-
-    This function takes such a constructor, remaining arguments are interpreted
-    as they are in _grid_import.from_file and calls the constructor with the
-    resulting arguments.
-
-    """
-    gfile = xtgeo._XTGeoFile(gfile, mode="rb")
-    if fformat == "eclipserun":
-        ecl_grid = grid_constructor(
-            **_grid_import.from_file(
-                xtgeo._XTGeoFile(gfile.name + ".EGRID", mode="rb"), fformat="egrid"
-            )
-        )
-        _grid_import_ecl.import_ecl_run(gfile.name, ecl_grid=ecl_grid, **kwargs)
-        return ecl_grid
-    return grid_constructor(**_grid_import.from_file(gfile, fformat, **kwargs))
-
-
-def grid_from_file(gfile, fformat=None, **kwargs):
-    """Read a grid (cornerpoint) from filelike and an returns a Grid() instance.
-
-    Args:
-        gfile (str or Path): File name to be imported. If fformat="eclipse_run"
-            then a fileroot name shall be input here, see example below.
-        fformat (str): File format egrid/roff/grdecl/bgrdecl/eclipserun/xtgcpgeom
-            (None is default and means "guess")
-        initprops (str list): Optional, and only applicable for file format
-            "eclipserun". Provide a list the names of the properties here. A
-            special value "all" can be get all properties found in the INIT file
-        restartprops (str list): Optional, see initprops
-        restartdates (int list): Optional, required if restartprops
-        ijkrange (list-like): Optional, only applicable for hdf files, see
-            :meth:`Grid.from_hdf`.
-        zerobased (bool): Optional, only applicable for hdf files, see
-            :meth:`Grid.from_hdf`.
-        mmap (bool): Optional, only applicable for xtgf files, see
-            :meth:`Grid.from_xtgf`.
-
-    Example::
-
-        >>> import xtgeo
-        >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-
-    Example using "eclipserun"::
-
-        >>> mycase = "REEK"  # meaning REEK.EGRID, REEK.INIT, REEK.UNRST
-        >>> xg = xtgeo.grid_from_file(
-        ...     reek_dir + "/" + mycase,
-        ...     fformat="eclipserun",
-        ...     initprops="all",
-        ... )
-        Grid ... filesrc='.../REEK.EGRID'
-
-    Raises:
-        OSError: if file is not found etc
-
-    """
-    return _handle_import(Grid, gfile, fformat, **kwargs)
-
-
-def grid_from_roxar(
-    project, gname, realisation=0, dimensions_only=False, info=False
-):  # pragma: no cover
-    """Read a 3D grid inside a RMS project and return a Grid() instance.
-
-    Args:
-        project (str or special): The RMS project or the project variable
-            from inside RMS.
-        gname (str): Name of Grid Model in RMS.
-        realisation (int): Realisation number.
-        dimensions_only (bool): If True, only the ncol, nrow, nlay will
-            read. The actual grid geometry will remain empty (None). This will
-            be much faster of only grid size info is needed, e.g.
-            for initalising a grid property.
-        info (bool): If true, only grid info
-
-    Example::
-
-        # inside RMS
-        import xtgeo
-        mygrid = xtgeo.grid_from_roxar(project, "REEK_SIM")
-
-    """
-    return Grid(
-        **_grid_roxapi.import_grid_roxapi(
-            project, gname, realisation, dimensions_only, info
-        )
-    )
-
-
-def create_box_grid(
-    dimension,
-    origin=(0.0, 0.0, 0.0),
-    oricenter=False,
-    increment=(1, 1, 1),
-    rotation=0.0,
-    flip=1,
-):
-    """Create a rectangular 'shoebox' grid from spec.
-
-    Args:
-        dimension (tuple of int): A tuple of (NCOL, NROW, NLAY)
-        origin (tuple of float): Startpoint of grid (x, y, z)
-        oricenter (bool): If False, startpoint is node, if True, use cell center
-        increment (tuple of float): Grid increments (xinc, yinc, zinc)
-        rotation (float): Roations in degrees, anticlock from X axis.
-        flip (int): If +1, grid origin is lower left and left-handed;
-                    if -1, origin is upper left and right-handed (row flip).
-
-    Returns:
-        Instance is updated (previous instance content will be erased)
-
-    .. versionadded:: 2.1
-    """
-    kwargs = _grid_etc1.create_box(
-        dimension=dimension,
-        origin=origin,
-        oricenter=oricenter,
-        increment=increment,
-        rotation=rotation,
-        flip=flip,
-    )
-
-    return Grid(**kwargs)
-
-
-# --------------------------------------------------------------------------------------
-# Comment on dual porosity grids:
-#
-# Simulation grids may hold a "dual poro" or/and a "dual perm" system. This is
-# supported here for EGRID format (only, so far), which:
-# * Index 5 in FILEHEAD will be 1 if dual poro is True
-# * Index 5 in FILEHEAD will be 2 if dual poro AND dual perm is True
-# * ACTNUM values will be: 0, 1, 2 (inactive) or 3 (active) instead of normal
-#   0 / 1 in the file:
-# * 0 both Fracture and Matrix are inactive
-# * 1 Matrix is active, Fracture is inactive (set to zero)
-# * 2 Matrix is inactive (set to zero), Fracture is active
-# * 3 Both Fracture and Matrix are active
-#
-#   However, XTGeo will convert this 0..3 scheme back to 0..1 scheme for ACTNUM!
-#   In case of dualporo/perm, a special property holding the initial actnum
-#   will be made, which is self._dualactnum
-#
-# The property self._dualporo is True in case of Dual Porosity
-# BOTH self._dualperm AND self._dualporo are True in case of Dual Permeability
-#
-# All properties in a dual p* system will be given a postfix "M" of "F", e.g.
-# PORO -->  POROM and POROF
-# --------------------------------------------------------------------------------------
-
-IJKRange = Tuple[int, int, int, int, int, int]
-
-
-def allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of RegularSurface and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(self, *args, **kwargs):
-        # Checking if we are doing an initialization
-        # from file and raise a deprecation warning if
-        # we are.
-        if "gfile" in kwargs or (
-            len(args) >= 1 and isinstance(args[0], (str, Path, _XTGeoFile))
-        ):
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "mygrid = xtgeo.grid_from_file('some_name.roff') instead",
-                DeprecationWarning,
-            )
-
-            def constructor(**kwargs):
-                func(self, **kwargs)
-                return self
-
-            _handle_import(constructor, *args, **kwargs)
-            return None
-
-        # Check if we are doing default value init
-        if len(args) == 0 and len(kwargs) == 0:
-            warnings.warn(
-                "Initializing default box grid is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "mygrid = xtgeo.create_box_grid() or, alternatively,"
-                "directly from file with mygrid = xtgeo.grid_from_file().",
-                DeprecationWarning,
-            )
-            kwargs = _grid_etc1.create_box(
-                dimension=(4, 3, 5),
-                origin=(10.0, 20.0, 1000.0),
-                oricenter=False,
-                increment=(100, 150, 5),
-                rotation=30.0,
-                flip=1,
-            )
-            return func(self, **kwargs)
-        return func(self, *args, **kwargs)
-
-    return wrapper
-
-
-class Grid(_Grid3D):
-    """Class for a 3D grid corner point geometry in XTGeo.
-
-    I.e. the geometric grid cells and the active cell indicator.
-
-    The grid geometry class instances are normally created when
-    importing a grid from file, as it is normally too complex to create from
-    scratch.
-
-    Args:
-        coordsv: numpy array of dtype float64 and dimensions (nx + 1, ny + 1, 6)
-            Giving the x,y,z values of the upper and lower corners in the grid.
-        zcornsv: numpy array of dtype float32 and dimensions (nx + 1, ny + 1, nz + 1, 4)
-            giving the sw, se, nw, ne corners along the i,jth corner line for
-            the kth layer.
-        actnumsv: numpy array of dtype int32 and dimensions (nx, ny, nz) giving
-            the activity number for each cell. 0 means inactive, 1 means
-            active. For dualporo=True/dualperm=True grids, value can also be 2
-            or 3 meaning rock volume only and pore volume only respectively.
-        dualporo (bool): True if dual porosity grid.
-        dualperm (bool): True if dual permeability grid.
-        subgrids: dictionary giving names to subset of layers. Has name as key and
-            list of layer indices as values. Defaults to no names given.
-        units: The length units the coordinates are in,
-            (either Units.CM, Units.METRES, Units.FEET for cm, metres and
-            feet respectively).  Default (None) is unitless.
-        filesrc: Optional filename of grid.
-        props: GridProperties instance containing the properties of the grid, defaults
-            to empty instance.
-        name: Optional name of the grid.
-        roxgrid: Roxar Grid the Grid originates from if any, defaults to no such grid.
-        roxindexer: Roxar grid indexer for the roxgrid. Defaults to no such indexer.
-
-    See Also:
-        The :class:`.GridProperty` and the :class:`.GridProperties` classes.
-
-    """
-
-    # pylint: disable=too-many-public-methods
-    @allow_deprecated_init
-    def __init__(
-        self,
-        coordsv: np.ndarray,
-        zcornsv: np.ndarray,
-        actnumsv: np.ndarray,
-        dualporo: bool = False,
-        dualperm: bool = False,
-        subgrids: OrderedDict = None,
-        units: Optional[Units] = None,
-        filesrc: Optional[Union[Path, str]] = None,
-        props: Optional[GridProperties] = None,
-        name: Optional[str] = None,
-        roxgrid=None,
-        roxindexer=None,
-    ):
-        coordsv = np.asarray(coordsv)
-        zcornsv = np.asarray(zcornsv)
-        actnumsv = np.asarray(actnumsv)
-        if coordsv.dtype != np.float64:
-            raise TypeError(
-                f"The dtype of the coordsv array must be float64, got {coordsv.dtype}"
-            )
-        if zcornsv.dtype != np.float32:
-            raise TypeError(
-                f"The dtype of the zcornsv array must be float32, got {zcornsv.dtype}"
-            )
-        if actnumsv.dtype != np.int32:
-            raise TypeError(
-                f"The dtype of the actnumsv array must be int32, got {actnumsv.dtype}"
-            )
-        if len(coordsv.shape) != 3 or coordsv.shape[2] != 6:
-            raise ValueError(
-                f"shape of coordsv should be (nx+1,ny+1,6), got {coordsv.shape}"
-            )
-        if len(zcornsv.shape) != 4 or zcornsv.shape[3] != 4:
-            raise ValueError(
-                f"shape of zcornsv should be (nx+1,ny+1,nz+1, 4), got {zcornsv.shape}"
-            )
-        if zcornsv.shape[0:2] != coordsv.shape[0:2]:
-            raise ValueError(
-                f"Mismatch between zcornsv and coordsv shape: {zcornsv.shape}"
-                f" vs {coordsv.shape}"
-            )
-        if np.any(np.asarray(zcornsv.shape[0:3]) != np.asarray(actnumsv.shape) + 1):
-            raise ValueError(
-                f"Mismatch between zcornsv and actnumsv shape: {zcornsv.shape}"
-                f" vs {actnumsv.shape}"
-            )
-
-        super().__init__(*actnumsv.shape)
-
-        self._reset(
-            coordsv=coordsv,
-            zcornsv=zcornsv,
-            actnumsv=actnumsv,
-            dualporo=dualporo,
-            dualperm=dualperm,
-            subgrids=subgrids,
-            units=units,
-            filesrc=filesrc,
-            props=props,
-            name=name,
-            roxgrid=roxgrid,
-            roxindexer=roxindexer,
-        )
-
-    def _reset(
-        self,
-        coordsv: np.ndarray,
-        zcornsv: np.ndarray,
-        actnumsv: np.ndarray,
-        dualporo: bool = False,
-        dualperm: bool = False,
-        subgrids: OrderedDict = None,
-        units: Optional[Units] = None,
-        filesrc: Optional[Union[Path, str]] = None,
-        props: Optional[GridProperties] = None,
-        name: Optional[str] = None,
-        roxgrid=None,
-        roxindexer=None,
-    ):
-        """This function only serves to allow deprecated initialization."""
-        # TODO: Remove once implicit initialization such as Grid().from_file()
-        # is removed
-        self._xtgformat = 2
-        self._ncol = actnumsv.shape[0]
-        self._nrow = actnumsv.shape[1]
-        self._nlay = actnumsv.shape[2]
-
-        self._coordsv = coordsv
-        self._zcornsv = zcornsv
-        self._actnumsv = actnumsv
-        self._dualporo = dualporo
-        self._dualperm = dualperm
-
-        self._filesrc = filesrc
-
-        if props is None:
-            self._props = GridProperties(props=[])
-        else:
-            self._props = props
-        self._name = name
-        self._subgrids = subgrids
-        self._ijk_handedness = None
-
-        self._dualactnum = None
-        if dualporo:
-            self._dualactnum = self.get_actnum(name="DUALACTNUM")
-            acttmp = self._dualactnum.copy()
-            acttmp.values[acttmp.values >= 1] = 1
-            self.set_actnum(acttmp)
-
-        self._metadata = xtgeo.MetaDataCPGeometry()
-        self._metadata.required = self
-
-        # Roxar api spesific:
-        self._roxgrid = roxgrid
-        self._roxindexer = roxindexer
-
-        self.units = units
-
-        self._tmp = {}
-
-    def __repr__(self):
-        """The __repr__ method."""
-        logger.info("Invoke __repr__ for grid")
-        myrp = (
-            f"{self.__class__.__name__} (id={id(self)}) ncol={self._ncol!r}, "
-            f"nrow={self._nrow!r}, nlay={self._nlay!r}, filesrc={self._filesrc!r}"
-        )
-        return myrp
-
-    def __str__(self):
-        """The __str__ method for user friendly print."""
-        logger.debug("Invoke __str__ for grid", stack_info=True)
-
-        return self.describe(flush=False)
-
-    # ==================================================================================
-    # Public Properties:
-    # ==================================================================================
-
-    @property
-    def metadata(self):
-        """obj: Return or set metadata instance of type MetaDataCPGeometry."""
-        return self._metadata
-
-    @metadata.setter
-    def metadata(self, obj):
-        # The current metadata object can be replaced. A bit dangerous so further
-        # check must be done to validate. TODO.
-        if not isinstance(obj, xtgeo.MetaDataCPGeometry):
-            raise ValueError("Input obj not an instance of MetaDataCPGeometry")
-
-        self._metadata = obj  # checking is currently missing! TODO
-
-    @property
-    def filesrc(self):
-        """str: Source for grid (filepath or name in RMS)."""
-        return self._filesrc
-
-    @property
-    def name(self):
-        """str: Name attribute of grid."""
-        return self._name
-
-    @name.setter
-    def name(self, name):
-        if isinstance(name, str):
-            self._name = name
-        else:
-            raise ValueError("Input name is not a text string")
-
-    @property
-    def ncol(self):
-        """int: Number of columns (read only)."""
-        return super().ncol
-
-    @property
-    def nrow(self):
-        """int: Number of rows (read only)."""
-        return super().nrow
-
-    @property
-    def nlay(self):
-        """int: Number of layers (read only)."""
-        return super().nlay
-
-    @property
-    def dimensions(self):
-        """3-tuple: The grid dimensions as a tuple of 3 integers (read only)."""
-        return (self._ncol, self._nrow, self._nlay)
-
-    @property
-    def vectordimensions(self):
-        """3-tuple: The storage grid array dimensions tuple of 3 integers (read only).
-
-        The tuple is (ncoord, nzcorn, nactnum).
-        """
-        ncoord = (self._ncol + 1) * (self._nrow + 1) * 2 * 3
-        nzcorn = self._ncol * self._nrow * (self._nlay + 1) * 4
-        ntot = self._ncol * self._nrow * self._nlay
-
-        return (ncoord, nzcorn, ntot)
-
-    @property
-    def ijk_handedness(self):
-        """str: IJK handedness for grids, "right" or "left".
-
-        For a non-rotated grid with K increasing with depth, 'left' is corner in
-        lower-left, while 'right' is origin in upper-left corner.
-
-        """
-        nflip = _grid_etc1.estimate_flip(self)
-        if nflip == 1:
-            self._ijk_handedness = "left"
-        elif nflip == -1:
-            self._ijk_handedness = "right"
-        else:
-            self._ijk_handedness = None  # cannot determine
-
-        return self._ijk_handedness
-
-    @ijk_handedness.setter
-    def ijk_handedness(self, value):
-        if value in ("right", "left"):
-            self.reverse_row_axis(ijk_handedness=value)
-        else:
-            raise ValueError("The value must be 'right' or 'left'")
-        self._ijk_handedness = value
-
-    @property
-    def subgrids(self):
-        """:obj:`list` of :obj:`int`: A dict with subgrid name and an array as value.
-
-        I.e. a dict on the form ``{"name1": [1, 2, 3, 4], "name2": [5, 6, 7],
-        "name3": [8, 9, 10]}``, here meaning 3 subgrids where upper is 4
-        cells vertically, then 3, then 3. The numbers must sum to NLAY.
-
-        The numbering in the arrays are 1 based; meaning uppermost layer is 1
-        (not 0).
-
-        None will be returned if no subgrid indexing is present.
-
-        See also :meth:`set_subgrids()` and :meth:`get_subgrids()` which
-        have a similar function, but differs a bit.
-
-        Note that this design is a bit different from the Roxar API, where
-        repeated sections are allowed, and where indices start from 0,
-        not one.
-        """
-        if self._subgrids is None:
-            return None
-
-        return self._subgrids
-
-    @subgrids.setter
-    def subgrids(self, sgrids):
-        if sgrids is None:
-            self._subgrids = None
-            return
-
-        if not isinstance(sgrids, OrderedDict):
-            raise ValueError("Input to subgrids must be an ordered dictionary")
-
-        lengths = 0
-        zarr = []
-        keys = []
-        for key, val in sgrids.items():
-            lengths += len(val)
-            keys.append(key)
-            zarr.extend(val)
-
-        if lengths != self._nlay:
-            raise ValueError(
-                f"Subgrids lengths <{lengths}> not equal NLAY <{self.nlay}>"
-            )
-
-        if set(zarr) != set(range(1, self._nlay + 1)):
-            raise ValueError(
-                f"Arrays are not valid as the do not sum to vertical range, {zarr}"
-            )
-
-        if len(keys) != len(set(keys)):
-            raise ValueError(f"Subgrid keys are not unique: {keys}")
-
-        self._subgrids = sgrids
-
-    @property
-    def nactive(self):
-        """int: Returns the number of active cells (read only)."""
-        return len(self.actnum_indices)
-
-    @property
-    def actnum_array(self):
-        """Returns the 3D ndarray which for active cells.
-
-        Values are 1 for active, 0 for inactive, in C order (read only).
-
-        """
-        actnumv = self.get_actnum().values
-        actnumv = ma.filled(actnumv, fill_value=0)
-
-        return actnumv
-
-    @property
-    def actnum_indices(self):
-        """:obj:np.ndrarray: Indices (1D array) for active cells (read only).
-
-        In dual poro/perm systems, this will be the active indices for the
-        matrix cells and/or fracture cells (i.e. actnum >= 1).
-        """
-        actnumv = self.get_actnum()
-        actnumv = np.ravel(actnumv.values)
-        return np.flatnonzero(actnumv)
-
-    @property
-    def ntotal(self):
-        """Returns the total number of cells (read only)."""
-        return self._ncol * self._nrow * self._nlay
-
-    @property
-    def dualporo(self):
-        """Boolean flag for dual porosity scheme (read only)."""
-        return self._dualporo
-
-    @property
-    def dualperm(self):
-        """Boolean flag for dual porosity scheme (read only)."""
-        return self._dualperm
-
-    @property
-    def gridprops(self):
-        """Return or set a XTGeo GridProperties objects attached to the Grid."""
-        # Note, internally, the _props is a GridProperties instance, which is
-        # a class that holds a list of properties.
-        # Note that the `props` methods below will deal with properties in a
-        # list context
-
-        return self._props
-
-    @gridprops.setter
-    def gridprops(self, gprops):
-        if not isinstance(gprops, GridProperties):
-            raise ValueError("Input must be a GridProperties instance")
-
-        self._props = gprops  # self._props is a GridProperties instance
-
-    @property
-    def props(self):
-        """Return or set a list of XTGeo GridProperty objects.
-
-        When setting, the dimension of the property object is checked,
-        and will raise an IndexError if it does not match the grid.
-
-        When setting props, the current property list is replaced.
-
-        See also :meth:`append_prop()` method to add a property to the current list.
-
-        """
-        # Note, internally, the _props is a GridProperties instance, which is
-        # a class that holds a list of properties.
-
-        prplist = None
-        if isinstance(self._props, GridProperties):
-            prplist = self._props.props
-        elif isinstance(self._props, list):
-            raise RuntimeError(
-                "self._props is a list, not a GridProperties " "instance"
-            )
-        return prplist
-
-    @props.setter
-    def props(self, plist):
-        if not isinstance(plist, list):
-            raise ValueError("Input to props must be a list")
-
-        for litem in plist:
-            if litem.dimensions != self.dimensions:
-                raise IndexError(
-                    f"Property NX NY NZ <{litem.name}> does not match grid!"
-                )
-
-        self._props.props = plist  # self._props is a GridProperties instance
-
-    @property
-    def propnames(self):
-        """Returns a list of property names that are hooked to a grid."""
-        plist = None
-        if self._props is not None:
-            plist = self._props.names
-
-        return plist
-
-    @property
-    def roxgrid(self):
-        """Get the Roxar native proj.grid_models[gname].get_grid() object."""
-        return self._roxgrid
-
-    @property
-    def roxindexer(self):
-        """The Roxar native proj.grid_models[gname].get_grid().grid_indexer object."""
-        return self._roxindexer
-
-    def generate_hash(self, hashmethod="md5"):
-        """Return a unique hash ID for current instance.
-
-        See :meth:`~xtgeo.common.sys.generic_hash()` for documentation.
-
-        .. versionadded:: 2.14
-        """
-        required = (
-            "ncol",
-            "nrow",
-            "nlay",
-            "coordsv",
-            "zcornsv",
-            "actnumsv",
-        )
-
-        gid = ""
-        for req in required:
-            gid += f"{getattr(self, '_' + req)}"
-
-        return xtgeo.common.sys.generic_hash(gid, hashmethod=hashmethod)
-
-    # ==================================================================================
-    # Create/import/export
-    # ==================================================================================
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.create_box_grid() instead",
-    )
-    def create_box(
-        self,
-        dimension=(10, 12, 6),
-        origin=(10.0, 20.0, 1000.0),
-        oricenter=False,
-        increment=(100, 150, 5),
-        rotation=30.0,
-        flip=1,
-    ):
-        """Create a rectangular 'shoebox' grid from spec.
-
-        Args:
-            dimension (tuple of int): A tuple of (NCOL, NROW, NLAY)
-            origin (tuple of float): Startpoint of grid (x, y, z)
-            oricenter (bool): If False, startpoint is node, if True, use cell center
-            increment (tuple of float): Grid increments (xinc, yinc, zinc)
-            rotation (float): Roations in degrees, anticlock from X axis.
-            flip (int): If +1, grid origin is lower left and left-handed;
-                        if -1, origin is upper left and right-handed (row flip).
-
-        Returns:
-            Instance is updated (previous instance content will be erased)
-
-        .. versionadded:: 2.1
-        """
-        kwargs = _grid_etc1.create_box(
-            dimension=dimension,
-            origin=origin,
-            oricenter=oricenter,
-            increment=increment,
-            rotation=rotation,
-            flip=flip,
-        )
-
-        self._reset(**kwargs)
-
-    def to_file(self, gfile, fformat="roff"):
-        """Export grid geometry to file, various vendor formats.
-
-        Args:
-            gfile (str): Name of output file
-            fformat (str): File format; roff/roff_binary/roff_ascii/
-                grdecl/bgrdecl/egrid.
-
-        Raises:
-            OSError: Directory does not exist
-
-        Example::
-            >>> grid = create_box_grid((2,2,2))
-            >>> grid.to_file(outdir + "/myfile.roff")
-        """
-        gfile = xtgeo._XTGeoFile(gfile, mode="wb")
-
-        if not gfile.memstream:
-            gfile.check_folder(raiseerror=OSError)
-
-        valid_formats = {
-            "roff": ["roff", "roff_binary", "roff_bin", "roffbin"],
-            "roff_ascii": ["roff_ascii", "roff_asc", "roffasc"],
-            "grdecl": ["grdecl"],
-            "bgrdecl": ["bgrdecl"],
-            "egrid": ["egrid"],
-            "fegrid": ["fegrid"],
-        }
-
-        if fformat in valid_formats["roff"]:
-            _grid_export.export_roff(self, gfile.name, "binary")
-        elif fformat in valid_formats["roff_ascii"]:
-            _grid_export.export_roff(self, gfile.name, "ascii")
-        elif fformat in valid_formats["grdecl"]:
-            _grid_export.export_grdecl(self, gfile.name, 1)
-        elif fformat in valid_formats["bgrdecl"]:
-            _grid_export.export_grdecl(self, gfile.name, 0)
-        elif fformat in valid_formats["egrid"]:
-            _grid_export.export_egrid(self, gfile.name)
-        elif fformat in valid_formats["fegrid"]:
-            _grid_export.export_fegrid(self, gfile.name)
-        else:
-            raise ValueError(
-                f"Invalid file format: {fformat}, valid options are: {valid_formats}"
-            )
-
-    def to_hdf(
-        self,
-        gfile: Union[str, Path],
-        compression: Optional[str] = None,
-        chunks: Optional[bool] = False,
-        subformat: Optional[int] = 844,
-    ) -> Path:
-        """Export grid geometry to HDF5 storage format (experimental!).
-
-        Args:
-            gfile: Name of output file
-            compression: Compression method, such as "blosc" or "lzf"
-            chunks: chunks settings
-            subformat: Format of output arrays in terms of bytes. E.g. 844 means
-                8 byte for COORD, 4 byte for ZCORNS, 4 byte for ACTNUM.
-
-        Raises:
-            OSError: Directory does not exist
-
-        Returns:
-            Used file object, or None if memory stream
-
-        Example:
-
-            >>> grid = create_box_grid((2,2,2))
-            >>> filename = grid.to_hdf(outdir + "/myfile_grid.h5")
-        """
-        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
-        gfile.check_folder(raiseerror=OSError)
-
-        _grid_export.export_hdf5_cpgeom(
-            self, gfile, compression=compression, chunks=chunks, subformat=subformat
-        )
-
-        return gfile.file
-
-    def to_xtgf(
-        self,
-        gfile: Union[str, Path],
-        subformat: Optional[int] = 844,
-    ) -> Path:
-        """Export grid geometry to xtgeo native binary file format (experimental!).
-
-        Args:
-            gfile: Name of output file
-            subformat: Format of output arryas in terms of bytes. E.g. 844 means
-                8 byte for COORD, 4 byte for ZCORNS, 4 byte for ACTNUM.
-
-        Raises:
-            OSError: Directory does not exist
-
-        Returns:
-            gfile (pathlib.Path): Used pathlib.Path file object, or None if
-                memory stream
-
-        Example::
-            >>> grid = create_box_grid((2,2,2))
-            >>> filename = grid.to_xtgf(outdir + "/myfile.xtg")
-        """
-        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
-        gfile.check_folder(raiseerror=OSError)
-
-        _grid_export.export_xtgcpgeom(self, gfile, subformat=subformat)
-
-        return gfile.file
-
-    def to_roxar(
-        self, project, gname, realisation=0, info=False, method="cpg"
-    ):  # pragma: no cover
-        """Export (upload) a grid from XTGeo to RMS via Roxar API.
-
-        Note:
-            When project is file path (direct access, outside RMS) then
-            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-            will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str or roxar._project): Inside RMS use the magic 'project',
-                else use path to RMS project, or a project reference
-            gname (str): Name of grid in RMS
-            realisation (int): Realisation umber, default 0
-            info (bool): TBD
-            method (str): Save approach
-
-        """
-        _grid_roxapi.export_grid_roxapi(
-            self, project, gname, realisation, info=info, method=method
-        )
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.grid_from_file() instead",
-    )
-    def from_file(
-        self,
-        gfile,
-        fformat=None,
-        **kwargs,
-    ):
-        """Import grid geometry from file, and makes an instance of this class.
-
-        If file extension is missing, then the extension will guess the fformat
-        key, e.g. fformat egrid will be guessed if ".EGRID". The "eclipserun"
-        will try to input INIT and UNRST file in addition the grid in "one go".
-
-        Arguments:
-            gfile (str or Path): File name to be imported. If fformat="eclipse_run"
-                then a fileroot name shall be input here, see example below.
-            fformat (str): File format egrid/roff/grdecl/bgrdecl/eclipserun/xtgcpgeom
-                (None is default and means "guess")
-            initprops (str list): Optional, and only applicable for file format
-                "eclipserun". Provide a list the names of the properties here. A
-                special value "all" can be get all properties found in the INIT file
-            restartprops (str list): Optional, see initprops
-            restartdates (int list): Optional, required if restartprops
-            ijkrange (list-like): Optional, only applicable for hdf files, see
-                :meth:`Grid.from_hdf`.
-            zerobased (bool): Optional, only applicable for hdf files, see
-                :meth:`Grid.from_hdf`.
-            mmap (bool): Optional, only applicable for xtgf files, see
-                :meth:`Grid.from_xtgf`.
-
-        Example::
-
-            >>> xg = Grid()
-            >>> xg.from_file(reek_dir + "/REEK.EGRID", fformat="egrid")
-            Grid ... filesrc='.../REEK.EGRID'
-
-        Example using "eclipserun"::
-
-            >>> mycase = "REEK"  # meaning REEK.EGRID, REEK.INIT, REEK.UNRST
-            >>> xg = Grid()
-            >>> xg.from_file(
-            ...     reek_dir + "/" + mycase,
-            ...     fformat="eclipserun",
-            ...     initprops="all",
-            ... )
-            Grid ... filesrc='.../REEK.EGRID'
-
-        Raises:
-            OSError: if file is not found etc
-        """
-
-        def constructor(*args, **kwargs):
-            self._reset(*args, **kwargs)
-            return self
-
-        _handle_import(constructor, gfile, fformat, **kwargs)
-        return self
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.grid_from_file() instead",
-    )
-    def from_hdf(self, gfile, ijkrange=None, zerobased=False):
-        """Import grid geometry from HDF5 file (experimental!).
-
-        Args:
-            gfile (str): Name of output file
-            ijkrange (list-like): Partial read, e.g. (1, 20, 1, 30, 1, 3) as
-                (i1, i2, j1, j2, k1, k2). Numbering scheme depends on `zerobased`,
-                where default is `eclipse-like` i.e. first cell is 1. Numbering
-                is inclusive for both ends. If ijkrange exceeds original range,
-                an Exception is raised. Using existing boundaries can be defaulted
-                by "min" and "max", e.g. (1, 20, 5, 10, "min", "max")
-            zerobased (bool): If True index in ijkrange is zero based.
-
-        Raises:
-            ValueError: The ijkrange spesification exceeds boundaries.
-            ValueError: The ijkrange list must have 6 elements
-
-        Example::
-
-            >>> xg = create_box_grid((20,20,5))
-            >>> filename = xg.to_hdf(outdir + "/myfile_grid.h5")
-            >>> xg.from_hdf(filename, ijkrange=(1, 10, 10, 15, 1, 4))
-        """
-        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
-
-        kwargs = _grid_import_xtgcpgeom.import_hdf5_cpgeom(
-            gfile, ijkrange=ijkrange, zerobased=zerobased
-        )
-        self._reset(**kwargs)
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.grid_from_file() instead",
-    )
-    def from_xtgf(self, gfile, mmap=False):
-        """Import grid geometry from native xtgeo file format (experimental!).
-
-        Args:
-            gfile (str): Name of output file
-            mmap (bool): If true, reading with memory mapping is active
-
-        Example::
-
-            >>> xg = create_box_grid((5,5,5))
-            >>> filename = xg.to_xtgf(outdir + "/myfile_grid.xtg")
-            >>> xg.from_xtgf(filename)
-        """
-        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
-
-        kwargs = _grid_import_xtgcpgeom.import_xtgcpgeom(gfile, mmap)
-        self._reset(**kwargs)
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.grid_from_roxar() instead",
-    )
-    def from_roxar(
-        self, projectname, gname, realisation=0, dimensions_only=False, info=False
-    ):  # pragma: no cover
-        """Import grid model geometry from RMS project, and makes an instance.
-
-        Args:
-            projectname (str): Name of RMS project
-            gname (str): Name of grid model
-            realisation (int): Realisation number.
-            dimensions_only (bool): If True, only the ncol, nrow, nlay will
-                read. The actual grid geometry will remain empty (None). This
-                will be much faster of only grid size info is needed, e.g.
-                for initalising a grid property.
-            info (bool): If True, various info will printed to screen. This
-                info will depend on version of ROXAPI, and is mainly a
-                developer/debugger feature. Default is False.
-
-
-        """
-        kwargs = _grid_roxapi.import_grid_roxapi(
-            projectname, gname, realisation, dimensions_only, info
-        )
-        self._reset(**kwargs)
-
-    def convert_units(self, units):
-        """
-        Convert the units of the grid.
-        Args:
-            units: The unit to convert to.
-        Raises:
-            ValueError: When the grid is unitless (no initial
-                unit information available).
-        """
-        old_grid_units = self.units
-        if old_grid_units is None:
-            raise ValueError("convert_units called on unitless grid.")
-        if old_grid_units == units:
-            return
-        factor = old_grid_units.conversion_factor(units)
-        self._coordsv *= factor
-        self._zcornsv *= factor
-        self.units = units
-
-    # ==================================================================================
-    # Various public methods
-    # ==================================================================================
-
-    def copy(self):
-        """Copy from one existing Grid instance to a new unique instance.
-
-        Note that associated properties will also be copied.
-
-        Example::
-
-            >>> grd = create_box_grid((5,5,5))
-            >>> newgrd = grd.copy()
-        """
-        logger.info("Copy a Grid instance")
-        other = _grid_etc1.copy(self)
-        return other
-
-    def describe(self, details=False, flush=True):
-        """Describe an instance by printing to stdout."""
-        logger.info("Print a description...")
-
-        dsc = XTGDescription()
-        dsc.title("Description of Grid instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("File source", self._filesrc)
-        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
-        dsc.txt("Number of active cells", self.nactive)
-
-        if details:
-            geom = self.get_geometrics(cellcenter=True, return_dict=True)
-
-            prp1 = []
-            for prp in ("xmin", "xmax", "ymin", "ymax", "zmin", "zmax"):
-                prp1.append(f"{geom[prp]:10.3f}")
-
-            prp2 = []
-            for prp in ("avg_dx", "avg_dy", "avg_dz", "avg_rotation"):
-                prp2.append(f"{geom[prp]:7.4f}")
-
-            geox = self.get_geometrics(
-                cellcenter=False, allcells=True, return_dict=True
-            )
-            prp3 = []
-            for prp in ("xmin", "xmax", "ymin", "ymax", "zmin", "zmax"):
-                prp3.append(f"{geox[prp]:10.3f}")
-
-            prp4 = []
-            for prp in ("avg_dx", "avg_dy", "avg_dz", "avg_rotation"):
-                prp4.append(f"{geox[prp]:7.4f}")
-
-            dsc.txt("For active cells, using cell centers:")
-            dsc.txt("Xmin, Xmax, Ymin, Ymax, Zmin, Zmax:", *prp1)
-            dsc.txt("Avg DX, Avg DY, Avg DZ, Avg rotation:", *prp2)
-            dsc.txt("For all cells, using cell corners:")
-            dsc.txt("Xmin, Xmax, Ymin, Ymax, Zmin, Zmax:", *prp3)
-            dsc.txt("Avg DX, Avg DY, Avg DZ, Avg rotation:", *prp4)
-
-        dsc.txt("Attached grid props objects (names)", self.propnames)
-
-        if details:
-            dsc.txt("Attached grid props objects (id)", self.props)
-        if self.subgrids:
-            dsc.txt("Number of subgrids", len(list(self.subgrids.keys())))
-        else:
-            dsc.txt("Number of subgrids", "No subgrids")
-        if details:
-            dsc.txt("Subgrids details", json.dumps(self.get_subgrids()))
-            dsc.txt("Subgrids with values array", self.subgrids)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    def get_dataframe(self, activeonly=True, ijk=True, xyz=True, doubleformat=False):
-        """Returns a Pandas dataframe for the grid and any attached grid properties.
-
-        Note that this dataframe method is rather similar to GridProperties
-        dataframe function, but have other defaults.
-
-        Args:
-            activeonly (bool): If True (default), return only active cells.
-            ijk (bool): If True (default), show cell indices, IX JY KZ columns
-            xyz (bool): If True (default), show cell center coordinates.
-            doubleformat (bool): If True, floats are 64 bit, otherwise 32 bit.
-                Note that coordinates (if xyz=True) is always 64 bit floats.
-
-        Returns:
-            A Pandas dataframe object
-
-        Example::
-
-            >>> import xtgeo
-            >>> grd = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID", fformat="egrid")
-            >>> names = ["SOIL", "SWAT", "PRESSURE"]
-            >>> dates = [19991201]
-            >>> xpr = xtgeo.gridproperties_from_file(
-            ...     reek_dir + "/REEK.UNRST",
-            ...     fformat="unrst",
-            ...     names=names,
-            ...     dates=dates,
-            ...     grid=grd,
-            ... )
-            >>> grd.gridprops = xpr  # attach properties to grid
-
-            >>> df = grd.get_dataframe()
-
-            >>> # save as CSV file
-            >>> df.to_csv(outdir + "/mygrid.csv")
-        """
-        return self.gridprops.get_dataframe(
-            grid=self,
-            activeonly=activeonly,
-            ijk=ijk,
-            xyz=xyz,
-            doubleformat=doubleformat,
-        )
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Method dataframe is deprecated, use get_dataframe instead.",
-    )
-    def dataframe(self, *args, **kwargs):
-        return self.get_dataframe(*args, **kwargs)
-
-    def get_vtk_esg_geometry_data(self):
-        """Get grid geometry data suitable for use with VTK's vtkExplicitStructuredGrid.
-
-        Builds and returns grid geometry data in a format tailored for use with VTK's
-        explicit structured grid (ESG). Essentially this entails building an
-        unstructured grid representation where all the grid cells are represented as
-        hexahedrons with explicit connectivities. The cell connectivity array refers
-        into the accompanying vertex array.
-
-        In VTK, cell order increases in I fastest, then J, then K.
-
-        The returned tuple contains:
-        - numpy array with dimensions in terms of points (not cells)
-        - vertex array, numpy array with vertex coordinates
-        - connectivity array for all the cells, numpy array with integer indices
-        - inactive cell indices, numpy array with integer indices
-
-        This function also tries to remove/weld duplicate vertices, but this is still
-        a work in progress.
-
-        Example usage with VTK::
-
-            dims, vert_arr, conn_arr, inact_arr = xtg_grid.get_vtk_esg_geometry_data()
-
-            vert_arr = vert_arr.reshape(-1, 3)
-            vtk_points = vtkPoints()
-            vtk_points.SetData(numpy_to_vtk(vert_arr, deep=1))
-
-            vtk_cell_array = vtkCellArray()
-            vtk_cell_array.SetData(8, numpy_to_vtkIdTypeArray(conn_arr, deep=1))
-
-            vtk_esgrid = vtkExplicitStructuredGrid()
-            vtk_esgrid.SetDimensions(dims)
-            vtk_esgrid.SetPoints(vtk_points)
-            vtk_esgrid.SetCells(vtk_cell_array)
-
-            vtk_esgrid.ComputeFacesConnectivityFlagsArray()
-
-            ghost_arr_vtk = vtk_esgrid.AllocateCellGhostArray()
-            ghost_arr_np = vtk_to_numpy(ghost_arr_vtk)
-            ghost_arr_np[inact_arr] = vtkDataSetAttributes.HIDDENCELL
-
-        .. versionadded:: 2.20
-        """
-        return _grid_etc1.get_vtk_esg_geometry_data(self)
-
-    def get_vtk_geometries(self):
-        """Get necessary arrays on correct layout for VTK ExplicitStructuredGrid usage.
-
-        Example::
-
-            import pyvista as pv
-            dim, crn, inactind = grd.get_vtk_geometries()
-            grid = pv.ExplicitStructuredGrid(dim, crn)
-            grid.flip_z(inplace=True)
-            grid.hide_cells(inactind, inplace=True)
-            grid.plot(show_edges=True)
-
-        Returns:
-            dims, corners, inactive_indices
-
-        .. versionadded:: 2.18
-        """
-
-        return _grid_etc1.get_vtk_geometries(self)
-
-    def append_prop(self, prop):
-        """Append a single property to the grid."""
-        if prop.dimensions != self.dimensions:
-            raise ValueError("Dimensions does not match")
-
-        self._props.append_props([prop])
-
-    def set_subgrids(self, sdict):
-        """Set the subgrid from a simplified ordered dictionary.
-
-        The simplified dictionary is on the form
-        {"name1": 3, "name2": 5}
-
-        Note that the input must be an OrderedDict!
-
-        """
-        if sdict is None:
-            return
-
-        if not isinstance(sdict, OrderedDict):
-            raise ValueError("Input sdict is not an OrderedDict")
-
-        newsub = OrderedDict()
-
-        inn1 = 1
-        for name, nsub in sdict.items():
-            inn2 = inn1 + nsub
-            newsub[name] = range(inn1, inn2)
-            inn1 = inn2
-
-        self.subgrids = newsub
-
-    def get_subgrids(self):
-        """Get the subgrids on a simplified ordered dictionary.
-
-        The simplified dictionary is on the form {"name1": 3, "name2": 5}
-        """
-        if not self.subgrids:
-            return None
-
-        newd = OrderedDict()
-        for name, subarr in self.subgrids.items():
-            newd[name] = len(subarr)
-
-        return newd
-
-    def rename_subgrids(self, names):
-        """Rename the names in the subgrids with the new names.
-
-        Args:
-            names (list): List of new names, length of list must be same as length of
-                subgrids
-
-
-        Example::
-
-            >>> from collections import OrderedDict
-            >>> grd = create_box_grid((3, 3, 3))
-            >>> grd.subgrids = OrderedDict(
-            ...     [("1", range(1,2)), ("2", range(2,3)), ("3", range(3,4))]
-            ... )
-            >>> grd.rename_subgrids(["Inky", "Tinky", "Pinky"])
-
-        Raises:
-            ValueError: Input names not a list or a tuple
-            ValueError: Lenght of names list not same as number of subgrids
-
-        .. versionadded:: 2.12
-        """
-        if not isinstance(names, (list, tuple)):
-            raise ValueError("Input names not a list or a tuple")
-
-        if len(names) != len(list(self.subgrids.keys())):
-            raise ValueError("Lenght of names list not same as number of subgrids")
-
-        subs = self.get_subgrids().copy()
-        for num, oldname in enumerate(self.subgrids.keys()):
-            subs[str(names[num])] = subs.pop(oldname)
-
-        self.set_subgrids(subs)
-
-    def estimate_design(self, nsub=None):
-        """Estimate design and simbox thickness of the grid or a subgrid.
-
-        If the grid consists of several subgrids, and nsub is not specified, then
-        a failure should be raised.
-
-        Args:
-            nsub (int or str): Subgrid index to check, either as a number (starting
-                with 1) or as subgrid name. If set to None, the whole grid will
-                examined.
-
-        Returns:
-            result (dict): where key "design" gives one letter in(P, T, B, X, M)
-                P=proportional, T=topconform, B=baseconform,
-                X=underdetermined, M=Mixed conform. Key "dzsimbox" is simbox thickness
-                estimate per cell. None if nsub is given, but subgrids are missing, or
-                nsub (name or number) is out of range.
-
-        Example::
-
-            >>> import xtgeo
-            >>> grd = xtgeo.grid_from_file(emerald_dir + "/emerald_hetero_grid.roff")
-            >>> print(grd.subgrids)
-            OrderedDict([('subgrid_0', range(1, 17)), ('subgrid_1', range(17, 47))])
-            >>> res = grd.estimate_design(nsub="subgrid_0")
-            >>> print("Subgrid design is", res["design"])
-            Subgrid design is P
-            >>> print("Subgrid simbox thickness is", res["dzsimbox"])
-            Subgrid simbox thickness is 2.548...
-
-
-
-        """
-        nsubname = None
-
-        if nsub is None and self.subgrids:
-            raise ValueError("Subgrids exists, nsub cannot be None")
-
-        if nsub is not None:
-            if not self.subgrids:
-                return None
-
-            if isinstance(nsub, int):
-                try:
-                    nsubname = list(self.subgrids.keys())[nsub - 1]
-                except IndexError:
-                    return None
-
-            elif isinstance(nsub, str):
-                nsubname = nsub
-            else:
-                raise ValueError("Key nsub of wrong type, must be a number or a name")
-
-            if nsubname not in self.subgrids.keys():
-                return None
-
-        res = _grid_etc1.estimate_design(self, nsubname)
-
-        return res
-
-    def estimate_flip(self):
-        """Estimate flip (handedness) of grid returns as 1 or -1.
-
-        The flip numbers are 1 for left-handed and -1 for right-handed.
-
-        .. seealso:: :py:attr:`~ijk_handedness`
-        """
-        return _grid_etc1.estimate_flip(self)
-
-    def subgrids_from_zoneprop(self, zoneprop):
-        """Estimate subgrid index from a zone property.
-
-        The new will esimate which will replace the current if any.
-
-        Args:
-            zoneprop(GridProperty): a XTGeo GridProperty instance.
-
-        Returns:
-            Will also return simplified dictionary is on the form
-                {"name1": 3, "name2": 5}
-        """
-        newd = OrderedDict()
-        _, _, k_index = self.get_ijk()
-        kval = k_index.values
-        zprval = zoneprop.values
-        minzone = int(zprval.min())
-        maxzone = int(zprval.max())
-
-        for izone in range(minzone, maxzone + 1):
-            mininzn = int(kval[zprval == izone].min())  # 1 base
-            maxinzn = int(kval[zprval == izone].max())  # 1 base
-            newd["zone" + str(izone)] = range(mininzn, maxinzn + 1)
-
-        self.subgrids = newd
-
-        return self.get_subgrids()
-
-    def get_zoneprop_from_subgrids(self):
-        """Make a XTGeo GridProperty instance for a Zone property subgrid index."""
-        raise NotImplementedError("Not yet; todo")
-
-    def get_actnum_indices(self, order="C", inverse=False):
-        """Returns the 1D ndarray which holds the indices for active cells.
-
-        Args:
-            order (str): "Either 'C' (default) or 'F' order).
-            inverse (bool): Default is False, returns indices for inactive cells
-                if True.
-
-        .. versionchanged:: 2.18 Added inverse option
-        """
-        actnumv = self.get_actnum().values.copy(order=order)
-        actnumv = np.ravel(actnumv, order=order)
-        if inverse:
-            actnumv -= 1
-            return np.flatnonzero(actnumv)
-        else:
-            return np.flatnonzero(actnumv)
-
-    def get_dualactnum_indices(self, order="C", fracture=False):
-        """Returns the 1D ndarray which holds the indices for matrix/fracture cases.
-
-        Args:
-            order (str): "Either 'C' (default) or 'F' order).
-            fracture (bool): If True use Fracture properties.
-        """
-        if not self._dualporo:
-            return None
-
-        actnumv = self._dualactnum.values.copy(order=order)
-        actnumv = np.ravel(actnumv, order=order)
-
-        if not fracture:
-            actnumvm = actnumv.copy()
-            actnumvm[(actnumv == 3) | (actnumv == 1)] = 1
-            actnumvm[(actnumv == 2) | (actnumv == 0)] = 0
-            ind = np.flatnonzero(actnumvm)
-        else:
-            actnumvf = actnumv.copy()
-            actnumvf[(actnumv == 3) | (actnumv == 2)] = 1
-            actnumvf[(actnumv == 1) | (actnumv == 0)] = 0
-            ind = np.flatnonzero(actnumvf)
-
-        return ind
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.Grid().gridprops instead",
-    )
-    def get_gridproperties(self):
-        """Return the :obj:`GridProperties` instance attached to the grid.
-
-        See also the :meth:`gridprops` property
-        """
-        return self._props
-
-    def get_prop_by_name(self, name):
-        """Gets a property object by name lookup, return None if not present."""
-        for obj in self.props:
-            if obj.name == name:
-                return obj
-
-        return None
-
-    def get_actnum(self, name="ACTNUM", asmasked=False, mask=None, dual=False):
-        """Return an ACTNUM GridProperty object.
-
-        Args:
-            name (str): name of property in the XTGeo GridProperty object.
-            asmasked (bool): Actnum is returned with all cells shown
-                as default. Use asmasked=True to make 0 entries masked.
-            mask (bool): Deprecated, use asmasked instead!
-            dual (bool): If True, and the grid is a dualporo/perm grid, an
-                extended ACTNUM is applied (numbers 0..3)
-
-        Example::
-
-            >>> import xtgeo
-            >>> mygrid = xtgeo.create_box_grid((2,2,2))
-            >>> act = mygrid.get_actnum()
-            >>> print("{}% of cells are active".format(act.values.mean() * 100))
-            100.0% of cells are active
-
-        .. versionchanged:: 2.6 Added ``dual`` keyword
-        """
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = self._evaluate_mask(mask)
-
-        if dual and self._dualactnum:
-            act = self._dualactnum.copy()
-        else:
-            act = xtgeo.grid3d.GridProperty(
-                ncol=self._ncol,
-                nrow=self._nrow,
-                nlay=self._nlay,
-                values=np.zeros((self._ncol, self._nrow, self._nlay), dtype=np.int32),
-                name=name,
-                discrete=True,
-            )
-
-            if self._xtgformat == 1:
-                values = _gridprop_lowlevel.f2c_order(self, self._actnumsv)
-            else:
-                values = self._actnumsv
-
-            act.values = values
-            act.mask_undef()
-
-        if asmasked:
-            act.values = ma.masked_equal(act.values, 0)
-
-        act.codes = {0: "0", 1: "1"}
-        if dual and self._dualactnum:
-            act.codes = {0: "0", 1: "1", 2: "2", 3: "3"}
-
-        # return the object
-        return act
-
-    def set_actnum(self, actnum):
-        """Modify the existing active cell index, ACTNUM.
-
-        Args:
-            actnum (GridProperty): a gridproperty instance with 1 for active
-                cells, 0 for inactive cells
-
-        Example::
-            >>> mygrid = create_box_grid((5,5,5))
-            >>> act = mygrid.get_actnum()
-            >>> act.values[:, :, :] = 1
-            >>> act.values[:, :, 4] = 0
-            >>> mygrid.set_actnum(act)
-        """
-        val1d = actnum.values.ravel()
-
-        if self._xtgformat == 1:
-            self._actnumsv = _gridprop_lowlevel.c2f_order(self, val1d)
-        else:
-            self._actnumsv = np.ma.filled(actnum.values, fill_value=0).astype(np.int32)
-
-    def get_dz(
-        self, name="dZ", flip=True, asmasked=True, mask=None, metric="z projection"
-    ):
-        """Return the dZ as GridProperty object.
-
-        Returns the average length of z direction edges for each
-        cell as a GridProperty. The length is by default the
-        z delta, ie. projected onto the z dimension (see the metric parameter).
-
-        Args:
-            name (str): name of property
-            flip (bool): Use False for Petrel grids were Z is negative down
-                (experimental)
-            asmasked (bool): True if only for active cells, False for all cells
-            mask (bool): Deprecated, use asmasked instead!
-            metric (str): One of the following metrics:
-                * "euclid": sqrt(dx^2 + dy^2 + dz^2)
-                * "horizontal": sqrt(dx^2 + dy^2)
-                * "east west vertical": sqrt(dy^2 + dz^2)
-                * "north south vertical": sqrt(dx^2 + dz^2)
-                * "x projection": dx
-                * "y projection": dy
-                * "z projection": dz
-
-        Returns:
-            A XTGeo GridProperty object dZ
-        """
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = self._evaluate_mask(mask)
-
-        deltaz = _grid_etc1.get_dz(
-            self, name=name, flip=flip, asmasked=asmasked, metric=metric
-        )
-
-        return deltaz
-
-    def get_dx(self, name="dX", asmasked=True, metric="horizontal"):
-        """Return the dX as GridProperty object.
-
-        Returns the average length of x direction edges for each
-        cell as a GridProperty. The length is by default horizontal
-        vector length (see the metric parameter).
-
-        Args:
-            name (str): names of properties
-            asmasked (bool). If True, make a np.ma array where inactive cells
-                are masked.
-            metric (str): One of the following metrics:
-                * "euclid": sqrt(dx^2 + dy^2 + dz^2)
-                * "horizontal": sqrt(dx^2 + dy^2)
-                * "east west vertical": sqrt(dy^2 + dz^2)
-                * "north south vertical": sqrt(dx^2 + dz^2)
-                * "x projection": dx
-                * "y projection": dy
-                * "z projection": dz
-
-        Returns:
-            XTGeo GridProperty objects containing dx.
-        """
-        return _grid_etc1.get_dx(self, name=name, asmasked=asmasked, metric=metric)
-
-    def get_dy(self, name="dY", asmasked=True, metric="horizontal"):
-        """Return the dY as GridProperty object.
-
-        Returns the average length of y direction edges for each
-        cell as a GridProperty. The length is by default horizontal
-        vector length (see the metric parameter).
-
-        Args:
-            name (str): names of properties
-            asmasked (bool). If True, make a np.ma array where inactive cells
-                are masked.
-            metric (str): One of the following metrics:
-                * "euclid": sqrt(dx^2 + dy^2 + dz^2)
-                * "horizontal": sqrt(dx^2 + dy^2)
-                * "east west vertical": sqrt(dy^2 + dz^2)
-                * "north south vertical": sqrt(dx^2 + dz^2)
-                * "x projection": dx
-                * "y projection": dy
-                * "z projection": dz
-
-        Returns:
-            Two XTGeo GridProperty objects (dx, dy).
-        """
-        return _grid_etc1.get_dy(self, name=name, asmasked=asmasked, metric=metric)
-
-    @deprecation.deprecated(
-        deprecated_in="3.0",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.Grid.get_dx() and/or xtgeo.Grid.get_dy() instead.",
-    )
-    def get_dxdy(self, names=("dX", "dY"), asmasked=False):
-        """Return the dX and dY as GridProperty object.
-
-        The values lengths are projected to a constant Z.
-
-        Args:
-            name (tuple): names of properties
-            asmasked (bool). If True, make a np.ma array where inactive cells
-                are masked.
-
-        Returns:
-            Two XTGeo GridProperty objects (dx, dy).
-            XTGeo GridProperty objects containing dy.
-        """
-        # return the property objects
-        return self.get_dx(name=names[0], asmasked=asmasked), self.get_dy(
-            name=names[1], asmasked=asmasked
-        )
-
-    def get_cell_volume(
-        self, ijk=(1, 1, 1), activeonly=True, zerobased=False, precision=2
-    ):
-        """Return the bulk volume for a given cell.
-
-        This method is currently *experimental*.
-
-        A bulk volume of a cornerpoint cell is actually a non-trivial and a non-unique
-        entity. The volume is approximated by dividing the cell (hexahedron) into
-        6 tetrehedrons; there is however a large number of ways to do this division.
-
-        As default (precision=2) an average of two different ways to divide the cell
-        into tetrahedrons is averaged.
-
-        Args:
-            ijk (tuple): A tuple of I J K (NB! cell counting starts from 1
-                unless zerobased is True).
-            activeonly (bool): Skip undef cells if True; return None for inactive.
-            precision (int): An even number indication precision level,where
-                a higher number means increased precision but also increased computing
-                time. Currently 1, 2, 4 are supported.
-
-        Returns:
-            Cell total bulk volume
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> print(grid.get_cell_volume(ijk=(10,13,2)))
-            107056...
-
-        .. versionadded:: 2.13 (as experimental)
-        """
-        vol = _grid_etc1.get_cell_volume(
-            self,
-            ijk=ijk,
-            activeonly=activeonly,
-            zerobased=zerobased,
-            precision=precision,
-        )
-
-        return vol
-
-    def get_bulk_volume(self, name="bulkvol", asmasked=True, precision=2):
-        """Return the geometric cell volume for all cells as a GridProperty object.
-
-        This method is currently *experimental*.
-
-        A bulk volume of a cornerpoint cell is actually a non-trivial and a non-unique
-        entity. The volume is approximated by dividing the cell (hexahedron) into
-        6 tetrehedrons; there is however a large number of ways to do this division.
-
-        As default (precision=2) an average of two different ways to divide the cell
-        into tetrahedrons is averaged.
-
-        Args:
-            name (str): name of property, default to "bulkvol"
-            asmasked (bool). If True, make a np.ma array where inactive cells
-                are masked. Otherwise a numpy array will all bulk for all cells is
-                returned
-            precision (int): An number indication precision level, where
-                a higher number means increased precision but also increased computing
-                time. Currently 1, 2 (default), 4 are supported.
-
-        Returns:
-            XTGeo GridProperty object
-
-        .. versionadded:: 2.13 (as experimental)
-
-        """
-        return _grid_etc1.get_bulk_volume(
-            self, name=name, asmasked=asmasked, precision=precision
-        )
-
-    def get_ijk(
-        self, names=("IX", "JY", "KZ"), asmasked=True, mask=None, zerobased=False
-    ):
-        """Returns 3 xtgeo.grid3d.GridProperty objects: I counter, J counter, K counter.
-
-        Args:
-            names: a 3 x tuple of names per property (default IX, JY, KZ).
-            asmasked: If True, UNDEF cells are masked, default is True
-            mask (bool): Deprecated, use asmasked instead!
-            zerobased: If True, counter start from 0, otherwise 1 (default=1).
-        """
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = self._evaluate_mask(mask)
-
-        ixc, jyc, kzc = _grid_etc1.get_ijk(
-            self, names=names, asmasked=asmasked, zerobased=zerobased
-        )
-
-        # return the objects
-        return ixc, jyc, kzc
-
-    def get_ijk_from_points(
-        self,
-        points,
-        activeonly=True,
-        zerobased=False,
-        dataframe=True,
-        includepoints=True,
-        columnnames=("IX", "JY", "KZ"),
-        fmt="int",
-        undef=-1,
-    ):
-        """Returns a list/dataframe of cell indices based on a Points() instance.
-
-        If a point is outside the grid, -1 values are returned
-
-        Args:
-            points (Points): A XTGeo Points instance
-            activeonly (bool): If True, UNDEF cells are not included
-            zerobased (bool): If True, counter start from 0, otherwise 1 (default=1).
-            dataframe (bool): If True result is Pandas dataframe, otherwise a list
-                of tuples
-            includepoints (bool): If True, include the input points in result
-            columnnames (tuple): Name of columns if dataframe is returned
-            fmt (str): Format of IJK arrays (int/float). Default is "int"
-            undef (int or float): Value to assign to undefined (outside) entries.
-
-        .. versionadded:: 2.6
-        .. versionchanged:: 2.8 Added keywords `columnnames`, `fmt`, `undef`
-        """
-        ijklist = _grid_etc1.get_ijk_from_points(
-            self,
-            points,
-            activeonly=activeonly,
-            zerobased=zerobased,
-            dataframe=dataframe,
-            includepoints=includepoints,
-            columnnames=columnnames,
-            fmt=fmt,
-            undef=undef,
-        )
-
-        # return the dataframe or list of tuples
-        return ijklist
-
-    def get_xyz(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS"), asmasked=True, mask=None):
-        """Returns 3 xtgeo.grid3d.GridProperty objects for x, y, z coordinates.
-
-        The values are mid cell values. Note that ACTNUM is
-        ignored, so these is also extracted for UNDEF cells (which may have
-        weird coordinates). However, the option asmasked=True will mask
-        the numpies for undef cells.
-
-        Args:
-            names: a 3 x tuple of names per property (default is X_UTME,
-            Y_UTMN, Z_TVDSS).
-            asmasked: If True, then inactive cells is masked (numpy.ma).
-            mask (bool): Deprecated, use asmasked instead!
-        """
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = self._evaluate_mask(mask)
-
-        xcoord, ycoord, zcoord = _grid_etc1.get_xyz(
-            self, names=names, asmasked=asmasked
-        )
-
-        # return the objects
-        return xcoord, ycoord, zcoord
-
-    def get_xyz_cell_corners(self, ijk=(1, 1, 1), activeonly=True, zerobased=False):
-        """Return a 8 * 3 tuple x, y, z for each corner.
-
-        .. code-block:: none
-
-           2       3
-           !~~~~~~~!
-           !  top  !
-           !~~~~~~~!    Listing corners with Python index (0 base)
-           0       1
-
-           6       7
-           !~~~~~~~!
-           !  base !
-           !~~~~~~~!
-           4       5
-
-        Args:
-            ijk (tuple): A tuple of I J K (NB! cell counting starts from 1
-                unless zerobased is True)
-            activeonly (bool): Skip undef cells if set to True.
-
-        Returns:
-            A tuple with 24 elements (x1, y1, z1, ... x8, y8, z8)
-                for 8 corners. None if cell is inactive and activeonly=True.
-
-        Example::
-
-            >>> grid = grid_from_file(reek_dir + "REEK.EGRID")
-            >>> print(grid.get_xyz_cell_corners(ijk=(10,13,2)))
-            (458704.10..., 1716.969970703125)
-
-        Raises:
-            RuntimeWarning if spesification is invalid.
-        """
-        clist = _grid_etc1.get_xyz_cell_corners(
-            self, ijk=ijk, activeonly=activeonly, zerobased=zerobased
-        )
-
-        return clist
-
-    def get_xyz_corners(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS")):
-        """Returns 8*3 (24) xtgeo.grid3d.GridProperty objects, x, y, z for each corner.
-
-        The values are cell corner values. Note that ACTNUM is
-        ignored, so these is also extracted for UNDEF cells (which may have
-        weird coordinates).
-
-        .. code-block:: none
-
-           2       3
-           !~~~~~~~!
-           !  top  !
-           !~~~~~~~!    Listing corners with Python index (0 base)
-           0       1
-
-           6       7
-           !~~~~~~~!
-           !  base !
-           !~~~~~~~!
-           4       5
-
-        Args:
-            names (list): Generic name of the properties, will have a
-                number added, e.g. X0, X1, etc.
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.create_box_grid((2,2,2))
-            >>> gps = grid.get_xyz_corners() # list of 24 grid properties
-            >>> len(gps)
-            24
-            >>> gps[0].values.tolist()
-            [[[0.0, 0.0], ... [[1.0, 1.0], [1.0, 1.0]]]
-
-
-        Raises:
-            RunetimeError if corners has wrong spesification
-        """
-        grid_props = _grid_etc1.get_xyz_corners(self, names=names)
-
-        # return the 24 objects in a long tuple (x1, y1, z1, ... x8, y8, z8)
-        return grid_props
-
-    def get_layer_slice(self, layer, top=True, activeonly=True):
-        """Get numpy arrays for cell coordinates e.g. for plotting.
-
-        In each cell there are 5 XY pairs, making a closed polygon as illustrated here:
-
-        XY3  <  XY2
-        !~~~~~~~!
-        !       ! ^
-        !~~~~~~~!
-        XY0 ->  XY1
-        XY4
-
-        Note that cell ordering is C ordering (row is fastest)
-
-        Args:
-            layer (int): K layer, starting with 1 as topmost
-            tip (bool): If True use top of cell, otherwise use base
-            activeonly (bool): If True, only return active cells
-
-        Returns:
-            layerarray (np): [[[X0, Y0], [X1, Y1]...[X4, Y4]], [[..][..]]...]
-            icarray (np): On the form [ic1, ic2, ...] where ic is cell count (C order)
-
-        Example:
-
-            Return two arrays forr cell corner for bottom layer::
-
-                grd = xtgeo.grid_from_file(REEKFILE)
-
-                parr, ibarr = grd.get_layer_slice(grd.nlay, top=False)
-
-        .. versionadded:: 2.3
-        """
-        return _grid_etc1.get_layer_slice(self, layer, top=top, activeonly=activeonly)
-
-    def get_geometrics(
-        self, allcells=False, cellcenter=True, return_dict=False, _ver=1
-    ):
-        """Get a list of grid geometrics such as origin, min, max, etc.
-
-        This returns a tuple: (xori, yori, zori, xmin, xmax, ymin, ymax, zmin,
-        zmax, avg_rotation, avg_dx, avg_dy, avg_dz, grid_regularity_flag)
-
-        If a dictionary is returned, the keys are as in the list above.
-
-        Args:
-            allcells (bool): If True, return also for inactive cells
-            cellcenter (bool): If True, use cell center, otherwise corner
-                coords
-            return_dict (bool): If True, return a dictionary instead of a
-                list, which is usually more convinient.
-            _ver (int): Private option; only for developer!
-
-        Raises: Nothing
-
-        Example::
-
-            >>> mygrid = grid_from_file(reek_dir + "REEK.EGRID")
-            >>> gstuff = mygrid.get_geometrics(return_dict=True)
-            >>> print(f"X min/max is {gstuff['xmin']:.2f} {gstuff['xmax']:.2f}")
-            X min/max is 456620.79 467106.33
-
-        """
-        gresult = _grid_etc1.get_geometrics(
-            self,
-            allcells=allcells,
-            cellcenter=cellcenter,
-            return_dict=return_dict,
-            _ver=_ver,
-        )
-
-        return gresult
-
-    def get_adjacent_cells(self, prop, val1, val2, activeonly=True):
-        """Get a discrete property which reports val1 properties vs neighbouring val2.
-
-        The result will be a new gridproperty, which in general has value 0
-        but 1 if criteria is met, and 2 if criteria is met but cells are
-        faulted.
-
-        Args:
-            prop (xtgeo.GridProperty): A discrete grid property, e.g region
-            val1 (int): Primary value to evaluate
-            val2 (int): Neighbourung value
-            activeonly (bool): If True, do not look at inactive cells
-
-        Raises: Nothing
-
-        """
-        presult = _grid_etc1.get_adjacent_cells(
-            self, prop, val1, val2, activeonly=activeonly
-        )
-
-        return presult
-
-    def get_gridquality_properties(self):
-        """Return a GridProperties() instance with grid quality measures.
-
-        These measures are currently:
-
-        * minangle_topbase (degrees) - minimum angle of top and base
-        * maxangle_topbase (degrees) - maximum angle of top and base
-        * minangle_topbase_proj (degrees) min angle projected (bird view)
-        * maxangle_topbase_proj (degrees) max angle projected (bird view)
-        * minangle_sides (degress) minimum angle, all side surfaces
-        * maxangle_sides (degress) maximum angle, all side surfaces
-        * collapsed (int) Integer, 1 of one or more corners are collpased in Z
-        * faulted (int) Integer, 1 if cell is faulted (one or more neighbours offset)
-        * negative_thickness (int) Integer, 1 if cell has negative thickness
-        * concave_proj (int) 1 if cell is concave seen from projected bird view
-
-        Example::
-
-            # store grid quality measures in RMS
-            gprops = grd.gridquality()
-            for gprop in gprops:
-                gprop.to_roxar(project, "MyGrid", gprop.name)
-
-
-        """
-        gprops = _grid_etc1.get_gridquality_properties(self)
-
-        return gprops
-
-    # =========================================================================
-    # Some more special operations that changes the grid or actnum
-    # =========================================================================
-    def activate_all(self):
-        """Activate all cells in the grid, by manipulating ACTNUM."""
-        self._actnumsv = np.ones(self.dimensions, dtype=np.int32)
-
-        if self._xtgformat == 1:
-            self._actnumsv = self._actnumsv.flatten()
-
-        self._tmp = {}
-
-    def inactivate_by_dz(self, threshold):
-        """Inactivate cells thinner than a given threshold."""
-        _grid_etc1.inactivate_by_dz(self, threshold)
-        self._tmp = {}
-
-    def inactivate_inside(self, poly, layer_range=None, inside=True, force_close=False):
-        """Inacativate grid inside a polygon.
-
-        The Polygons instance may consist of several polygons. If a polygon
-        is open, then the flag force_close will close any that are not open
-        when doing the operations in the grid.
-
-        Args:
-            poly(Polygons): A polygons object
-            layer_range (tuple): A tuple of two ints, upper layer = 1, e.g.
-                (1, 14). Note that base layer count is 1 (not zero)
-            inside (bool): True if remove inside polygon
-            force_close (bool): If True then force polygons to be closed.
-
-        Raises:
-            RuntimeError: If a problems with one or more polygons.
-            ValueError: If Polygon is not a XTGeo object
-        """
-        _grid_etc1.inactivate_inside(
-            self, poly, layer_range=layer_range, inside=inside, force_close=force_close
-        )
-        self._tmp = {}
-
-    def inactivate_outside(self, poly, layer_range=None, force_close=False):
-        """Inacativate grid outside a polygon."""
-        self.inactivate_inside(
-            poly, layer_range=layer_range, inside=False, force_close=force_close
-        )
-        self._tmp = {}
-
-    def collapse_inactive_cells(self):
-        """Collapse inactive layers where, for I J with other active cells."""
-        _grid_etc1.collapse_inactive_cells(self)
-        self._tmp = {}
-
-    def crop(self, colcrop, rowcrop, laycrop, props=None):
-        """Reduce the grid size by cropping.
-
-        The new grid will get new dimensions.
-
-        If props is "all" then all properties assosiated (linked) to then
-        grid are also cropped, and the instances are updated.
-
-        Args:
-            colcrop (tuple): A tuple on the form (i1, i2)
-                where 1 represents start number, and 2 represent end. The range
-                is inclusive for both ends, and the number start index is 1 based.
-            rowcrop (tuple): A tuple on the form (j1, j2)
-            laycrop (tuple): A tuple on the form (k1, k2)
-            props (list or str): None is default, while properties can be listed.
-                If "all", then all GridProperty objects which are linked to the
-                Grid instance are updated.
-
-        Returns:
-            The instance is updated (cropped)
-
-        Example::
-
-            >>> import xtgeo
-            >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> mygrid.crop((3, 6), (4, 20), (1, 10))
-            >>> mygrid.to_file(outdir + "/gf_reduced.roff")
-
-        """
-        _grid_etc1.crop(self, (colcrop, rowcrop, laycrop), props=props)
-        self._tmp = {}
-
-    def reduce_to_one_layer(self):
-        """Reduce the grid to one single layer.
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> grid.nlay
-            14
-            >>> grid.reduce_to_one_layer()
-            >>> grid.nlay
-            1
-
-        """
-        _grid_etc1.reduce_to_one_layer(self)
-        self._tmp = {}
-
-    def translate_coordinates(self, translate=(0, 0, 0), flip=(1, 1, 1)):
-        """Translate (move) and/or flip grid coordinates in 3D.
-
-        By 'flip' here, it means that the full coordinate array are multiplied
-        with -1.
-
-        Args:
-            translate (tuple): Translation distance in X, Y, Z coordinates
-            flip (tuple): Flip array. The flip values must be 1 or -1.
-
-        Raises:
-            RuntimeError: If translation goes wrong for unknown reasons
-        """
-        _grid_etc1.translate_coordinates(self, translate=translate, flip=flip)
-        self._tmp = {}
-
-    def reverse_row_axis(self, ijk_handedness=None):
-        """Reverse the row axis (J indices).
-
-        This means that IJK system will switched between a left vs right handed system.
-        It is here (by using ijk_handedness key), possible to set a wanted stated.
-
-        Note that properties that are assosiated with the grid (through the
-        :py:attr:`~gridprops` or :py:attr:`~props` attribute) will also be
-        reversed (which is desirable).
-
-        Args:
-            ijk_handedness (str): If set to "right" or "left", do only reverse rows if
-                handedness is not already achieved.
-
-        Example::
-
-            grd = xtgeo.Grid("somefile.roff")
-            prop1 = xtgeo.GridProperty("somepropfile1.roff")
-            prop2 = xtgeo.GridProperty("somepropfile2.roff")
-
-            grd.props = [prop1, prop2]
-
-            # secure that the grid geometry is IJK right-handed
-            grd.reverse_row_axis(ijk_handedness="right")
-
-        .. versionadded:: 2.5
-
-        """
-        _grid_etc1.reverse_row_axis(self, ijk_handedness=ijk_handedness)
-        self._tmp = {}
-
-    def make_zconsistent(self, zsep=1e-5):
-        """Make the 3D grid consistent in Z, by a minimal gap (zsep).
-
-        Args:
-            zsep (float): Minimum gap
-        """
-        _grid_etc1.make_zconsistent(self, zsep)
-        self._tmp = {}
-
-    def convert_to_hybrid(
-        self,
-        nhdiv=10,
-        toplevel=1000.0,
-        bottomlevel=1100.0,
-        region=None,
-        region_number=None,
-    ):
-        """Convert to hybrid grid, either globally or in a selected region.
-
-        This function will convert the internal structure in the corner point grid,
-        so that the cells between two levels ``toplevel`` and ``bottomlevel`` become
-        horizontal, which can be useful in flow simulators when e.g. liquid
-        contact movements are dominating. See example on `usage in the Troll field`_.
-
-        Note that the resulting hybrid will have an increased number of layers.
-        If the initial grid has N layers, and the number of horizontal layers
-        is NHDIV, then the result grid will have N * 2 + NHDIV layers.
-
-        .. image:: images/hybridgrid2.jpg
-           :width: 600
-           :align: center
-
-        Args:
-            nhdiv (int): Number of hybrid layers.
-            toplevel (float): Top of hybrid grid.
-            bottomlevel (float): Base of hybrid grid.
-            region (GridProperty, optional): Region property (if needed).
-            region_number (int): Which region to apply hybrid grid in if region.
-
-        Example:
-            Create a hybridgrid from file, based on a GRDECL file (no region)::
-
-               import xtgeo
-               grd = xtgeo.grid_from_file("simgrid.grdecl", fformat="grdecl")
-               grd.convert_to_hybrid(nhdiv=12, toplevel=2200, bottomlevel=2250)
-               # save in binary GRDECL fmt:
-               grd.to_file("simgrid_hybrid.bgrdecl", fformat="bgrdecl")
-
-        See Also:
-               :ref:`hybrid` example.
-
-        .. _usage in the Troll field: https://doi.org/10.2118/148023-MS
-
-        """
-        _grid_hybrid.make_hybridgrid(
-            self,
-            nhdiv=nhdiv,
-            toplevel=toplevel,
-            bottomlevel=bottomlevel,
-            region=region,
-            region_number=region_number,
-        )
-        self._tmp = {}
-
-    def refine_vertically(self, rfactor, zoneprop=None):
-        """Refine vertically, proportionally.
-
-        The rfactor can be a scalar or a dictionary.
-
-        If rfactor is a dict and zoneprop is None, then the current
-        subgrids array is used. If zoneprop is defined, the
-        current subgrid index will be redefined for the case. A warning will
-        be issued if subgrids are defined, but the give zone
-        property is inconsistent with this.
-
-        Also, if a zoneprop is defined but no current subgrids in the grid,
-        then subgrids will be added to the grid, if more than 1 subgrid.
-
-        Args:
-            self (object): A grid XTGeo object
-            rfactor (scalar or dict): Refinement factor, if dict, then the
-                dictionary must be consistent with self.subgrids if this is
-                present.
-            zoneprop (GridProperty): Zone property; must be defined if rfactor
-                is a dict
-
-        Returns:
-            ValueError: if..
-            RuntimeError: if mismatch in dimensions for rfactor and zoneprop
-
-
-        Examples::
-
-            # refine vertically all by factor 3
-
-            grd.refine_vertically(3)
-
-            # refine by using a dictionary; note that subgrids must exist!
-            # and that subgrids that are not mentioned will have value 1
-            # in refinement (1 is meaning no refinement)
-
-            grd.refine_vertically({1: 3, 2: 4, 4: 1})
-
-            # refine by using a a dictionary and a zonelog. If subgrids exists
-            # but are inconsistent with the zonelog; the current subgrids will
-            # be redefined, and a warning will be issued! Note also that ranges
-            # in the dictionary rfactor and the zone property must be aligned.
-
-            grd.refine_vertically({1: 3, 2: 4, 4: 0}, zoneprop=myzone)
-
-        """
-        _grid_refine.refine_vertically(self, rfactor, zoneprop=zoneprop)
-        self._tmp = {}
-
-    def report_zone_mismatch(
-        self,
-        well=None,
-        zonelogname="ZONELOG",
-        zoneprop=None,
-        onelayergrid=None,
-        zonelogrange=(0, 9999),
-        zonelogshift=0,
-        depthrange=None,
-        perflogname=None,
-        perflogrange=(1, 9999),
-        filterlogname=None,
-        filterlogrange=(1e-32, 9999.0),
-        resultformat=1,
-    ):
-        """Reports mismatch between wells and a zone.
-
-        Approaches on matching:
-            1. Use the well zonelog as basis, and compare sampled zone with that
-               interval. This means that zone cells outside well range will not be
-               counted
-            2. Compare intervals with wellzonation in range or grid zonations in
-               range. This gives a wider comparison, and will capture cases
-               where grid zonations is outside well zonation
-
-        .. image:: images/zone-well-mismatch-plain.svg
-           :width: 200
-           :align: center
-
-        Note if `zonelogname` and/or `filterlogname` and/or `perflogname` is given,
-        and such log(s) are not present, then this function will return ``None``.
-
-        Args:
-            well (Well): a XTGeo well object
-            zonelogname (str): Name of the zone logger
-            zoneprop (GridProperty): Grid property instance to use for
-                zonation
-            zonelogrange (tuple): zone log range, from - to (inclusive)
-            onelayergrid (Grid): Redundant from version 2.8, please skip!
-            zonelogshift (int): Deviation (numerical shift) between grid and zonelog,
-                e.g. if Zone property starts with 1 and this corresponds to a zonelog
-                index of 3 in the well, the shift shall be -2.
-            depthrange (tuple): Interval for search in TVD depth, to speed up
-            perflogname (str): Name of perforation log to filter on (> 0 default).
-            perflogrange (tuple): Range of values where perforations are present.
-            filterlogname (str): General filter, work as perflog, filter on values > 0
-            filterlogrange (tuple): Range of values where filter shall be present.
-            resultformat (int): If 1, consider the zonelogrange in the well as
-                basis for match ratio, return (percent, match count, total count).
-                If 2 then a dictionary is returned with various result members
-
-        Returns:
-            res (tuple or dict): report dependent on `resultformat`
-                * A tuple with 3 members:
-                    (match_as_percent, number of matches, total count) approach 1
-                * A dictionary with keys:
-                    * MATCH1 - match as percent, approach 1
-                    * MCOUNT1 - number of match samples approach 1
-                    * TCOUNT1 - total number of samples approach 1
-                    * MATCH2 - match as percent, approach 2
-                    * MCOUNT2 - a.a for option 2
-                    * TCOUNT2 - a.a. for option 2
-                    * WELLINTV - a Well() instance for the actual interval
-                * None, if perflogname or zonelogname of filtername is given, but
-                  the log does not exists for the well
-
-        Example::
-
-            g1 = xtgeo.Grid("gullfaks2.roff")
-
-            z = xtgeo.GridProperty(gullfaks2_zone.roff", name="Zone")
-
-            w2 = xtgeo.Well("34_10-1.w", zonelogname="Zonelog")
-
-            w3 = xtgeo.Well("34_10-B-21_B.w", zonelogname="Zonelog"))
-
-            wells = [w2, w3]
-
-            for w in wells:
-                response = g1.report_zone_mismatch(
-                    well=w, zonelogname="ZONELOG", zoneprop=z,
-                    zonelogrange=(0, 19), depthrange=(1700, 9999))
-
-                print(response)
-
-        .. versionchanged:: 2.8 Added several new keys and better precision in result
-        .. versionchanged:: 2.11 Added ``perflogrange`` and ``filterlogrange``
-        """
-        reports = _grid_wellzone.report_zone_mismatch(
-            self,
-            well=well,
-            zonelogname=zonelogname,
-            zoneprop=zoneprop,
-            onelayergrid=onelayergrid,
-            zonelogrange=zonelogrange,
-            zonelogshift=zonelogshift,
-            depthrange=depthrange,
-            perflogname=perflogname,
-            perflogrange=perflogrange,
-            filterlogname=filterlogname,
-            filterlogrange=filterlogrange,
-            resultformat=resultformat,
-        )
-
-        return reports
-
-    # ==================================================================================
-    # Extract a fence/randomline by sampling, ready for plotting with e.g. matplotlib
-    # ==================================================================================
-    def get_randomline(
-        self,
-        fencespec,
-        prop,
-        zmin=None,
-        zmax=None,
-        zincrement=1.0,
-        hincrement=None,
-        atleast=5,
-        nextend=2,
-    ):
-        """Get a sampled randomline from a fence spesification.
-
-        This randomline will be a 2D numpy with depth on the vertical
-        axis, and length along as horizontal axis. Undefined values will have
-        the np.nan value.
-
-        The input fencespec is either a 2D numpy where each row is X, Y, Z, HLEN,
-        where X, Y are UTM coordinates, Z is depth/time, and HLEN is a
-        length along the fence, or a Polygons instance.
-
-        If input fencspec is a numpy 2D, it is important that the HLEN array
-        has a constant increment and ideally a sampling that is less than the
-        Grid resolution. If a Polygons() instance, this is automated if hincrement is
-        None, and ignored if hincrement is False.
-
-        Args:
-            fencespec (:obj:`~numpy.ndarray` or :class:`~xtgeo.xyz.polygons.Polygons`):
-                2D numpy with X, Y, Z, HLEN as rows or a xtgeo Polygons() object.
-            prop (GridProperty or str): The grid property object, or name, which shall
-                be plotted.
-            zmin (float): Minimum Z (default is Grid Z minima/origin)
-            zmax (float): Maximum Z (default is Grid Z maximum)
-            zincrement (float): Sampling vertically, default is 1.0
-            hincrement (float or bool): Resampling horizontally. This applies only
-                if the fencespec is a Polygons() instance. If None (default),
-                the distance will be deduced automatically. If False, then it assumes
-                the Polygons can be used as-is.
-            atleast (int): Minimum number of horizontal samples (only if
-                fencespec is a Polygons instance and hincrement != False)
-            nextend (int): Extend with nextend * hincrement in both ends (only if
-                fencespec is a Polygons instance and hincrement != False)
-
-        Returns:
-            A tuple: (hmin, hmax, vmin, vmax, ndarray2d)
-
-        Raises:
-            ValueError: Input fence is not according to spec.
-
-        Example::
-
-            mygrid = xtgeo.Grid("somegrid.roff")
-            poro = xtgeo.GridProperty("someporo.roff")
-            mywell = xtgeo.Well("somewell.rmswell")
-            fence = mywell.get_fence_polyline(sampling=5, tvdmin=1750, asnumpy=True)
-            (hmin, hmax, vmin, vmax, arr) = mygrid.get_randomline(
-                 fence, poro, zmin=1750, zmax=1850, zincrement=0.5,
-            )
-            # matplotlib ...
-            plt.imshow(arr, cmap="rainbow", extent=(hmin1, hmax1, vmax1, vmin1))
-
-        .. versionadded:: 2.1
-
-        .. seealso::
-           Class :class:`~xtgeo.xyz.polygons.Polygons`
-              The method :meth:`~xtgeo.xyz.polygons.Polygons.get_fence()` which can be
-              used to pregenerate `fencespec`
-
-        """
-        if not isinstance(fencespec, (np.ndarray, xtgeo.Polygons)):
-            raise ValueError("fencespec must be a numpy or a Polygons() object")
-        logger.info("Getting randomline...")
-
-        res = _grid3d_fence.get_randomline(
-            self,
-            fencespec,
-            prop,
-            zmin=zmin,
-            zmax=zmax,
-            zincrement=zincrement,
-            hincrement=hincrement,
-            atleast=atleast,
-            nextend=nextend,
-        )
-        logger.info("Getting randomline... DONE")
-        return res
-
-    # ----------------------------------------------------------------------------------
-    # Special private functions; these may only live for while
-    # ----------------------------------------------------------------------------------
-
-    def _convert_xtgformat2to1(self):
-        """Convert arrays from new structure xtgformat=2 to legacy xtgformat=1."""
-        _grid_etc1._convert_xtgformat2to1(self)
-
-    def _convert_xtgformat1to2(self):
-        """Convert arrays from old structure xtgformat=1 to new xtgformat=2."""
-        _grid_etc1._convert_xtgformat1to2(self)
-
-    def _xtgformat1(self):
-        """Shortform... arrays from new structure xtgformat=2 to legacy xtgformat=1."""
-        self._convert_xtgformat2to1()
-
-    def _xtgformat2(self):
-        """Shortform... arrays from old structure xtgformat=1 to new xtgformat=2."""
-        self._convert_xtgformat1to2()
+# -*- coding: utf-8 -*-
+"""Module/class for 3D grids (corner point geometry) with XTGeo."""
+
+import functools
+import json
+import warnings
+from collections import OrderedDict
+from pathlib import Path
+from typing import Optional, Tuple, Union
+
+import deprecation
+import numpy as np
+import numpy.ma as ma
+
+import xtgeo
+from xtgeo.common import XTGDescription, _XTGeoFile
+
+from . import (
+    _grid3d_fence,
+    _grid_etc1,
+    _grid_export,
+    _grid_hybrid,
+    _grid_import,
+    _grid_import_ecl,
+    _grid_import_xtgcpgeom,
+    _grid_refine,
+    _grid_roxapi,
+    _grid_wellzone,
+    _gridprop_lowlevel,
+)
+from ._ecl_grid import Units
+from ._grid3d import _Grid3D
+from .grid_properties import GridProperties
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+# --------------------------------------------------------------------------------------
+# Comment on "asmasked" vs "activeonly:
+#
+# "asmasked"=True will return a np.ma array, while "asmasked" = False will
+# return a np.ndarray
+#
+# The "activeonly" will filter out masked entries, or use None or np.nan
+# if "activeonly" is False.
+#
+# Use word "zerobased" for a bool regarding if startcell basis is 1 or 0
+#
+# For functions with mask=... ,they should be replaced with asmasked=...
+# --------------------------------------------------------------------------------------
+
+
+# METHODS as wrappers to class init + import
+def _handle_import(grid_constructor, gfile, fformat=None, **kwargs):
+    """Handles the import given a constructor.
+
+    For backwards compatability we need to call different constructors
+    with grid __init__ parameters (As returned by _grid_import.from_file).
+    These are generally either the _reset method of an instance or Grid().
+
+    This function takes such a constructor, remaining arguments are interpreted
+    as they are in _grid_import.from_file and calls the constructor with the
+    resulting arguments.
+
+    """
+    gfile = xtgeo._XTGeoFile(gfile, mode="rb")
+    if fformat == "eclipserun":
+        ecl_grid = grid_constructor(
+            **_grid_import.from_file(
+                xtgeo._XTGeoFile(gfile.name + ".EGRID", mode="rb"), fformat="egrid"
+            )
+        )
+        _grid_import_ecl.import_ecl_run(gfile.name, ecl_grid=ecl_grid, **kwargs)
+        return ecl_grid
+    return grid_constructor(**_grid_import.from_file(gfile, fformat, **kwargs))
+
+
+def grid_from_file(gfile, fformat=None, **kwargs):
+    """Read a grid (cornerpoint) from filelike and an returns a Grid() instance.
+
+    Args:
+        gfile (str or Path): File name to be imported. If fformat="eclipse_run"
+            then a fileroot name shall be input here, see example below.
+        fformat (str): File format egrid/roff/grdecl/bgrdecl/eclipserun/xtgcpgeom
+            (None is default and means "guess")
+        initprops (str list): Optional, and only applicable for file format
+            "eclipserun". Provide a list the names of the properties here. A
+            special value "all" can be get all properties found in the INIT file
+        restartprops (str list): Optional, see initprops
+        restartdates (int list): Optional, required if restartprops
+        ijkrange (list-like): Optional, only applicable for hdf files, see
+            :meth:`Grid.from_hdf`.
+        zerobased (bool): Optional, only applicable for hdf files, see
+            :meth:`Grid.from_hdf`.
+        mmap (bool): Optional, only applicable for xtgf files, see
+            :meth:`Grid.from_xtgf`.
+
+    Example::
+
+        >>> import xtgeo
+        >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+
+    Example using "eclipserun"::
+
+        >>> mycase = "REEK"  # meaning REEK.EGRID, REEK.INIT, REEK.UNRST
+        >>> xg = xtgeo.grid_from_file(
+        ...     reek_dir + "/" + mycase,
+        ...     fformat="eclipserun",
+        ...     initprops="all",
+        ... )
+        Grid ... filesrc='.../REEK.EGRID'
+
+    Raises:
+        OSError: if file is not found etc
+
+    """
+    return _handle_import(Grid, gfile, fformat, **kwargs)
+
+
+def grid_from_roxar(
+    project, gname, realisation=0, dimensions_only=False, info=False
+):  # pragma: no cover
+    """Read a 3D grid inside a RMS project and return a Grid() instance.
+
+    Args:
+        project (str or special): The RMS project or the project variable
+            from inside RMS.
+        gname (str): Name of Grid Model in RMS.
+        realisation (int): Realisation number.
+        dimensions_only (bool): If True, only the ncol, nrow, nlay will
+            read. The actual grid geometry will remain empty (None). This will
+            be much faster of only grid size info is needed, e.g.
+            for initalising a grid property.
+        info (bool): If true, only grid info
+
+    Example::
+
+        # inside RMS
+        import xtgeo
+        mygrid = xtgeo.grid_from_roxar(project, "REEK_SIM")
+
+    """
+    return Grid(
+        **_grid_roxapi.import_grid_roxapi(
+            project, gname, realisation, dimensions_only, info
+        )
+    )
+
+
+def create_box_grid(
+    dimension,
+    origin=(0.0, 0.0, 0.0),
+    oricenter=False,
+    increment=(1, 1, 1),
+    rotation=0.0,
+    flip=1,
+):
+    """Create a rectangular 'shoebox' grid from spec.
+
+    Args:
+        dimension (tuple of int): A tuple of (NCOL, NROW, NLAY)
+        origin (tuple of float): Startpoint of grid (x, y, z)
+        oricenter (bool): If False, startpoint is node, if True, use cell center
+        increment (tuple of float): Grid increments (xinc, yinc, zinc)
+        rotation (float): Roations in degrees, anticlock from X axis.
+        flip (int): If +1, grid origin is lower left and left-handed;
+                    if -1, origin is upper left and right-handed (row flip).
+
+    Returns:
+        Instance is updated (previous instance content will be erased)
+
+    .. versionadded:: 2.1
+    """
+    kwargs = _grid_etc1.create_box(
+        dimension=dimension,
+        origin=origin,
+        oricenter=oricenter,
+        increment=increment,
+        rotation=rotation,
+        flip=flip,
+    )
+
+    return Grid(**kwargs)
+
+
+# --------------------------------------------------------------------------------------
+# Comment on dual porosity grids:
+#
+# Simulation grids may hold a "dual poro" or/and a "dual perm" system. This is
+# supported here for EGRID format (only, so far), which:
+# * Index 5 in FILEHEAD will be 1 if dual poro is True
+# * Index 5 in FILEHEAD will be 2 if dual poro AND dual perm is True
+# * ACTNUM values will be: 0, 1, 2 (inactive) or 3 (active) instead of normal
+#   0 / 1 in the file:
+# * 0 both Fracture and Matrix are inactive
+# * 1 Matrix is active, Fracture is inactive (set to zero)
+# * 2 Matrix is inactive (set to zero), Fracture is active
+# * 3 Both Fracture and Matrix are active
+#
+#   However, XTGeo will convert this 0..3 scheme back to 0..1 scheme for ACTNUM!
+#   In case of dualporo/perm, a special property holding the initial actnum
+#   will be made, which is self._dualactnum
+#
+# The property self._dualporo is True in case of Dual Porosity
+# BOTH self._dualperm AND self._dualporo are True in case of Dual Permeability
+#
+# All properties in a dual p* system will be given a postfix "M" of "F", e.g.
+# PORO -->  POROM and POROF
+# --------------------------------------------------------------------------------------
+
+IJKRange = Tuple[int, int, int, int, int, int]
+
+
+def allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of RegularSurface and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(self, *args, **kwargs):
+        # Checking if we are doing an initialization
+        # from file and raise a deprecation warning if
+        # we are.
+        if "gfile" in kwargs or (
+            len(args) >= 1 and isinstance(args[0], (str, Path, _XTGeoFile))
+        ):
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "mygrid = xtgeo.grid_from_file('some_name.roff') instead",
+                DeprecationWarning,
+            )
+
+            def constructor(**kwargs):
+                func(self, **kwargs)
+                return self
+
+            _handle_import(constructor, *args, **kwargs)
+            return None
+
+        # Check if we are doing default value init
+        if len(args) == 0 and len(kwargs) == 0:
+            warnings.warn(
+                "Initializing default box grid is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "mygrid = xtgeo.create_box_grid() or, alternatively,"
+                "directly from file with mygrid = xtgeo.grid_from_file().",
+                DeprecationWarning,
+            )
+            kwargs = _grid_etc1.create_box(
+                dimension=(4, 3, 5),
+                origin=(10.0, 20.0, 1000.0),
+                oricenter=False,
+                increment=(100, 150, 5),
+                rotation=30.0,
+                flip=1,
+            )
+            return func(self, **kwargs)
+        return func(self, *args, **kwargs)
+
+    return wrapper
+
+
+class Grid(_Grid3D):
+    """Class for a 3D grid corner point geometry in XTGeo.
+
+    I.e. the geometric grid cells and the active cell indicator.
+
+    The grid geometry class instances are normally created when
+    importing a grid from file, as it is normally too complex to create from
+    scratch.
+
+    Args:
+        coordsv: numpy array of dtype float64 and dimensions (nx + 1, ny + 1, 6)
+            Giving the x,y,z values of the upper and lower corners in the grid.
+        zcornsv: numpy array of dtype float32 and dimensions (nx + 1, ny + 1, nz + 1, 4)
+            giving the sw, se, nw, ne corners along the i,jth corner line for
+            the kth layer.
+        actnumsv: numpy array of dtype int32 and dimensions (nx, ny, nz) giving
+            the activity number for each cell. 0 means inactive, 1 means
+            active. For dualporo=True/dualperm=True grids, value can also be 2
+            or 3 meaning rock volume only and pore volume only respectively.
+        dualporo (bool): True if dual porosity grid.
+        dualperm (bool): True if dual permeability grid.
+        subgrids: dictionary giving names to subset of layers. Has name as key and
+            list of layer indices as values. Defaults to no names given.
+        units: The length units the coordinates are in,
+            (either Units.CM, Units.METRES, Units.FEET for cm, metres and
+            feet respectively).  Default (None) is unitless.
+        filesrc: Optional filename of grid.
+        props: GridProperties instance containing the properties of the grid, defaults
+            to empty instance.
+        name: Optional name of the grid.
+        roxgrid: Roxar Grid the Grid originates from if any, defaults to no such grid.
+        roxindexer: Roxar grid indexer for the roxgrid. Defaults to no such indexer.
+
+    See Also:
+        The :class:`.GridProperty` and the :class:`.GridProperties` classes.
+
+    """
+
+    # pylint: disable=too-many-public-methods
+    @allow_deprecated_init
+    def __init__(
+        self,
+        coordsv: np.ndarray,
+        zcornsv: np.ndarray,
+        actnumsv: np.ndarray,
+        dualporo: bool = False,
+        dualperm: bool = False,
+        subgrids: OrderedDict = None,
+        units: Optional[Units] = None,
+        filesrc: Optional[Union[Path, str]] = None,
+        props: Optional[GridProperties] = None,
+        name: Optional[str] = None,
+        roxgrid=None,
+        roxindexer=None,
+    ):
+        coordsv = np.asarray(coordsv)
+        zcornsv = np.asarray(zcornsv)
+        actnumsv = np.asarray(actnumsv)
+        if coordsv.dtype != np.float64:
+            raise TypeError(
+                f"The dtype of the coordsv array must be float64, got {coordsv.dtype}"
+            )
+        if zcornsv.dtype != np.float32:
+            raise TypeError(
+                f"The dtype of the zcornsv array must be float32, got {zcornsv.dtype}"
+            )
+        if actnumsv.dtype != np.int32:
+            raise TypeError(
+                f"The dtype of the actnumsv array must be int32, got {actnumsv.dtype}"
+            )
+        if len(coordsv.shape) != 3 or coordsv.shape[2] != 6:
+            raise ValueError(
+                f"shape of coordsv should be (nx+1,ny+1,6), got {coordsv.shape}"
+            )
+        if len(zcornsv.shape) != 4 or zcornsv.shape[3] != 4:
+            raise ValueError(
+                f"shape of zcornsv should be (nx+1,ny+1,nz+1, 4), got {zcornsv.shape}"
+            )
+        if zcornsv.shape[0:2] != coordsv.shape[0:2]:
+            raise ValueError(
+                f"Mismatch between zcornsv and coordsv shape: {zcornsv.shape}"
+                f" vs {coordsv.shape}"
+            )
+        if np.any(np.asarray(zcornsv.shape[0:3]) != np.asarray(actnumsv.shape) + 1):
+            raise ValueError(
+                f"Mismatch between zcornsv and actnumsv shape: {zcornsv.shape}"
+                f" vs {actnumsv.shape}"
+            )
+
+        super().__init__(*actnumsv.shape)
+
+        self._reset(
+            coordsv=coordsv,
+            zcornsv=zcornsv,
+            actnumsv=actnumsv,
+            dualporo=dualporo,
+            dualperm=dualperm,
+            subgrids=subgrids,
+            units=units,
+            filesrc=filesrc,
+            props=props,
+            name=name,
+            roxgrid=roxgrid,
+            roxindexer=roxindexer,
+        )
+
+    def _reset(
+        self,
+        coordsv: np.ndarray,
+        zcornsv: np.ndarray,
+        actnumsv: np.ndarray,
+        dualporo: bool = False,
+        dualperm: bool = False,
+        subgrids: OrderedDict = None,
+        units: Optional[Units] = None,
+        filesrc: Optional[Union[Path, str]] = None,
+        props: Optional[GridProperties] = None,
+        name: Optional[str] = None,
+        roxgrid=None,
+        roxindexer=None,
+    ):
+        """This function only serves to allow deprecated initialization."""
+        # TODO: Remove once implicit initialization such as Grid().from_file()
+        # is removed
+        self._xtgformat = 2
+        self._ncol = actnumsv.shape[0]
+        self._nrow = actnumsv.shape[1]
+        self._nlay = actnumsv.shape[2]
+
+        self._coordsv = coordsv
+        self._zcornsv = zcornsv
+        self._actnumsv = actnumsv
+        self._dualporo = dualporo
+        self._dualperm = dualperm
+
+        self._filesrc = filesrc
+
+        if props is None:
+            self._props = GridProperties(props=[])
+        else:
+            self._props = props
+        self._name = name
+        self._subgrids = subgrids
+        self._ijk_handedness = None
+
+        self._dualactnum = None
+        if dualporo:
+            self._dualactnum = self.get_actnum(name="DUALACTNUM")
+            acttmp = self._dualactnum.copy()
+            acttmp.values[acttmp.values >= 1] = 1
+            self.set_actnum(acttmp)
+
+        self._metadata = xtgeo.MetaDataCPGeometry()
+        self._metadata.required = self
+
+        # Roxar api spesific:
+        self._roxgrid = roxgrid
+        self._roxindexer = roxindexer
+
+        self.units = units
+
+        self._tmp = {}
+
+    def __repr__(self):
+        """The __repr__ method."""
+        logger.info("Invoke __repr__ for grid")
+        myrp = (
+            f"{self.__class__.__name__} (id={id(self)}) ncol={self._ncol!r}, "
+            f"nrow={self._nrow!r}, nlay={self._nlay!r}, filesrc={self._filesrc!r}"
+        )
+        return myrp
+
+    def __str__(self):
+        """The __str__ method for user friendly print."""
+        logger.debug("Invoke __str__ for grid", stack_info=True)
+
+        return self.describe(flush=False)
+
+    # ==================================================================================
+    # Public Properties:
+    # ==================================================================================
+
+    @property
+    def metadata(self):
+        """obj: Return or set metadata instance of type MetaDataCPGeometry."""
+        return self._metadata
+
+    @metadata.setter
+    def metadata(self, obj):
+        # The current metadata object can be replaced. A bit dangerous so further
+        # check must be done to validate. TODO.
+        if not isinstance(obj, xtgeo.MetaDataCPGeometry):
+            raise ValueError("Input obj not an instance of MetaDataCPGeometry")
+
+        self._metadata = obj  # checking is currently missing! TODO
+
+    @property
+    def filesrc(self):
+        """str: Source for grid (filepath or name in RMS)."""
+        return self._filesrc
+
+    @property
+    def name(self):
+        """str: Name attribute of grid."""
+        return self._name
+
+    @name.setter
+    def name(self, name):
+        if isinstance(name, str):
+            self._name = name
+        else:
+            raise ValueError("Input name is not a text string")
+
+    @property
+    def ncol(self):
+        """int: Number of columns (read only)."""
+        return super().ncol
+
+    @property
+    def nrow(self):
+        """int: Number of rows (read only)."""
+        return super().nrow
+
+    @property
+    def nlay(self):
+        """int: Number of layers (read only)."""
+        return super().nlay
+
+    @property
+    def dimensions(self):
+        """3-tuple: The grid dimensions as a tuple of 3 integers (read only)."""
+        return (self._ncol, self._nrow, self._nlay)
+
+    @property
+    def vectordimensions(self):
+        """3-tuple: The storage grid array dimensions tuple of 3 integers (read only).
+
+        The tuple is (ncoord, nzcorn, nactnum).
+        """
+        ncoord = (self._ncol + 1) * (self._nrow + 1) * 2 * 3
+        nzcorn = self._ncol * self._nrow * (self._nlay + 1) * 4
+        ntot = self._ncol * self._nrow * self._nlay
+
+        return (ncoord, nzcorn, ntot)
+
+    @property
+    def ijk_handedness(self):
+        """str: IJK handedness for grids, "right" or "left".
+
+        For a non-rotated grid with K increasing with depth, 'left' is corner in
+        lower-left, while 'right' is origin in upper-left corner.
+
+        """
+        nflip = _grid_etc1.estimate_flip(self)
+        if nflip == 1:
+            self._ijk_handedness = "left"
+        elif nflip == -1:
+            self._ijk_handedness = "right"
+        else:
+            self._ijk_handedness = None  # cannot determine
+
+        return self._ijk_handedness
+
+    @ijk_handedness.setter
+    def ijk_handedness(self, value):
+        if value in ("right", "left"):
+            self.reverse_row_axis(ijk_handedness=value)
+        else:
+            raise ValueError("The value must be 'right' or 'left'")
+        self._ijk_handedness = value
+
+    @property
+    def subgrids(self):
+        """:obj:`list` of :obj:`int`: A dict with subgrid name and an array as value.
+
+        I.e. a dict on the form ``{"name1": [1, 2, 3, 4], "name2": [5, 6, 7],
+        "name3": [8, 9, 10]}``, here meaning 3 subgrids where upper is 4
+        cells vertically, then 3, then 3. The numbers must sum to NLAY.
+
+        The numbering in the arrays are 1 based; meaning uppermost layer is 1
+        (not 0).
+
+        None will be returned if no subgrid indexing is present.
+
+        See also :meth:`set_subgrids()` and :meth:`get_subgrids()` which
+        have a similar function, but differs a bit.
+
+        Note that this design is a bit different from the Roxar API, where
+        repeated sections are allowed, and where indices start from 0,
+        not one.
+        """
+        if self._subgrids is None:
+            return None
+
+        return self._subgrids
+
+    @subgrids.setter
+    def subgrids(self, sgrids):
+        if sgrids is None:
+            self._subgrids = None
+            return
+
+        if not isinstance(sgrids, OrderedDict):
+            raise ValueError("Input to subgrids must be an ordered dictionary")
+
+        lengths = 0
+        zarr = []
+        keys = []
+        for key, val in sgrids.items():
+            lengths += len(val)
+            keys.append(key)
+            zarr.extend(val)
+
+        if lengths != self._nlay:
+            raise ValueError(
+                f"Subgrids lengths <{lengths}> not equal NLAY <{self.nlay}>"
+            )
+
+        if set(zarr) != set(range(1, self._nlay + 1)):
+            raise ValueError(
+                f"Arrays are not valid as the do not sum to vertical range, {zarr}"
+            )
+
+        if len(keys) != len(set(keys)):
+            raise ValueError(f"Subgrid keys are not unique: {keys}")
+
+        self._subgrids = sgrids
+
+    @property
+    def nactive(self):
+        """int: Returns the number of active cells (read only)."""
+        return len(self.actnum_indices)
+
+    @property
+    def actnum_array(self):
+        """Returns the 3D ndarray which for active cells.
+
+        Values are 1 for active, 0 for inactive, in C order (read only).
+
+        """
+        actnumv = self.get_actnum().values
+        actnumv = ma.filled(actnumv, fill_value=0)
+
+        return actnumv
+
+    @property
+    def actnum_indices(self):
+        """:obj:np.ndrarray: Indices (1D array) for active cells (read only).
+
+        In dual poro/perm systems, this will be the active indices for the
+        matrix cells and/or fracture cells (i.e. actnum >= 1).
+        """
+        actnumv = self.get_actnum()
+        actnumv = np.ravel(actnumv.values)
+        return np.flatnonzero(actnumv)
+
+    @property
+    def ntotal(self):
+        """Returns the total number of cells (read only)."""
+        return self._ncol * self._nrow * self._nlay
+
+    @property
+    def dualporo(self):
+        """Boolean flag for dual porosity scheme (read only)."""
+        return self._dualporo
+
+    @property
+    def dualperm(self):
+        """Boolean flag for dual porosity scheme (read only)."""
+        return self._dualperm
+
+    @property
+    def gridprops(self):
+        """Return or set a XTGeo GridProperties objects attached to the Grid."""
+        # Note, internally, the _props is a GridProperties instance, which is
+        # a class that holds a list of properties.
+        # Note that the `props` methods below will deal with properties in a
+        # list context
+
+        return self._props
+
+    @gridprops.setter
+    def gridprops(self, gprops):
+        if not isinstance(gprops, GridProperties):
+            raise ValueError("Input must be a GridProperties instance")
+
+        self._props = gprops  # self._props is a GridProperties instance
+
+    @property
+    def props(self):
+        """Return or set a list of XTGeo GridProperty objects.
+
+        When setting, the dimension of the property object is checked,
+        and will raise an IndexError if it does not match the grid.
+
+        When setting props, the current property list is replaced.
+
+        See also :meth:`append_prop()` method to add a property to the current list.
+
+        """
+        # Note, internally, the _props is a GridProperties instance, which is
+        # a class that holds a list of properties.
+
+        prplist = None
+        if isinstance(self._props, GridProperties):
+            prplist = self._props.props
+        elif isinstance(self._props, list):
+            raise RuntimeError(
+                "self._props is a list, not a GridProperties " "instance"
+            )
+        return prplist
+
+    @props.setter
+    def props(self, plist):
+        if not isinstance(plist, list):
+            raise ValueError("Input to props must be a list")
+
+        for litem in plist:
+            if litem.dimensions != self.dimensions:
+                raise IndexError(
+                    f"Property NX NY NZ <{litem.name}> does not match grid!"
+                )
+
+        self._props.props = plist  # self._props is a GridProperties instance
+
+    @property
+    def propnames(self):
+        """Returns a list of property names that are hooked to a grid."""
+        plist = None
+        if self._props is not None:
+            plist = self._props.names
+
+        return plist
+
+    @property
+    def roxgrid(self):
+        """Get the Roxar native proj.grid_models[gname].get_grid() object."""
+        return self._roxgrid
+
+    @property
+    def roxindexer(self):
+        """The Roxar native proj.grid_models[gname].get_grid().grid_indexer object."""
+        return self._roxindexer
+
+    def generate_hash(self, hashmethod="md5"):
+        """Return a unique hash ID for current instance.
+
+        See :meth:`~xtgeo.common.sys.generic_hash()` for documentation.
+
+        .. versionadded:: 2.14
+        """
+        required = (
+            "ncol",
+            "nrow",
+            "nlay",
+            "coordsv",
+            "zcornsv",
+            "actnumsv",
+        )
+
+        gid = ""
+        for req in required:
+            gid += f"{getattr(self, '_' + req)}"
+
+        return xtgeo.common.sys.generic_hash(gid, hashmethod=hashmethod)
+
+    # ==================================================================================
+    # Create/import/export
+    # ==================================================================================
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.create_box_grid() instead",
+    )
+    def create_box(
+        self,
+        dimension=(10, 12, 6),
+        origin=(10.0, 20.0, 1000.0),
+        oricenter=False,
+        increment=(100, 150, 5),
+        rotation=30.0,
+        flip=1,
+    ):
+        """Create a rectangular 'shoebox' grid from spec.
+
+        Args:
+            dimension (tuple of int): A tuple of (NCOL, NROW, NLAY)
+            origin (tuple of float): Startpoint of grid (x, y, z)
+            oricenter (bool): If False, startpoint is node, if True, use cell center
+            increment (tuple of float): Grid increments (xinc, yinc, zinc)
+            rotation (float): Roations in degrees, anticlock from X axis.
+            flip (int): If +1, grid origin is lower left and left-handed;
+                        if -1, origin is upper left and right-handed (row flip).
+
+        Returns:
+            Instance is updated (previous instance content will be erased)
+
+        .. versionadded:: 2.1
+        """
+        kwargs = _grid_etc1.create_box(
+            dimension=dimension,
+            origin=origin,
+            oricenter=oricenter,
+            increment=increment,
+            rotation=rotation,
+            flip=flip,
+        )
+
+        self._reset(**kwargs)
+
+    def to_file(self, gfile, fformat="roff"):
+        """Export grid geometry to file, various vendor formats.
+
+        Args:
+            gfile (str): Name of output file
+            fformat (str): File format; roff/roff_binary/roff_ascii/
+                grdecl/bgrdecl/egrid.
+
+        Raises:
+            OSError: Directory does not exist
+
+        Example::
+            >>> grid = create_box_grid((2,2,2))
+            >>> grid.to_file(outdir + "/myfile.roff")
+        """
+        gfile = xtgeo._XTGeoFile(gfile, mode="wb")
+
+        if not gfile.memstream:
+            gfile.check_folder(raiseerror=OSError)
+
+        valid_formats = {
+            "roff": ["roff", "roff_binary", "roff_bin", "roffbin"],
+            "roff_ascii": ["roff_ascii", "roff_asc", "roffasc"],
+            "grdecl": ["grdecl"],
+            "bgrdecl": ["bgrdecl"],
+            "egrid": ["egrid"],
+            "fegrid": ["fegrid"],
+        }
+
+        if fformat in valid_formats["roff"]:
+            _grid_export.export_roff(self, gfile.name, "binary")
+        elif fformat in valid_formats["roff_ascii"]:
+            _grid_export.export_roff(self, gfile.name, "ascii")
+        elif fformat in valid_formats["grdecl"]:
+            _grid_export.export_grdecl(self, gfile.name, 1)
+        elif fformat in valid_formats["bgrdecl"]:
+            _grid_export.export_grdecl(self, gfile.name, 0)
+        elif fformat in valid_formats["egrid"]:
+            _grid_export.export_egrid(self, gfile.name)
+        elif fformat in valid_formats["fegrid"]:
+            _grid_export.export_fegrid(self, gfile.name)
+        else:
+            raise ValueError(
+                f"Invalid file format: {fformat}, valid options are: {valid_formats}"
+            )
+
+    def to_hdf(
+        self,
+        gfile: Union[str, Path],
+        compression: Optional[str] = None,
+        chunks: Optional[bool] = False,
+        subformat: Optional[int] = 844,
+    ) -> Path:
+        """Export grid geometry to HDF5 storage format (experimental!).
+
+        Args:
+            gfile: Name of output file
+            compression: Compression method, such as "blosc" or "lzf"
+            chunks: chunks settings
+            subformat: Format of output arrays in terms of bytes. E.g. 844 means
+                8 byte for COORD, 4 byte for ZCORNS, 4 byte for ACTNUM.
+
+        Raises:
+            OSError: Directory does not exist
+
+        Returns:
+            Used file object, or None if memory stream
+
+        Example:
+
+            >>> grid = create_box_grid((2,2,2))
+            >>> filename = grid.to_hdf(outdir + "/myfile_grid.h5")
+        """
+        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
+        gfile.check_folder(raiseerror=OSError)
+
+        _grid_export.export_hdf5_cpgeom(
+            self, gfile, compression=compression, chunks=chunks, subformat=subformat
+        )
+
+        return gfile.file
+
+    def to_xtgf(
+        self,
+        gfile: Union[str, Path],
+        subformat: Optional[int] = 844,
+    ) -> Path:
+        """Export grid geometry to xtgeo native binary file format (experimental!).
+
+        Args:
+            gfile: Name of output file
+            subformat: Format of output arryas in terms of bytes. E.g. 844 means
+                8 byte for COORD, 4 byte for ZCORNS, 4 byte for ACTNUM.
+
+        Raises:
+            OSError: Directory does not exist
+
+        Returns:
+            gfile (pathlib.Path): Used pathlib.Path file object, or None if
+                memory stream
+
+        Example::
+            >>> grid = create_box_grid((2,2,2))
+            >>> filename = grid.to_xtgf(outdir + "/myfile.xtg")
+        """
+        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
+        gfile.check_folder(raiseerror=OSError)
+
+        _grid_export.export_xtgcpgeom(self, gfile, subformat=subformat)
+
+        return gfile.file
+
+    def to_roxar(
+        self, project, gname, realisation=0, info=False, method="cpg"
+    ):  # pragma: no cover
+        """Export (upload) a grid from XTGeo to RMS via Roxar API.
+
+        Note:
+            When project is file path (direct access, outside RMS) then
+            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+            will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str or roxar._project): Inside RMS use the magic 'project',
+                else use path to RMS project, or a project reference
+            gname (str): Name of grid in RMS
+            realisation (int): Realisation umber, default 0
+            info (bool): TBD
+            method (str): Save approach
+
+        """
+        _grid_roxapi.export_grid_roxapi(
+            self, project, gname, realisation, info=info, method=method
+        )
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.grid_from_file() instead",
+    )
+    def from_file(
+        self,
+        gfile,
+        fformat=None,
+        **kwargs,
+    ):
+        """Import grid geometry from file, and makes an instance of this class.
+
+        If file extension is missing, then the extension will guess the fformat
+        key, e.g. fformat egrid will be guessed if ".EGRID". The "eclipserun"
+        will try to input INIT and UNRST file in addition the grid in "one go".
+
+        Arguments:
+            gfile (str or Path): File name to be imported. If fformat="eclipse_run"
+                then a fileroot name shall be input here, see example below.
+            fformat (str): File format egrid/roff/grdecl/bgrdecl/eclipserun/xtgcpgeom
+                (None is default and means "guess")
+            initprops (str list): Optional, and only applicable for file format
+                "eclipserun". Provide a list the names of the properties here. A
+                special value "all" can be get all properties found in the INIT file
+            restartprops (str list): Optional, see initprops
+            restartdates (int list): Optional, required if restartprops
+            ijkrange (list-like): Optional, only applicable for hdf files, see
+                :meth:`Grid.from_hdf`.
+            zerobased (bool): Optional, only applicable for hdf files, see
+                :meth:`Grid.from_hdf`.
+            mmap (bool): Optional, only applicable for xtgf files, see
+                :meth:`Grid.from_xtgf`.
+
+        Example::
+
+            >>> xg = Grid()
+            >>> xg.from_file(reek_dir + "/REEK.EGRID", fformat="egrid")
+            Grid ... filesrc='.../REEK.EGRID'
+
+        Example using "eclipserun"::
+
+            >>> mycase = "REEK"  # meaning REEK.EGRID, REEK.INIT, REEK.UNRST
+            >>> xg = Grid()
+            >>> xg.from_file(
+            ...     reek_dir + "/" + mycase,
+            ...     fformat="eclipserun",
+            ...     initprops="all",
+            ... )
+            Grid ... filesrc='.../REEK.EGRID'
+
+        Raises:
+            OSError: if file is not found etc
+        """
+
+        def constructor(*args, **kwargs):
+            self._reset(*args, **kwargs)
+            return self
+
+        _handle_import(constructor, gfile, fformat, **kwargs)
+        return self
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.grid_from_file() instead",
+    )
+    def from_hdf(self, gfile, ijkrange=None, zerobased=False):
+        """Import grid geometry from HDF5 file (experimental!).
+
+        Args:
+            gfile (str): Name of output file
+            ijkrange (list-like): Partial read, e.g. (1, 20, 1, 30, 1, 3) as
+                (i1, i2, j1, j2, k1, k2). Numbering scheme depends on `zerobased`,
+                where default is `eclipse-like` i.e. first cell is 1. Numbering
+                is inclusive for both ends. If ijkrange exceeds original range,
+                an Exception is raised. Using existing boundaries can be defaulted
+                by "min" and "max", e.g. (1, 20, 5, 10, "min", "max")
+            zerobased (bool): If True index in ijkrange is zero based.
+
+        Raises:
+            ValueError: The ijkrange spesification exceeds boundaries.
+            ValueError: The ijkrange list must have 6 elements
+
+        Example::
+
+            >>> xg = create_box_grid((20,20,5))
+            >>> filename = xg.to_hdf(outdir + "/myfile_grid.h5")
+            >>> xg.from_hdf(filename, ijkrange=(1, 10, 10, 15, 1, 4))
+        """
+        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
+
+        kwargs = _grid_import_xtgcpgeom.import_hdf5_cpgeom(
+            gfile, ijkrange=ijkrange, zerobased=zerobased
+        )
+        self._reset(**kwargs)
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.grid_from_file() instead",
+    )
+    def from_xtgf(self, gfile, mmap=False):
+        """Import grid geometry from native xtgeo file format (experimental!).
+
+        Args:
+            gfile (str): Name of output file
+            mmap (bool): If true, reading with memory mapping is active
+
+        Example::
+
+            >>> xg = create_box_grid((5,5,5))
+            >>> filename = xg.to_xtgf(outdir + "/myfile_grid.xtg")
+            >>> xg.from_xtgf(filename)
+        """
+        gfile = xtgeo._XTGeoFile(gfile, mode="wb", obj=self)
+
+        kwargs = _grid_import_xtgcpgeom.import_xtgcpgeom(gfile, mmap)
+        self._reset(**kwargs)
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.grid_from_roxar() instead",
+    )
+    def from_roxar(
+        self, projectname, gname, realisation=0, dimensions_only=False, info=False
+    ):  # pragma: no cover
+        """Import grid model geometry from RMS project, and makes an instance.
+
+        Args:
+            projectname (str): Name of RMS project
+            gname (str): Name of grid model
+            realisation (int): Realisation number.
+            dimensions_only (bool): If True, only the ncol, nrow, nlay will
+                read. The actual grid geometry will remain empty (None). This
+                will be much faster of only grid size info is needed, e.g.
+                for initalising a grid property.
+            info (bool): If True, various info will printed to screen. This
+                info will depend on version of ROXAPI, and is mainly a
+                developer/debugger feature. Default is False.
+
+
+        """
+        kwargs = _grid_roxapi.import_grid_roxapi(
+            projectname, gname, realisation, dimensions_only, info
+        )
+        self._reset(**kwargs)
+
+    def convert_units(self, units):
+        """
+        Convert the units of the grid.
+        Args:
+            units: The unit to convert to.
+        Raises:
+            ValueError: When the grid is unitless (no initial
+                unit information available).
+        """
+        old_grid_units = self.units
+        if old_grid_units is None:
+            raise ValueError("convert_units called on unitless grid.")
+        if old_grid_units == units:
+            return
+        factor = old_grid_units.conversion_factor(units)
+        self._coordsv *= factor
+        self._zcornsv *= factor
+        self.units = units
+
+    # ==================================================================================
+    # Various public methods
+    # ==================================================================================
+
+    def copy(self):
+        """Copy from one existing Grid instance to a new unique instance.
+
+        Note that associated properties will also be copied.
+
+        Example::
+
+            >>> grd = create_box_grid((5,5,5))
+            >>> newgrd = grd.copy()
+        """
+        logger.info("Copy a Grid instance")
+        other = _grid_etc1.copy(self)
+        return other
+
+    def describe(self, details=False, flush=True):
+        """Describe an instance by printing to stdout."""
+        logger.info("Print a description...")
+
+        dsc = XTGDescription()
+        dsc.title("Description of Grid instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("File source", self._filesrc)
+        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
+        dsc.txt("Number of active cells", self.nactive)
+
+        if details:
+            geom = self.get_geometrics(cellcenter=True, return_dict=True)
+
+            prp1 = []
+            for prp in ("xmin", "xmax", "ymin", "ymax", "zmin", "zmax"):
+                prp1.append(f"{geom[prp]:10.3f}")
+
+            prp2 = []
+            for prp in ("avg_dx", "avg_dy", "avg_dz", "avg_rotation"):
+                prp2.append(f"{geom[prp]:7.4f}")
+
+            geox = self.get_geometrics(
+                cellcenter=False, allcells=True, return_dict=True
+            )
+            prp3 = []
+            for prp in ("xmin", "xmax", "ymin", "ymax", "zmin", "zmax"):
+                prp3.append(f"{geox[prp]:10.3f}")
+
+            prp4 = []
+            for prp in ("avg_dx", "avg_dy", "avg_dz", "avg_rotation"):
+                prp4.append(f"{geox[prp]:7.4f}")
+
+            dsc.txt("For active cells, using cell centers:")
+            dsc.txt("Xmin, Xmax, Ymin, Ymax, Zmin, Zmax:", *prp1)
+            dsc.txt("Avg DX, Avg DY, Avg DZ, Avg rotation:", *prp2)
+            dsc.txt("For all cells, using cell corners:")
+            dsc.txt("Xmin, Xmax, Ymin, Ymax, Zmin, Zmax:", *prp3)
+            dsc.txt("Avg DX, Avg DY, Avg DZ, Avg rotation:", *prp4)
+
+        dsc.txt("Attached grid props objects (names)", self.propnames)
+
+        if details:
+            dsc.txt("Attached grid props objects (id)", self.props)
+        if self.subgrids:
+            dsc.txt("Number of subgrids", len(list(self.subgrids.keys())))
+        else:
+            dsc.txt("Number of subgrids", "No subgrids")
+        if details:
+            dsc.txt("Subgrids details", json.dumps(self.get_subgrids()))
+            dsc.txt("Subgrids with values array", self.subgrids)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    def get_dataframe(self, activeonly=True, ijk=True, xyz=True, doubleformat=False):
+        """Returns a Pandas dataframe for the grid and any attached grid properties.
+
+        Note that this dataframe method is rather similar to GridProperties
+        dataframe function, but have other defaults.
+
+        Args:
+            activeonly (bool): If True (default), return only active cells.
+            ijk (bool): If True (default), show cell indices, IX JY KZ columns
+            xyz (bool): If True (default), show cell center coordinates.
+            doubleformat (bool): If True, floats are 64 bit, otherwise 32 bit.
+                Note that coordinates (if xyz=True) is always 64 bit floats.
+
+        Returns:
+            A Pandas dataframe object
+
+        Example::
+
+            >>> import xtgeo
+            >>> grd = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID", fformat="egrid")
+            >>> names = ["SOIL", "SWAT", "PRESSURE"]
+            >>> dates = [19991201]
+            >>> xpr = xtgeo.gridproperties_from_file(
+            ...     reek_dir + "/REEK.UNRST",
+            ...     fformat="unrst",
+            ...     names=names,
+            ...     dates=dates,
+            ...     grid=grd,
+            ... )
+            >>> grd.gridprops = xpr  # attach properties to grid
+
+            >>> df = grd.get_dataframe()
+
+            >>> # save as CSV file
+            >>> df.to_csv(outdir + "/mygrid.csv")
+        """
+        return self.gridprops.get_dataframe(
+            grid=self,
+            activeonly=activeonly,
+            ijk=ijk,
+            xyz=xyz,
+            doubleformat=doubleformat,
+        )
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Method dataframe is deprecated, use get_dataframe instead.",
+    )
+    def dataframe(self, *args, **kwargs):
+        return self.get_dataframe(*args, **kwargs)
+
+    def get_vtk_esg_geometry_data(self):
+        """Get grid geometry data suitable for use with VTK's vtkExplicitStructuredGrid.
+
+        Builds and returns grid geometry data in a format tailored for use with VTK's
+        explicit structured grid (ESG). Essentially this entails building an
+        unstructured grid representation where all the grid cells are represented as
+        hexahedrons with explicit connectivities. The cell connectivity array refers
+        into the accompanying vertex array.
+
+        In VTK, cell order increases in I fastest, then J, then K.
+
+        The returned tuple contains:
+        - numpy array with dimensions in terms of points (not cells)
+        - vertex array, numpy array with vertex coordinates
+        - connectivity array for all the cells, numpy array with integer indices
+        - inactive cell indices, numpy array with integer indices
+
+        This function also tries to remove/weld duplicate vertices, but this is still
+        a work in progress.
+
+        Example usage with VTK::
+
+            dims, vert_arr, conn_arr, inact_arr = xtg_grid.get_vtk_esg_geometry_data()
+
+            vert_arr = vert_arr.reshape(-1, 3)
+            vtk_points = vtkPoints()
+            vtk_points.SetData(numpy_to_vtk(vert_arr, deep=1))
+
+            vtk_cell_array = vtkCellArray()
+            vtk_cell_array.SetData(8, numpy_to_vtkIdTypeArray(conn_arr, deep=1))
+
+            vtk_esgrid = vtkExplicitStructuredGrid()
+            vtk_esgrid.SetDimensions(dims)
+            vtk_esgrid.SetPoints(vtk_points)
+            vtk_esgrid.SetCells(vtk_cell_array)
+
+            vtk_esgrid.ComputeFacesConnectivityFlagsArray()
+
+            ghost_arr_vtk = vtk_esgrid.AllocateCellGhostArray()
+            ghost_arr_np = vtk_to_numpy(ghost_arr_vtk)
+            ghost_arr_np[inact_arr] = vtkDataSetAttributes.HIDDENCELL
+
+        .. versionadded:: 2.20
+        """
+        return _grid_etc1.get_vtk_esg_geometry_data(self)
+
+    def get_vtk_geometries(self):
+        """Get necessary arrays on correct layout for VTK ExplicitStructuredGrid usage.
+
+        Example::
+
+            import pyvista as pv
+            dim, crn, inactind = grd.get_vtk_geometries()
+            grid = pv.ExplicitStructuredGrid(dim, crn)
+            grid.flip_z(inplace=True)
+            grid.hide_cells(inactind, inplace=True)
+            grid.plot(show_edges=True)
+
+        Returns:
+            dims, corners, inactive_indices
+
+        .. versionadded:: 2.18
+        """
+
+        return _grid_etc1.get_vtk_geometries(self)
+
+    def append_prop(self, prop):
+        """Append a single property to the grid."""
+        if prop.dimensions != self.dimensions:
+            raise ValueError("Dimensions does not match")
+
+        self._props.append_props([prop])
+
+    def set_subgrids(self, sdict):
+        """Set the subgrid from a simplified ordered dictionary.
+
+        The simplified dictionary is on the form
+        {"name1": 3, "name2": 5}
+
+        Note that the input must be an OrderedDict!
+
+        """
+        if sdict is None:
+            return
+
+        if not isinstance(sdict, OrderedDict):
+            raise ValueError("Input sdict is not an OrderedDict")
+
+        newsub = OrderedDict()
+
+        inn1 = 1
+        for name, nsub in sdict.items():
+            inn2 = inn1 + nsub
+            newsub[name] = range(inn1, inn2)
+            inn1 = inn2
+
+        self.subgrids = newsub
+
+    def get_subgrids(self):
+        """Get the subgrids on a simplified ordered dictionary.
+
+        The simplified dictionary is on the form {"name1": 3, "name2": 5}
+        """
+        if not self.subgrids:
+            return None
+
+        newd = OrderedDict()
+        for name, subarr in self.subgrids.items():
+            newd[name] = len(subarr)
+
+        return newd
+
+    def rename_subgrids(self, names):
+        """Rename the names in the subgrids with the new names.
+
+        Args:
+            names (list): List of new names, length of list must be same as length of
+                subgrids
+
+
+        Example::
+
+            >>> from collections import OrderedDict
+            >>> grd = create_box_grid((3, 3, 3))
+            >>> grd.subgrids = OrderedDict(
+            ...     [("1", range(1,2)), ("2", range(2,3)), ("3", range(3,4))]
+            ... )
+            >>> grd.rename_subgrids(["Inky", "Tinky", "Pinky"])
+
+        Raises:
+            ValueError: Input names not a list or a tuple
+            ValueError: Lenght of names list not same as number of subgrids
+
+        .. versionadded:: 2.12
+        """
+        if not isinstance(names, (list, tuple)):
+            raise ValueError("Input names not a list or a tuple")
+
+        if len(names) != len(list(self.subgrids.keys())):
+            raise ValueError("Lenght of names list not same as number of subgrids")
+
+        subs = self.get_subgrids().copy()
+        for num, oldname in enumerate(self.subgrids.keys()):
+            subs[str(names[num])] = subs.pop(oldname)
+
+        self.set_subgrids(subs)
+
+    def estimate_design(self, nsub=None):
+        """Estimate design and simbox thickness of the grid or a subgrid.
+
+        If the grid consists of several subgrids, and nsub is not specified, then
+        a failure should be raised.
+
+        Args:
+            nsub (int or str): Subgrid index to check, either as a number (starting
+                with 1) or as subgrid name. If set to None, the whole grid will
+                examined.
+
+        Returns:
+            result (dict): where key "design" gives one letter in(P, T, B, X, M)
+                P=proportional, T=topconform, B=baseconform,
+                X=underdetermined, M=Mixed conform. Key "dzsimbox" is simbox thickness
+                estimate per cell. None if nsub is given, but subgrids are missing, or
+                nsub (name or number) is out of range.
+
+        Example::
+
+            >>> import xtgeo
+            >>> grd = xtgeo.grid_from_file(emerald_dir + "/emerald_hetero_grid.roff")
+            >>> print(grd.subgrids)
+            OrderedDict([('subgrid_0', range(1, 17)), ('subgrid_1', range(17, 47))])
+            >>> res = grd.estimate_design(nsub="subgrid_0")
+            >>> print("Subgrid design is", res["design"])
+            Subgrid design is P
+            >>> print("Subgrid simbox thickness is", res["dzsimbox"])
+            Subgrid simbox thickness is 2.548...
+
+
+
+        """
+        nsubname = None
+
+        if nsub is None and self.subgrids:
+            raise ValueError("Subgrids exists, nsub cannot be None")
+
+        if nsub is not None:
+            if not self.subgrids:
+                return None
+
+            if isinstance(nsub, int):
+                try:
+                    nsubname = list(self.subgrids.keys())[nsub - 1]
+                except IndexError:
+                    return None
+
+            elif isinstance(nsub, str):
+                nsubname = nsub
+            else:
+                raise ValueError("Key nsub of wrong type, must be a number or a name")
+
+            if nsubname not in self.subgrids.keys():
+                return None
+
+        res = _grid_etc1.estimate_design(self, nsubname)
+
+        return res
+
+    def estimate_flip(self):
+        """Estimate flip (handedness) of grid returns as 1 or -1.
+
+        The flip numbers are 1 for left-handed and -1 for right-handed.
+
+        .. seealso:: :py:attr:`~ijk_handedness`
+        """
+        return _grid_etc1.estimate_flip(self)
+
+    def subgrids_from_zoneprop(self, zoneprop):
+        """Estimate subgrid index from a zone property.
+
+        The new will esimate which will replace the current if any.
+
+        Args:
+            zoneprop(GridProperty): a XTGeo GridProperty instance.
+
+        Returns:
+            Will also return simplified dictionary is on the form
+                {"name1": 3, "name2": 5}
+        """
+        newd = OrderedDict()
+        _, _, k_index = self.get_ijk()
+        kval = k_index.values
+        zprval = zoneprop.values
+        minzone = int(zprval.min())
+        maxzone = int(zprval.max())
+
+        for izone in range(minzone, maxzone + 1):
+            mininzn = int(kval[zprval == izone].min())  # 1 base
+            maxinzn = int(kval[zprval == izone].max())  # 1 base
+            newd["zone" + str(izone)] = range(mininzn, maxinzn + 1)
+
+        self.subgrids = newd
+
+        return self.get_subgrids()
+
+    def get_zoneprop_from_subgrids(self):
+        """Make a XTGeo GridProperty instance for a Zone property subgrid index."""
+        raise NotImplementedError("Not yet; todo")
+
+    def get_actnum_indices(self, order="C", inverse=False):
+        """Returns the 1D ndarray which holds the indices for active cells.
+
+        Args:
+            order (str): "Either 'C' (default) or 'F' order).
+            inverse (bool): Default is False, returns indices for inactive cells
+                if True.
+
+        .. versionchanged:: 2.18 Added inverse option
+        """
+        actnumv = self.get_actnum().values.copy(order=order)
+        actnumv = np.ravel(actnumv, order=order)
+        if inverse:
+            actnumv -= 1
+            return np.flatnonzero(actnumv)
+        else:
+            return np.flatnonzero(actnumv)
+
+    def get_dualactnum_indices(self, order="C", fracture=False):
+        """Returns the 1D ndarray which holds the indices for matrix/fracture cases.
+
+        Args:
+            order (str): "Either 'C' (default) or 'F' order).
+            fracture (bool): If True use Fracture properties.
+        """
+        if not self._dualporo:
+            return None
+
+        actnumv = self._dualactnum.values.copy(order=order)
+        actnumv = np.ravel(actnumv, order=order)
+
+        if not fracture:
+            actnumvm = actnumv.copy()
+            actnumvm[(actnumv == 3) | (actnumv == 1)] = 1
+            actnumvm[(actnumv == 2) | (actnumv == 0)] = 0
+            ind = np.flatnonzero(actnumvm)
+        else:
+            actnumvf = actnumv.copy()
+            actnumvf[(actnumv == 3) | (actnumv == 2)] = 1
+            actnumvf[(actnumv == 1) | (actnumv == 0)] = 0
+            ind = np.flatnonzero(actnumvf)
+
+        return ind
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.Grid().gridprops instead",
+    )
+    def get_gridproperties(self):
+        """Return the :obj:`GridProperties` instance attached to the grid.
+
+        See also the :meth:`gridprops` property
+        """
+        return self._props
+
+    def get_prop_by_name(self, name):
+        """Gets a property object by name lookup, return None if not present."""
+        for obj in self.props:
+            if obj.name == name:
+                return obj
+
+        return None
+
+    def get_actnum(self, name="ACTNUM", asmasked=False, mask=None, dual=False):
+        """Return an ACTNUM GridProperty object.
+
+        Args:
+            name (str): name of property in the XTGeo GridProperty object.
+            asmasked (bool): Actnum is returned with all cells shown
+                as default. Use asmasked=True to make 0 entries masked.
+            mask (bool): Deprecated, use asmasked instead!
+            dual (bool): If True, and the grid is a dualporo/perm grid, an
+                extended ACTNUM is applied (numbers 0..3)
+
+        Example::
+
+            >>> import xtgeo
+            >>> mygrid = xtgeo.create_box_grid((2,2,2))
+            >>> act = mygrid.get_actnum()
+            >>> print("{}% of cells are active".format(act.values.mean() * 100))
+            100.0% of cells are active
+
+        .. versionchanged:: 2.6 Added ``dual`` keyword
+        """
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = self._evaluate_mask(mask)
+
+        if dual and self._dualactnum:
+            act = self._dualactnum.copy()
+        else:
+            act = xtgeo.grid3d.GridProperty(
+                ncol=self._ncol,
+                nrow=self._nrow,
+                nlay=self._nlay,
+                values=np.zeros((self._ncol, self._nrow, self._nlay), dtype=np.int32),
+                name=name,
+                discrete=True,
+            )
+
+            if self._xtgformat == 1:
+                values = _gridprop_lowlevel.f2c_order(self, self._actnumsv)
+            else:
+                values = self._actnumsv
+
+            act.values = values
+            act.mask_undef()
+
+        if asmasked:
+            act.values = ma.masked_equal(act.values, 0)
+
+        act.codes = {0: "0", 1: "1"}
+        if dual and self._dualactnum:
+            act.codes = {0: "0", 1: "1", 2: "2", 3: "3"}
+
+        # return the object
+        return act
+
+    def set_actnum(self, actnum):
+        """Modify the existing active cell index, ACTNUM.
+
+        Args:
+            actnum (GridProperty): a gridproperty instance with 1 for active
+                cells, 0 for inactive cells
+
+        Example::
+            >>> mygrid = create_box_grid((5,5,5))
+            >>> act = mygrid.get_actnum()
+            >>> act.values[:, :, :] = 1
+            >>> act.values[:, :, 4] = 0
+            >>> mygrid.set_actnum(act)
+        """
+        val1d = actnum.values.ravel()
+
+        if self._xtgformat == 1:
+            self._actnumsv = _gridprop_lowlevel.c2f_order(self, val1d)
+        else:
+            self._actnumsv = np.ma.filled(actnum.values, fill_value=0).astype(np.int32)
+
+    def get_dz(
+        self, name="dZ", flip=True, asmasked=True, mask=None, metric="z projection"
+    ):
+        """Return the dZ as GridProperty object.
+
+        Returns the average length of z direction edges for each
+        cell as a GridProperty. The length is by default the
+        z delta, ie. projected onto the z dimension (see the metric parameter).
+
+        Args:
+            name (str): name of property
+            flip (bool): Use False for Petrel grids were Z is negative down
+                (experimental)
+            asmasked (bool): True if only for active cells, False for all cells
+            mask (bool): Deprecated, use asmasked instead!
+            metric (str): One of the following metrics:
+                * "euclid": sqrt(dx^2 + dy^2 + dz^2)
+                * "horizontal": sqrt(dx^2 + dy^2)
+                * "east west vertical": sqrt(dy^2 + dz^2)
+                * "north south vertical": sqrt(dx^2 + dz^2)
+                * "x projection": dx
+                * "y projection": dy
+                * "z projection": dz
+
+        Returns:
+            A XTGeo GridProperty object dZ
+        """
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = self._evaluate_mask(mask)
+
+        deltaz = _grid_etc1.get_dz(
+            self, name=name, flip=flip, asmasked=asmasked, metric=metric
+        )
+
+        return deltaz
+
+    def get_dx(self, name="dX", asmasked=True, metric="horizontal"):
+        """Return the dX as GridProperty object.
+
+        Returns the average length of x direction edges for each
+        cell as a GridProperty. The length is by default horizontal
+        vector length (see the metric parameter).
+
+        Args:
+            name (str): names of properties
+            asmasked (bool). If True, make a np.ma array where inactive cells
+                are masked.
+            metric (str): One of the following metrics:
+                * "euclid": sqrt(dx^2 + dy^2 + dz^2)
+                * "horizontal": sqrt(dx^2 + dy^2)
+                * "east west vertical": sqrt(dy^2 + dz^2)
+                * "north south vertical": sqrt(dx^2 + dz^2)
+                * "x projection": dx
+                * "y projection": dy
+                * "z projection": dz
+
+        Returns:
+            XTGeo GridProperty objects containing dx.
+        """
+        return _grid_etc1.get_dx(self, name=name, asmasked=asmasked, metric=metric)
+
+    def get_dy(self, name="dY", asmasked=True, metric="horizontal"):
+        """Return the dY as GridProperty object.
+
+        Returns the average length of y direction edges for each
+        cell as a GridProperty. The length is by default horizontal
+        vector length (see the metric parameter).
+
+        Args:
+            name (str): names of properties
+            asmasked (bool). If True, make a np.ma array where inactive cells
+                are masked.
+            metric (str): One of the following metrics:
+                * "euclid": sqrt(dx^2 + dy^2 + dz^2)
+                * "horizontal": sqrt(dx^2 + dy^2)
+                * "east west vertical": sqrt(dy^2 + dz^2)
+                * "north south vertical": sqrt(dx^2 + dz^2)
+                * "x projection": dx
+                * "y projection": dy
+                * "z projection": dz
+
+        Returns:
+            Two XTGeo GridProperty objects (dx, dy).
+        """
+        return _grid_etc1.get_dy(self, name=name, asmasked=asmasked, metric=metric)
+
+    @deprecation.deprecated(
+        deprecated_in="3.0",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.Grid.get_dx() and/or xtgeo.Grid.get_dy() instead.",
+    )
+    def get_dxdy(self, names=("dX", "dY"), asmasked=False):
+        """Return the dX and dY as GridProperty object.
+
+        The values lengths are projected to a constant Z.
+
+        Args:
+            name (tuple): names of properties
+            asmasked (bool). If True, make a np.ma array where inactive cells
+                are masked.
+
+        Returns:
+            Two XTGeo GridProperty objects (dx, dy).
+            XTGeo GridProperty objects containing dy.
+        """
+        # return the property objects
+        return self.get_dx(name=names[0], asmasked=asmasked), self.get_dy(
+            name=names[1], asmasked=asmasked
+        )
+
+    def get_cell_volume(
+        self, ijk=(1, 1, 1), activeonly=True, zerobased=False, precision=2
+    ):
+        """Return the bulk volume for a given cell.
+
+        This method is currently *experimental*.
+
+        A bulk volume of a cornerpoint cell is actually a non-trivial and a non-unique
+        entity. The volume is approximated by dividing the cell (hexahedron) into
+        6 tetrehedrons; there is however a large number of ways to do this division.
+
+        As default (precision=2) an average of two different ways to divide the cell
+        into tetrahedrons is averaged.
+
+        Args:
+            ijk (tuple): A tuple of I J K (NB! cell counting starts from 1
+                unless zerobased is True).
+            activeonly (bool): Skip undef cells if True; return None for inactive.
+            precision (int): An even number indication precision level,where
+                a higher number means increased precision but also increased computing
+                time. Currently 1, 2, 4 are supported.
+
+        Returns:
+            Cell total bulk volume
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> print(grid.get_cell_volume(ijk=(10,13,2)))
+            107056...
+
+        .. versionadded:: 2.13 (as experimental)
+        """
+        vol = _grid_etc1.get_cell_volume(
+            self,
+            ijk=ijk,
+            activeonly=activeonly,
+            zerobased=zerobased,
+            precision=precision,
+        )
+
+        return vol
+
+    def get_bulk_volume(self, name="bulkvol", asmasked=True, precision=2):
+        """Return the geometric cell volume for all cells as a GridProperty object.
+
+        This method is currently *experimental*.
+
+        A bulk volume of a cornerpoint cell is actually a non-trivial and a non-unique
+        entity. The volume is approximated by dividing the cell (hexahedron) into
+        6 tetrehedrons; there is however a large number of ways to do this division.
+
+        As default (precision=2) an average of two different ways to divide the cell
+        into tetrahedrons is averaged.
+
+        Args:
+            name (str): name of property, default to "bulkvol"
+            asmasked (bool). If True, make a np.ma array where inactive cells
+                are masked. Otherwise a numpy array will all bulk for all cells is
+                returned
+            precision (int): An number indication precision level, where
+                a higher number means increased precision but also increased computing
+                time. Currently 1, 2 (default), 4 are supported.
+
+        Returns:
+            XTGeo GridProperty object
+
+        .. versionadded:: 2.13 (as experimental)
+
+        """
+        return _grid_etc1.get_bulk_volume(
+            self, name=name, asmasked=asmasked, precision=precision
+        )
+
+    def get_ijk(
+        self, names=("IX", "JY", "KZ"), asmasked=True, mask=None, zerobased=False
+    ):
+        """Returns 3 xtgeo.grid3d.GridProperty objects: I counter, J counter, K counter.
+
+        Args:
+            names: a 3 x tuple of names per property (default IX, JY, KZ).
+            asmasked: If True, UNDEF cells are masked, default is True
+            mask (bool): Deprecated, use asmasked instead!
+            zerobased: If True, counter start from 0, otherwise 1 (default=1).
+        """
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = self._evaluate_mask(mask)
+
+        ixc, jyc, kzc = _grid_etc1.get_ijk(
+            self, names=names, asmasked=asmasked, zerobased=zerobased
+        )
+
+        # return the objects
+        return ixc, jyc, kzc
+
+    def get_ijk_from_points(
+        self,
+        points,
+        activeonly=True,
+        zerobased=False,
+        dataframe=True,
+        includepoints=True,
+        columnnames=("IX", "JY", "KZ"),
+        fmt="int",
+        undef=-1,
+    ):
+        """Returns a list/dataframe of cell indices based on a Points() instance.
+
+        If a point is outside the grid, -1 values are returned
+
+        Args:
+            points (Points): A XTGeo Points instance
+            activeonly (bool): If True, UNDEF cells are not included
+            zerobased (bool): If True, counter start from 0, otherwise 1 (default=1).
+            dataframe (bool): If True result is Pandas dataframe, otherwise a list
+                of tuples
+            includepoints (bool): If True, include the input points in result
+            columnnames (tuple): Name of columns if dataframe is returned
+            fmt (str): Format of IJK arrays (int/float). Default is "int"
+            undef (int or float): Value to assign to undefined (outside) entries.
+
+        .. versionadded:: 2.6
+        .. versionchanged:: 2.8 Added keywords `columnnames`, `fmt`, `undef`
+        """
+        ijklist = _grid_etc1.get_ijk_from_points(
+            self,
+            points,
+            activeonly=activeonly,
+            zerobased=zerobased,
+            dataframe=dataframe,
+            includepoints=includepoints,
+            columnnames=columnnames,
+            fmt=fmt,
+            undef=undef,
+        )
+
+        # return the dataframe or list of tuples
+        return ijklist
+
+    def get_xyz(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS"), asmasked=True, mask=None):
+        """Returns 3 xtgeo.grid3d.GridProperty objects for x, y, z coordinates.
+
+        The values are mid cell values. Note that ACTNUM is
+        ignored, so these is also extracted for UNDEF cells (which may have
+        weird coordinates). However, the option asmasked=True will mask
+        the numpies for undef cells.
+
+        Args:
+            names: a 3 x tuple of names per property (default is X_UTME,
+            Y_UTMN, Z_TVDSS).
+            asmasked: If True, then inactive cells is masked (numpy.ma).
+            mask (bool): Deprecated, use asmasked instead!
+        """
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = self._evaluate_mask(mask)
+
+        xcoord, ycoord, zcoord = _grid_etc1.get_xyz(
+            self, names=names, asmasked=asmasked
+        )
+
+        # return the objects
+        return xcoord, ycoord, zcoord
+
+    def get_xyz_cell_corners(self, ijk=(1, 1, 1), activeonly=True, zerobased=False):
+        """Return a 8 * 3 tuple x, y, z for each corner.
+
+        .. code-block:: none
+
+           2       3
+           !~~~~~~~!
+           !  top  !
+           !~~~~~~~!    Listing corners with Python index (0 base)
+           0       1
+
+           6       7
+           !~~~~~~~!
+           !  base !
+           !~~~~~~~!
+           4       5
+
+        Args:
+            ijk (tuple): A tuple of I J K (NB! cell counting starts from 1
+                unless zerobased is True)
+            activeonly (bool): Skip undef cells if set to True.
+
+        Returns:
+            A tuple with 24 elements (x1, y1, z1, ... x8, y8, z8)
+                for 8 corners. None if cell is inactive and activeonly=True.
+
+        Example::
+
+            >>> grid = grid_from_file(reek_dir + "REEK.EGRID")
+            >>> print(grid.get_xyz_cell_corners(ijk=(10,13,2)))
+            (458704.10..., 1716.969970703125)
+
+        Raises:
+            RuntimeWarning if spesification is invalid.
+        """
+        clist = _grid_etc1.get_xyz_cell_corners(
+            self, ijk=ijk, activeonly=activeonly, zerobased=zerobased
+        )
+
+        return clist
+
+    def get_xyz_corners(self, names=("X_UTME", "Y_UTMN", "Z_TVDSS")):
+        """Returns 8*3 (24) xtgeo.grid3d.GridProperty objects, x, y, z for each corner.
+
+        The values are cell corner values. Note that ACTNUM is
+        ignored, so these is also extracted for UNDEF cells (which may have
+        weird coordinates).
+
+        .. code-block:: none
+
+           2       3
+           !~~~~~~~!
+           !  top  !
+           !~~~~~~~!    Listing corners with Python index (0 base)
+           0       1
+
+           6       7
+           !~~~~~~~!
+           !  base !
+           !~~~~~~~!
+           4       5
+
+        Args:
+            names (list): Generic name of the properties, will have a
+                number added, e.g. X0, X1, etc.
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.create_box_grid((2,2,2))
+            >>> gps = grid.get_xyz_corners() # list of 24 grid properties
+            >>> len(gps)
+            24
+            >>> gps[0].values.tolist()
+            [[[0.0, 0.0], ... [[1.0, 1.0], [1.0, 1.0]]]
+
+
+        Raises:
+            RunetimeError if corners has wrong spesification
+        """
+        grid_props = _grid_etc1.get_xyz_corners(self, names=names)
+
+        # return the 24 objects in a long tuple (x1, y1, z1, ... x8, y8, z8)
+        return grid_props
+
+    def get_layer_slice(self, layer, top=True, activeonly=True):
+        """Get numpy arrays for cell coordinates e.g. for plotting.
+
+        In each cell there are 5 XY pairs, making a closed polygon as illustrated here:
+
+        XY3  <  XY2
+        !~~~~~~~!
+        !       ! ^
+        !~~~~~~~!
+        XY0 ->  XY1
+        XY4
+
+        Note that cell ordering is C ordering (row is fastest)
+
+        Args:
+            layer (int): K layer, starting with 1 as topmost
+            tip (bool): If True use top of cell, otherwise use base
+            activeonly (bool): If True, only return active cells
+
+        Returns:
+            layerarray (np): [[[X0, Y0], [X1, Y1]...[X4, Y4]], [[..][..]]...]
+            icarray (np): On the form [ic1, ic2, ...] where ic is cell count (C order)
+
+        Example:
+
+            Return two arrays forr cell corner for bottom layer::
+
+                grd = xtgeo.grid_from_file(REEKFILE)
+
+                parr, ibarr = grd.get_layer_slice(grd.nlay, top=False)
+
+        .. versionadded:: 2.3
+        """
+        return _grid_etc1.get_layer_slice(self, layer, top=top, activeonly=activeonly)
+
+    def get_geometrics(
+        self, allcells=False, cellcenter=True, return_dict=False, _ver=1
+    ):
+        """Get a list of grid geometrics such as origin, min, max, etc.
+
+        This returns a tuple: (xori, yori, zori, xmin, xmax, ymin, ymax, zmin,
+        zmax, avg_rotation, avg_dx, avg_dy, avg_dz, grid_regularity_flag)
+
+        If a dictionary is returned, the keys are as in the list above.
+
+        Args:
+            allcells (bool): If True, return also for inactive cells
+            cellcenter (bool): If True, use cell center, otherwise corner
+                coords
+            return_dict (bool): If True, return a dictionary instead of a
+                list, which is usually more convinient.
+            _ver (int): Private option; only for developer!
+
+        Raises: Nothing
+
+        Example::
+
+            >>> mygrid = grid_from_file(reek_dir + "REEK.EGRID")
+            >>> gstuff = mygrid.get_geometrics(return_dict=True)
+            >>> print(f"X min/max is {gstuff['xmin']:.2f} {gstuff['xmax']:.2f}")
+            X min/max is 456620.79 467106.33
+
+        """
+        gresult = _grid_etc1.get_geometrics(
+            self,
+            allcells=allcells,
+            cellcenter=cellcenter,
+            return_dict=return_dict,
+            _ver=_ver,
+        )
+
+        return gresult
+
+    def get_adjacent_cells(self, prop, val1, val2, activeonly=True):
+        """Get a discrete property which reports val1 properties vs neighbouring val2.
+
+        The result will be a new gridproperty, which in general has value 0
+        but 1 if criteria is met, and 2 if criteria is met but cells are
+        faulted.
+
+        Args:
+            prop (xtgeo.GridProperty): A discrete grid property, e.g region
+            val1 (int): Primary value to evaluate
+            val2 (int): Neighbourung value
+            activeonly (bool): If True, do not look at inactive cells
+
+        Raises: Nothing
+
+        """
+        presult = _grid_etc1.get_adjacent_cells(
+            self, prop, val1, val2, activeonly=activeonly
+        )
+
+        return presult
+
+    def get_gridquality_properties(self):
+        """Return a GridProperties() instance with grid quality measures.
+
+        These measures are currently:
+
+        * minangle_topbase (degrees) - minimum angle of top and base
+        * maxangle_topbase (degrees) - maximum angle of top and base
+        * minangle_topbase_proj (degrees) min angle projected (bird view)
+        * maxangle_topbase_proj (degrees) max angle projected (bird view)
+        * minangle_sides (degress) minimum angle, all side surfaces
+        * maxangle_sides (degress) maximum angle, all side surfaces
+        * collapsed (int) Integer, 1 of one or more corners are collpased in Z
+        * faulted (int) Integer, 1 if cell is faulted (one or more neighbours offset)
+        * negative_thickness (int) Integer, 1 if cell has negative thickness
+        * concave_proj (int) 1 if cell is concave seen from projected bird view
+
+        Example::
+
+            # store grid quality measures in RMS
+            gprops = grd.gridquality()
+            for gprop in gprops:
+                gprop.to_roxar(project, "MyGrid", gprop.name)
+
+
+        """
+        gprops = _grid_etc1.get_gridquality_properties(self)
+
+        return gprops
+
+    # =========================================================================
+    # Some more special operations that changes the grid or actnum
+    # =========================================================================
+    def activate_all(self):
+        """Activate all cells in the grid, by manipulating ACTNUM."""
+        self._actnumsv = np.ones(self.dimensions, dtype=np.int32)
+
+        if self._xtgformat == 1:
+            self._actnumsv = self._actnumsv.flatten()
+
+        self._tmp = {}
+
+    def inactivate_by_dz(self, threshold):
+        """Inactivate cells thinner than a given threshold."""
+        _grid_etc1.inactivate_by_dz(self, threshold)
+        self._tmp = {}
+
+    def inactivate_inside(self, poly, layer_range=None, inside=True, force_close=False):
+        """Inacativate grid inside a polygon.
+
+        The Polygons instance may consist of several polygons. If a polygon
+        is open, then the flag force_close will close any that are not open
+        when doing the operations in the grid.
+
+        Args:
+            poly(Polygons): A polygons object
+            layer_range (tuple): A tuple of two ints, upper layer = 1, e.g.
+                (1, 14). Note that base layer count is 1 (not zero)
+            inside (bool): True if remove inside polygon
+            force_close (bool): If True then force polygons to be closed.
+
+        Raises:
+            RuntimeError: If a problems with one or more polygons.
+            ValueError: If Polygon is not a XTGeo object
+        """
+        _grid_etc1.inactivate_inside(
+            self, poly, layer_range=layer_range, inside=inside, force_close=force_close
+        )
+        self._tmp = {}
+
+    def inactivate_outside(self, poly, layer_range=None, force_close=False):
+        """Inacativate grid outside a polygon."""
+        self.inactivate_inside(
+            poly, layer_range=layer_range, inside=False, force_close=force_close
+        )
+        self._tmp = {}
+
+    def collapse_inactive_cells(self):
+        """Collapse inactive layers where, for I J with other active cells."""
+        _grid_etc1.collapse_inactive_cells(self)
+        self._tmp = {}
+
+    def crop(self, colcrop, rowcrop, laycrop, props=None):
+        """Reduce the grid size by cropping.
+
+        The new grid will get new dimensions.
+
+        If props is "all" then all properties assosiated (linked) to then
+        grid are also cropped, and the instances are updated.
+
+        Args:
+            colcrop (tuple): A tuple on the form (i1, i2)
+                where 1 represents start number, and 2 represent end. The range
+                is inclusive for both ends, and the number start index is 1 based.
+            rowcrop (tuple): A tuple on the form (j1, j2)
+            laycrop (tuple): A tuple on the form (k1, k2)
+            props (list or str): None is default, while properties can be listed.
+                If "all", then all GridProperty objects which are linked to the
+                Grid instance are updated.
+
+        Returns:
+            The instance is updated (cropped)
+
+        Example::
+
+            >>> import xtgeo
+            >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> mygrid.crop((3, 6), (4, 20), (1, 10))
+            >>> mygrid.to_file(outdir + "/gf_reduced.roff")
+
+        """
+        _grid_etc1.crop(self, (colcrop, rowcrop, laycrop), props=props)
+        self._tmp = {}
+
+    def reduce_to_one_layer(self):
+        """Reduce the grid to one single layer.
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> grid.nlay
+            14
+            >>> grid.reduce_to_one_layer()
+            >>> grid.nlay
+            1
+
+        """
+        _grid_etc1.reduce_to_one_layer(self)
+        self._tmp = {}
+
+    def translate_coordinates(self, translate=(0, 0, 0), flip=(1, 1, 1)):
+        """Translate (move) and/or flip grid coordinates in 3D.
+
+        By 'flip' here, it means that the full coordinate array are multiplied
+        with -1.
+
+        Args:
+            translate (tuple): Translation distance in X, Y, Z coordinates
+            flip (tuple): Flip array. The flip values must be 1 or -1.
+
+        Raises:
+            RuntimeError: If translation goes wrong for unknown reasons
+        """
+        _grid_etc1.translate_coordinates(self, translate=translate, flip=flip)
+        self._tmp = {}
+
+    def reverse_row_axis(self, ijk_handedness=None):
+        """Reverse the row axis (J indices).
+
+        This means that IJK system will switched between a left vs right handed system.
+        It is here (by using ijk_handedness key), possible to set a wanted stated.
+
+        Note that properties that are assosiated with the grid (through the
+        :py:attr:`~gridprops` or :py:attr:`~props` attribute) will also be
+        reversed (which is desirable).
+
+        Args:
+            ijk_handedness (str): If set to "right" or "left", do only reverse rows if
+                handedness is not already achieved.
+
+        Example::
+
+            grd = xtgeo.Grid("somefile.roff")
+            prop1 = xtgeo.GridProperty("somepropfile1.roff")
+            prop2 = xtgeo.GridProperty("somepropfile2.roff")
+
+            grd.props = [prop1, prop2]
+
+            # secure that the grid geometry is IJK right-handed
+            grd.reverse_row_axis(ijk_handedness="right")
+
+        .. versionadded:: 2.5
+
+        """
+        _grid_etc1.reverse_row_axis(self, ijk_handedness=ijk_handedness)
+        self._tmp = {}
+
+    def make_zconsistent(self, zsep=1e-5):
+        """Make the 3D grid consistent in Z, by a minimal gap (zsep).
+
+        Args:
+            zsep (float): Minimum gap
+        """
+        _grid_etc1.make_zconsistent(self, zsep)
+        self._tmp = {}
+
+    def convert_to_hybrid(
+        self,
+        nhdiv=10,
+        toplevel=1000.0,
+        bottomlevel=1100.0,
+        region=None,
+        region_number=None,
+    ):
+        """Convert to hybrid grid, either globally or in a selected region.
+
+        This function will convert the internal structure in the corner point grid,
+        so that the cells between two levels ``toplevel`` and ``bottomlevel`` become
+        horizontal, which can be useful in flow simulators when e.g. liquid
+        contact movements are dominating. See example on `usage in the Troll field`_.
+
+        Note that the resulting hybrid will have an increased number of layers.
+        If the initial grid has N layers, and the number of horizontal layers
+        is NHDIV, then the result grid will have N * 2 + NHDIV layers.
+
+        .. image:: images/hybridgrid2.jpg
+           :width: 600
+           :align: center
+
+        Args:
+            nhdiv (int): Number of hybrid layers.
+            toplevel (float): Top of hybrid grid.
+            bottomlevel (float): Base of hybrid grid.
+            region (GridProperty, optional): Region property (if needed).
+            region_number (int): Which region to apply hybrid grid in if region.
+
+        Example:
+            Create a hybridgrid from file, based on a GRDECL file (no region)::
+
+               import xtgeo
+               grd = xtgeo.grid_from_file("simgrid.grdecl", fformat="grdecl")
+               grd.convert_to_hybrid(nhdiv=12, toplevel=2200, bottomlevel=2250)
+               # save in binary GRDECL fmt:
+               grd.to_file("simgrid_hybrid.bgrdecl", fformat="bgrdecl")
+
+        See Also:
+               :ref:`hybrid` example.
+
+        .. _usage in the Troll field: https://doi.org/10.2118/148023-MS
+
+        """
+        _grid_hybrid.make_hybridgrid(
+            self,
+            nhdiv=nhdiv,
+            toplevel=toplevel,
+            bottomlevel=bottomlevel,
+            region=region,
+            region_number=region_number,
+        )
+        self._tmp = {}
+
+    def refine_vertically(self, rfactor, zoneprop=None):
+        """Refine vertically, proportionally.
+
+        The rfactor can be a scalar or a dictionary.
+
+        If rfactor is a dict and zoneprop is None, then the current
+        subgrids array is used. If zoneprop is defined, the
+        current subgrid index will be redefined for the case. A warning will
+        be issued if subgrids are defined, but the give zone
+        property is inconsistent with this.
+
+        Also, if a zoneprop is defined but no current subgrids in the grid,
+        then subgrids will be added to the grid, if more than 1 subgrid.
+
+        Args:
+            self (object): A grid XTGeo object
+            rfactor (scalar or dict): Refinement factor, if dict, then the
+                dictionary must be consistent with self.subgrids if this is
+                present.
+            zoneprop (GridProperty): Zone property; must be defined if rfactor
+                is a dict
+
+        Returns:
+            ValueError: if..
+            RuntimeError: if mismatch in dimensions for rfactor and zoneprop
+
+
+        Examples::
+
+            # refine vertically all by factor 3
+
+            grd.refine_vertically(3)
+
+            # refine by using a dictionary; note that subgrids must exist!
+            # and that subgrids that are not mentioned will have value 1
+            # in refinement (1 is meaning no refinement)
+
+            grd.refine_vertically({1: 3, 2: 4, 4: 1})
+
+            # refine by using a a dictionary and a zonelog. If subgrids exists
+            # but are inconsistent with the zonelog; the current subgrids will
+            # be redefined, and a warning will be issued! Note also that ranges
+            # in the dictionary rfactor and the zone property must be aligned.
+
+            grd.refine_vertically({1: 3, 2: 4, 4: 0}, zoneprop=myzone)
+
+        """
+        _grid_refine.refine_vertically(self, rfactor, zoneprop=zoneprop)
+        self._tmp = {}
+
+    def report_zone_mismatch(
+        self,
+        well=None,
+        zonelogname="ZONELOG",
+        zoneprop=None,
+        onelayergrid=None,
+        zonelogrange=(0, 9999),
+        zonelogshift=0,
+        depthrange=None,
+        perflogname=None,
+        perflogrange=(1, 9999),
+        filterlogname=None,
+        filterlogrange=(1e-32, 9999.0),
+        resultformat=1,
+    ):
+        """Reports mismatch between wells and a zone.
+
+        Approaches on matching:
+            1. Use the well zonelog as basis, and compare sampled zone with that
+               interval. This means that zone cells outside well range will not be
+               counted
+            2. Compare intervals with wellzonation in range or grid zonations in
+               range. This gives a wider comparison, and will capture cases
+               where grid zonations is outside well zonation
+
+        .. image:: images/zone-well-mismatch-plain.svg
+           :width: 200
+           :align: center
+
+        Note if `zonelogname` and/or `filterlogname` and/or `perflogname` is given,
+        and such log(s) are not present, then this function will return ``None``.
+
+        Args:
+            well (Well): a XTGeo well object
+            zonelogname (str): Name of the zone logger
+            zoneprop (GridProperty): Grid property instance to use for
+                zonation
+            zonelogrange (tuple): zone log range, from - to (inclusive)
+            onelayergrid (Grid): Redundant from version 2.8, please skip!
+            zonelogshift (int): Deviation (numerical shift) between grid and zonelog,
+                e.g. if Zone property starts with 1 and this corresponds to a zonelog
+                index of 3 in the well, the shift shall be -2.
+            depthrange (tuple): Interval for search in TVD depth, to speed up
+            perflogname (str): Name of perforation log to filter on (> 0 default).
+            perflogrange (tuple): Range of values where perforations are present.
+            filterlogname (str): General filter, work as perflog, filter on values > 0
+            filterlogrange (tuple): Range of values where filter shall be present.
+            resultformat (int): If 1, consider the zonelogrange in the well as
+                basis for match ratio, return (percent, match count, total count).
+                If 2 then a dictionary is returned with various result members
+
+        Returns:
+            res (tuple or dict): report dependent on `resultformat`
+                * A tuple with 3 members:
+                    (match_as_percent, number of matches, total count) approach 1
+                * A dictionary with keys:
+                    * MATCH1 - match as percent, approach 1
+                    * MCOUNT1 - number of match samples approach 1
+                    * TCOUNT1 - total number of samples approach 1
+                    * MATCH2 - match as percent, approach 2
+                    * MCOUNT2 - a.a for option 2
+                    * TCOUNT2 - a.a. for option 2
+                    * WELLINTV - a Well() instance for the actual interval
+                * None, if perflogname or zonelogname of filtername is given, but
+                  the log does not exists for the well
+
+        Example::
+
+            g1 = xtgeo.Grid("gullfaks2.roff")
+
+            z = xtgeo.GridProperty(gullfaks2_zone.roff", name="Zone")
+
+            w2 = xtgeo.Well("34_10-1.w", zonelogname="Zonelog")
+
+            w3 = xtgeo.Well("34_10-B-21_B.w", zonelogname="Zonelog"))
+
+            wells = [w2, w3]
+
+            for w in wells:
+                response = g1.report_zone_mismatch(
+                    well=w, zonelogname="ZONELOG", zoneprop=z,
+                    zonelogrange=(0, 19), depthrange=(1700, 9999))
+
+                print(response)
+
+        .. versionchanged:: 2.8 Added several new keys and better precision in result
+        .. versionchanged:: 2.11 Added ``perflogrange`` and ``filterlogrange``
+        """
+        reports = _grid_wellzone.report_zone_mismatch(
+            self,
+            well=well,
+            zonelogname=zonelogname,
+            zoneprop=zoneprop,
+            onelayergrid=onelayergrid,
+            zonelogrange=zonelogrange,
+            zonelogshift=zonelogshift,
+            depthrange=depthrange,
+            perflogname=perflogname,
+            perflogrange=perflogrange,
+            filterlogname=filterlogname,
+            filterlogrange=filterlogrange,
+            resultformat=resultformat,
+        )
+
+        return reports
+
+    # ==================================================================================
+    # Extract a fence/randomline by sampling, ready for plotting with e.g. matplotlib
+    # ==================================================================================
+    def get_randomline(
+        self,
+        fencespec,
+        prop,
+        zmin=None,
+        zmax=None,
+        zincrement=1.0,
+        hincrement=None,
+        atleast=5,
+        nextend=2,
+    ):
+        """Get a sampled randomline from a fence spesification.
+
+        This randomline will be a 2D numpy with depth on the vertical
+        axis, and length along as horizontal axis. Undefined values will have
+        the np.nan value.
+
+        The input fencespec is either a 2D numpy where each row is X, Y, Z, HLEN,
+        where X, Y are UTM coordinates, Z is depth/time, and HLEN is a
+        length along the fence, or a Polygons instance.
+
+        If input fencspec is a numpy 2D, it is important that the HLEN array
+        has a constant increment and ideally a sampling that is less than the
+        Grid resolution. If a Polygons() instance, this is automated if hincrement is
+        None, and ignored if hincrement is False.
+
+        Args:
+            fencespec (:obj:`~numpy.ndarray` or :class:`~xtgeo.xyz.polygons.Polygons`):
+                2D numpy with X, Y, Z, HLEN as rows or a xtgeo Polygons() object.
+            prop (GridProperty or str): The grid property object, or name, which shall
+                be plotted.
+            zmin (float): Minimum Z (default is Grid Z minima/origin)
+            zmax (float): Maximum Z (default is Grid Z maximum)
+            zincrement (float): Sampling vertically, default is 1.0
+            hincrement (float or bool): Resampling horizontally. This applies only
+                if the fencespec is a Polygons() instance. If None (default),
+                the distance will be deduced automatically. If False, then it assumes
+                the Polygons can be used as-is.
+            atleast (int): Minimum number of horizontal samples (only if
+                fencespec is a Polygons instance and hincrement != False)
+            nextend (int): Extend with nextend * hincrement in both ends (only if
+                fencespec is a Polygons instance and hincrement != False)
+
+        Returns:
+            A tuple: (hmin, hmax, vmin, vmax, ndarray2d)
+
+        Raises:
+            ValueError: Input fence is not according to spec.
+
+        Example::
+
+            mygrid = xtgeo.Grid("somegrid.roff")
+            poro = xtgeo.GridProperty("someporo.roff")
+            mywell = xtgeo.Well("somewell.rmswell")
+            fence = mywell.get_fence_polyline(sampling=5, tvdmin=1750, asnumpy=True)
+            (hmin, hmax, vmin, vmax, arr) = mygrid.get_randomline(
+                 fence, poro, zmin=1750, zmax=1850, zincrement=0.5,
+            )
+            # matplotlib ...
+            plt.imshow(arr, cmap="rainbow", extent=(hmin1, hmax1, vmax1, vmin1))
+
+        .. versionadded:: 2.1
+
+        .. seealso::
+           Class :class:`~xtgeo.xyz.polygons.Polygons`
+              The method :meth:`~xtgeo.xyz.polygons.Polygons.get_fence()` which can be
+              used to pregenerate `fencespec`
+
+        """
+        if not isinstance(fencespec, (np.ndarray, xtgeo.Polygons)):
+            raise ValueError("fencespec must be a numpy or a Polygons() object")
+        logger.info("Getting randomline...")
+
+        res = _grid3d_fence.get_randomline(
+            self,
+            fencespec,
+            prop,
+            zmin=zmin,
+            zmax=zmax,
+            zincrement=zincrement,
+            hincrement=hincrement,
+            atleast=atleast,
+            nextend=nextend,
+        )
+        logger.info("Getting randomline... DONE")
+        return res
+
+    # ----------------------------------------------------------------------------------
+    # Special private functions; these may only live for while
+    # ----------------------------------------------------------------------------------
+
+    def _convert_xtgformat2to1(self):
+        """Convert arrays from new structure xtgformat=2 to legacy xtgformat=1."""
+        _grid_etc1._convert_xtgformat2to1(self)
+
+    def _convert_xtgformat1to2(self):
+        """Convert arrays from old structure xtgformat=1 to new xtgformat=2."""
+        _grid_etc1._convert_xtgformat1to2(self)
+
+    def _xtgformat1(self):
+        """Shortform... arrays from new structure xtgformat=2 to legacy xtgformat=1."""
+        self._convert_xtgformat2to1()
+
+    def _xtgformat2(self):
+        """Shortform... arrays from old structure xtgformat=1 to new xtgformat=2."""
+        self._convert_xtgformat1to2()
```

## xtgeo/grid3d/grid_properties.py

 * *Ordering differences only*

```diff
@@ -1,811 +1,811 @@
-# -*- coding: utf-8 -*-
-
-"""Module for Grid Properties."""
-import hashlib
-import warnings
-from typing import List, Optional
-
-import deprecation
-import numpy as np
-import pandas as pd
-
-import xtgeo
-from xtgeo.common import XTGDescription, XTGeoDialog
-from xtgeo.common.constants import MAXDATES, MAXKEYWORDS
-
-from . import _grid3d_utils as utils
-from . import _grid_etc1, _gridprops_import_eclrun, _gridprops_import_roff
-from ._grid3d import _Grid3D
-from .grid_property import GridProperty
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def gridproperties_from_file(
-    pfile,
-    fformat=None,
-    names=None,
-    dates=None,
-    grid=None,
-    namestyle=0,
-    strict=(True, False),
-):
-    """Import grid properties from file.
-
-    In case of names='all' then all vectors which have a valid length
-    (number of total or active cells in the grid) will be read
-
-    Args:
-        pfile (str or Path): Name of file with properties
-        fformat (str): roff/init/unrst
-        names: list of property names, e.g. ['PORO', 'PERMX'] or 'all'
-        dates: list of dates on YYYYMMDD format, for restart files, or 'all'
-        grid (obj): The grid geometry object (optional if ROFF)
-        namestyle (int): 0 (default) for style SWAT_20110223,
-            1 for SWAT--2011_02_23 (applies to restart only)
-        strict (tuple of (bool, bool)): If (True, False) (default) then an
-            Error is raised if keyword name is not found, or a key-date combination
-            is not found. However, the dates will processed so that non-valid dates
-            are skipped (still, invalid key-date combinations may occur!).
-            If (True, True) all keywords and dates are tried, while (False, False)
-            means that that only valid entries are imported, more or less silently.
-            Saturations keywords SWAT/SOIL/SGAS are not evaluated as they may be
-            derived.
-    Example::
-        >>> grd = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID", fformat='egrid')
-        >>> gps = xtgeo.gridproperties_from_file(
-        ...     reek_dir + "/REEK.INIT",
-        ...     fformat='init',
-        ...     names=["PORO", "PERMX"],
-        ...     grid=grd,
-        ... )
-    """
-    pfile = xtgeo._XTGeoFile(pfile, mode="rb")
-
-    pfile.check_file(raiseerror=ValueError)
-
-    if fformat is None or fformat == "guess":
-        fformat = pfile.detect_fformat()
-    else:
-        fformat = pfile.generic_format_by_proposal(fformat)  # default
-
-    if fformat.lower() in ["roff_ascii", "roff_binary"]:
-        props = _gridprops_import_roff.import_roff_gridproperties(
-            pfile, names, strict=strict
-        )
-        return GridProperties(props=props)
-
-    elif fformat.lower() == "init":
-        return GridProperties(
-            props=_gridprops_import_eclrun.import_ecl_init_gridproperties(
-                pfile,
-                grid=grid,
-                names=names,
-                strict=strict[0],
-                maxkeys=MAXKEYWORDS,
-            )
-        )
-    elif fformat.lower() == "unrst":
-        return GridProperties(
-            props=_gridprops_import_eclrun.import_ecl_restart_gridproperties(
-                pfile,
-                dates=dates,
-                grid=grid,
-                names=names,
-                namestyle=namestyle,
-                strict=strict,
-                maxkeys=MAXKEYWORDS,
-            )
-        )
-    else:
-        raise ValueError("Invalid file format {fformat}")
-
-
-# --------------------------------------------------------------------------------------
-# Comment on 'asmasked' vs 'activeonly:
-#
-# 'asmasked'=True will return a np.ma array, while 'asmasked' = False will
-# return a np.ndarray
-#
-# The 'activeonly' will filter out masked entries, or use None or np.nan
-# if 'activeonly' is False.
-#
-# Use word 'zerobased' for a bool regrading startcell basis is 1 or 0
-#
-# For functions with mask=... ,they should be replaced with asmasked=...
-# --------------------------------------------------------------------------------------
-
-
-def gridproperties_dataframe(
-    gridproperties, grid=None, activeonly=True, ijk=False, xyz=False, doubleformat=False
-):  # pylint: disable=too-many-branches, too-many-statements
-    """Returns a Pandas dataframe table for the properties.
-
-    Similar to :meth:`GridProperties.get_dataframe()` but takes any list of
-    grid properties as its first argument.
-
-    Args:
-        gridproperties: List (also GridProperties or iterable) of GridProperty
-            to create dataframe of.
-        activeonly (bool): If True, return only active cells, NB!
-            If True, will require a grid instance (see grid key)
-        ijk (bool): If True, show cell indices, IX JY KZ columns
-        xyz (bool): If True, show cell center coordinates (needs grid).
-        doubleformat (bool): If True, floats are 64 bit, otherwise 32 bit.
-            Note that coordinates (if xyz=True) is always 64 bit floats.
-        grid (Grid): The grid geometry object. This is required for the
-            xyz option.
-    Returns:
-        Pandas dataframe object
-    Examples::
-        >>> grd = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID", fformat='egrid')
-        >>> names = ['SOIL', 'SWAT', 'PRESSURE']
-        >>> dates = [19991201]
-        >>> gps = xtgeo.gridproperties_from_file(
-        ...     reek_dir + "/REEK.UNRST",
-        ...     fformat='unrst',
-        ...     names=names,
-        ...     dates=dates,
-        ...     grid=grd
-        ... )
-        >>> df = xtgeo.gridproperties_dataframe(gps, grid=grd)
-    """
-
-    proplist = list(gridproperties)
-
-    dataframe_dict = dict()
-    if ijk:
-        if activeonly:
-            if grid:
-                ix, jy, kz = _grid_etc1.get_ijk(grid)
-                dataframe_dict["IX"] = ix.get_active_npvalues1d()
-                dataframe_dict["JY"] = jy.get_active_npvalues1d()
-                dataframe_dict["KZ"] = kz.get_active_npvalues1d()
-            elif proplist:
-                ix, jy, kz = _grid_etc1.get_ijk(proplist[0])
-                dataframe_dict["IX"] = ix.get_active_npvalues1d()
-                dataframe_dict["JY"] = jy.get_active_npvalues1d()
-                dataframe_dict["KZ"] = kz.get_active_npvalues1d()
-        else:
-            if not grid:
-                raise ValueError(
-                    "You ask for active_only but no Grid is present. Use grid=..."
-                )
-            act = grid.get_actnum(dual=True)
-            ix, jy, kz = grid.get_ijk(asmasked=False)
-            dataframe_dict["ACTNUM"] = act.values1d
-            dataframe_dict["IX"] = ix.values1d
-            dataframe_dict["JY"] = jy.values1d
-            dataframe_dict["KZ"] = kz.values1d
-
-    if xyz:
-        if not grid:
-            raise ValueError("You ask for xyz but no Grid is present. Use grid=...")
-
-        xc, yc, zc = grid.get_xyz(asmasked=activeonly)
-        if activeonly:
-            dataframe_dict["X_UTME"] = xc.get_active_npvalues1d()
-            dataframe_dict["Y_UTMN"] = yc.get_active_npvalues1d()
-            dataframe_dict["Z_TVDSS"] = zc.get_active_npvalues1d()
-        else:
-            dataframe_dict["X_UTME"] = xc.values1d
-            dataframe_dict["Y_UTMN"] = yc.values1d
-            dataframe_dict["Z_TVDSS"] = zc.values1d
-
-    for prop in gridproperties:
-        if activeonly:
-            vector = prop.get_active_npvalues1d()
-        else:
-            vector = prop.values1d.copy()
-            # mask values not supported in Pandas:
-            if prop.isdiscrete:
-                vector = vector.filled(fill_value=0)
-            else:
-                vector = vector.filled(fill_value=np.nan)
-
-        if doubleformat:
-            vector = vector.astype(np.float64)
-        else:
-            vector = vector.astype(np.float32)
-
-        dataframe_dict[prop.name] = vector
-
-    return pd.DataFrame.from_dict(dataframe_dict)
-
-
-class GridProperties(_Grid3D):
-    """Class for a collection of 3D grid props, belonging to the same grid topology.
-
-     It is a thin wrapper on a list that 1) checks that the GridProperties
-     belong to the same Grid (loosely). 2) Contains operations that can be
-     called on lists of GridProperty objects for easy discoverability.
-
-     Examples::
-         >>> import xtgeo
-         >>> # Create an
-         >>> grid_properties = xtgeo.GridProperties(props=[])
-         >>> # Get the dataframe via the gridproperties object
-         >>> grid_properties.get_dataframe()
-         Empty DataFrame...
-         >>> # Convert the gridproperties to a list
-         >>> grid_properties_list = list(grid_properties)
-         >>> # Get the dataframe of the list:
-         >>> gridproperties_dataframe(grid_properties_list)
-         Empty DataFrame...
-
-    Args:
-        ncol: Deprecated argument.
-        nrow: Deprecated argument.
-        nlay: Deprecated argument.
-        props: The list of GridProperty objects.
-
-    See Also:
-        The :class:`GridProperty` class.
-    """
-
-    def __init__(
-        self,
-        ncol: Optional[int] = None,
-        nrow: Optional[int] = None,
-        nlay: Optional[int] = None,
-        props: List[GridProperty] = None,
-    ):
-        dims_given = False
-        if ncol is not None:
-            warnings.warn(
-                "Initializing GridProperties with ncol is deprecated.",
-                DeprecationWarning,
-            )
-            dims_given = True
-        else:
-            ncol = 4
-        if nrow is not None:
-            warnings.warn(
-                "Initializing GridProperties with nrow is deprecated.",
-                DeprecationWarning,
-            )
-            dims_given = True
-        else:
-            nrow = 3
-        if nlay is not None:
-            warnings.warn(
-                "Initializing GridProperties with nlay is deprecated.",
-                DeprecationWarning,
-            )
-            dims_given = True
-        else:
-            nlay = 5
-
-        if props:
-            if dims_given:
-                raise ValueError(
-                    "Giving both ncol/nrow/nlay and props list is not supported. "
-                    "Please give just props as ncol/nrow/nlay is deprecated."
-                )
-            ncol, nrow, nlay = props[0].dimensions
-
-        super().__init__(ncol, nrow, nlay)
-
-        # The _names field is just kept for backwards
-        # compatability until the names setter has been
-        # deprecated
-        self._names = []
-
-        self.props = props or []
-
-    def __repr__(self):  # noqa: D105
-        myrp = (
-            f"{self.__class__.__name__} (id={id(self)}) ncol={self._ncol!r}, "
-            f"nrow={self._nrow!r}, nlay={self._nlay!r}, filesrc={self.names!r}"
-        )
-        return myrp
-
-    def __str__(self):
-        """str: User friendly print."""
-        return self.describe(flush=False)
-
-    def __contains__(self, name):
-        """bool: Emulate 'if "PORO" in props'."""
-        prop = self.get_prop_by_name(name, raiseserror=False)
-        if prop:
-            return True
-
-        return False
-
-    def __getitem__(self, name):  # noqa: D105
-        prop = self.get_prop_by_name(name, raiseserror=False)
-        if prop is None:
-            raise KeyError(f"Key {name} does not exist")
-
-        return prop
-
-    def __iter__(self):  # noqa: D105
-        return iter(self._props)
-
-    @property
-    def names(self):
-        """Returns a list of property names.
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> props = GridProperties()
-            >>> props.from_file(
-            ...     reek_dir + "/REEK.INIT",
-            ...     fformat="init",
-            ...     names=["PERMX"],
-            ...     grid=grid,
-            ... )
-
-            >>> namelist = props.names
-            >>> for name in namelist:
-            ...     print ('Property name is {}'.format(name))
-            Property name is PERMX
-
-        """
-        return self._names
-
-    @names.setter
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="The behavior of the setting names would before create an alias name "
-        "the behavior of which was not always consistent. "
-        "Note that setting names on the GridProperties has "
-        "_no effect_ on the behavior of its methods except the names getter."
-        "This name aliasing is now going away. "
-        "In order to change the name of properties, "
-        "use\nfor p in gridprops:\n    p.name = newname",
-    )
-    def names(self, nameslist):
-        if len(nameslist) != len(self._props):
-            raise ValueError("Number of names does not match number of properties")
-
-        # look for duplicates
-        if len(nameslist) > len(set(nameslist)):
-            raise ValueError("List of names contains duplicates; names must be unique")
-
-        self._names = nameslist
-
-    @property
-    def props(self):
-        """Returns a list of XTGeo GridProperty objects, None if empty.
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> myprops = GridProperties()
-            >>> myprops.from_file(
-            ...     reek_dir + "/REEK.INIT",
-            ...     fformat="init",
-            ...     names=["PERMX"],
-            ...     grid=grid,
-            ... )
-
-            >>> proplist = myprops.props
-            >>> for prop in proplist:
-            ...     print ('Property object name is {}'.format(prop.name))
-            Property object name is PERMX
-
-            >>> # adding a property, e.g. get ACTNUM as a property from the grid
-            >>> actn = grid.get_actnum()  # this will get actn as a GridProperty
-            >>> myprops.append_props([actn])
-        """
-        if not self._props:
-            return None
-
-        return self._props
-
-    @props.setter
-    def props(self, propslist):
-        self._props = propslist
-        if propslist:
-            self._ncol = propslist[0].ncol
-            self._nrow = propslist[0].nrow
-            self._nlay = propslist[0].nlay
-        self._names = [p.name for p in self._props]
-        self._consistency_check()
-
-    @property
-    def dates(self):
-        """Returns a list of valid (found) dates after import.
-
-        Returns None if no dates present
-
-        Note:
-            See also :meth:`GridProperties.scan_dates` for scanning available dates
-            in advance
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> props = GridProperties()
-            >>> props.from_file(
-            ...     reek_dir + "/REEK.INIT",
-            ...     fformat="init",
-            ...     names=["PERMX"],
-            ...     grid=grid,
-            ... )
-
-            >>> datelist = props.dates
-            >>> for date in datelist:
-            ...     print ('Date applied is {}'.format(date))
-            Date applied is 19991201
-
-        .. versionchanged:: 2.16 dates is no longer an alias (undocumented behavior),
-            and simply return the dates of the underlying list of GridProperty.
-        """
-        if not self.props:
-            return None
-
-        return [p.date for p in self.props]
-
-    # Copy, and etc aka setters and getters
-
-    def copy(self):
-        """Copy a GridProperties instance to a new unique instance.
-
-        Note that the GridProperty instances will also be unique.
-        """
-
-        gps = GridProperties(props=[p.copy() for p in self.props] if self.props else [])
-        gps._names = self._names.copy()
-        return gps
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout."""
-        dsc = XTGDescription()
-
-        dsc.title("Description of GridProperties instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
-        dsc.txt("Attached grid props objects (names)", self.names)
-
-        if flush:
-            dsc.flush()
-            return None
-        return dsc.astext()
-
-    def generate_hash(self):
-        """str: Return a unique hash ID for current gridproperties instance.
-
-        .. versionadded:: 2.10
-        """
-        mhash = hashlib.sha256()
-
-        hashinput = ""
-        for prop in self._props:
-            gid = (
-                f"{prop.ncol}{prop.nrow}{prop.nlay}{prop.values.mean()}"
-                f"{prop.values.min()}{prop.values.max()}"
-            )
-            hashinput += gid
-
-        mhash.update(hashinput.encode())
-        return mhash.hexdigest()
-
-    def get_prop_by_name(self, name, raiseserror=True):
-        """Find and return a property object (GridProperty) by name.
-
-        Args:
-            name (str): Name of property to look for
-            raiseserror (bool): If True, raises a ValueError if not found, otherwise
-                return None
-
-        """
-        for prop in self._props:
-            logger.debug("Look for %s, actual is %s", name, prop.name)
-            if prop.name == name:
-                logger.debug(repr(prop))
-                return prop
-
-        if raiseserror:
-            raise ValueError(f"Cannot find property with name <{name}>")
-
-        return None
-
-    def append_props(self, proplist):
-        """Add a list of GridProperty objects to current GridProperties instance."""
-        if not self._props and proplist:
-            self._ncol = proplist[0].ncol
-            self._nrow = proplist[0].nrow
-            self._nlay = proplist[0].nlay
-        self._props += proplist
-        self._names = [p.name for p in self._props]
-        self._consistency_check()
-
-    def get_ijk(
-        self, names=("IX", "JY", "KZ"), zerobased=False, asmasked=False, mask=None
-    ):
-        """Returns 3 xtgeo.grid3d.GridProperty objects: I counter, J counter, K counter.
-
-        Args:
-            names: a 3 x tuple of names per property (default IX, JY, KZ).
-            asmasked: If True, then active cells only.
-            mask: If True, then active cells only (deprecated).
-            zerobased: If True, counter start from 0, otherwise 1 (default=1).
-        """
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = super()._evaluate_mask(mask)
-
-        return _grid_etc1.get_ijk(
-            self, names=names, zerobased=zerobased, asmasked=asmasked
-        )
-
-    def get_actnum(self, name="ACTNUM", asmasked=False, mask=None):
-        """Return an ACTNUM GridProperty object.
-
-        Args:
-            name (str): name of property in the XTGeo GridProperty object.
-            asmasked (bool): ACTNUM is returned with all cells
-                as default. Use asmasked=True to make 0 entries masked.
-            mask (bool): Deprecated, use asmasked instead.
-
-        Example::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> myprops = GridProperties()
-            >>> myprops.from_file(
-            ...     reek_dir + "/REEK.INIT",
-            ...     fformat="init",
-            ...     names=["PERMX"],
-            ...     grid=grid,
-            ... )
-            >>> act = myprops.get_actnum()
-            >>> print('{}% of cells are active'.format(act.values.mean() * 100))
-            99.99...% of cells are active
-
-        Returns:
-            A GridProperty instance of ACTNUM, or None if no props present.
-        """
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = super()._evaluate_mask(mask)
-
-        # borrow function from GridProperty class:
-        if self._props:
-            return self._props[0].get_actnum(name=name, asmasked=asmasked)
-
-        warnings.warn("No gridproperty in list", UserWarning)
-        return None
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.gridproperties_from_file() instead",
-    )
-    def from_file(
-        self,
-        pfile,
-        fformat="roff",
-        names=None,
-        dates=None,
-        grid=None,
-        namestyle=0,
-        strict=(True, False),
-    ):
-        """Import grid properties from file in one go.
-
-        This class is particulary useful for Eclipse INIT and RESTART files.
-
-        In case of names='all' then all vectors which have a valid length
-        (number of total or active cells in the grid) will be read
-
-        Args:
-            pfile (str or Path): Name of file with properties
-            fformat (str): roff/init/unrst
-            names: list of property names, e.g. ['PORO', 'PERMX'] or 'all'
-            dates: list of dates on YYYYMMDD format, for restart files, or 'all'
-            grid (obj): The grid geometry object (optional if ROFF)
-            namestyle (int): 0 (default) for style SWAT_20110223,
-                1 for SWAT--2011_02_23 (applies to restart only)
-            strict (tuple of (bool, bool)): If (True, False) (default) then an
-                Error is raised if keyword name is not found, or a key-date combination
-                is not found. However, the dates will processed so that non-valid dates
-                are skipped (still, invalid key-date combinations may occur!).
-                If (True, True) all keywords and dates are tried, while (False, False)
-                means that that only valid entries are imported, more or less silently.
-                Saturations keywords SWAT/SOIL/SGAS are not evaluated as they may be
-                derived.
-
-        Example::
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> props = GridProperties()
-            >>> props.from_file(
-            ...     reek_dir + "/REEK.INIT",
-            ...     fformat="init",
-            ...     names=["PERMX"],
-            ...     grid=grid,
-            ... )
-
-
-        Raises:
-            FileNotFoundError: if input file is not found
-            DateNotFoundError: The date is not found
-            KeywordNotFoundError: The keyword is not found
-            KeywordFoundDateNotFoundError: The keyword but not date found
-
-        .. versionadded:: 2.13 Added strict key
-        """
-
-        self.append_props(
-            list(
-                gridproperties_from_file(
-                    pfile=pfile,
-                    fformat=fformat,
-                    names=names,
-                    dates=dates,
-                    grid=grid,
-                    namestyle=namestyle,
-                    strict=strict,
-                )
-            )
-        )
-
-    def get_dataframe(
-        self, activeonly=False, ijk=False, xyz=False, doubleformat=False, grid=None
-    ):
-        """Returns a Pandas dataframe table for the properties.
-
-        See also :func:`xtgeo.gridproperties_dataframe()`
-
-        Args:
-            activeonly (bool): If True, return only active cells, NB!
-                If True, will require a grid instance (see grid key)
-            ijk (bool): If True, show cell indices, IX JY KZ columns
-            xyz (bool): If True, show cell center coordinates (needs grid).
-            doubleformat (bool): If True, floats are 64 bit, otherwise 32 bit.
-                Note that coordinates (if xyz=True) is always 64 bit floats.
-            grid (Grid): The grid geometry object. This is required for the
-                xyz option.
-
-        Returns:
-            Pandas dataframe object
-
-        Examples::
-
-            >>> import xtgeo
-            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> pps.grid_properties_from_file(
-            ...     reek_dir + "/REEK.UNRST",
-            ...     fformat="unrst",
-            ...     names=['SOIL', 'SWAT', 'PRESSURE'],
-            ...     dates=[19991201],
-            ...     grid=grid,
-            ... )
-            >>> df = pps.get_dataframe(activeonly=False, ijk=True, xyz=True, grid=grid)
-            >>> print(df)
-                   ACTNUM  IX  JY  ...  SOIL_19991201  SWAT_19991201  PRESSURE_19991201
-            0           1   1   1  ...            0.0            1.0         341.694183
-            1           1   1   1  ...            0.0            1.0         342.097107
-            2           1   1   1  ...            0.0            1.0         342.500061
-            3           1   1   1  ...            0.0            1.0         342.902954
-            4           1   1   1  ...            0.0            1.0         343.305908
-            ...
-
-        """
-        return gridproperties_dataframe(
-            self,
-            activeonly=activeonly,
-            ijk=ijk,
-            xyz=xyz,
-            doubleformat=doubleformat,
-            grid=grid,
-        )
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use GridProperty.get_dataframe() instead",
-    )
-    def dataframe(self, *args, **kwargs):
-        return self.get_dataframe(*args, **kwargs)
-
-    def _consistency_check(self):
-        for p in self._props:
-            if (p.ncol, p.nrow, p.nlay) != (self.ncol, self.nrow, self.nlay):
-                raise ValueError("Mismatching dimensions in GridProperties members.")
-
-    @staticmethod
-    def scan_keywords(
-        pfile, fformat="xecl", maxkeys=MAXKEYWORDS, dataframe=False, dates=False
-    ):
-        """Quick scan of keywords in Eclipse binary files, or ROFF binary files.
-
-        For Eclipse files:
-        Returns a list of tuples (or dataframe), e.g. ('PRESSURE',
-        'REAL', 355299, 3582700), where (keyword, type, no_of_values,
-        byteposition_in_file)
-
-        For ROFF files
-        Returns a list of tuples (or dataframe), e.g.
-        ('translate!xoffset', 'float', 1, 3582700),
-        where (keyword, type, no_of_values, byteposition_in_file).
-
-        For Eclipse, the byteposition is to the KEYWORD, while for ROFF
-        the byte position is to the beginning of the actual data.
-
-        Args:
-            pfile (str): Name or a filehandle to file with properties
-            fformat (str): xecl (Eclipse INIT, RESTART, ...) or roff for
-                ROFF binary,
-            maxkeys (int): Maximum number of keys
-            dataframe (bool): If True, return a Pandas dataframe instead
-            dates (bool): if True, the date is the last column (only
-                menaingful for restart files). Default is False.
-
-        Return:
-            A list of tuples or dataframe with keyword info
-
-        Example::
-            >>> dlist = GridProperties.scan_keywords(reek_dir + "/REEK.UNRST")
-
-        """
-        pfile = xtgeo._XTGeoFile(pfile)
-        pfile.check_file(raiseerror=ValueError)
-
-        return utils.scan_keywords(
-            pfile,
-            fformat=fformat,
-            maxkeys=maxkeys,
-            dataframe=dataframe,
-            dates=dates,
-        )
-
-    @staticmethod
-    def scan_dates(
-        pfile, fformat="unrst", maxdates=MAXDATES, dataframe=False, datesonly=False
-    ):
-        """Quick scan dates in a simulation restart file.
-
-        Args:
-            pfile (str): Name of file or file handle with properties
-            fformat (str): unrst (so far)
-            maxdates (int): Maximum number of dates to collect
-            dataframe (bool): If True, return a Pandas dataframe instead
-            datesonly (bool): If True, SEQNUM is skipped,
-
-        Return:
-            A list of tuples or a dataframe with (seqno, date),
-            date is on YYYYMMDD form. If datesonly is True and dataframe is False,
-            the returning list will be a simple list of dates.
-
-        Example::
-            >>> dlist = GridProperties.scan_dates(reek_dir + "/REEK.UNRST")
-            >>> #or getting all dates a simple list:
-            >>> dlist = GridProperties.scan_dates(
-            ... reek_dir + "/REEK.UNRST",
-            ... datesonly=True)
-
-        .. versionchanged:: 2.13 Added datesonly keyword
-        """
-        logger.info("Format supported as default is %s", fformat)
-
-        pfile = xtgeo._XTGeoFile(pfile)
-        pfile.check_file(raiseerror=ValueError)
-
-        dlist = utils.scan_dates(pfile, maxdates=maxdates, dataframe=dataframe)
-
-        if datesonly and dataframe:
-            dlist.drop("SEQNUM", axis=1, inplace=True)
-
-        if datesonly and not dataframe:
-            dlist = [date for (_, date) in dlist]
-
-        return dlist
+# -*- coding: utf-8 -*-
+
+"""Module for Grid Properties."""
+import hashlib
+import warnings
+from typing import List, Optional
+
+import deprecation
+import numpy as np
+import pandas as pd
+
+import xtgeo
+from xtgeo.common import XTGDescription, XTGeoDialog
+from xtgeo.common.constants import MAXDATES, MAXKEYWORDS
+
+from . import _grid3d_utils as utils
+from . import _grid_etc1, _gridprops_import_eclrun, _gridprops_import_roff
+from ._grid3d import _Grid3D
+from .grid_property import GridProperty
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def gridproperties_from_file(
+    pfile,
+    fformat=None,
+    names=None,
+    dates=None,
+    grid=None,
+    namestyle=0,
+    strict=(True, False),
+):
+    """Import grid properties from file.
+
+    In case of names='all' then all vectors which have a valid length
+    (number of total or active cells in the grid) will be read
+
+    Args:
+        pfile (str or Path): Name of file with properties
+        fformat (str): roff/init/unrst
+        names: list of property names, e.g. ['PORO', 'PERMX'] or 'all'
+        dates: list of dates on YYYYMMDD format, for restart files, or 'all'
+        grid (obj): The grid geometry object (optional if ROFF)
+        namestyle (int): 0 (default) for style SWAT_20110223,
+            1 for SWAT--2011_02_23 (applies to restart only)
+        strict (tuple of (bool, bool)): If (True, False) (default) then an
+            Error is raised if keyword name is not found, or a key-date combination
+            is not found. However, the dates will processed so that non-valid dates
+            are skipped (still, invalid key-date combinations may occur!).
+            If (True, True) all keywords and dates are tried, while (False, False)
+            means that that only valid entries are imported, more or less silently.
+            Saturations keywords SWAT/SOIL/SGAS are not evaluated as they may be
+            derived.
+    Example::
+        >>> grd = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID", fformat='egrid')
+        >>> gps = xtgeo.gridproperties_from_file(
+        ...     reek_dir + "/REEK.INIT",
+        ...     fformat='init',
+        ...     names=["PORO", "PERMX"],
+        ...     grid=grd,
+        ... )
+    """
+    pfile = xtgeo._XTGeoFile(pfile, mode="rb")
+
+    pfile.check_file(raiseerror=ValueError)
+
+    if fformat is None or fformat == "guess":
+        fformat = pfile.detect_fformat()
+    else:
+        fformat = pfile.generic_format_by_proposal(fformat)  # default
+
+    if fformat.lower() in ["roff_ascii", "roff_binary"]:
+        props = _gridprops_import_roff.import_roff_gridproperties(
+            pfile, names, strict=strict
+        )
+        return GridProperties(props=props)
+
+    elif fformat.lower() == "init":
+        return GridProperties(
+            props=_gridprops_import_eclrun.import_ecl_init_gridproperties(
+                pfile,
+                grid=grid,
+                names=names,
+                strict=strict[0],
+                maxkeys=MAXKEYWORDS,
+            )
+        )
+    elif fformat.lower() == "unrst":
+        return GridProperties(
+            props=_gridprops_import_eclrun.import_ecl_restart_gridproperties(
+                pfile,
+                dates=dates,
+                grid=grid,
+                names=names,
+                namestyle=namestyle,
+                strict=strict,
+                maxkeys=MAXKEYWORDS,
+            )
+        )
+    else:
+        raise ValueError("Invalid file format {fformat}")
+
+
+# --------------------------------------------------------------------------------------
+# Comment on 'asmasked' vs 'activeonly:
+#
+# 'asmasked'=True will return a np.ma array, while 'asmasked' = False will
+# return a np.ndarray
+#
+# The 'activeonly' will filter out masked entries, or use None or np.nan
+# if 'activeonly' is False.
+#
+# Use word 'zerobased' for a bool regrading startcell basis is 1 or 0
+#
+# For functions with mask=... ,they should be replaced with asmasked=...
+# --------------------------------------------------------------------------------------
+
+
+def gridproperties_dataframe(
+    gridproperties, grid=None, activeonly=True, ijk=False, xyz=False, doubleformat=False
+):  # pylint: disable=too-many-branches, too-many-statements
+    """Returns a Pandas dataframe table for the properties.
+
+    Similar to :meth:`GridProperties.get_dataframe()` but takes any list of
+    grid properties as its first argument.
+
+    Args:
+        gridproperties: List (also GridProperties or iterable) of GridProperty
+            to create dataframe of.
+        activeonly (bool): If True, return only active cells, NB!
+            If True, will require a grid instance (see grid key)
+        ijk (bool): If True, show cell indices, IX JY KZ columns
+        xyz (bool): If True, show cell center coordinates (needs grid).
+        doubleformat (bool): If True, floats are 64 bit, otherwise 32 bit.
+            Note that coordinates (if xyz=True) is always 64 bit floats.
+        grid (Grid): The grid geometry object. This is required for the
+            xyz option.
+    Returns:
+        Pandas dataframe object
+    Examples::
+        >>> grd = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID", fformat='egrid')
+        >>> names = ['SOIL', 'SWAT', 'PRESSURE']
+        >>> dates = [19991201]
+        >>> gps = xtgeo.gridproperties_from_file(
+        ...     reek_dir + "/REEK.UNRST",
+        ...     fformat='unrst',
+        ...     names=names,
+        ...     dates=dates,
+        ...     grid=grd
+        ... )
+        >>> df = xtgeo.gridproperties_dataframe(gps, grid=grd)
+    """
+
+    proplist = list(gridproperties)
+
+    dataframe_dict = dict()
+    if ijk:
+        if activeonly:
+            if grid:
+                ix, jy, kz = _grid_etc1.get_ijk(grid)
+                dataframe_dict["IX"] = ix.get_active_npvalues1d()
+                dataframe_dict["JY"] = jy.get_active_npvalues1d()
+                dataframe_dict["KZ"] = kz.get_active_npvalues1d()
+            elif proplist:
+                ix, jy, kz = _grid_etc1.get_ijk(proplist[0])
+                dataframe_dict["IX"] = ix.get_active_npvalues1d()
+                dataframe_dict["JY"] = jy.get_active_npvalues1d()
+                dataframe_dict["KZ"] = kz.get_active_npvalues1d()
+        else:
+            if not grid:
+                raise ValueError(
+                    "You ask for active_only but no Grid is present. Use grid=..."
+                )
+            act = grid.get_actnum(dual=True)
+            ix, jy, kz = grid.get_ijk(asmasked=False)
+            dataframe_dict["ACTNUM"] = act.values1d
+            dataframe_dict["IX"] = ix.values1d
+            dataframe_dict["JY"] = jy.values1d
+            dataframe_dict["KZ"] = kz.values1d
+
+    if xyz:
+        if not grid:
+            raise ValueError("You ask for xyz but no Grid is present. Use grid=...")
+
+        xc, yc, zc = grid.get_xyz(asmasked=activeonly)
+        if activeonly:
+            dataframe_dict["X_UTME"] = xc.get_active_npvalues1d()
+            dataframe_dict["Y_UTMN"] = yc.get_active_npvalues1d()
+            dataframe_dict["Z_TVDSS"] = zc.get_active_npvalues1d()
+        else:
+            dataframe_dict["X_UTME"] = xc.values1d
+            dataframe_dict["Y_UTMN"] = yc.values1d
+            dataframe_dict["Z_TVDSS"] = zc.values1d
+
+    for prop in gridproperties:
+        if activeonly:
+            vector = prop.get_active_npvalues1d()
+        else:
+            vector = prop.values1d.copy()
+            # mask values not supported in Pandas:
+            if prop.isdiscrete:
+                vector = vector.filled(fill_value=0)
+            else:
+                vector = vector.filled(fill_value=np.nan)
+
+        if doubleformat:
+            vector = vector.astype(np.float64)
+        else:
+            vector = vector.astype(np.float32)
+
+        dataframe_dict[prop.name] = vector
+
+    return pd.DataFrame.from_dict(dataframe_dict)
+
+
+class GridProperties(_Grid3D):
+    """Class for a collection of 3D grid props, belonging to the same grid topology.
+
+     It is a thin wrapper on a list that 1) checks that the GridProperties
+     belong to the same Grid (loosely). 2) Contains operations that can be
+     called on lists of GridProperty objects for easy discoverability.
+
+     Examples::
+         >>> import xtgeo
+         >>> # Create an
+         >>> grid_properties = xtgeo.GridProperties(props=[])
+         >>> # Get the dataframe via the gridproperties object
+         >>> grid_properties.get_dataframe()
+         Empty DataFrame...
+         >>> # Convert the gridproperties to a list
+         >>> grid_properties_list = list(grid_properties)
+         >>> # Get the dataframe of the list:
+         >>> gridproperties_dataframe(grid_properties_list)
+         Empty DataFrame...
+
+    Args:
+        ncol: Deprecated argument.
+        nrow: Deprecated argument.
+        nlay: Deprecated argument.
+        props: The list of GridProperty objects.
+
+    See Also:
+        The :class:`GridProperty` class.
+    """
+
+    def __init__(
+        self,
+        ncol: Optional[int] = None,
+        nrow: Optional[int] = None,
+        nlay: Optional[int] = None,
+        props: List[GridProperty] = None,
+    ):
+        dims_given = False
+        if ncol is not None:
+            warnings.warn(
+                "Initializing GridProperties with ncol is deprecated.",
+                DeprecationWarning,
+            )
+            dims_given = True
+        else:
+            ncol = 4
+        if nrow is not None:
+            warnings.warn(
+                "Initializing GridProperties with nrow is deprecated.",
+                DeprecationWarning,
+            )
+            dims_given = True
+        else:
+            nrow = 3
+        if nlay is not None:
+            warnings.warn(
+                "Initializing GridProperties with nlay is deprecated.",
+                DeprecationWarning,
+            )
+            dims_given = True
+        else:
+            nlay = 5
+
+        if props:
+            if dims_given:
+                raise ValueError(
+                    "Giving both ncol/nrow/nlay and props list is not supported. "
+                    "Please give just props as ncol/nrow/nlay is deprecated."
+                )
+            ncol, nrow, nlay = props[0].dimensions
+
+        super().__init__(ncol, nrow, nlay)
+
+        # The _names field is just kept for backwards
+        # compatability until the names setter has been
+        # deprecated
+        self._names = []
+
+        self.props = props or []
+
+    def __repr__(self):  # noqa: D105
+        myrp = (
+            f"{self.__class__.__name__} (id={id(self)}) ncol={self._ncol!r}, "
+            f"nrow={self._nrow!r}, nlay={self._nlay!r}, filesrc={self.names!r}"
+        )
+        return myrp
+
+    def __str__(self):
+        """str: User friendly print."""
+        return self.describe(flush=False)
+
+    def __contains__(self, name):
+        """bool: Emulate 'if "PORO" in props'."""
+        prop = self.get_prop_by_name(name, raiseserror=False)
+        if prop:
+            return True
+
+        return False
+
+    def __getitem__(self, name):  # noqa: D105
+        prop = self.get_prop_by_name(name, raiseserror=False)
+        if prop is None:
+            raise KeyError(f"Key {name} does not exist")
+
+        return prop
+
+    def __iter__(self):  # noqa: D105
+        return iter(self._props)
+
+    @property
+    def names(self):
+        """Returns a list of property names.
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> props = GridProperties()
+            >>> props.from_file(
+            ...     reek_dir + "/REEK.INIT",
+            ...     fformat="init",
+            ...     names=["PERMX"],
+            ...     grid=grid,
+            ... )
+
+            >>> namelist = props.names
+            >>> for name in namelist:
+            ...     print ('Property name is {}'.format(name))
+            Property name is PERMX
+
+        """
+        return self._names
+
+    @names.setter
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="The behavior of the setting names would before create an alias name "
+        "the behavior of which was not always consistent. "
+        "Note that setting names on the GridProperties has "
+        "_no effect_ on the behavior of its methods except the names getter."
+        "This name aliasing is now going away. "
+        "In order to change the name of properties, "
+        "use\nfor p in gridprops:\n    p.name = newname",
+    )
+    def names(self, nameslist):
+        if len(nameslist) != len(self._props):
+            raise ValueError("Number of names does not match number of properties")
+
+        # look for duplicates
+        if len(nameslist) > len(set(nameslist)):
+            raise ValueError("List of names contains duplicates; names must be unique")
+
+        self._names = nameslist
+
+    @property
+    def props(self):
+        """Returns a list of XTGeo GridProperty objects, None if empty.
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> myprops = GridProperties()
+            >>> myprops.from_file(
+            ...     reek_dir + "/REEK.INIT",
+            ...     fformat="init",
+            ...     names=["PERMX"],
+            ...     grid=grid,
+            ... )
+
+            >>> proplist = myprops.props
+            >>> for prop in proplist:
+            ...     print ('Property object name is {}'.format(prop.name))
+            Property object name is PERMX
+
+            >>> # adding a property, e.g. get ACTNUM as a property from the grid
+            >>> actn = grid.get_actnum()  # this will get actn as a GridProperty
+            >>> myprops.append_props([actn])
+        """
+        if not self._props:
+            return None
+
+        return self._props
+
+    @props.setter
+    def props(self, propslist):
+        self._props = propslist
+        if propslist:
+            self._ncol = propslist[0].ncol
+            self._nrow = propslist[0].nrow
+            self._nlay = propslist[0].nlay
+        self._names = [p.name for p in self._props]
+        self._consistency_check()
+
+    @property
+    def dates(self):
+        """Returns a list of valid (found) dates after import.
+
+        Returns None if no dates present
+
+        Note:
+            See also :meth:`GridProperties.scan_dates` for scanning available dates
+            in advance
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> props = GridProperties()
+            >>> props.from_file(
+            ...     reek_dir + "/REEK.INIT",
+            ...     fformat="init",
+            ...     names=["PERMX"],
+            ...     grid=grid,
+            ... )
+
+            >>> datelist = props.dates
+            >>> for date in datelist:
+            ...     print ('Date applied is {}'.format(date))
+            Date applied is 19991201
+
+        .. versionchanged:: 2.16 dates is no longer an alias (undocumented behavior),
+            and simply return the dates of the underlying list of GridProperty.
+        """
+        if not self.props:
+            return None
+
+        return [p.date for p in self.props]
+
+    # Copy, and etc aka setters and getters
+
+    def copy(self):
+        """Copy a GridProperties instance to a new unique instance.
+
+        Note that the GridProperty instances will also be unique.
+        """
+
+        gps = GridProperties(props=[p.copy() for p in self.props] if self.props else [])
+        gps._names = self._names.copy()
+        return gps
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout."""
+        dsc = XTGDescription()
+
+        dsc.title("Description of GridProperties instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
+        dsc.txt("Attached grid props objects (names)", self.names)
+
+        if flush:
+            dsc.flush()
+            return None
+        return dsc.astext()
+
+    def generate_hash(self):
+        """str: Return a unique hash ID for current gridproperties instance.
+
+        .. versionadded:: 2.10
+        """
+        mhash = hashlib.sha256()
+
+        hashinput = ""
+        for prop in self._props:
+            gid = (
+                f"{prop.ncol}{prop.nrow}{prop.nlay}{prop.values.mean()}"
+                f"{prop.values.min()}{prop.values.max()}"
+            )
+            hashinput += gid
+
+        mhash.update(hashinput.encode())
+        return mhash.hexdigest()
+
+    def get_prop_by_name(self, name, raiseserror=True):
+        """Find and return a property object (GridProperty) by name.
+
+        Args:
+            name (str): Name of property to look for
+            raiseserror (bool): If True, raises a ValueError if not found, otherwise
+                return None
+
+        """
+        for prop in self._props:
+            logger.debug("Look for %s, actual is %s", name, prop.name)
+            if prop.name == name:
+                logger.debug(repr(prop))
+                return prop
+
+        if raiseserror:
+            raise ValueError(f"Cannot find property with name <{name}>")
+
+        return None
+
+    def append_props(self, proplist):
+        """Add a list of GridProperty objects to current GridProperties instance."""
+        if not self._props and proplist:
+            self._ncol = proplist[0].ncol
+            self._nrow = proplist[0].nrow
+            self._nlay = proplist[0].nlay
+        self._props += proplist
+        self._names = [p.name for p in self._props]
+        self._consistency_check()
+
+    def get_ijk(
+        self, names=("IX", "JY", "KZ"), zerobased=False, asmasked=False, mask=None
+    ):
+        """Returns 3 xtgeo.grid3d.GridProperty objects: I counter, J counter, K counter.
+
+        Args:
+            names: a 3 x tuple of names per property (default IX, JY, KZ).
+            asmasked: If True, then active cells only.
+            mask: If True, then active cells only (deprecated).
+            zerobased: If True, counter start from 0, otherwise 1 (default=1).
+        """
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = super()._evaluate_mask(mask)
+
+        return _grid_etc1.get_ijk(
+            self, names=names, zerobased=zerobased, asmasked=asmasked
+        )
+
+    def get_actnum(self, name="ACTNUM", asmasked=False, mask=None):
+        """Return an ACTNUM GridProperty object.
+
+        Args:
+            name (str): name of property in the XTGeo GridProperty object.
+            asmasked (bool): ACTNUM is returned with all cells
+                as default. Use asmasked=True to make 0 entries masked.
+            mask (bool): Deprecated, use asmasked instead.
+
+        Example::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> myprops = GridProperties()
+            >>> myprops.from_file(
+            ...     reek_dir + "/REEK.INIT",
+            ...     fformat="init",
+            ...     names=["PERMX"],
+            ...     grid=grid,
+            ... )
+            >>> act = myprops.get_actnum()
+            >>> print('{}% of cells are active'.format(act.values.mean() * 100))
+            99.99...% of cells are active
+
+        Returns:
+            A GridProperty instance of ACTNUM, or None if no props present.
+        """
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = super()._evaluate_mask(mask)
+
+        # borrow function from GridProperty class:
+        if self._props:
+            return self._props[0].get_actnum(name=name, asmasked=asmasked)
+
+        warnings.warn("No gridproperty in list", UserWarning)
+        return None
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.gridproperties_from_file() instead",
+    )
+    def from_file(
+        self,
+        pfile,
+        fformat="roff",
+        names=None,
+        dates=None,
+        grid=None,
+        namestyle=0,
+        strict=(True, False),
+    ):
+        """Import grid properties from file in one go.
+
+        This class is particulary useful for Eclipse INIT and RESTART files.
+
+        In case of names='all' then all vectors which have a valid length
+        (number of total or active cells in the grid) will be read
+
+        Args:
+            pfile (str or Path): Name of file with properties
+            fformat (str): roff/init/unrst
+            names: list of property names, e.g. ['PORO', 'PERMX'] or 'all'
+            dates: list of dates on YYYYMMDD format, for restart files, or 'all'
+            grid (obj): The grid geometry object (optional if ROFF)
+            namestyle (int): 0 (default) for style SWAT_20110223,
+                1 for SWAT--2011_02_23 (applies to restart only)
+            strict (tuple of (bool, bool)): If (True, False) (default) then an
+                Error is raised if keyword name is not found, or a key-date combination
+                is not found. However, the dates will processed so that non-valid dates
+                are skipped (still, invalid key-date combinations may occur!).
+                If (True, True) all keywords and dates are tried, while (False, False)
+                means that that only valid entries are imported, more or less silently.
+                Saturations keywords SWAT/SOIL/SGAS are not evaluated as they may be
+                derived.
+
+        Example::
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> props = GridProperties()
+            >>> props.from_file(
+            ...     reek_dir + "/REEK.INIT",
+            ...     fformat="init",
+            ...     names=["PERMX"],
+            ...     grid=grid,
+            ... )
+
+
+        Raises:
+            FileNotFoundError: if input file is not found
+            DateNotFoundError: The date is not found
+            KeywordNotFoundError: The keyword is not found
+            KeywordFoundDateNotFoundError: The keyword but not date found
+
+        .. versionadded:: 2.13 Added strict key
+        """
+
+        self.append_props(
+            list(
+                gridproperties_from_file(
+                    pfile=pfile,
+                    fformat=fformat,
+                    names=names,
+                    dates=dates,
+                    grid=grid,
+                    namestyle=namestyle,
+                    strict=strict,
+                )
+            )
+        )
+
+    def get_dataframe(
+        self, activeonly=False, ijk=False, xyz=False, doubleformat=False, grid=None
+    ):
+        """Returns a Pandas dataframe table for the properties.
+
+        See also :func:`xtgeo.gridproperties_dataframe()`
+
+        Args:
+            activeonly (bool): If True, return only active cells, NB!
+                If True, will require a grid instance (see grid key)
+            ijk (bool): If True, show cell indices, IX JY KZ columns
+            xyz (bool): If True, show cell center coordinates (needs grid).
+            doubleformat (bool): If True, floats are 64 bit, otherwise 32 bit.
+                Note that coordinates (if xyz=True) is always 64 bit floats.
+            grid (Grid): The grid geometry object. This is required for the
+                xyz option.
+
+        Returns:
+            Pandas dataframe object
+
+        Examples::
+
+            >>> import xtgeo
+            >>> grid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> pps.grid_properties_from_file(
+            ...     reek_dir + "/REEK.UNRST",
+            ...     fformat="unrst",
+            ...     names=['SOIL', 'SWAT', 'PRESSURE'],
+            ...     dates=[19991201],
+            ...     grid=grid,
+            ... )
+            >>> df = pps.get_dataframe(activeonly=False, ijk=True, xyz=True, grid=grid)
+            >>> print(df)
+                   ACTNUM  IX  JY  ...  SOIL_19991201  SWAT_19991201  PRESSURE_19991201
+            0           1   1   1  ...            0.0            1.0         341.694183
+            1           1   1   1  ...            0.0            1.0         342.097107
+            2           1   1   1  ...            0.0            1.0         342.500061
+            3           1   1   1  ...            0.0            1.0         342.902954
+            4           1   1   1  ...            0.0            1.0         343.305908
+            ...
+
+        """
+        return gridproperties_dataframe(
+            self,
+            activeonly=activeonly,
+            ijk=ijk,
+            xyz=xyz,
+            doubleformat=doubleformat,
+            grid=grid,
+        )
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use GridProperty.get_dataframe() instead",
+    )
+    def dataframe(self, *args, **kwargs):
+        return self.get_dataframe(*args, **kwargs)
+
+    def _consistency_check(self):
+        for p in self._props:
+            if (p.ncol, p.nrow, p.nlay) != (self.ncol, self.nrow, self.nlay):
+                raise ValueError("Mismatching dimensions in GridProperties members.")
+
+    @staticmethod
+    def scan_keywords(
+        pfile, fformat="xecl", maxkeys=MAXKEYWORDS, dataframe=False, dates=False
+    ):
+        """Quick scan of keywords in Eclipse binary files, or ROFF binary files.
+
+        For Eclipse files:
+        Returns a list of tuples (or dataframe), e.g. ('PRESSURE',
+        'REAL', 355299, 3582700), where (keyword, type, no_of_values,
+        byteposition_in_file)
+
+        For ROFF files
+        Returns a list of tuples (or dataframe), e.g.
+        ('translate!xoffset', 'float', 1, 3582700),
+        where (keyword, type, no_of_values, byteposition_in_file).
+
+        For Eclipse, the byteposition is to the KEYWORD, while for ROFF
+        the byte position is to the beginning of the actual data.
+
+        Args:
+            pfile (str): Name or a filehandle to file with properties
+            fformat (str): xecl (Eclipse INIT, RESTART, ...) or roff for
+                ROFF binary,
+            maxkeys (int): Maximum number of keys
+            dataframe (bool): If True, return a Pandas dataframe instead
+            dates (bool): if True, the date is the last column (only
+                menaingful for restart files). Default is False.
+
+        Return:
+            A list of tuples or dataframe with keyword info
+
+        Example::
+            >>> dlist = GridProperties.scan_keywords(reek_dir + "/REEK.UNRST")
+
+        """
+        pfile = xtgeo._XTGeoFile(pfile)
+        pfile.check_file(raiseerror=ValueError)
+
+        return utils.scan_keywords(
+            pfile,
+            fformat=fformat,
+            maxkeys=maxkeys,
+            dataframe=dataframe,
+            dates=dates,
+        )
+
+    @staticmethod
+    def scan_dates(
+        pfile, fformat="unrst", maxdates=MAXDATES, dataframe=False, datesonly=False
+    ):
+        """Quick scan dates in a simulation restart file.
+
+        Args:
+            pfile (str): Name of file or file handle with properties
+            fformat (str): unrst (so far)
+            maxdates (int): Maximum number of dates to collect
+            dataframe (bool): If True, return a Pandas dataframe instead
+            datesonly (bool): If True, SEQNUM is skipped,
+
+        Return:
+            A list of tuples or a dataframe with (seqno, date),
+            date is on YYYYMMDD form. If datesonly is True and dataframe is False,
+            the returning list will be a simple list of dates.
+
+        Example::
+            >>> dlist = GridProperties.scan_dates(reek_dir + "/REEK.UNRST")
+            >>> #or getting all dates a simple list:
+            >>> dlist = GridProperties.scan_dates(
+            ... reek_dir + "/REEK.UNRST",
+            ... datesonly=True)
+
+        .. versionchanged:: 2.13 Added datesonly keyword
+        """
+        logger.info("Format supported as default is %s", fformat)
+
+        pfile = xtgeo._XTGeoFile(pfile)
+        pfile.check_file(raiseerror=ValueError)
+
+        dlist = utils.scan_dates(pfile, maxdates=maxdates, dataframe=dataframe)
+
+        if datesonly and dataframe:
+            dlist.drop("SEQNUM", axis=1, inplace=True)
+
+        if datesonly and not dataframe:
+            dlist = [date for (_, date) in dlist]
+
+        return dlist
```

## xtgeo/grid3d/grid_property.py

 * *Ordering differences only*

```diff
@@ -1,1425 +1,1425 @@
-# -*- coding: utf-8 -*-
-"""Module for a 3D grid property."""
-
-
-import copy
-import functools
-import hashlib
-import io
-import numbers
-import pathlib
-import warnings
-from types import FunctionType
-from typing import Any, Optional, Union
-
-import numpy as np
-
-import xtgeo
-
-from . import (
-    _gridprop_export,
-    _gridprop_lowlevel,
-    _gridprop_op1,
-    _gridprop_roxapi,
-    _gridprop_value_init,
-)
-from ._grid3d import _Grid3D
-from ._gridprop_import_eclrun import (
-    import_gridprop_from_init,
-    import_gridprop_from_restart,
-)
-from ._gridprop_import_grdecl import import_bgrdecl_prop, import_grdecl_prop
-from ._gridprop_import_roff import import_roff
-from ._gridprop_import_xtgcpprop import import_xtgcpprop
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-# --------------------------------------------------------------------------------------
-# Comment on 'asmasked' vs 'activeonly:
-#
-# 'asmasked'=True will return a np.ma array, while 'asmasked' = False will
-# return a np.ndarray
-#
-# The 'activeonly' will filter out masked entries, or use None or np.nan
-# if 'activeonly' is False.
-#
-# Use word 'zerobased' for a bool regrading startcell basis is 1 or 0
-#
-# For functions with mask=... ,they should be replaced with asmasked=...
-# --------------------------------------------------------------------------------------
-
-# pylint: disable=logging-format-interpolation, too-many-public-methods
-
-# ======================================================================================
-# Functions outside the class, for rapid access. Will be exposed as
-# xxx = xtgeo.gridproperty_from_file. pylint: disable=fixme
-# ======================================================================================
-
-
-def _data_reader_factory(fformat):
-    if fformat in ["roff_binary", "roff_ascii"]:
-        return import_roff
-    elif fformat in ["finit", "init"]:
-        return import_gridprop_from_init
-
-    elif fformat in ["funrst", "unrst"]:
-        return functools.partial(import_gridprop_from_restart, fformat=fformat)
-    elif fformat == "grdecl":
-        return import_grdecl_prop
-
-    elif fformat == "bgrdecl":
-        return import_bgrdecl_prop
-
-    elif fformat in ["xtg"]:
-        return import_xtgcpprop
-    else:
-        raise ValueError(f"Invalid grid property file format {fformat}")
-
-
-def gridproperty_from_file(*args, **kwargs):
-    """Make a GridProperty instance directly from file import.
-
-    For arguments, see :func:`GridProperty.from_file()`
-
-    Args:
-        pfile (str): Property file
-        args: See :func:`GridProperty.from_file()`
-        kwargs: See :func:`GridProperty.from_file()`.
-
-    Example::
-
-        >>> import xtgeo
-        >>> myporo = xtgeo.gridproperty_from_file(
-        ...    reek_dir + '/reek_sim_poro.roff',
-        ...    name="PORO"
-        ... )
-    """
-    return GridProperty._read_file(*args, **kwargs)
-
-
-def gridproperty_from_roxar(
-    project, gname, pname, realisation=0, faciescodes=False
-):  # pragma: no cover
-    """Make a GridProperty instance directly inside RMS.
-
-    For arguments, see :func:`GridProperty.from_roxar()`
-
-    Example::
-
-        import xtgeo
-        myporo = xtgeo.gridproperty_from_roxar(project, 'Geogrid', 'Poro')
-
-    """
-    return GridProperty._read_roxar(
-        project, gname, pname, realisation=realisation, faciescodes=faciescodes
-    )
-
-
-def allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of GridProperty and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(self, *args, **kwargs):
-        # Check if dummy values are to be used
-        if (
-            all(param not in kwargs for param in ["values", "ncol", "nrow", "nlay"])
-            and len(args) == 0
-        ):
-            warnings.warn(
-                "Default initialization of GridProperty without values and dimension "
-                "is deprecated and will be removed in xtgeo version 4.0",
-                DeprecationWarning,
-            )
-            return func(
-                self,
-                *args,
-                ncol=4,
-                nrow=3,
-                nlay=5,
-                values=_gridprop_value_init.gridproperty_dummy_values(
-                    kwargs.get("discrete", False)
-                ),
-                **kwargs,
-            )
-
-        # Checking if we are doing an initialization
-        # from file and raise a deprecation warning if
-        # we are.
-        if "pfile" in kwargs or (
-            len(args) >= 1
-            and isinstance(args[0], (str, pathlib.Path, xtgeo._XTGeoFile))
-        ):
-            pfile = kwargs.get("pfile", args[0])
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "myprop = xtgeo.gridproperty_from_file('some_name.roff') instead",
-                DeprecationWarning,
-            )
-            fformat = kwargs.get("fformat", None)
-            mfile = xtgeo._XTGeoFile(pfile)
-            if fformat is None or fformat == "guess":
-                fformat = mfile.detect_fformat()
-            else:
-                fformat = mfile.generic_format_by_proposal(fformat)  # default
-
-            if "pfile" in kwargs:
-                del kwargs["pfile"]
-            if "fformat" in kwargs:
-                del kwargs["fformat"]
-            if len(args) >= 1 and isinstance(
-                args[0], (str, pathlib.Path, xtgeo._XTGeoFile)
-            ):
-                args = args[min(len(args), 2) :]
-
-            kwargs = _data_reader_factory(fformat)(mfile, *args, **kwargs)
-            kwargs["filesrc"] = mfile.file
-            return func(self, **kwargs)
-        return func(self, *args, **kwargs)
-
-    return wrapper
-
-
-class GridProperty(_Grid3D):
-    """Class for a single 3D grid property, e.g porosity or facies.
-
-    An GridProperty instance may or may not 'belong' to a grid (geometry) object.
-    E.g. for ROFF input, ncol, nrow, nlay are given in the import file and the grid
-    geometry file is not needed. For many Eclipse files, the grid geometry is needed
-    as this holds the active number indices (ACTNUM).
-
-    Normally the instance is created when importing a grid
-    property from file, but it can also be created directly, as e.g.::
-
-        poro = GridProperty(ncol=233, nrow=122, nlay=32)
-
-    The grid property values ``someinstance.values`` by themselves is a 3D masked
-    numpy usually as either float64 (double) or int32 (if discrete), and undefined
-    cells are displayed as masked. The internal array order is now C_CONTIGUOUS.
-    (i.e. not in Eclipse manner). A 1D view (C order) is achieved by the
-    values1d property, e.g.::
-
-       poronumpy = poro.values1d
-
-    .. versionchanged:: 2.6 Possible to make GridProperty instance directly from Grid()
-    .. versionchanged:: 2.8 Possible to base it on existing GridProperty() instance
-
-    """
-
-    @allow_deprecated_init
-    def __init__(
-        self,
-        gridlike=None,
-        ncol: Optional[int] = None,
-        nrow: Optional[int] = None,
-        nlay: Optional[int] = None,
-        name: Optional[str] = "unknown",
-        discrete: Optional[bool] = False,
-        date: Optional[str] = None,
-        grid: Optional[Any] = None,
-        linkgeometry: Optional[bool] = True,
-        fracture: Optional[bool] = False,
-        codes: Optional[dict] = None,
-        dualporo: Optional[bool] = False,
-        dualperm: Optional[bool] = False,
-        roxar_dtype: Optional[Any] = None,
-        values: Optional[Union[np.ndarray, float, int]] = None,
-        roxorigin: bool = False,
-        filesrc: Optional[str] = None,
-    ):
-        """Instantating.
-
-            Args:
-                pfile: Input file-like or a Grid/GridProperty instance or leave blank.
-                fformat: File format input, default is ``guess``.
-                ncol: Number of columns (nx) defaults to 4.
-                nrow: Number of rows (ny) defaults to 3.
-                ncol: Number of layers (nz) defaults to 5.
-                name: Name of property.
-                discrete: True or False.
-                date: Date on YYYYMMDD form.
-                grid: Attached grid object
-                linkgeometry: If True, establish a link between GridProperty and Grid
-                fracture: True if fracture option (relevant for flow simulator data)
-                codes: Codes in case a discrete property e.g. {1: "Sand", 4: "Shale"}
-                dualporo: True if dual porosity system.
-                dualperm: True if dual porosity and dual permeability system.
-                roxar_dtype: Spesify roxar datatype e.g. np.uint8
-                values: Values to apply (will not be applied if a file-like is input)
-
-        Returns:
-            A GridProperty object instance.
-
-        Raises:
-            RuntimeError: if something goes wrong (e.g. file not found)
-
-        Examples::
-
-            from xtgeo.grid3d import GridProperty
-            myprop = GridProperty()
-            myprop.from_file('emerald.roff', name='PORO')
-
-            # or
-
-            values = np.ma.ones((12, 17, 10), dtype=np.float64),
-            myprop = GridProperty(ncol=12, nrow=17, nlay=10,
-                                  values=values, discrete=False,
-                                  name='MyValue')
-
-            # or
-
-            myprop = GridProperty('emerald.roff', name='PORO')
-
-            # or create properties from a Grid() instance
-
-            mygrid = Grid("grid.roff")
-            myprop1 = GridProperty(mygrid, name='PORO')
-            myprop2 = GridProperty(mygrid, name='FACIES', discrete=True, values=1,
-                                   linkgeometry=True)  # alternative 1
-            myprop2.geometry = mygrid  # alternative 2 to link grid geometry to property
-
-            # from Grid instance:
-            grd = Grid("somefile_grid_file")
-            myprop = GridProperty(grd, values=99, discrete=True)  # based on grd
-
-            # or from existing GridProperty instance:
-            myprop2 = GridProperty(myprop, values=99, discrete=False)  # based on myprop
-
-        """
-
-        super().__init__(ncol, nrow, nlay)
-
-        self._reset(
-            gridlike,
-            ncol,
-            nrow,
-            nlay,
-            name,
-            discrete,
-            date,
-            grid,
-            linkgeometry,
-            fracture,
-            codes,
-            dualporo,
-            dualperm,
-            roxar_dtype,
-            values,
-            roxorigin,
-            filesrc,
-        )
-
-    def _reset(
-        self,
-        gridlike=None,
-        ncol: Optional[int] = None,
-        nrow: Optional[int] = None,
-        nlay: Optional[int] = None,
-        name: Optional[str] = "unknown",
-        discrete: Optional[bool] = False,
-        date: Optional[str] = None,
-        grid: Optional[Any] = None,
-        linkgeometry: Optional[bool] = True,
-        fracture: Optional[bool] = False,
-        codes: Optional[dict] = None,
-        dualporo: Optional[bool] = False,
-        dualperm: Optional[bool] = False,
-        roxar_dtype: Optional[Any] = None,
-        values: Optional[Union[np.ndarray, float, int]] = None,
-        roxorigin: bool = False,
-        filesrc: Optional[str] = None,
-    ):
-        """Instantating.
-
-            Args:
-                pfile: Input file-like or a Grid/GridProperty instance or leave blank.
-                fformat: File format input, default is ``guess``.
-                ncol: Number of columns (nx) defaults to 4.
-                nrow: Number of rows (ny) defaults to 3.
-                ncol: Number of layers (nz) defaults to 5.
-                name: Name of property.
-                discrete: True or False.
-                date: Date on YYYYMMDD form.
-                grid: Attached grid object
-                linkgeometry: If True, establish a link between GridProperty and Grid
-                fracture: True if fracture option (relevant for flow simulator data)
-                codes: Codes in case a discrete property e.g. {1: "Sand", 4: "Shale"}
-                dualporo: True if dual porosity system.
-                dualperm: True if dual porosity and dual permeability system.
-                roxar_dtype: Spesify roxar datatype e.g. np.uint8
-                values: Values to apply (will not be applied if a file-like is input)
-
-        Returns:
-            A GridProperty object instance.
-
-        Raises:
-            RuntimeError: if something goes wrong (e.g. file not found)
-
-        Examples::
-
-            from xtgeo.grid3d import GridProperty
-            myprop = GridProperty()
-            myprop.from_file('emerald.roff', name='PORO')
-
-            # or
-
-            values = np.ma.ones((12, 17, 10), dtype=np.float64),
-            myprop = GridProperty(ncol=12, nrow=17, nlay=10,
-                                  values=values, discrete=False,
-                                  name='MyValue')
-
-            # or
-
-            myprop = GridProperty('emerald.roff', name='PORO')
-
-            # or create properties from a Grid() instance
-
-            mygrid = Grid("grid.roff")
-            myprop1 = GridProperty(mygrid, name='PORO')
-            myprop2 = GridProperty(mygrid, name='FACIES', discrete=True, values=1,
-                                   linkgeometry=True)  # alternative 1
-            myprop2.geometry = mygrid  # alternative 2 to link grid geometry to property
-
-            # from Grid instance:
-            grd = Grid("somefile_grid_file")
-            myprop = GridProperty(grd, values=99, discrete=True)  # based on grd
-
-            # or from existing GridProperty instance:
-            myprop2 = GridProperty(myprop, values=99, discrete=False)  # based on myprop
-
-        """
-
-        super().__init__()
-
-        self._ncol = ncol
-        self._nrow = nrow
-        self._nlay = nlay
-
-        # instance attributes defaults:
-        self._name = name
-        self._date = date
-        self._isdiscrete = discrete
-        self._geometry = grid
-        self._fracture = fracture
-        self._codes = dict() if codes is None else codes
-
-        # not primary input:
-        self._dualporo = dualporo
-        self._dualperm = dualperm
-
-        self._filesrc = filesrc
-        self._roxorigin = roxorigin  # true if the object comes from the ROXAPI
-        if roxar_dtype is None:
-            if discrete:
-                self._roxar_dtype = np.uint8
-            else:
-                self._roxar_dtype = np.float32
-        else:
-            self.roxar_dtype = roxar_dtype
-        self._values = values
-
-        self._undef = xtgeo.UNDEF_INT if discrete else xtgeo.UNDEF
-
-        self._set_initial_dimensions(gridlike, (ncol, nrow, nlay))
-
-        self._values = _gridprop_value_init.gridproperty_non_dummy_values(
-            gridlike, self.dimensions, values, discrete
-        )
-
-        if isinstance(gridlike, xtgeo.grid3d.Grid):
-            if linkgeometry:
-                # assosiate this grid property with grid instance. This is not default
-                # since sunch links may affect garbish collection
-                self.geometry = gridlike
-            gridlike.append_prop(self)
-
-        self._metadata = xtgeo.MetaDataCPProperty()
-
-    def _set_initial_dimensions(self, gridlike, input_dimensions):
-        """Sets the initial dimensions either from input, grid or default.
-
-        If a gridlike is given, we use its dimensions, but make sure it matches
-        the input dimensions if given (not None). Otherwise, dimensions are either
-        set to the input dimensions or defaulted.
-
-        """
-        if gridlike is not None:
-            self._ncol = gridlike.ncol
-            self._nrow = gridlike.nrow
-            self._nlay = gridlike.nlay
-            self._check_dimensions_match(*input_dimensions)
-        else:
-            ncol, nrow, nlay = input_dimensions
-            if ncol is None:
-                self._ncol = 4
-            else:
-                self._ncol = ncol
-            if nrow is None:
-                self._nrow = 3
-            else:
-                self._nrow = nrow
-            if nlay is None:
-                self._nlay = 5
-            else:
-                self._nlay = nlay
-
-    def _check_dimensions_match(self, ncol, nrow, nlay):
-        """
-        Raises:
-            ValueError: if given dimensions are not None and do not
-                match dimensions of the GridProperty
-        """
-        if ncol is not None and self._ncol != ncol:
-            raise ValueError(
-                f"mismatching column dimension given: {ncol} vs {self._ncol}"
-            )
-        if nrow is not None and self._nrow != nrow:
-            raise ValueError(f"mismatching row dimension given: {nrow} vs {self._nrow}")
-        if nlay is not None and self._nlay != nlay:
-            raise ValueError(
-                f"mismatching layer dimension given: {nlay} vs {self._nlay}"
-            )
-
-    def __del__(self):
-        logger.debug("DELETING property instance %s", self.name)
-
-    def __repr__(self):
-        myrp = (
-            f"{self.__class__.__name__} (id={id(self)}) ncol={self._ncol!r}, "
-            f"nrow={self._nrow!r}, nlay={self._nlay!r}, filesrc={self._filesrc!r}"
-        )
-        return myrp
-
-    def __str__(self):
-        # user friendly print
-        return self.describe(flush=False)
-
-    # ==================================================================================
-    # Properties
-    # Some proprerties such as ncol, nrow, nlay are from the Super class
-    # ==================================================================================
-
-    @property
-    def metadata(self):
-        """Return metadata object instance of type MetaDataRegularSurface."""
-        return self._metadata
-
-    @metadata.setter
-    def metadata(self, obj):
-        # The current metadata object can be replaced. A bit dangerous so further
-        # check must be done to validate. TODO.
-        if not isinstance(obj, xtgeo.MetaDataCPProperty):
-            raise ValueError("Input obj not an instance of MetaDataCPProperty")
-
-        self._metadata = obj  # checking is currently missing! TODO
-
-    @property
-    def name(self):
-        """Returns or rename the property name."""
-        return self._name
-
-    @name.setter
-    def name(self, name):
-        self._name = name
-
-    @property
-    def dimensions(self):
-        """3-tuple: The grid dimensions as a tuple of 3 integers (read only)"""
-        return (self._ncol, self._nrow, self._nlay)
-
-    @property
-    def nactive(self):
-        """int: Returns the number of active cells (read only)."""
-        return len(self.actnum_indices)
-
-    @property
-    def geometry(self):
-        """Returns or set the linked geometry, i.e. the Grid instance)"""
-        return self._geometry
-
-    @geometry.setter
-    def geometry(self, geom):
-        if geom is None:
-            self._geometry = None
-        elif isinstance(geom, xtgeo.grid3d.Grid) and geom.dimensions == self.dimensions:
-            self._geometry = geom
-        else:
-            raise ValueError("Could not set geometry; wrong type or size")
-
-    @property
-    def actnum_indices(self):
-        """Returns the 1D ndarray which holds the indices for active cells
-        given in 1D, C order (read only).
-
-        """
-        actnumv = self.get_actnum()
-        actnumv = np.ravel(actnumv.values)
-        return np.flatnonzero(actnumv)
-
-    @property
-    def isdiscrete(self):
-        """Return True if property is discrete.
-
-        This can also be used to convert from continuous to discrete
-        or from discrete to continuous::
-
-            myprop.isdiscrete = False
-        """
-
-        return self._isdiscrete
-
-    @isdiscrete.setter
-    def isdiscrete(self, flag):
-        if not isinstance(flag, bool):
-            raise ValueError("Input to {__name__} must be a bool")
-
-        if flag is self._isdiscrete:
-            pass
-        else:
-            if flag is True and self._isdiscrete is False:
-                self.continuous_to_discrete()
-            else:
-                self.discrete_to_continuous()
-
-    @property
-    def dtype(self):
-        """Return or set the values numpy dtype.
-
-        When setting, note that the the dtype must correspond to the
-        `isdiscrete` property. Hence dtype cannot alter isdiscrete status
-
-        Example::
-
-            if myprop.isdiscrete:
-                myprop.dtype = np.uint16
-
-
-        """
-        return self._values.dtype
-
-    @dtype.setter
-    def dtype(self, dtype):
-        allowedfloat = [np.float16, np.float32, np.float64]
-        allowedint = [np.uint8, np.uint16, np.int16, np.int32, np.int64]
-
-        okv = True
-        msg = ""
-        if self.isdiscrete:
-            if dtype in allowedint:
-                self.values = self.values.astype(dtype)
-            else:
-                okv = False
-                msg = f"{__name__}: Wrong input for dtype. Use one of {allowedint}!"
-        else:
-            if dtype in allowedfloat:
-                self.values = self.values.astype(dtype)
-            else:
-                okv = False
-                msg = f"{__name__}: Wrong input for dtype. Use one of {allowedfloat}!"
-
-        if not okv:
-            raise ValueError(msg)
-
-    @property
-    def filesrc(self):
-        """Return or set file src (if any)"""
-        return self._filesrc
-
-    @filesrc.setter
-    def filesrc(self, src):
-        self._filesrc = src
-
-    @property
-    def roxar_dtype(self):
-        """Return or set the roxar dtype (if any)"""
-        return self._roxar_dtype
-
-    @roxar_dtype.setter
-    def roxar_dtype(self, dtype):
-        allowed = [np.uint16, np.uint8, np.float32]
-        if dtype in allowed:
-            self._roxar_dtype = dtype
-        else:
-            raise ValueError(
-                f"{__name__}: Wrong input for roxar_dtype. Use one of {allowed}!"
-            )
-
-    @property
-    def date(self) -> Optional[str]:
-        """Returns or rename the property date as string on YYYYMMDD format."""
-        return self._date
-
-    @date.setter
-    def date(self, date: Optional[str]):
-        self._date = date
-
-    @property
-    def codes(self) -> dict:
-        """The property codes as a dictionary."""
-        return self._codes
-
-    @codes.setter
-    def codes(self, cdict):
-        if isinstance(cdict, dict):
-            self._codes = copy.deepcopy(cdict)
-        else:
-            raise ValueError(
-                "The codes must be a python dictionary, current input "
-                f"is type: {type(cdict)}"
-            )
-
-    @property
-    def ncodes(self) -> int:
-        """Number of codes if discrete grid property (read only)."""
-        return len(self._codes)
-
-    @property
-    def values(self) -> np.ma.MaskedArray:
-        """Return or set the grid property as a masked 3D numpy array"""
-        return self._values
-
-    @values.setter
-    def values(self, values):
-        values = self.ensure_correct_values(self.ncol, self.nrow, self.nlay, values)
-
-        self._values = values
-
-    @property
-    def ntotal(self) -> int:
-        """Returns total number of cells ncol*nrow*nlay (read only)"""
-        return self._ncol * self._nrow * self._nlay
-
-    @property
-    def roxorigin(self):
-        """Returns True if the property comes from ROXAPI"""
-        return self._roxorigin
-
-    @roxorigin.setter
-    def roxorigin(self, val):
-        if isinstance(val, bool):
-            self._roxorigin = val
-        else:
-            raise ValueError("Input to roxorigin must be True or False")
-
-    @property
-    def values3d(self):
-        """For backward compatibility (use values instead)"""
-        return self._values
-
-    @values3d.setter
-    def values3d(self, values):
-        # kept for backwards compatibility
-        self.values = values
-
-    @property
-    def values1d(self):
-        """Returns a 1D view of values (masked numpy) (read only)."""
-        return self._values.reshape(-1)
-
-    @property
-    def undef(self):
-        """Get the actual undef value for floats or ints
-        numpy arrays (read only).
-        """
-        if self._isdiscrete:
-            return xtgeo.UNDEF_INT
-
-        return xtgeo.UNDEF
-
-    @property
-    def undef_limit(self):
-        """Returns the undef limit number, which is slightly less than the
-        undef value.
-
-        Hence for numerical precision, one can force undef values
-        to a given number, e.g.::
-
-           x[x<x.undef_limit]=999
-
-        Undef limit values cannot be changed (read only).
-
-        """
-        if self._isdiscrete:
-            return xtgeo.UNDEF_INT_LIMIT
-
-        return xtgeo.UNDEF_LIMIT
-
-    # ==================================================================================
-    # Class and special methods
-    # ==================================================================================
-
-    def generate_hash(self):
-        """str: Return a unique hash ID for current grid; can e.g. be used to compare
-        two gridproperty instances with same source.
-
-        .. versionadded:: 2.10
-        """
-
-        mhash = hashlib.sha256()
-        gid = f"{self._filesrc}{self._ncol}{self._nrow}{self._nlay}"
-        f"{self._values.mean()}{self._values.min()}{self._values.max()}"
-
-        mhash.update(gid.encode())
-        return mhash.hexdigest()
-
-    @classmethod
-    def methods(cls):
-        """Returns the names of the methods in the class.
-
-        >>> print(GridProperty.methods())
-        METHODS for GridProperty():
-        ======================
-        __init__
-        _reset
-        _set_initial_dimensions
-        _check_dimensions_match
-        ...
-        """
-        mets = [x for x, y in cls.__dict__.items() if isinstance(y, FunctionType)]
-
-        txt = "METHODS for GridProperty():\n======================\n"
-        for met in mets:
-            txt += str(met) + "\n"
-
-        return txt
-
-    def ensure_correct_values(self, ncol, nrow, nlay, invalues):
-        """Ensures that values is a 3D masked numpy (ncol, nrol, nlay).
-
-        Args:
-            ncol (int): Number of columns.
-            nrow (int): Number of rows.
-            nlay (int): Number of layers.
-            invalues (array or scalar): Values to process.
-
-        Return:
-            values (MaskedArray): Numpy masked array on correct format.
-
-        """
-
-        currentmask = None
-        if self._values is not None:
-            if isinstance(self._values, np.ma.MaskedArray):
-                currentmask = np.ma.getmaskarray(self._values)
-
-        if isinstance(invalues, numbers.Number):
-            vals = np.ma.zeros((ncol, nrow, nlay), order="C", dtype=self.dtype)
-            vals = np.ma.array(vals, mask=currentmask)
-            values = vals + invalues
-            invalues = values
-
-        if not isinstance(invalues, np.ma.MaskedArray):
-            values = np.ma.array(invalues, mask=currentmask, order="C")
-        else:
-            values = invalues  # new mask is possible
-
-        if values.shape != (ncol, nrow, nlay):
-            try:
-                values = np.ma.reshape(values, (ncol, nrow, nlay), order="C")
-            except ValueError as emsg:
-                xtg.error(f"Cannot reshape array: {emsg}")
-                raise
-
-        # replace any undef or nan with mask
-        values = np.ma.masked_greater(values, self.undef_limit)
-        values = np.ma.masked_invalid(values)
-
-        if not values.flags.c_contiguous:
-            mask = np.ma.getmaskarray(values)
-            mask = np.asanyarray(mask, order="C")
-            values = np.asanyarray(values, order="C")
-            values = np.ma.array(values, mask=mask, order="C")
-
-        # the self._isdiscrete property shall win over numpy dtype
-        if "int" in str(values.dtype) and not self._isdiscrete:
-            values = values.astype(np.float64)
-
-        if "float" in str(values.dtype) and self._isdiscrete:
-            values = values.astype(np.int32)
-
-        return values
-
-    # ==================================================================================
-    # Import and export
-    # ==================================================================================
-
-    def from_file(self, pfile, fformat=None, **kwargs):  # _roffapiv for devel.
-        """
-        Import grid property from file, and makes an instance of this class.
-
-        Note that the the property may be linked to its geometrical grid,
-        through the ``grid=`` option. Sometimes this is required, for instance
-        for most Eclipse input.
-
-        Args:
-            pfile (str): name of file to be imported
-            fformat (str): file format to be used roff/init/unrst/grdecl
-                (None is default, which means "guess" from file extension).
-            name (str): name of property to import
-            date (int or str): For restart files, date on YYYYMMDD format. Also
-                the YYYY-MM-DD form is allowed (string), and for Eclipse,
-                mnemonics like 'first', 'last' is also allowed.
-            grid (Grid object): Grid Object for checks (optional for ROFF,
-                required for Eclipse).
-            gridlink (bool): If True, and grid is not None, a link from the grid
-                instance to the property is made. If False, no such link is made.
-                Avoiding gridlink is recommended when running statistics of multiple
-                realisations of a property.
-            fracture (bool): Only applicable for DUAL POROSITY systems, if True
-                then the fracture property is read; if False then the matrix
-                property is read. Names will be appended with "M" or "F"
-            ijrange (list-like): A list of 4 number: (i1, i2, j1, j2) for subrange
-                of cells to read. Only applicable for xtgcpprop format.
-            zerobased (bool): Input if cells counts are zero- or one-based in
-                ijrange. Only applicable for xtgcpprop format.
-
-        Examples::
-
-           x = GridProperty()
-           x.from_file('somefile.roff', fformat='roff')
-           #
-           mygrid = Grid('ECL.EGRID')
-           pressure_1 = GridProperty()
-           pressure_1.from_file('ECL.UNRST', name='PRESSURE', date='first',
-                                grid=mygrid)
-
-        Returns:
-           True if success, otherwise False
-
-        .. versionchanged:: 2.8 Added gridlink option, default is True
-        """
-        pfile = xtgeo._XTGeoFile(pfile)
-        if fformat is None or fformat == "guess":
-            fformat = pfile.detect_fformat()
-        else:
-            fformat = pfile.generic_format_by_proposal(fformat)  # default
-        kwargs = _data_reader_factory(fformat)(pfile, **kwargs)
-        kwargs["filesrc"] = pfile.file
-        self._reset(**kwargs)
-        return self
-
-    @classmethod
-    def _read_file(
-        cls,
-        pfile: Union[str, pathlib.Path, io.BytesIO, io.StringIO],
-        fformat: Optional[str] = None,
-        **kwargs,
-    ):
-        pfile = xtgeo._XTGeoFile(pfile)
-        if fformat is None or fformat == "guess":
-            fformat = pfile.detect_fformat()
-        else:
-            fformat = pfile.generic_format_by_proposal(fformat)  # default
-        kwargs = _data_reader_factory(fformat)(pfile, **kwargs)
-        kwargs["filesrc"] = pfile.file
-        return cls(**kwargs)
-
-    def to_file(
-        self, pfile, fformat="roff", name=None, append=False, dtype=None, fmt=None
-    ):
-        """Export the grid property to file.
-
-        Args:
-            pfile (str or Path): File name or pathlib.Path to export to
-            fformat (str): The file format to be used. Default is
-                roff binary , else roff_ascii/grdecl/bgrdecl
-            name (str): If provided, will explicitly give property name;
-                else the existing name of the instance will used.
-            append (bool): Append to existing file, only for (b)grdecl formats.
-            dtype (str): Data type; this is valid only for grdecl or bgrdecl
-                formats, where default is None which means 'float32' for
-                floating point number and 'int32' for discrete properties.
-                Other choices are 'float64' which are 'DOUB' entries in
-                Eclipse formats.
-            fmt (str): Format for ascii grdecl format, default is None. If spesified,
-                the user is responsible for a valid format specifier, e.g. "%8.4f"
-
-        Example::
-
-            # This example demonstrates that file formats can be mixed
-            rgrid = Grid('reek.roff')
-            poro = GridProperty('reek_poro.grdecl', grid=rgrid, name='PORO')
-
-            poro.values += 0.05
-
-            poro.to_file('reek_export_poro.bgrdecl', format='bgrdecl')
-
-        .. versionadded:: 2.13  Key `fmt` was added and default format for float output
-            to grdecl is now "%e" if `fmt=None`
-
-        """
-
-        _gridprop_export.to_file(
-            self,
-            pfile,
-            fformat=fformat,
-            name=name,
-            append=append,
-            dtype=dtype,
-            fmt=fmt,
-        )
-
-    def from_roxar(
-        self, projectname, gname, pname, realisation=0, faciescodes=False
-    ):  # pragma: no cover
-        """Import grid model property from RMS project, and makes an instance.
-
-        Arguments:
-            projectname (str): Name of RMS project; use pure 'project'
-                if inside RMS
-            gfile (str): Name of grid model
-            pfile (str): Name of grid property
-            realisation (int): Realisation number (default 0; first)
-            faciescodes (bool): If a Roxar property is of the special ``body_facies``
-                type (e.g. result from a channel facies object modelling), the
-                default is to get the body code values. If faciescodes is True,
-                the facies code values will be read instead. For other roxar properties
-                this key is not relevant.
-
-        .. versionadded:: 2.12  Key `faciescodes` was added
-
-        """
-
-        self._reset(
-            **_gridprop_roxapi.import_prop_roxapi(
-                projectname, gname, pname, realisation, faciescodes
-            )
-        )
-
-    @classmethod
-    def _read_roxar(
-        cls, projectname, gname, pname, realisation=0, faciescodes=False
-    ):  # pragma: no cover
-        return cls(
-            **_gridprop_roxapi.import_prop_roxapi(
-                projectname, gname, pname, realisation, faciescodes
-            )
-        )
-
-    def to_roxar(
-        self, project, gname, pname, realisation=0, casting="unsafe"
-    ):  # pragma: no cover
-        """Store a grid model property into a RMS project.
-
-        Note:
-            When project is file path (direct access, outside RMS) then
-            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-            will not be saved until the user do an explicit project save action.
-
-        Note:
-            Beware values casting, see ``casting`` key.
-            Default is "unsafe" which may create issues if your property has
-            values that is outside the valid range. I.e. for float values XTGeo
-            normally use `float64` (8 byte) while roxar use `float32` (4 byte).
-            With extreme values, e.g. 10e40, such values will be truncated if
-            "unsafe" casting. More common is casting issues with discrete as
-            Roxar (RMS) often use `uint8` which only allow values in range 1..256.
-
-        Args:
-            project (str or roxar._project): Inside RMS use the magic 'project',
-                else use path to RMS project, or a project reference
-            gfile (str): Name of grid model
-            pfile (str): Name of grid property
-            projectname (str): Name of RMS project (None if inside a project)
-            realisation (int): Realisation number (default 0 first)
-            casting (str): This refers to numpy `astype(... casting=...)` settings.
-
-
-        .. versionchanged: 2.10
-            Key `saveproject` has been removed and will have no effect
-
-        .. versionadded:: 2.12 Key `casting` was added
-
-        """
-        _gridprop_roxapi.export_prop_roxapi(
-            self, project, gname, pname, realisation=realisation, casting=casting
-        )
-
-    # ==================================================================================
-    # Various public methods
-    # ==================================================================================
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout"""
-
-        dsc = xtgeo.common.XTGDescription()
-        dsc.title("Description of GridProperty instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("Name", self.name)
-        dsc.txt("Date", self.date)
-        dsc.txt("File source", self._filesrc)
-        dsc.txt("Discrete status", self._isdiscrete)
-        dsc.txt("Codes", self._codes)
-        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
-        np.set_printoptions(threshold=16)
-        dsc.txt("Values", self._values.reshape(-1), self._values.dtype)
-        np.set_printoptions(threshold=1000)
-        dsc.txt(
-            "Values, mean, stdev, minimum, maximum",
-            self.values.mean(),
-            self.values.std(),
-            self.values.min(),
-            self.values.max(),
-        )
-        itemsize = self.values.itemsize
-        msize = float(self.values.size * itemsize) / (1024 * 1024 * 1024)
-        dsc.txt("Roxar datatype", self.roxar_dtype)
-        dsc.txt("Minimum memory usage of array (GB)", msize)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    def get_npvalues3d(self, fill_value=None):
-        """Get a pure numpy copy (not masked) copy of the values, 3D shape.
-
-        Note that Numpy dtype will be reset; int32 if discrete or float64 if
-        continuous. The reason for this is to avoid inconsistensies regarding
-        UNDEF values.
-
-        If fill_value is not None, than the returning dtype is always `np.float64`.
-
-        Args:
-            fill_value: Value of masked entries. Default is None which
-                means the XTGeo UNDEF value (a high number), different
-                for a continuous or discrete property
-        """
-        # this is a function, not a property by design
-
-        if fill_value is None:
-            if self._isdiscrete:
-                fvalue = xtgeo.UNDEF_INT
-                dtype = np.int32
-            else:
-                fvalue = xtgeo.UNDEF
-                dtype = np.float64
-        else:
-            fvalue = fill_value
-            dtype = np.float64
-
-        val = self.values.copy().astype(dtype)
-        npv3d = np.ma.filled(val, fill_value=fvalue)
-        del val
-
-        return npv3d
-
-    def get_actnum(self, name="ACTNUM", asmasked=False, mask=None):
-        """Return an ACTNUM GridProperty object.
-
-        Note that this method is similar to, but not identical to,
-        the job with same name in Grid(). Here, the maskedarray of the values
-        is applied to deduce the ACTNUM array.
-
-        Args:
-            name (str): name of property in the XTGeo GridProperty object.
-            asmasked (bool): Actnum is returned with all cells shown
-                as default. Use asmasked=True to make 0 entries masked.
-            mask (bool): Deprecated, use asmasked instead!
-
-        Example::
-
-            act = mygrid.get_actnum()
-            print('{}% cells are active'.format(act.values.mean() * 100))
-        """
-
-        if mask is not None:
-            xtg.warndeprecated(
-                "The mask option is deprecated,"
-                "and will be removed in version 4.0. Use asmasked instead."
-            )
-            asmasked = super()._evaluate_mask(mask)
-
-        act = GridProperty(
-            ncol=self._ncol, nrow=self._nrow, nlay=self._nlay, name=name, discrete=True
-        )
-
-        orig = self.values
-        vact = np.ones(self.values.shape)
-        vact[orig.mask] = 0
-
-        if asmasked:
-            vact = np.ma.masked_equal(vact, 0)
-
-        act.values = vact.astype(np.int32)
-        act.isdiscrete = True
-        act.codes = {0: "0", 1: "1"}
-
-        # return the object
-        return act
-
-    def get_active_npvalues1d(self):
-        """Return the grid property as a 1D numpy array (copy), active
-        cells only.
-        """
-
-        return self.get_npvalues1d(activeonly=True)
-
-    def get_npvalues1d(self, activeonly=False, fill_value=np.nan, order="C"):
-        """Return the grid property as a 1D numpy array (copy) for active or all
-        cells, but inactive have a fill value.
-
-        Args:
-            activeonly (bool): If True, then only return active cells
-            fill_value (float): Fill value for inactive cells
-            order (str): Array internal order; default is "C", alternative is "F"
-
-        .. versionadded:: 2.3
-        .. versionchanged:: 2.8 Added `fill_value` and `order`
-        """
-        vact = self.values1d.copy()
-
-        if order == "F":
-            vact = _gridprop_lowlevel.c2f_order(self, vact)
-
-        if activeonly:
-            return vact.compressed()  # safer than vact[~vact.mask] if no masked
-
-        return vact.filled(fill_value)
-
-    def copy(self, newname=None):
-        """Copy a xtgeo.grid3d.GridProperty() object to another instance.
-
-        ::
-
-            >>> import xtgeo
-            >>> myporo = xtgeo.gridproperty_from_file(
-            ...    reek_dir + '/reek_sim_poro.roff',
-            ...    name="PORO"
-            ... )
-            >>> mycopy = myporo.copy(newname='XPROP')
-            >>> print(mycopy.name)
-            XPROP
-
-        """
-
-        if newname is None:
-            newname = self.name
-
-        xprop = GridProperty(
-            ncol=self._ncol,
-            nrow=self._nrow,
-            nlay=self._nlay,
-            values=self._values.copy(),
-            name=newname,
-        )
-
-        xprop.geometry = self._geometry
-        xprop.isdiscrete = self._isdiscrete
-        xprop.codes = self._codes
-        xprop.date = self._date
-        xprop.roxorigin = self._roxorigin
-        xprop.roxar_dtype = self._roxar_dtype
-
-        xprop.filesrc = self._filesrc
-
-        return xprop
-
-    def mask_undef(self):
-        """Make UNDEF values masked."""
-        if self._isdiscrete:
-            self._values = np.ma.masked_greater(self._values, xtgeo.UNDEF_INT_LIMIT)
-        else:
-            self._values = np.ma.masked_greater(self._values, xtgeo.UNDEF_LIMIT)
-
-    def crop(self, spec):
-        """Crop a property, see method under grid"""
-
-        (ic1, ic2), (jc1, jc2), (kc1, kc2) = spec
-
-        # compute size of new cropped grid
-        self._ncol = ic2 - ic1 + 1
-        self._nrow = jc2 - jc1 + 1
-        self._nlay = kc2 - kc1 + 1
-
-        newvalues = self.values.copy()
-
-        self.values = newvalues[ic1 - 1 : ic2, jc1 - 1 : jc2, kc1 - 1 : kc2]
-
-    def get_xy_value_lists(self, grid=None, activeonly=True):
-        """Get lists of xy coords and values for Webportal format.
-
-        The coordinates are on the form (two cells)::
-
-            [[[(x1,y1), (x2,y2), (x3,y3), (x4,y4)],
-            [(x5,y5), (x6,y6), (x7,y7), (x8,y8)]]]
-
-        Args:
-            grid (object): The XTGeo Grid object for the property
-            activeonly (bool): If true (default), active cells only,
-                otherwise cell geometries will be listed and property will
-                have value -999 in undefined cells.
-
-        Example::
-
-            grid = Grid()
-            grid.from_file('../xtgeo-testdata/3dgrids/bri/b_grid.roff')
-            prop = GridProperty()
-            prop.from_file('../xtgeo-testdata/3dgrids/bri/b_poro.roff',
-                           grid=grid, name='PORO')
-
-            clist, valuelist = prop.get_xy_value_lists(grid=grid,
-                                                       activeonly=False)
-
-
-        """
-
-        clist, vlist = _gridprop_op1.get_xy_value_lists(
-            self, grid=grid, mask=activeonly
-        )
-        return clist, vlist
-
-    def get_values_by_ijk(self, iarr, jarr, karr, base=1):
-        """Get a 1D ndarray of values by I J K arrays.
-
-        This could for instance be a well path where I J K
-        exists as well logs.
-
-        Note that the input arrays have 1 as base as default
-
-        Args:
-            iarr (ndarray): Numpy array of I
-            jarr (ndarray): Numpy array of J
-            karr (ndarray): Numpy array of K
-            base (int): Should be 1 or 0, dependent on what
-                number base the input arrays has.
-
-        Returns:
-            pvalues (ndarray): A 1D numpy array of property values,
-                with NaN if undefined
-
-        """
-        res = np.zeros(iarr.shape, dtype="float64")
-        res = np.ma.masked_equal(res, 0)  # mask all
-
-        # get indices where defined (note the , after valids)
-        (valids,) = np.where(~np.isnan(iarr))
-
-        iarr = iarr[~np.isnan(iarr)]
-        jarr = jarr[~np.isnan(jarr)]
-        karr = karr[~np.isnan(karr)]
-
-        try:
-            res[valids] = self.values[
-                iarr.astype("int") - base,
-                jarr.astype("int") - base,
-                karr.astype("int") - base,
-            ]
-
-            return np.ma.filled(res, fill_value=np.nan)
-
-        except IndexError as ier:
-            xtg.warn(f"Error {ier}, return None")
-            return None
-        except:  # noqa
-            xtg.warn("Unexpected error")
-            raise
-
-    def discrete_to_continuous(self):
-        """Convert from discrete to continuous values"""
-
-        if self.isdiscrete:
-            logger.info("Converting to continuous ...")
-            val = self._values.copy()
-            val = val.astype("float64")
-            self._values = val
-            self._isdiscrete = False
-            self._codes = {}
-            self._roxar_dtype = np.float32
-        else:
-            logger.info("No need to convert, already continuous")
-
-    def continuous_to_discrete(self):
-        """Convert from continuous to discrete values"""
-
-        if not self.isdiscrete:
-            logger.info("Converting to discrete ...")
-            val = self._values.copy()
-            val = val.astype(np.int32)
-            self._values = val
-            self._isdiscrete = True
-
-            # make the code list
-            uniq = np.unique(val).tolist()
-            codes = dict(zip(uniq, uniq))
-            codes = {k: str(v) for k, v in codes.items()}  # val as strings
-            self._codes = codes
-            self._roxar_dtype = np.uint16
-        else:
-            logger.info("No need to convert, already discrete")
-
-    # ==================================================================================
-    # Operations restricted to inside/outside polygons
-    # ==================================================================================
-
-    def operation_polygons(self, poly, value, opname="add", inside=True):
-        """A generic function for doing 3D grid property operations
-        restricted to inside or outside polygon(s).
-
-        This method requires that the property geometry is known
-        (prop.geometry is set to a grid instance)
-
-        Args:
-            poly (Polygons): A XTGeo Polygons instance
-            value (float): Value to add, subtract etc
-            opname (str): Name of operation... 'add', 'sub', etc
-            inside (bool): If True do operation inside polygons; else outside.
-        """
-
-        if self.geometry is None:
-            msg = """
-            You need to link the property to a grid geometry:"
-
-                myprop.geometry = mygrid
-
-            """
-            xtg.warnuser(msg)
-            raise ValueError("The geometry attribute is not set")
-
-        _gridprop_op1.operation_polygons(
-            self, poly, value, opname=opname, inside=inside
-        )
-
-    # shortforms
-    def add_inside(self, poly, value):
-        """Add a value (scalar) inside polygons"""
-        self.operation_polygons(poly, value, opname="add", inside=True)
-
-    def add_outside(self, poly, value):
-        """Add a value (scalar) outside polygons"""
-        self.operation_polygons(poly, value, opname="add", inside=False)
-
-    def sub_inside(self, poly, value):
-        """Subtract a value (scalar) inside polygons"""
-        self.operation_polygons(poly, value, opname="sub", inside=True)
-
-    def sub_outside(self, poly, value):
-        """Subtract a value (scalar) outside polygons"""
-        self.operation_polygons(poly, value, opname="sub", inside=False)
-
-    def mul_inside(self, poly, value):
-        """Multiply a value (scalar) inside polygons"""
-        self.operation_polygons(poly, value, opname="mul", inside=True)
-
-    def mul_outside(self, poly, value):
-        """Multiply a value (scalar) outside polygons"""
-        self.operation_polygons(poly, value, opname="mul", inside=False)
-
-    def div_inside(self, poly, value):
-        """Divide a value (scalar) inside polygons"""
-        self.operation_polygons(poly, value, opname="div", inside=True)
-
-    def div_outside(self, poly, value):
-        """Divide a value (scalar) outside polygons"""
-        self.operation_polygons(poly, value, opname="div", inside=False)
-
-    def set_inside(self, poly, value):
-        """Set a value (scalar) inside polygons"""
-        self.operation_polygons(poly, value, opname="set", inside=True)
-
-    def set_outside(self, poly, value):
-        """Set a value (scalar) outside polygons"""
-        self.operation_polygons(poly, value, opname="set", inside=False)
+# -*- coding: utf-8 -*-
+"""Module for a 3D grid property."""
+
+
+import copy
+import functools
+import hashlib
+import io
+import numbers
+import pathlib
+import warnings
+from types import FunctionType
+from typing import Any, Optional, Union
+
+import numpy as np
+
+import xtgeo
+
+from . import (
+    _gridprop_export,
+    _gridprop_lowlevel,
+    _gridprop_op1,
+    _gridprop_roxapi,
+    _gridprop_value_init,
+)
+from ._grid3d import _Grid3D
+from ._gridprop_import_eclrun import (
+    import_gridprop_from_init,
+    import_gridprop_from_restart,
+)
+from ._gridprop_import_grdecl import import_bgrdecl_prop, import_grdecl_prop
+from ._gridprop_import_roff import import_roff
+from ._gridprop_import_xtgcpprop import import_xtgcpprop
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+# --------------------------------------------------------------------------------------
+# Comment on 'asmasked' vs 'activeonly:
+#
+# 'asmasked'=True will return a np.ma array, while 'asmasked' = False will
+# return a np.ndarray
+#
+# The 'activeonly' will filter out masked entries, or use None or np.nan
+# if 'activeonly' is False.
+#
+# Use word 'zerobased' for a bool regrading startcell basis is 1 or 0
+#
+# For functions with mask=... ,they should be replaced with asmasked=...
+# --------------------------------------------------------------------------------------
+
+# pylint: disable=logging-format-interpolation, too-many-public-methods
+
+# ======================================================================================
+# Functions outside the class, for rapid access. Will be exposed as
+# xxx = xtgeo.gridproperty_from_file. pylint: disable=fixme
+# ======================================================================================
+
+
+def _data_reader_factory(fformat):
+    if fformat in ["roff_binary", "roff_ascii"]:
+        return import_roff
+    elif fformat in ["finit", "init"]:
+        return import_gridprop_from_init
+
+    elif fformat in ["funrst", "unrst"]:
+        return functools.partial(import_gridprop_from_restart, fformat=fformat)
+    elif fformat == "grdecl":
+        return import_grdecl_prop
+
+    elif fformat == "bgrdecl":
+        return import_bgrdecl_prop
+
+    elif fformat in ["xtg"]:
+        return import_xtgcpprop
+    else:
+        raise ValueError(f"Invalid grid property file format {fformat}")
+
+
+def gridproperty_from_file(*args, **kwargs):
+    """Make a GridProperty instance directly from file import.
+
+    For arguments, see :func:`GridProperty.from_file()`
+
+    Args:
+        pfile (str): Property file
+        args: See :func:`GridProperty.from_file()`
+        kwargs: See :func:`GridProperty.from_file()`.
+
+    Example::
+
+        >>> import xtgeo
+        >>> myporo = xtgeo.gridproperty_from_file(
+        ...    reek_dir + '/reek_sim_poro.roff',
+        ...    name="PORO"
+        ... )
+    """
+    return GridProperty._read_file(*args, **kwargs)
+
+
+def gridproperty_from_roxar(
+    project, gname, pname, realisation=0, faciescodes=False
+):  # pragma: no cover
+    """Make a GridProperty instance directly inside RMS.
+
+    For arguments, see :func:`GridProperty.from_roxar()`
+
+    Example::
+
+        import xtgeo
+        myporo = xtgeo.gridproperty_from_roxar(project, 'Geogrid', 'Poro')
+
+    """
+    return GridProperty._read_roxar(
+        project, gname, pname, realisation=realisation, faciescodes=faciescodes
+    )
+
+
+def allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of GridProperty and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(self, *args, **kwargs):
+        # Check if dummy values are to be used
+        if (
+            all(param not in kwargs for param in ["values", "ncol", "nrow", "nlay"])
+            and len(args) == 0
+        ):
+            warnings.warn(
+                "Default initialization of GridProperty without values and dimension "
+                "is deprecated and will be removed in xtgeo version 4.0",
+                DeprecationWarning,
+            )
+            return func(
+                self,
+                *args,
+                ncol=4,
+                nrow=3,
+                nlay=5,
+                values=_gridprop_value_init.gridproperty_dummy_values(
+                    kwargs.get("discrete", False)
+                ),
+                **kwargs,
+            )
+
+        # Checking if we are doing an initialization
+        # from file and raise a deprecation warning if
+        # we are.
+        if "pfile" in kwargs or (
+            len(args) >= 1
+            and isinstance(args[0], (str, pathlib.Path, xtgeo._XTGeoFile))
+        ):
+            pfile = kwargs.get("pfile", args[0])
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "myprop = xtgeo.gridproperty_from_file('some_name.roff') instead",
+                DeprecationWarning,
+            )
+            fformat = kwargs.get("fformat", None)
+            mfile = xtgeo._XTGeoFile(pfile)
+            if fformat is None or fformat == "guess":
+                fformat = mfile.detect_fformat()
+            else:
+                fformat = mfile.generic_format_by_proposal(fformat)  # default
+
+            if "pfile" in kwargs:
+                del kwargs["pfile"]
+            if "fformat" in kwargs:
+                del kwargs["fformat"]
+            if len(args) >= 1 and isinstance(
+                args[0], (str, pathlib.Path, xtgeo._XTGeoFile)
+            ):
+                args = args[min(len(args), 2) :]
+
+            kwargs = _data_reader_factory(fformat)(mfile, *args, **kwargs)
+            kwargs["filesrc"] = mfile.file
+            return func(self, **kwargs)
+        return func(self, *args, **kwargs)
+
+    return wrapper
+
+
+class GridProperty(_Grid3D):
+    """Class for a single 3D grid property, e.g porosity or facies.
+
+    An GridProperty instance may or may not 'belong' to a grid (geometry) object.
+    E.g. for ROFF input, ncol, nrow, nlay are given in the import file and the grid
+    geometry file is not needed. For many Eclipse files, the grid geometry is needed
+    as this holds the active number indices (ACTNUM).
+
+    Normally the instance is created when importing a grid
+    property from file, but it can also be created directly, as e.g.::
+
+        poro = GridProperty(ncol=233, nrow=122, nlay=32)
+
+    The grid property values ``someinstance.values`` by themselves is a 3D masked
+    numpy usually as either float64 (double) or int32 (if discrete), and undefined
+    cells are displayed as masked. The internal array order is now C_CONTIGUOUS.
+    (i.e. not in Eclipse manner). A 1D view (C order) is achieved by the
+    values1d property, e.g.::
+
+       poronumpy = poro.values1d
+
+    .. versionchanged:: 2.6 Possible to make GridProperty instance directly from Grid()
+    .. versionchanged:: 2.8 Possible to base it on existing GridProperty() instance
+
+    """
+
+    @allow_deprecated_init
+    def __init__(
+        self,
+        gridlike=None,
+        ncol: Optional[int] = None,
+        nrow: Optional[int] = None,
+        nlay: Optional[int] = None,
+        name: Optional[str] = "unknown",
+        discrete: Optional[bool] = False,
+        date: Optional[str] = None,
+        grid: Optional[Any] = None,
+        linkgeometry: Optional[bool] = True,
+        fracture: Optional[bool] = False,
+        codes: Optional[dict] = None,
+        dualporo: Optional[bool] = False,
+        dualperm: Optional[bool] = False,
+        roxar_dtype: Optional[Any] = None,
+        values: Optional[Union[np.ndarray, float, int]] = None,
+        roxorigin: bool = False,
+        filesrc: Optional[str] = None,
+    ):
+        """Instantating.
+
+            Args:
+                pfile: Input file-like or a Grid/GridProperty instance or leave blank.
+                fformat: File format input, default is ``guess``.
+                ncol: Number of columns (nx) defaults to 4.
+                nrow: Number of rows (ny) defaults to 3.
+                ncol: Number of layers (nz) defaults to 5.
+                name: Name of property.
+                discrete: True or False.
+                date: Date on YYYYMMDD form.
+                grid: Attached grid object
+                linkgeometry: If True, establish a link between GridProperty and Grid
+                fracture: True if fracture option (relevant for flow simulator data)
+                codes: Codes in case a discrete property e.g. {1: "Sand", 4: "Shale"}
+                dualporo: True if dual porosity system.
+                dualperm: True if dual porosity and dual permeability system.
+                roxar_dtype: Spesify roxar datatype e.g. np.uint8
+                values: Values to apply (will not be applied if a file-like is input)
+
+        Returns:
+            A GridProperty object instance.
+
+        Raises:
+            RuntimeError: if something goes wrong (e.g. file not found)
+
+        Examples::
+
+            from xtgeo.grid3d import GridProperty
+            myprop = GridProperty()
+            myprop.from_file('emerald.roff', name='PORO')
+
+            # or
+
+            values = np.ma.ones((12, 17, 10), dtype=np.float64),
+            myprop = GridProperty(ncol=12, nrow=17, nlay=10,
+                                  values=values, discrete=False,
+                                  name='MyValue')
+
+            # or
+
+            myprop = GridProperty('emerald.roff', name='PORO')
+
+            # or create properties from a Grid() instance
+
+            mygrid = Grid("grid.roff")
+            myprop1 = GridProperty(mygrid, name='PORO')
+            myprop2 = GridProperty(mygrid, name='FACIES', discrete=True, values=1,
+                                   linkgeometry=True)  # alternative 1
+            myprop2.geometry = mygrid  # alternative 2 to link grid geometry to property
+
+            # from Grid instance:
+            grd = Grid("somefile_grid_file")
+            myprop = GridProperty(grd, values=99, discrete=True)  # based on grd
+
+            # or from existing GridProperty instance:
+            myprop2 = GridProperty(myprop, values=99, discrete=False)  # based on myprop
+
+        """
+
+        super().__init__(ncol, nrow, nlay)
+
+        self._reset(
+            gridlike,
+            ncol,
+            nrow,
+            nlay,
+            name,
+            discrete,
+            date,
+            grid,
+            linkgeometry,
+            fracture,
+            codes,
+            dualporo,
+            dualperm,
+            roxar_dtype,
+            values,
+            roxorigin,
+            filesrc,
+        )
+
+    def _reset(
+        self,
+        gridlike=None,
+        ncol: Optional[int] = None,
+        nrow: Optional[int] = None,
+        nlay: Optional[int] = None,
+        name: Optional[str] = "unknown",
+        discrete: Optional[bool] = False,
+        date: Optional[str] = None,
+        grid: Optional[Any] = None,
+        linkgeometry: Optional[bool] = True,
+        fracture: Optional[bool] = False,
+        codes: Optional[dict] = None,
+        dualporo: Optional[bool] = False,
+        dualperm: Optional[bool] = False,
+        roxar_dtype: Optional[Any] = None,
+        values: Optional[Union[np.ndarray, float, int]] = None,
+        roxorigin: bool = False,
+        filesrc: Optional[str] = None,
+    ):
+        """Instantating.
+
+            Args:
+                pfile: Input file-like or a Grid/GridProperty instance or leave blank.
+                fformat: File format input, default is ``guess``.
+                ncol: Number of columns (nx) defaults to 4.
+                nrow: Number of rows (ny) defaults to 3.
+                ncol: Number of layers (nz) defaults to 5.
+                name: Name of property.
+                discrete: True or False.
+                date: Date on YYYYMMDD form.
+                grid: Attached grid object
+                linkgeometry: If True, establish a link between GridProperty and Grid
+                fracture: True if fracture option (relevant for flow simulator data)
+                codes: Codes in case a discrete property e.g. {1: "Sand", 4: "Shale"}
+                dualporo: True if dual porosity system.
+                dualperm: True if dual porosity and dual permeability system.
+                roxar_dtype: Spesify roxar datatype e.g. np.uint8
+                values: Values to apply (will not be applied if a file-like is input)
+
+        Returns:
+            A GridProperty object instance.
+
+        Raises:
+            RuntimeError: if something goes wrong (e.g. file not found)
+
+        Examples::
+
+            from xtgeo.grid3d import GridProperty
+            myprop = GridProperty()
+            myprop.from_file('emerald.roff', name='PORO')
+
+            # or
+
+            values = np.ma.ones((12, 17, 10), dtype=np.float64),
+            myprop = GridProperty(ncol=12, nrow=17, nlay=10,
+                                  values=values, discrete=False,
+                                  name='MyValue')
+
+            # or
+
+            myprop = GridProperty('emerald.roff', name='PORO')
+
+            # or create properties from a Grid() instance
+
+            mygrid = Grid("grid.roff")
+            myprop1 = GridProperty(mygrid, name='PORO')
+            myprop2 = GridProperty(mygrid, name='FACIES', discrete=True, values=1,
+                                   linkgeometry=True)  # alternative 1
+            myprop2.geometry = mygrid  # alternative 2 to link grid geometry to property
+
+            # from Grid instance:
+            grd = Grid("somefile_grid_file")
+            myprop = GridProperty(grd, values=99, discrete=True)  # based on grd
+
+            # or from existing GridProperty instance:
+            myprop2 = GridProperty(myprop, values=99, discrete=False)  # based on myprop
+
+        """
+
+        super().__init__()
+
+        self._ncol = ncol
+        self._nrow = nrow
+        self._nlay = nlay
+
+        # instance attributes defaults:
+        self._name = name
+        self._date = date
+        self._isdiscrete = discrete
+        self._geometry = grid
+        self._fracture = fracture
+        self._codes = dict() if codes is None else codes
+
+        # not primary input:
+        self._dualporo = dualporo
+        self._dualperm = dualperm
+
+        self._filesrc = filesrc
+        self._roxorigin = roxorigin  # true if the object comes from the ROXAPI
+        if roxar_dtype is None:
+            if discrete:
+                self._roxar_dtype = np.uint8
+            else:
+                self._roxar_dtype = np.float32
+        else:
+            self.roxar_dtype = roxar_dtype
+        self._values = values
+
+        self._undef = xtgeo.UNDEF_INT if discrete else xtgeo.UNDEF
+
+        self._set_initial_dimensions(gridlike, (ncol, nrow, nlay))
+
+        self._values = _gridprop_value_init.gridproperty_non_dummy_values(
+            gridlike, self.dimensions, values, discrete
+        )
+
+        if isinstance(gridlike, xtgeo.grid3d.Grid):
+            if linkgeometry:
+                # assosiate this grid property with grid instance. This is not default
+                # since sunch links may affect garbish collection
+                self.geometry = gridlike
+            gridlike.append_prop(self)
+
+        self._metadata = xtgeo.MetaDataCPProperty()
+
+    def _set_initial_dimensions(self, gridlike, input_dimensions):
+        """Sets the initial dimensions either from input, grid or default.
+
+        If a gridlike is given, we use its dimensions, but make sure it matches
+        the input dimensions if given (not None). Otherwise, dimensions are either
+        set to the input dimensions or defaulted.
+
+        """
+        if gridlike is not None:
+            self._ncol = gridlike.ncol
+            self._nrow = gridlike.nrow
+            self._nlay = gridlike.nlay
+            self._check_dimensions_match(*input_dimensions)
+        else:
+            ncol, nrow, nlay = input_dimensions
+            if ncol is None:
+                self._ncol = 4
+            else:
+                self._ncol = ncol
+            if nrow is None:
+                self._nrow = 3
+            else:
+                self._nrow = nrow
+            if nlay is None:
+                self._nlay = 5
+            else:
+                self._nlay = nlay
+
+    def _check_dimensions_match(self, ncol, nrow, nlay):
+        """
+        Raises:
+            ValueError: if given dimensions are not None and do not
+                match dimensions of the GridProperty
+        """
+        if ncol is not None and self._ncol != ncol:
+            raise ValueError(
+                f"mismatching column dimension given: {ncol} vs {self._ncol}"
+            )
+        if nrow is not None and self._nrow != nrow:
+            raise ValueError(f"mismatching row dimension given: {nrow} vs {self._nrow}")
+        if nlay is not None and self._nlay != nlay:
+            raise ValueError(
+                f"mismatching layer dimension given: {nlay} vs {self._nlay}"
+            )
+
+    def __del__(self):
+        logger.debug("DELETING property instance %s", self.name)
+
+    def __repr__(self):
+        myrp = (
+            f"{self.__class__.__name__} (id={id(self)}) ncol={self._ncol!r}, "
+            f"nrow={self._nrow!r}, nlay={self._nlay!r}, filesrc={self._filesrc!r}"
+        )
+        return myrp
+
+    def __str__(self):
+        # user friendly print
+        return self.describe(flush=False)
+
+    # ==================================================================================
+    # Properties
+    # Some proprerties such as ncol, nrow, nlay are from the Super class
+    # ==================================================================================
+
+    @property
+    def metadata(self):
+        """Return metadata object instance of type MetaDataRegularSurface."""
+        return self._metadata
+
+    @metadata.setter
+    def metadata(self, obj):
+        # The current metadata object can be replaced. A bit dangerous so further
+        # check must be done to validate. TODO.
+        if not isinstance(obj, xtgeo.MetaDataCPProperty):
+            raise ValueError("Input obj not an instance of MetaDataCPProperty")
+
+        self._metadata = obj  # checking is currently missing! TODO
+
+    @property
+    def name(self):
+        """Returns or rename the property name."""
+        return self._name
+
+    @name.setter
+    def name(self, name):
+        self._name = name
+
+    @property
+    def dimensions(self):
+        """3-tuple: The grid dimensions as a tuple of 3 integers (read only)"""
+        return (self._ncol, self._nrow, self._nlay)
+
+    @property
+    def nactive(self):
+        """int: Returns the number of active cells (read only)."""
+        return len(self.actnum_indices)
+
+    @property
+    def geometry(self):
+        """Returns or set the linked geometry, i.e. the Grid instance)"""
+        return self._geometry
+
+    @geometry.setter
+    def geometry(self, geom):
+        if geom is None:
+            self._geometry = None
+        elif isinstance(geom, xtgeo.grid3d.Grid) and geom.dimensions == self.dimensions:
+            self._geometry = geom
+        else:
+            raise ValueError("Could not set geometry; wrong type or size")
+
+    @property
+    def actnum_indices(self):
+        """Returns the 1D ndarray which holds the indices for active cells
+        given in 1D, C order (read only).
+
+        """
+        actnumv = self.get_actnum()
+        actnumv = np.ravel(actnumv.values)
+        return np.flatnonzero(actnumv)
+
+    @property
+    def isdiscrete(self):
+        """Return True if property is discrete.
+
+        This can also be used to convert from continuous to discrete
+        or from discrete to continuous::
+
+            myprop.isdiscrete = False
+        """
+
+        return self._isdiscrete
+
+    @isdiscrete.setter
+    def isdiscrete(self, flag):
+        if not isinstance(flag, bool):
+            raise ValueError("Input to {__name__} must be a bool")
+
+        if flag is self._isdiscrete:
+            pass
+        else:
+            if flag is True and self._isdiscrete is False:
+                self.continuous_to_discrete()
+            else:
+                self.discrete_to_continuous()
+
+    @property
+    def dtype(self):
+        """Return or set the values numpy dtype.
+
+        When setting, note that the the dtype must correspond to the
+        `isdiscrete` property. Hence dtype cannot alter isdiscrete status
+
+        Example::
+
+            if myprop.isdiscrete:
+                myprop.dtype = np.uint16
+
+
+        """
+        return self._values.dtype
+
+    @dtype.setter
+    def dtype(self, dtype):
+        allowedfloat = [np.float16, np.float32, np.float64]
+        allowedint = [np.uint8, np.uint16, np.int16, np.int32, np.int64]
+
+        okv = True
+        msg = ""
+        if self.isdiscrete:
+            if dtype in allowedint:
+                self.values = self.values.astype(dtype)
+            else:
+                okv = False
+                msg = f"{__name__}: Wrong input for dtype. Use one of {allowedint}!"
+        else:
+            if dtype in allowedfloat:
+                self.values = self.values.astype(dtype)
+            else:
+                okv = False
+                msg = f"{__name__}: Wrong input for dtype. Use one of {allowedfloat}!"
+
+        if not okv:
+            raise ValueError(msg)
+
+    @property
+    def filesrc(self):
+        """Return or set file src (if any)"""
+        return self._filesrc
+
+    @filesrc.setter
+    def filesrc(self, src):
+        self._filesrc = src
+
+    @property
+    def roxar_dtype(self):
+        """Return or set the roxar dtype (if any)"""
+        return self._roxar_dtype
+
+    @roxar_dtype.setter
+    def roxar_dtype(self, dtype):
+        allowed = [np.uint16, np.uint8, np.float32]
+        if dtype in allowed:
+            self._roxar_dtype = dtype
+        else:
+            raise ValueError(
+                f"{__name__}: Wrong input for roxar_dtype. Use one of {allowed}!"
+            )
+
+    @property
+    def date(self) -> Optional[str]:
+        """Returns or rename the property date as string on YYYYMMDD format."""
+        return self._date
+
+    @date.setter
+    def date(self, date: Optional[str]):
+        self._date = date
+
+    @property
+    def codes(self) -> dict:
+        """The property codes as a dictionary."""
+        return self._codes
+
+    @codes.setter
+    def codes(self, cdict):
+        if isinstance(cdict, dict):
+            self._codes = copy.deepcopy(cdict)
+        else:
+            raise ValueError(
+                "The codes must be a python dictionary, current input "
+                f"is type: {type(cdict)}"
+            )
+
+    @property
+    def ncodes(self) -> int:
+        """Number of codes if discrete grid property (read only)."""
+        return len(self._codes)
+
+    @property
+    def values(self) -> np.ma.MaskedArray:
+        """Return or set the grid property as a masked 3D numpy array"""
+        return self._values
+
+    @values.setter
+    def values(self, values):
+        values = self.ensure_correct_values(self.ncol, self.nrow, self.nlay, values)
+
+        self._values = values
+
+    @property
+    def ntotal(self) -> int:
+        """Returns total number of cells ncol*nrow*nlay (read only)"""
+        return self._ncol * self._nrow * self._nlay
+
+    @property
+    def roxorigin(self):
+        """Returns True if the property comes from ROXAPI"""
+        return self._roxorigin
+
+    @roxorigin.setter
+    def roxorigin(self, val):
+        if isinstance(val, bool):
+            self._roxorigin = val
+        else:
+            raise ValueError("Input to roxorigin must be True or False")
+
+    @property
+    def values3d(self):
+        """For backward compatibility (use values instead)"""
+        return self._values
+
+    @values3d.setter
+    def values3d(self, values):
+        # kept for backwards compatibility
+        self.values = values
+
+    @property
+    def values1d(self):
+        """Returns a 1D view of values (masked numpy) (read only)."""
+        return self._values.reshape(-1)
+
+    @property
+    def undef(self):
+        """Get the actual undef value for floats or ints
+        numpy arrays (read only).
+        """
+        if self._isdiscrete:
+            return xtgeo.UNDEF_INT
+
+        return xtgeo.UNDEF
+
+    @property
+    def undef_limit(self):
+        """Returns the undef limit number, which is slightly less than the
+        undef value.
+
+        Hence for numerical precision, one can force undef values
+        to a given number, e.g.::
+
+           x[x<x.undef_limit]=999
+
+        Undef limit values cannot be changed (read only).
+
+        """
+        if self._isdiscrete:
+            return xtgeo.UNDEF_INT_LIMIT
+
+        return xtgeo.UNDEF_LIMIT
+
+    # ==================================================================================
+    # Class and special methods
+    # ==================================================================================
+
+    def generate_hash(self):
+        """str: Return a unique hash ID for current grid; can e.g. be used to compare
+        two gridproperty instances with same source.
+
+        .. versionadded:: 2.10
+        """
+
+        mhash = hashlib.sha256()
+        gid = f"{self._filesrc}{self._ncol}{self._nrow}{self._nlay}"
+        f"{self._values.mean()}{self._values.min()}{self._values.max()}"
+
+        mhash.update(gid.encode())
+        return mhash.hexdigest()
+
+    @classmethod
+    def methods(cls):
+        """Returns the names of the methods in the class.
+
+        >>> print(GridProperty.methods())
+        METHODS for GridProperty():
+        ======================
+        __init__
+        _reset
+        _set_initial_dimensions
+        _check_dimensions_match
+        ...
+        """
+        mets = [x for x, y in cls.__dict__.items() if isinstance(y, FunctionType)]
+
+        txt = "METHODS for GridProperty():\n======================\n"
+        for met in mets:
+            txt += str(met) + "\n"
+
+        return txt
+
+    def ensure_correct_values(self, ncol, nrow, nlay, invalues):
+        """Ensures that values is a 3D masked numpy (ncol, nrol, nlay).
+
+        Args:
+            ncol (int): Number of columns.
+            nrow (int): Number of rows.
+            nlay (int): Number of layers.
+            invalues (array or scalar): Values to process.
+
+        Return:
+            values (MaskedArray): Numpy masked array on correct format.
+
+        """
+
+        currentmask = None
+        if self._values is not None:
+            if isinstance(self._values, np.ma.MaskedArray):
+                currentmask = np.ma.getmaskarray(self._values)
+
+        if isinstance(invalues, numbers.Number):
+            vals = np.ma.zeros((ncol, nrow, nlay), order="C", dtype=self.dtype)
+            vals = np.ma.array(vals, mask=currentmask)
+            values = vals + invalues
+            invalues = values
+
+        if not isinstance(invalues, np.ma.MaskedArray):
+            values = np.ma.array(invalues, mask=currentmask, order="C")
+        else:
+            values = invalues  # new mask is possible
+
+        if values.shape != (ncol, nrow, nlay):
+            try:
+                values = np.ma.reshape(values, (ncol, nrow, nlay), order="C")
+            except ValueError as emsg:
+                xtg.error(f"Cannot reshape array: {emsg}")
+                raise
+
+        # replace any undef or nan with mask
+        values = np.ma.masked_greater(values, self.undef_limit)
+        values = np.ma.masked_invalid(values)
+
+        if not values.flags.c_contiguous:
+            mask = np.ma.getmaskarray(values)
+            mask = np.asanyarray(mask, order="C")
+            values = np.asanyarray(values, order="C")
+            values = np.ma.array(values, mask=mask, order="C")
+
+        # the self._isdiscrete property shall win over numpy dtype
+        if "int" in str(values.dtype) and not self._isdiscrete:
+            values = values.astype(np.float64)
+
+        if "float" in str(values.dtype) and self._isdiscrete:
+            values = values.astype(np.int32)
+
+        return values
+
+    # ==================================================================================
+    # Import and export
+    # ==================================================================================
+
+    def from_file(self, pfile, fformat=None, **kwargs):  # _roffapiv for devel.
+        """
+        Import grid property from file, and makes an instance of this class.
+
+        Note that the the property may be linked to its geometrical grid,
+        through the ``grid=`` option. Sometimes this is required, for instance
+        for most Eclipse input.
+
+        Args:
+            pfile (str): name of file to be imported
+            fformat (str): file format to be used roff/init/unrst/grdecl
+                (None is default, which means "guess" from file extension).
+            name (str): name of property to import
+            date (int or str): For restart files, date on YYYYMMDD format. Also
+                the YYYY-MM-DD form is allowed (string), and for Eclipse,
+                mnemonics like 'first', 'last' is also allowed.
+            grid (Grid object): Grid Object for checks (optional for ROFF,
+                required for Eclipse).
+            gridlink (bool): If True, and grid is not None, a link from the grid
+                instance to the property is made. If False, no such link is made.
+                Avoiding gridlink is recommended when running statistics of multiple
+                realisations of a property.
+            fracture (bool): Only applicable for DUAL POROSITY systems, if True
+                then the fracture property is read; if False then the matrix
+                property is read. Names will be appended with "M" or "F"
+            ijrange (list-like): A list of 4 number: (i1, i2, j1, j2) for subrange
+                of cells to read. Only applicable for xtgcpprop format.
+            zerobased (bool): Input if cells counts are zero- or one-based in
+                ijrange. Only applicable for xtgcpprop format.
+
+        Examples::
+
+           x = GridProperty()
+           x.from_file('somefile.roff', fformat='roff')
+           #
+           mygrid = Grid('ECL.EGRID')
+           pressure_1 = GridProperty()
+           pressure_1.from_file('ECL.UNRST', name='PRESSURE', date='first',
+                                grid=mygrid)
+
+        Returns:
+           True if success, otherwise False
+
+        .. versionchanged:: 2.8 Added gridlink option, default is True
+        """
+        pfile = xtgeo._XTGeoFile(pfile)
+        if fformat is None or fformat == "guess":
+            fformat = pfile.detect_fformat()
+        else:
+            fformat = pfile.generic_format_by_proposal(fformat)  # default
+        kwargs = _data_reader_factory(fformat)(pfile, **kwargs)
+        kwargs["filesrc"] = pfile.file
+        self._reset(**kwargs)
+        return self
+
+    @classmethod
+    def _read_file(
+        cls,
+        pfile: Union[str, pathlib.Path, io.BytesIO, io.StringIO],
+        fformat: Optional[str] = None,
+        **kwargs,
+    ):
+        pfile = xtgeo._XTGeoFile(pfile)
+        if fformat is None or fformat == "guess":
+            fformat = pfile.detect_fformat()
+        else:
+            fformat = pfile.generic_format_by_proposal(fformat)  # default
+        kwargs = _data_reader_factory(fformat)(pfile, **kwargs)
+        kwargs["filesrc"] = pfile.file
+        return cls(**kwargs)
+
+    def to_file(
+        self, pfile, fformat="roff", name=None, append=False, dtype=None, fmt=None
+    ):
+        """Export the grid property to file.
+
+        Args:
+            pfile (str or Path): File name or pathlib.Path to export to
+            fformat (str): The file format to be used. Default is
+                roff binary , else roff_ascii/grdecl/bgrdecl
+            name (str): If provided, will explicitly give property name;
+                else the existing name of the instance will used.
+            append (bool): Append to existing file, only for (b)grdecl formats.
+            dtype (str): Data type; this is valid only for grdecl or bgrdecl
+                formats, where default is None which means 'float32' for
+                floating point number and 'int32' for discrete properties.
+                Other choices are 'float64' which are 'DOUB' entries in
+                Eclipse formats.
+            fmt (str): Format for ascii grdecl format, default is None. If spesified,
+                the user is responsible for a valid format specifier, e.g. "%8.4f"
+
+        Example::
+
+            # This example demonstrates that file formats can be mixed
+            rgrid = Grid('reek.roff')
+            poro = GridProperty('reek_poro.grdecl', grid=rgrid, name='PORO')
+
+            poro.values += 0.05
+
+            poro.to_file('reek_export_poro.bgrdecl', format='bgrdecl')
+
+        .. versionadded:: 2.13  Key `fmt` was added and default format for float output
+            to grdecl is now "%e" if `fmt=None`
+
+        """
+
+        _gridprop_export.to_file(
+            self,
+            pfile,
+            fformat=fformat,
+            name=name,
+            append=append,
+            dtype=dtype,
+            fmt=fmt,
+        )
+
+    def from_roxar(
+        self, projectname, gname, pname, realisation=0, faciescodes=False
+    ):  # pragma: no cover
+        """Import grid model property from RMS project, and makes an instance.
+
+        Arguments:
+            projectname (str): Name of RMS project; use pure 'project'
+                if inside RMS
+            gfile (str): Name of grid model
+            pfile (str): Name of grid property
+            realisation (int): Realisation number (default 0; first)
+            faciescodes (bool): If a Roxar property is of the special ``body_facies``
+                type (e.g. result from a channel facies object modelling), the
+                default is to get the body code values. If faciescodes is True,
+                the facies code values will be read instead. For other roxar properties
+                this key is not relevant.
+
+        .. versionadded:: 2.12  Key `faciescodes` was added
+
+        """
+
+        self._reset(
+            **_gridprop_roxapi.import_prop_roxapi(
+                projectname, gname, pname, realisation, faciescodes
+            )
+        )
+
+    @classmethod
+    def _read_roxar(
+        cls, projectname, gname, pname, realisation=0, faciescodes=False
+    ):  # pragma: no cover
+        return cls(
+            **_gridprop_roxapi.import_prop_roxapi(
+                projectname, gname, pname, realisation, faciescodes
+            )
+        )
+
+    def to_roxar(
+        self, project, gname, pname, realisation=0, casting="unsafe"
+    ):  # pragma: no cover
+        """Store a grid model property into a RMS project.
+
+        Note:
+            When project is file path (direct access, outside RMS) then
+            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+            will not be saved until the user do an explicit project save action.
+
+        Note:
+            Beware values casting, see ``casting`` key.
+            Default is "unsafe" which may create issues if your property has
+            values that is outside the valid range. I.e. for float values XTGeo
+            normally use `float64` (8 byte) while roxar use `float32` (4 byte).
+            With extreme values, e.g. 10e40, such values will be truncated if
+            "unsafe" casting. More common is casting issues with discrete as
+            Roxar (RMS) often use `uint8` which only allow values in range 1..256.
+
+        Args:
+            project (str or roxar._project): Inside RMS use the magic 'project',
+                else use path to RMS project, or a project reference
+            gfile (str): Name of grid model
+            pfile (str): Name of grid property
+            projectname (str): Name of RMS project (None if inside a project)
+            realisation (int): Realisation number (default 0 first)
+            casting (str): This refers to numpy `astype(... casting=...)` settings.
+
+
+        .. versionchanged: 2.10
+            Key `saveproject` has been removed and will have no effect
+
+        .. versionadded:: 2.12 Key `casting` was added
+
+        """
+        _gridprop_roxapi.export_prop_roxapi(
+            self, project, gname, pname, realisation=realisation, casting=casting
+        )
+
+    # ==================================================================================
+    # Various public methods
+    # ==================================================================================
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout"""
+
+        dsc = xtgeo.common.XTGDescription()
+        dsc.title("Description of GridProperty instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("Name", self.name)
+        dsc.txt("Date", self.date)
+        dsc.txt("File source", self._filesrc)
+        dsc.txt("Discrete status", self._isdiscrete)
+        dsc.txt("Codes", self._codes)
+        dsc.txt("Shape: NCOL, NROW, NLAY", self.ncol, self.nrow, self.nlay)
+        np.set_printoptions(threshold=16)
+        dsc.txt("Values", self._values.reshape(-1), self._values.dtype)
+        np.set_printoptions(threshold=1000)
+        dsc.txt(
+            "Values, mean, stdev, minimum, maximum",
+            self.values.mean(),
+            self.values.std(),
+            self.values.min(),
+            self.values.max(),
+        )
+        itemsize = self.values.itemsize
+        msize = float(self.values.size * itemsize) / (1024 * 1024 * 1024)
+        dsc.txt("Roxar datatype", self.roxar_dtype)
+        dsc.txt("Minimum memory usage of array (GB)", msize)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    def get_npvalues3d(self, fill_value=None):
+        """Get a pure numpy copy (not masked) copy of the values, 3D shape.
+
+        Note that Numpy dtype will be reset; int32 if discrete or float64 if
+        continuous. The reason for this is to avoid inconsistensies regarding
+        UNDEF values.
+
+        If fill_value is not None, than the returning dtype is always `np.float64`.
+
+        Args:
+            fill_value: Value of masked entries. Default is None which
+                means the XTGeo UNDEF value (a high number), different
+                for a continuous or discrete property
+        """
+        # this is a function, not a property by design
+
+        if fill_value is None:
+            if self._isdiscrete:
+                fvalue = xtgeo.UNDEF_INT
+                dtype = np.int32
+            else:
+                fvalue = xtgeo.UNDEF
+                dtype = np.float64
+        else:
+            fvalue = fill_value
+            dtype = np.float64
+
+        val = self.values.copy().astype(dtype)
+        npv3d = np.ma.filled(val, fill_value=fvalue)
+        del val
+
+        return npv3d
+
+    def get_actnum(self, name="ACTNUM", asmasked=False, mask=None):
+        """Return an ACTNUM GridProperty object.
+
+        Note that this method is similar to, but not identical to,
+        the job with same name in Grid(). Here, the maskedarray of the values
+        is applied to deduce the ACTNUM array.
+
+        Args:
+            name (str): name of property in the XTGeo GridProperty object.
+            asmasked (bool): Actnum is returned with all cells shown
+                as default. Use asmasked=True to make 0 entries masked.
+            mask (bool): Deprecated, use asmasked instead!
+
+        Example::
+
+            act = mygrid.get_actnum()
+            print('{}% cells are active'.format(act.values.mean() * 100))
+        """
+
+        if mask is not None:
+            xtg.warndeprecated(
+                "The mask option is deprecated,"
+                "and will be removed in version 4.0. Use asmasked instead."
+            )
+            asmasked = super()._evaluate_mask(mask)
+
+        act = GridProperty(
+            ncol=self._ncol, nrow=self._nrow, nlay=self._nlay, name=name, discrete=True
+        )
+
+        orig = self.values
+        vact = np.ones(self.values.shape)
+        vact[orig.mask] = 0
+
+        if asmasked:
+            vact = np.ma.masked_equal(vact, 0)
+
+        act.values = vact.astype(np.int32)
+        act.isdiscrete = True
+        act.codes = {0: "0", 1: "1"}
+
+        # return the object
+        return act
+
+    def get_active_npvalues1d(self):
+        """Return the grid property as a 1D numpy array (copy), active
+        cells only.
+        """
+
+        return self.get_npvalues1d(activeonly=True)
+
+    def get_npvalues1d(self, activeonly=False, fill_value=np.nan, order="C"):
+        """Return the grid property as a 1D numpy array (copy) for active or all
+        cells, but inactive have a fill value.
+
+        Args:
+            activeonly (bool): If True, then only return active cells
+            fill_value (float): Fill value for inactive cells
+            order (str): Array internal order; default is "C", alternative is "F"
+
+        .. versionadded:: 2.3
+        .. versionchanged:: 2.8 Added `fill_value` and `order`
+        """
+        vact = self.values1d.copy()
+
+        if order == "F":
+            vact = _gridprop_lowlevel.c2f_order(self, vact)
+
+        if activeonly:
+            return vact.compressed()  # safer than vact[~vact.mask] if no masked
+
+        return vact.filled(fill_value)
+
+    def copy(self, newname=None):
+        """Copy a xtgeo.grid3d.GridProperty() object to another instance.
+
+        ::
+
+            >>> import xtgeo
+            >>> myporo = xtgeo.gridproperty_from_file(
+            ...    reek_dir + '/reek_sim_poro.roff',
+            ...    name="PORO"
+            ... )
+            >>> mycopy = myporo.copy(newname='XPROP')
+            >>> print(mycopy.name)
+            XPROP
+
+        """
+
+        if newname is None:
+            newname = self.name
+
+        xprop = GridProperty(
+            ncol=self._ncol,
+            nrow=self._nrow,
+            nlay=self._nlay,
+            values=self._values.copy(),
+            name=newname,
+        )
+
+        xprop.geometry = self._geometry
+        xprop.isdiscrete = self._isdiscrete
+        xprop.codes = self._codes
+        xprop.date = self._date
+        xprop.roxorigin = self._roxorigin
+        xprop.roxar_dtype = self._roxar_dtype
+
+        xprop.filesrc = self._filesrc
+
+        return xprop
+
+    def mask_undef(self):
+        """Make UNDEF values masked."""
+        if self._isdiscrete:
+            self._values = np.ma.masked_greater(self._values, xtgeo.UNDEF_INT_LIMIT)
+        else:
+            self._values = np.ma.masked_greater(self._values, xtgeo.UNDEF_LIMIT)
+
+    def crop(self, spec):
+        """Crop a property, see method under grid"""
+
+        (ic1, ic2), (jc1, jc2), (kc1, kc2) = spec
+
+        # compute size of new cropped grid
+        self._ncol = ic2 - ic1 + 1
+        self._nrow = jc2 - jc1 + 1
+        self._nlay = kc2 - kc1 + 1
+
+        newvalues = self.values.copy()
+
+        self.values = newvalues[ic1 - 1 : ic2, jc1 - 1 : jc2, kc1 - 1 : kc2]
+
+    def get_xy_value_lists(self, grid=None, activeonly=True):
+        """Get lists of xy coords and values for Webportal format.
+
+        The coordinates are on the form (two cells)::
+
+            [[[(x1,y1), (x2,y2), (x3,y3), (x4,y4)],
+            [(x5,y5), (x6,y6), (x7,y7), (x8,y8)]]]
+
+        Args:
+            grid (object): The XTGeo Grid object for the property
+            activeonly (bool): If true (default), active cells only,
+                otherwise cell geometries will be listed and property will
+                have value -999 in undefined cells.
+
+        Example::
+
+            grid = Grid()
+            grid.from_file('../xtgeo-testdata/3dgrids/bri/b_grid.roff')
+            prop = GridProperty()
+            prop.from_file('../xtgeo-testdata/3dgrids/bri/b_poro.roff',
+                           grid=grid, name='PORO')
+
+            clist, valuelist = prop.get_xy_value_lists(grid=grid,
+                                                       activeonly=False)
+
+
+        """
+
+        clist, vlist = _gridprop_op1.get_xy_value_lists(
+            self, grid=grid, mask=activeonly
+        )
+        return clist, vlist
+
+    def get_values_by_ijk(self, iarr, jarr, karr, base=1):
+        """Get a 1D ndarray of values by I J K arrays.
+
+        This could for instance be a well path where I J K
+        exists as well logs.
+
+        Note that the input arrays have 1 as base as default
+
+        Args:
+            iarr (ndarray): Numpy array of I
+            jarr (ndarray): Numpy array of J
+            karr (ndarray): Numpy array of K
+            base (int): Should be 1 or 0, dependent on what
+                number base the input arrays has.
+
+        Returns:
+            pvalues (ndarray): A 1D numpy array of property values,
+                with NaN if undefined
+
+        """
+        res = np.zeros(iarr.shape, dtype="float64")
+        res = np.ma.masked_equal(res, 0)  # mask all
+
+        # get indices where defined (note the , after valids)
+        (valids,) = np.where(~np.isnan(iarr))
+
+        iarr = iarr[~np.isnan(iarr)]
+        jarr = jarr[~np.isnan(jarr)]
+        karr = karr[~np.isnan(karr)]
+
+        try:
+            res[valids] = self.values[
+                iarr.astype("int") - base,
+                jarr.astype("int") - base,
+                karr.astype("int") - base,
+            ]
+
+            return np.ma.filled(res, fill_value=np.nan)
+
+        except IndexError as ier:
+            xtg.warn(f"Error {ier}, return None")
+            return None
+        except:  # noqa
+            xtg.warn("Unexpected error")
+            raise
+
+    def discrete_to_continuous(self):
+        """Convert from discrete to continuous values"""
+
+        if self.isdiscrete:
+            logger.info("Converting to continuous ...")
+            val = self._values.copy()
+            val = val.astype("float64")
+            self._values = val
+            self._isdiscrete = False
+            self._codes = {}
+            self._roxar_dtype = np.float32
+        else:
+            logger.info("No need to convert, already continuous")
+
+    def continuous_to_discrete(self):
+        """Convert from continuous to discrete values"""
+
+        if not self.isdiscrete:
+            logger.info("Converting to discrete ...")
+            val = self._values.copy()
+            val = val.astype(np.int32)
+            self._values = val
+            self._isdiscrete = True
+
+            # make the code list
+            uniq = np.unique(val).tolist()
+            codes = dict(zip(uniq, uniq))
+            codes = {k: str(v) for k, v in codes.items()}  # val as strings
+            self._codes = codes
+            self._roxar_dtype = np.uint16
+        else:
+            logger.info("No need to convert, already discrete")
+
+    # ==================================================================================
+    # Operations restricted to inside/outside polygons
+    # ==================================================================================
+
+    def operation_polygons(self, poly, value, opname="add", inside=True):
+        """A generic function for doing 3D grid property operations
+        restricted to inside or outside polygon(s).
+
+        This method requires that the property geometry is known
+        (prop.geometry is set to a grid instance)
+
+        Args:
+            poly (Polygons): A XTGeo Polygons instance
+            value (float): Value to add, subtract etc
+            opname (str): Name of operation... 'add', 'sub', etc
+            inside (bool): If True do operation inside polygons; else outside.
+        """
+
+        if self.geometry is None:
+            msg = """
+            You need to link the property to a grid geometry:"
+
+                myprop.geometry = mygrid
+
+            """
+            xtg.warnuser(msg)
+            raise ValueError("The geometry attribute is not set")
+
+        _gridprop_op1.operation_polygons(
+            self, poly, value, opname=opname, inside=inside
+        )
+
+    # shortforms
+    def add_inside(self, poly, value):
+        """Add a value (scalar) inside polygons"""
+        self.operation_polygons(poly, value, opname="add", inside=True)
+
+    def add_outside(self, poly, value):
+        """Add a value (scalar) outside polygons"""
+        self.operation_polygons(poly, value, opname="add", inside=False)
+
+    def sub_inside(self, poly, value):
+        """Subtract a value (scalar) inside polygons"""
+        self.operation_polygons(poly, value, opname="sub", inside=True)
+
+    def sub_outside(self, poly, value):
+        """Subtract a value (scalar) outside polygons"""
+        self.operation_polygons(poly, value, opname="sub", inside=False)
+
+    def mul_inside(self, poly, value):
+        """Multiply a value (scalar) inside polygons"""
+        self.operation_polygons(poly, value, opname="mul", inside=True)
+
+    def mul_outside(self, poly, value):
+        """Multiply a value (scalar) outside polygons"""
+        self.operation_polygons(poly, value, opname="mul", inside=False)
+
+    def div_inside(self, poly, value):
+        """Divide a value (scalar) inside polygons"""
+        self.operation_polygons(poly, value, opname="div", inside=True)
+
+    def div_outside(self, poly, value):
+        """Divide a value (scalar) outside polygons"""
+        self.operation_polygons(poly, value, opname="div", inside=False)
+
+    def set_inside(self, poly, value):
+        """Set a value (scalar) inside polygons"""
+        self.operation_polygons(poly, value, opname="set", inside=True)
+
+    def set_outside(self, poly, value):
+        """Set a value (scalar) outside polygons"""
+        self.operation_polygons(poly, value, opname="set", inside=False)
```

## xtgeo/metadata/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-# -*- coding: utf-8 -*-
-# flake8: noqa
-"""XTGeo metadata package."""
-
-from xtgeo.metadata.metadata import MetaDataRegularSurface
-from xtgeo.metadata.metadata import MetaDataRegularCube
+# -*- coding: utf-8 -*-
+# flake8: noqa
+"""XTGeo metadata package."""
+
+from xtgeo.metadata.metadata import MetaDataRegularSurface
+from xtgeo.metadata.metadata import MetaDataRegularCube
```

## xtgeo/metadata/metadata.py

 * *Ordering differences only*

```diff
@@ -1,444 +1,444 @@
-# -*- coding: utf-8 -*-
-"""The metadata module, currently experimental.
-
-The metadata works through the various datatypes in XTGeo. For example::
-
-    >>> import xtgeo
-    >>> surf = xtgeo.surface_from_file(surface_dir + "/topreek_rota.gri")
-    >>> surf.metadata.required
-    OrderedDict([('ncol', 554),...
-    >>> surf.metadata.optional.mean = surf.values.mean()
-
-"""
-# import datetime
-from collections import OrderedDict
-
-# from datetime import date
-import xtgeo
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class _OptionalMetaData:
-    """Optional metadata are not required, but keys are limited.
-
-    A limited sets of possible keys are available, and they can modified. This
-    class can also have validation methods.
-    """
-
-    __slots__ = (
-        "_name",
-        "_shortname",
-        "_datatype",
-        "_md5sum",
-        "_description",
-        "_crs",
-        "_datetime",
-        "_deltadatetime",
-        "_visuals",
-        "_domain",
-        "_user",
-        "_field",
-        "_source",
-        "_modelid",
-        "_ensembleid",
-        "_units",
-        "_mean",
-        "_stddev",
-        "_percentiles",
-    )
-
-    def __init__(self) -> None:
-        self._name = "A Longer Descriptive Name e.g. from SMDA"
-        self._shortname = "TheShortName"
-        self._datatype = None
-        self._md5sum = None
-        self._description = "Some description"
-        self._crs = None
-        self._datetime = None
-        self._deltadatetime = None
-        self._visuals = {"colortable": "rainbow", "lower": None, "upper": None}
-        self._domain = "depth"
-        self._units = "metric"
-        self._mean = None
-        self._stddev = None
-        self._percentiles = None
-        self._user = "anonymous"
-        self._field = "nofield"
-        self._ensembleid = None
-        self._modelid = None
-        self._source = "unknown"
-
-    @property
-    def name(self):
-        return self._name
-
-    @name.setter
-    def name(self, newname):
-        # TODO: validation
-        self._name = newname
-
-    @property
-    def datetime(self):
-        return self._datetime
-
-    @datetime.setter
-    def datetime(self, newdate):
-        # TODO: validation
-        self._datetime = newdate
-
-    @property
-    def shortname(self):
-        return self._shortname
-
-    @shortname.setter
-    def shortname(self, newname):
-        if not isinstance(newname, str):
-            raise ValueError("The shortname must be a string.")
-        if len(newname) >= 32:
-            raise ValueError("The shortname length must less or equal 32 letters.")
-
-        self._shortname = newname
-
-    @property
-    def description(self):
-        return self._description
-
-    @description.setter
-    def description(self, newstr):
-        if not isinstance(newstr, str):
-            raise ValueError("The description must be a string.")
-        if len(newstr) >= 64:
-            raise ValueError("The description length must less or equal 64 letters.")
-        invalids = r"/$<>[]:\&%"
-        if set(invalids).intersection(newstr):
-            raise ValueError("The description constains invalid characters such as /.")
-
-        self._description = newstr
-
-    @property
-    def md5sum(self):
-        """Set or get the md5 checksum of file content.
-
-        See generate_hash() method in e.g. RegularSurface.
-        """
-        return self._md5sum
-
-    @md5sum.setter
-    def md5sum(self, newhash):
-        # TODO: validation
-        self._md5sum = newhash
-
-    def get_meta(self):
-        """Return metadata as an OrderedDict."""
-        meta = OrderedDict()
-        for key in self.__slots__:
-            newkey = key[1:]
-            meta[newkey] = getattr(self, key)
-
-        return meta
-
-
-class MetaData:
-    """Generic metadata class, not intended to be used directly."""
-
-    def __init__(self):
-        """Generic metadata class __init__, not be used directly."""
-        self._required = OrderedDict()
-        self._optional = _OptionalMetaData()
-        self._freeform = OrderedDict()
-
-        self._freeform = {"smda": "whatever"}
-
-    def get_metadata(self):
-        """Get all metadata that are present."""
-        allmeta = OrderedDict()
-        allmeta["_required_"] = self._required
-        allmeta["_optional_"] = self._optional.get_meta()
-        allmeta["_freeform_"] = self._freeform
-        return allmeta
-
-    @property
-    def optional(self):
-        """Return or set optional metadata.
-
-        When setting optional names, it can be done in several ways...
-
-        surf.metadata.optional.name = "New name"
-        """
-        # return a copy of the instance; the reason for this is to avoid manipulation
-        # without validation
-        return self._optional.get_meta()
-
-    @optional.setter
-    def optional(self, indict):
-        # setting the optional key, including validation
-        if not isinstance(indict, dict):
-            raise ValueError(f"Input must be a dictionary, not a {type(indict)}")
-
-        for key, value in indict.items():
-            setattr(self._optional, "_" + key, value)
-
-    @property
-    def opt(self):
-        """Return the metadata optional instance.
-
-        This makes access to the _OptionalMetaData instance.
-
-        Example::
-            >>> import xtgeo
-            >>> surf = xtgeo.surface_from_file(surface_dir + "/topreek_rota.gri")
-            >>> surf.metadata.opt.shortname = "TopValysar"
-
-        """
-        return self._optional
-
-    @optional.setter
-    def optional(self, indict):
-        # setting the optional key, including validation
-        if not isinstance(indict, dict):
-            raise ValueError(f"Input must be a dictionary, not a {type(indict)}")
-
-        for key, value in indict.items():
-            setattr(self._optional, "_" + key, value)
-
-    @property
-    def freeform(self):
-        """Get or set the current freeform metadata dictionary."""
-        return self._freeform
-
-    @freeform.setter
-    def freeform(self, adict):
-        """Freeform is a whatever you want set, without any validation."""
-        self._freeform = adict.copy()
-
-    def generate_fmu_name(self):
-        """Generate FMU name on form xxxx--yyyy--date but no suffix."""
-        fname = ""
-        first = "prefix"
-        fname += first
-        fname += "--"
-        fname += self._optional._shortname.lower()
-        if self._optional._datetime:
-            fname += "--"
-            fname += str(self._optional._datetime)
-        return fname
-
-
-class MetaDataRegularSurface(MetaData):
-    """Metadata for RegularSurface() objects."""
-
-    REQUIRED = OrderedDict(
-        [
-            ("ncol", 1),
-            ("nrow", 1),
-            ("xori", 0.0),
-            ("yori", 0.0),
-            ("xinc", 1.0),
-            ("yinc", 1.0),
-            ("yflip", 1),
-            ("rotation", 0.0),
-            ("undef", xtgeo.UNDEF),
-        ]
-    )
-
-    def __init__(self):
-        """Docstring."""
-        super().__init__()
-        self._required = __class__.REQUIRED
-        self._optional._datatype = "Regular Surface"
-
-    @property
-    def required(self):
-        """Get of set required metadata."""
-        return self._required
-
-    @required.setter
-    def required(self, obj):
-        if not isinstance(obj, xtgeo.RegularSurface):
-            raise ValueError("Input object is not a RegularSurface()")
-
-        self._required["ncol"] = obj.ncol
-        self._required["nrow"] = obj.nrow
-        self._required["xori"] = obj.xori
-        self._required["yori"] = obj.yori
-        self._required["xinc"] = obj.xinc
-        self._required["yinc"] = obj.yinc
-        self._required["yflip"] = obj.yflip
-        self._required["rotation"] = obj.rotation
-        self._required["undef"] = obj.undef
-
-
-class MetaDataRegularCube(MetaData):
-    """Metadata for Cube() objects."""
-
-    # allowed optional keys; these are set to avoid discussions
-    REQUIRED = OrderedDict(
-        [
-            ("ncol", 1),
-            ("nrow", 1),
-            ("nlay", 1),
-            ("xori", 0.0),
-            ("yori", 0.0),
-            ("zori", 0.0),
-            ("xinc", 1.0),
-            ("yinc", 1.0),
-            ("zinc", 1.0),
-            ("yflip", 1),
-            ("zflip", 1),
-            ("rotation", 0.0),
-            ("undef", xtgeo.UNDEF),
-        ]
-    )
-
-    def __init__(self):
-        """Docstring."""
-        super().__init__()
-        self._required = __class__.REQUIRED
-        self._optional._datatype = "Regular Cube"
-
-    @property
-    def required(self):
-        """Get of set required metadata."""
-        return self._required
-
-    @required.setter
-    def required(self, obj):
-        if not isinstance(obj, xtgeo.Cube):
-            raise ValueError("Input object is not a regular Cube()")
-
-        self._required["ncol"] = obj.ncol
-        self._required["nrow"] = obj.nrow
-        self._required["nlay"] = obj.nlay
-        self._required["xori"] = obj.xori
-        self._required["yori"] = obj.yori
-        self._required["zori"] = obj.zori
-        self._required["xinc"] = obj.xinc
-        self._required["yinc"] = obj.yinc
-        self._required["zinc"] = obj.zinc
-        self._required["yflip"] = obj.yflip
-        self._required["zflip"] = 1
-        self._required["rotation"] = obj.rotation
-        self._required["undef"] = obj.undef
-
-
-class MetaDataCPGeometry(MetaData):
-    """Metadata for Grid() objects of type simplified CornerPoint Geometry."""
-
-    REQUIRED = OrderedDict(
-        [
-            ("ncol", 1),
-            ("nrow", 1),
-            ("nlay", 1),
-            ("xshift", 0.0),
-            ("yshift", 0.0),
-            ("zshift", 0.0),
-            ("xscale", 1.0),
-            ("yscale", 1.0),
-            ("zscale", 1.0),
-        ]
-    )
-
-    def __init__(self):
-        """Docstring."""
-        super().__init__()
-        self._required = __class__.REQUIRED
-        self._optional._datatype = "CornerPoint GridGeometry"
-
-    @property
-    def required(self):
-        """Get of set required metadata."""
-        return self._required
-
-    @required.setter
-    def required(self, obj):
-        if not isinstance(obj, xtgeo.Grid):
-            raise ValueError("Input object is not a Grid()")
-
-        self._required["ncol"] = obj.ncol
-        self._required["nrow"] = obj.nrow
-        self._required["nlay"] = obj.nlay
-        self._required["xshift"] = 0.0  # hardcoded so far
-        self._required["yshift"] = 0.0
-        self._required["zshift"] = 0.0
-        self._required["xscale"] = 1.0
-        self._required["yscale"] = 1.0
-        self._required["zscale"] = 1.0
-        self._required["subgrids"] = obj.get_subgrids()
-
-
-class MetaDataCPProperty(MetaData):
-    """Metadata for GridProperty() objects belonging to CPGeometry."""
-
-    REQUIRED = OrderedDict(
-        [
-            ("ncol", 1),
-            ("nrow", 1),
-            ("nlay", 1),
-            ("codes", None),
-            ("discrete", False),
-        ]
-    )
-
-    def __init__(self):
-        """Docstring."""
-        super().__init__()
-        self._required = __class__.REQUIRED
-        self._optional._datatype = "CornerPoint GridProperty"
-
-    @property
-    def required(self):
-        """Get of set required metadata."""
-        return self._required
-
-    @required.setter
-    def required(self, obj):
-        if not isinstance(obj, xtgeo.GridProperty):
-            raise ValueError("Input object is not a GridProperty()")
-
-        self._required["ncol"] = obj.ncol
-        self._required["nrow"] = obj.nrow
-        self._required["nlay"] = obj.nlay
-        self._required["codes"] = obj.codes
-        self._required["discrete"] = obj.isdiscrete
-
-
-class MetaDataWell(MetaData):
-    """Metadata for single Well() objects."""
-
-    REQUIRED = OrderedDict(
-        [
-            ("rkb", 0.0),
-            ("xpos", 0.0),
-            ("ypos", 0.0),
-            ("name", "noname"),
-            ("wlogs", dict()),
-            ("mdlogname", None),
-            ("zonelogname", None),
-        ]
-    )
-
-    def __init__(self):
-        """Initialisation for Well metadata."""
-        super().__init__()
-        self._required = __class__.REQUIRED
-        self._optional._datatype = "Well"
-
-    @property
-    def required(self):
-        """Get of set required metadata."""
-        return self._required
-
-    @required.setter
-    def required(self, obj):
-        if not isinstance(obj, xtgeo.Well):
-            raise ValueError("Input object is not a Well() instance!")
-
-        self._required["rkb"] = obj.rkb
-        self._required["xpos"] = obj.xpos
-        self._required["ypos"] = obj.ypos
-        self._required["name"] = obj.wname
-        self._required["wlogs"] = obj.get_wlogs()
-        self._required["mdlogname"] = obj.mdlogname
-        self._required["zonelogname"] = obj.zonelogname
+# -*- coding: utf-8 -*-
+"""The metadata module, currently experimental.
+
+The metadata works through the various datatypes in XTGeo. For example::
+
+    >>> import xtgeo
+    >>> surf = xtgeo.surface_from_file(surface_dir + "/topreek_rota.gri")
+    >>> surf.metadata.required
+    OrderedDict([('ncol', 554),...
+    >>> surf.metadata.optional.mean = surf.values.mean()
+
+"""
+# import datetime
+from collections import OrderedDict
+
+# from datetime import date
+import xtgeo
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class _OptionalMetaData:
+    """Optional metadata are not required, but keys are limited.
+
+    A limited sets of possible keys are available, and they can modified. This
+    class can also have validation methods.
+    """
+
+    __slots__ = (
+        "_name",
+        "_shortname",
+        "_datatype",
+        "_md5sum",
+        "_description",
+        "_crs",
+        "_datetime",
+        "_deltadatetime",
+        "_visuals",
+        "_domain",
+        "_user",
+        "_field",
+        "_source",
+        "_modelid",
+        "_ensembleid",
+        "_units",
+        "_mean",
+        "_stddev",
+        "_percentiles",
+    )
+
+    def __init__(self) -> None:
+        self._name = "A Longer Descriptive Name e.g. from SMDA"
+        self._shortname = "TheShortName"
+        self._datatype = None
+        self._md5sum = None
+        self._description = "Some description"
+        self._crs = None
+        self._datetime = None
+        self._deltadatetime = None
+        self._visuals = {"colortable": "rainbow", "lower": None, "upper": None}
+        self._domain = "depth"
+        self._units = "metric"
+        self._mean = None
+        self._stddev = None
+        self._percentiles = None
+        self._user = "anonymous"
+        self._field = "nofield"
+        self._ensembleid = None
+        self._modelid = None
+        self._source = "unknown"
+
+    @property
+    def name(self):
+        return self._name
+
+    @name.setter
+    def name(self, newname):
+        # TODO: validation
+        self._name = newname
+
+    @property
+    def datetime(self):
+        return self._datetime
+
+    @datetime.setter
+    def datetime(self, newdate):
+        # TODO: validation
+        self._datetime = newdate
+
+    @property
+    def shortname(self):
+        return self._shortname
+
+    @shortname.setter
+    def shortname(self, newname):
+        if not isinstance(newname, str):
+            raise ValueError("The shortname must be a string.")
+        if len(newname) >= 32:
+            raise ValueError("The shortname length must less or equal 32 letters.")
+
+        self._shortname = newname
+
+    @property
+    def description(self):
+        return self._description
+
+    @description.setter
+    def description(self, newstr):
+        if not isinstance(newstr, str):
+            raise ValueError("The description must be a string.")
+        if len(newstr) >= 64:
+            raise ValueError("The description length must less or equal 64 letters.")
+        invalids = r"/$<>[]:\&%"
+        if set(invalids).intersection(newstr):
+            raise ValueError("The description constains invalid characters such as /.")
+
+        self._description = newstr
+
+    @property
+    def md5sum(self):
+        """Set or get the md5 checksum of file content.
+
+        See generate_hash() method in e.g. RegularSurface.
+        """
+        return self._md5sum
+
+    @md5sum.setter
+    def md5sum(self, newhash):
+        # TODO: validation
+        self._md5sum = newhash
+
+    def get_meta(self):
+        """Return metadata as an OrderedDict."""
+        meta = OrderedDict()
+        for key in self.__slots__:
+            newkey = key[1:]
+            meta[newkey] = getattr(self, key)
+
+        return meta
+
+
+class MetaData:
+    """Generic metadata class, not intended to be used directly."""
+
+    def __init__(self):
+        """Generic metadata class __init__, not be used directly."""
+        self._required = OrderedDict()
+        self._optional = _OptionalMetaData()
+        self._freeform = OrderedDict()
+
+        self._freeform = {"smda": "whatever"}
+
+    def get_metadata(self):
+        """Get all metadata that are present."""
+        allmeta = OrderedDict()
+        allmeta["_required_"] = self._required
+        allmeta["_optional_"] = self._optional.get_meta()
+        allmeta["_freeform_"] = self._freeform
+        return allmeta
+
+    @property
+    def optional(self):
+        """Return or set optional metadata.
+
+        When setting optional names, it can be done in several ways...
+
+        surf.metadata.optional.name = "New name"
+        """
+        # return a copy of the instance; the reason for this is to avoid manipulation
+        # without validation
+        return self._optional.get_meta()
+
+    @optional.setter
+    def optional(self, indict):
+        # setting the optional key, including validation
+        if not isinstance(indict, dict):
+            raise ValueError(f"Input must be a dictionary, not a {type(indict)}")
+
+        for key, value in indict.items():
+            setattr(self._optional, "_" + key, value)
+
+    @property
+    def opt(self):
+        """Return the metadata optional instance.
+
+        This makes access to the _OptionalMetaData instance.
+
+        Example::
+            >>> import xtgeo
+            >>> surf = xtgeo.surface_from_file(surface_dir + "/topreek_rota.gri")
+            >>> surf.metadata.opt.shortname = "TopValysar"
+
+        """
+        return self._optional
+
+    @optional.setter
+    def optional(self, indict):
+        # setting the optional key, including validation
+        if not isinstance(indict, dict):
+            raise ValueError(f"Input must be a dictionary, not a {type(indict)}")
+
+        for key, value in indict.items():
+            setattr(self._optional, "_" + key, value)
+
+    @property
+    def freeform(self):
+        """Get or set the current freeform metadata dictionary."""
+        return self._freeform
+
+    @freeform.setter
+    def freeform(self, adict):
+        """Freeform is a whatever you want set, without any validation."""
+        self._freeform = adict.copy()
+
+    def generate_fmu_name(self):
+        """Generate FMU name on form xxxx--yyyy--date but no suffix."""
+        fname = ""
+        first = "prefix"
+        fname += first
+        fname += "--"
+        fname += self._optional._shortname.lower()
+        if self._optional._datetime:
+            fname += "--"
+            fname += str(self._optional._datetime)
+        return fname
+
+
+class MetaDataRegularSurface(MetaData):
+    """Metadata for RegularSurface() objects."""
+
+    REQUIRED = OrderedDict(
+        [
+            ("ncol", 1),
+            ("nrow", 1),
+            ("xori", 0.0),
+            ("yori", 0.0),
+            ("xinc", 1.0),
+            ("yinc", 1.0),
+            ("yflip", 1),
+            ("rotation", 0.0),
+            ("undef", xtgeo.UNDEF),
+        ]
+    )
+
+    def __init__(self):
+        """Docstring."""
+        super().__init__()
+        self._required = __class__.REQUIRED
+        self._optional._datatype = "Regular Surface"
+
+    @property
+    def required(self):
+        """Get of set required metadata."""
+        return self._required
+
+    @required.setter
+    def required(self, obj):
+        if not isinstance(obj, xtgeo.RegularSurface):
+            raise ValueError("Input object is not a RegularSurface()")
+
+        self._required["ncol"] = obj.ncol
+        self._required["nrow"] = obj.nrow
+        self._required["xori"] = obj.xori
+        self._required["yori"] = obj.yori
+        self._required["xinc"] = obj.xinc
+        self._required["yinc"] = obj.yinc
+        self._required["yflip"] = obj.yflip
+        self._required["rotation"] = obj.rotation
+        self._required["undef"] = obj.undef
+
+
+class MetaDataRegularCube(MetaData):
+    """Metadata for Cube() objects."""
+
+    # allowed optional keys; these are set to avoid discussions
+    REQUIRED = OrderedDict(
+        [
+            ("ncol", 1),
+            ("nrow", 1),
+            ("nlay", 1),
+            ("xori", 0.0),
+            ("yori", 0.0),
+            ("zori", 0.0),
+            ("xinc", 1.0),
+            ("yinc", 1.0),
+            ("zinc", 1.0),
+            ("yflip", 1),
+            ("zflip", 1),
+            ("rotation", 0.0),
+            ("undef", xtgeo.UNDEF),
+        ]
+    )
+
+    def __init__(self):
+        """Docstring."""
+        super().__init__()
+        self._required = __class__.REQUIRED
+        self._optional._datatype = "Regular Cube"
+
+    @property
+    def required(self):
+        """Get of set required metadata."""
+        return self._required
+
+    @required.setter
+    def required(self, obj):
+        if not isinstance(obj, xtgeo.Cube):
+            raise ValueError("Input object is not a regular Cube()")
+
+        self._required["ncol"] = obj.ncol
+        self._required["nrow"] = obj.nrow
+        self._required["nlay"] = obj.nlay
+        self._required["xori"] = obj.xori
+        self._required["yori"] = obj.yori
+        self._required["zori"] = obj.zori
+        self._required["xinc"] = obj.xinc
+        self._required["yinc"] = obj.yinc
+        self._required["zinc"] = obj.zinc
+        self._required["yflip"] = obj.yflip
+        self._required["zflip"] = 1
+        self._required["rotation"] = obj.rotation
+        self._required["undef"] = obj.undef
+
+
+class MetaDataCPGeometry(MetaData):
+    """Metadata for Grid() objects of type simplified CornerPoint Geometry."""
+
+    REQUIRED = OrderedDict(
+        [
+            ("ncol", 1),
+            ("nrow", 1),
+            ("nlay", 1),
+            ("xshift", 0.0),
+            ("yshift", 0.0),
+            ("zshift", 0.0),
+            ("xscale", 1.0),
+            ("yscale", 1.0),
+            ("zscale", 1.0),
+        ]
+    )
+
+    def __init__(self):
+        """Docstring."""
+        super().__init__()
+        self._required = __class__.REQUIRED
+        self._optional._datatype = "CornerPoint GridGeometry"
+
+    @property
+    def required(self):
+        """Get of set required metadata."""
+        return self._required
+
+    @required.setter
+    def required(self, obj):
+        if not isinstance(obj, xtgeo.Grid):
+            raise ValueError("Input object is not a Grid()")
+
+        self._required["ncol"] = obj.ncol
+        self._required["nrow"] = obj.nrow
+        self._required["nlay"] = obj.nlay
+        self._required["xshift"] = 0.0  # hardcoded so far
+        self._required["yshift"] = 0.0
+        self._required["zshift"] = 0.0
+        self._required["xscale"] = 1.0
+        self._required["yscale"] = 1.0
+        self._required["zscale"] = 1.0
+        self._required["subgrids"] = obj.get_subgrids()
+
+
+class MetaDataCPProperty(MetaData):
+    """Metadata for GridProperty() objects belonging to CPGeometry."""
+
+    REQUIRED = OrderedDict(
+        [
+            ("ncol", 1),
+            ("nrow", 1),
+            ("nlay", 1),
+            ("codes", None),
+            ("discrete", False),
+        ]
+    )
+
+    def __init__(self):
+        """Docstring."""
+        super().__init__()
+        self._required = __class__.REQUIRED
+        self._optional._datatype = "CornerPoint GridProperty"
+
+    @property
+    def required(self):
+        """Get of set required metadata."""
+        return self._required
+
+    @required.setter
+    def required(self, obj):
+        if not isinstance(obj, xtgeo.GridProperty):
+            raise ValueError("Input object is not a GridProperty()")
+
+        self._required["ncol"] = obj.ncol
+        self._required["nrow"] = obj.nrow
+        self._required["nlay"] = obj.nlay
+        self._required["codes"] = obj.codes
+        self._required["discrete"] = obj.isdiscrete
+
+
+class MetaDataWell(MetaData):
+    """Metadata for single Well() objects."""
+
+    REQUIRED = OrderedDict(
+        [
+            ("rkb", 0.0),
+            ("xpos", 0.0),
+            ("ypos", 0.0),
+            ("name", "noname"),
+            ("wlogs", dict()),
+            ("mdlogname", None),
+            ("zonelogname", None),
+        ]
+    )
+
+    def __init__(self):
+        """Initialisation for Well metadata."""
+        super().__init__()
+        self._required = __class__.REQUIRED
+        self._optional._datatype = "Well"
+
+    @property
+    def required(self):
+        """Get of set required metadata."""
+        return self._required
+
+    @required.setter
+    def required(self, obj):
+        if not isinstance(obj, xtgeo.Well):
+            raise ValueError("Input object is not a Well() instance!")
+
+        self._required["rkb"] = obj.rkb
+        self._required["xpos"] = obj.xpos
+        self._required["ypos"] = obj.ypos
+        self._required["name"] = obj.wname
+        self._required["wlogs"] = obj.get_wlogs()
+        self._required["mdlogname"] = obj.mdlogname
+        self._required["zonelogname"] = obj.zonelogname
```

## xtgeo/plot/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-# -*- coding: utf-8 -*-
-"""The XTGeo plot package"""
-
-
-# flake8: noqa
-from xtgeo.plot.xsection import XSection
-from xtgeo.plot.xtmap import Map
-from xtgeo.plot.grid3d_slice import Grid3DSlice
-
-# from ._colortables import random, random40, xtgeocolors, colorsfromfile
+# -*- coding: utf-8 -*-
+"""The XTGeo plot package"""
+
+
+# flake8: noqa
+from xtgeo.plot.xsection import XSection
+from xtgeo.plot.xtmap import Map
+from xtgeo.plot.grid3d_slice import Grid3DSlice
+
+# from ._colortables import random, random40, xtgeocolors, colorsfromfile
```

## xtgeo/plot/_colortables.py

 * *Ordering differences only*

```diff
@@ -1,355 +1,355 @@
-# -*- coding: utf-8 -*-
-"""Module for color tables."""
-
-import os
-import random
-
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def random40():
-    """Random but fixed color table with 40 entries, mostly for testing."""
-
-    ctable = [
-        (0.83345071168299956, 0.2382011018575051, 0.6457693086399422),
-        (0.541018453798349, 0.48038587236806063, 0.9151873938263322),
-        (0.10882072036344093, 0.8838633180878654, 0.5133890790690832),
-        (0.0021118859288566, 0.23207523745480785, 0.87105494758907984),
-        (0.9167955510272826, 0.06636986706409276, 0.2085761696435795),
-        (0.95445330127180916, 0.92537092140778787, 0.5643674283342163),
-        (0.1118142610865344, 0.8753812704140146, 0.882396282596847),
-        (0.9169300859386396, 0.94501442557231312, 0.01407158657305927),
-        (0.878693250908883, 0.11222139418674728, 0.6167801456569036),
-        (0.5256939400784498, 0.11435373675297966, 0.48354476358995946),
-        (0.48340383516545826, 0.6410572769763058, 0.29403657486119683),
-        (0.311139625063827, 0.9263058894491734, 0.7287039866021682),
-        (0.8598753478111007, 0.9018735026443733, 0.6313058592961296),
-        (0.43793800204923883, 0.2921469762701936, 0.2098830453305408),
-        (0.7381272212655949, 0.1156423111704514, 0.2452716196308139),
-        (0.21717885649022806, 0.3046216797738889, 0.6714686237358712),
-        (0.0070595461262333, 0.714345667698068, 0.34723053474857124),
-        (0.9745984699708928, 0.4730239548162848, 0.15007860849878418),
-        (0.6833692521550699, 0.4228448482259841, 0.0044103854819418364),
-        (0.5691320165209793, 0.08796666338494852, 0.8035539289654177),
-        (0.34042801546149426, 0.47713419001045765, 0.20879105326096692),
-        (0.3085826005434883, 0.5079488187213768, 0.4608596310424611),
-        (0.9396543635576965, 0.0064011918231481335, 0.2812656314040958),
-        (0.5217398018516609, 0.2575024696206665, 0.4677374482174591),
-        (0.42127435507233524, 0.8568338738437583, 0.7638805644700677),
-        (0.09602707599420568, 0.3864069927590945, 0.6039576328011579),
-        (0.16897839817549498, 0.7065043632174942, 0.8423968298858847),
-        (0.2986608595521294, 0.6791125573730088, 0.5589263091038545),
-        (0.445032995863182, 0.3024296426041809, 0.887087647343199),
-        (0.40061174588633763, 0.8547027289892815, 0.822969921567935),
-        (0.6547824166299984, 0.9159072024952073, 0.5999250835064326),
-        (0.6234796486190842, 0.9312014083676132, 0.2705486337045906),
-        (0.46405899784500193, 0.5740384701517647, 0.6154775367548058),
-        (0.5479496115847091, 0.607922568141698, 0.48482790495317296),
-        (0.950508478466332, 0.15137736802206792, 0.277696533485587),
-        (0.27606819502763125, 0.7150787664103754, 0.9004516278146157),
-        (0.41462932348169335, 0.1546152675658281, 0.6024802500176701),
-        (0.26316035181248487, 0.9924296950259774, 0.012858089574406706),
-        (0.6058186938654891, 0.8263245802609571, 0.5632477188449058),
-        (0.18715791502874501, 0.8748052205404021, 0.43694647042971213),
-    ]
-
-    return ctable
-
-
-def randomc(nlen):
-    """Get a truly random color table with nlen entries."""
-
-    ctable = []
-    for _ic in range(nlen):
-        red = random.uniform(0, 1)
-        green = random.uniform(0, 1)
-        blue = random.uniform(0, 1)
-        ctable.append((red, green, blue))
-
-    return ctable
-
-
-def xtgeocolors():
-    """Get the XTGeo color table which is basic inherited from old IRAP."""
-
-    ctable = [
-        (0.500, 0.500, 0.500),  # CGM colour 0 is "dummy" grey
-        (0.000, 0.000, 0.000),
-        (1.000, 0.000, 0.000),
-        (0.000, 1.000, 0.000),
-        (0.000, 0.000, 1.000),
-        (0.000, 1.000, 1.000),
-        (1.000, 0.408, 0.729),
-        (1.000, 1.000, 0.000),
-        (0.800, 0.196, 0.600),
-        (1.000, 0.800, 0.498),
-        (1.000, 0.000, 1.000),
-        (0.576, 0.859, 0.439),
-        (0.498, 1.000, 0.000),
-        (0.800, 0.498, 0.196),
-        (0.498, 0.000, 1.000),
-        (1.000, 0.498, 0.000),
-        (0.918, 0.678, 0.918),
-        (0.000, 0.000, 0.000),
-        (0.059, 0.059, 0.059),
-        (0.129, 0.129, 0.129),
-        (0.200, 0.200, 0.200),
-        (0.259, 0.259, 0.259),
-        (0.322, 0.322, 0.322),
-        (0.388, 0.388, 0.388),
-        (0.471, 0.471, 0.471),
-        (0.541, 0.541, 0.541),
-        (0.612, 0.612, 0.612),
-        (0.671, 0.671, 0.671),
-        (0.729, 0.729, 0.729),
-        (0.788, 0.788, 0.788),
-        (0.851, 0.851, 0.851),
-        (0.929, 0.929, 0.929),
-        (1.000, 1.000, 1.000),
-        (0.000, 0.000, 0.000),
-        (0.141, 0.000, 0.000),
-        (0.282, 0.000, 0.000),
-        (0.424, 0.000, 0.000),
-        (0.565, 0.000, 0.000),
-        (0.706, 0.000, 0.000),
-        (0.847, 0.000, 0.000),
-        (0.988, 0.000, 0.000),
-        (0.988, 0.122, 0.122),
-        (0.988, 0.243, 0.243),
-        (0.988, 0.365, 0.365),
-        (0.988, 0.486, 0.486),
-        (0.988, 0.608, 0.608),
-        (0.988, 0.729, 0.729),
-        (0.988, 0.851, 0.851),
-        (0.988, 0.973, 0.973),
-        (0.000, 0.000, 0.000),
-        (0.000, 0.141, 0.000),
-        (0.000, 0.282, 0.000),
-        (0.000, 0.424, 0.000),
-        (0.000, 0.565, 0.000),
-        (0.000, 0.706, 0.000),
-        (0.000, 0.847, 0.000),
-        (0.000, 0.988, 0.000),
-        (0.122, 0.988, 0.122),
-        (0.243, 0.988, 0.243),
-        (0.365, 0.988, 0.365),
-        (0.486, 0.988, 0.486),
-        (0.608, 0.988, 0.608),
-        (0.729, 0.988, 0.729),
-        (0.851, 0.988, 0.851),
-        (0.973, 0.988, 0.973),
-        (0.000, 0.000, 0.000),
-        (0.000, 0.000, 0.141),
-        (0.000, 0.000, 0.282),
-        (0.000, 0.000, 0.424),
-        (0.000, 0.000, 0.565),
-        (0.000, 0.000, 0.706),
-        (0.000, 0.000, 0.847),
-        (0.000, 0.000, 0.988),
-        (0.122, 0.122, 0.988),
-        (0.243, 0.243, 0.988),
-        (0.365, 0.365, 0.988),
-        (0.486, 0.486, 0.988),
-        (0.608, 0.608, 0.988),
-        (0.729, 0.729, 0.988),
-        (0.851, 0.851, 0.988),
-        (0.973, 0.973, 0.988),
-        (0.000, 0.000, 0.000),
-        (0.141, 0.141, 0.000),
-        (0.282, 0.282, 0.000),
-        (0.424, 0.424, 0.000),
-        (0.565, 0.565, 0.000),
-        (0.706, 0.706, 0.000),
-        (0.847, 0.847, 0.000),
-        (0.988, 0.988, 0.000),
-        (0.988, 0.988, 0.122),
-        (0.988, 0.988, 0.243),
-        (0.988, 0.988, 0.365),
-        (0.988, 0.988, 0.486),
-        (0.988, 0.988, 0.608),
-        (0.988, 0.988, 0.729),
-        (0.988, 0.988, 0.851),
-        (0.988, 0.988, 0.973),
-        (0.298, 0.298, 1.000),
-        (0.549, 0.298, 1.000),
-        (0.749, 0.298, 1.000),
-        (0.867, 0.298, 1.000),
-        (0.929, 0.298, 1.000),
-        (1.000, 0.298, 1.000),
-        (1.000, 0.298, 0.929),
-        (1.000, 0.298, 0.867),
-        (1.000, 0.298, 0.749),
-        (1.000, 0.298, 0.549),
-        (1.000, 0.298, 0.298),
-        (1.000, 0.549, 0.298),
-        (1.000, 0.749, 0.298),
-        (1.000, 0.867, 0.298),
-        (1.000, 0.929, 0.298),
-        (1.000, 1.000, 0.298),
-        (0.929, 1.000, 0.298),
-        (0.867, 1.000, 0.298),
-        (0.749, 1.000, 0.298),
-        (0.549, 1.000, 0.298),
-        (0.298, 1.000, 0.298),
-        (0.298, 1.000, 0.549),
-        (0.298, 1.000, 0.749),
-        (0.298, 1.000, 0.867),
-        (0.298, 1.000, 0.929),
-        (0.298, 1.000, 1.000),
-        (0.298, 0.929, 1.000),
-        (0.298, 0.867, 1.000),
-        (0.298, 0.749, 1.000),
-        (0.298, 0.549, 1.000),
-        (1.000, 1.000, 1.000),
-        (1.000, 1.000, 1.000),
-        (0.208, 0.208, 0.698),
-        (0.384, 0.208, 0.698),
-        (0.522, 0.208, 0.698),
-        (0.608, 0.208, 0.698),
-        (0.651, 0.208, 0.698),
-        (0.698, 0.208, 0.698),
-        (0.698, 0.208, 0.651),
-        (0.698, 0.208, 0.608),
-        (0.698, 0.208, 0.522),
-        (0.698, 0.208, 0.384),
-        (0.698, 0.208, 0.208),
-        (0.698, 0.384, 0.208),
-        (0.698, 0.522, 0.208),
-        (0.698, 0.608, 0.208),
-        (0.698, 0.651, 0.208),
-        (0.698, 0.698, 0.208),
-        (0.651, 0.698, 0.208),
-        (0.608, 0.698, 0.208),
-        (0.522, 0.698, 0.208),
-        (0.384, 0.698, 0.208),
-        (0.208, 0.698, 0.208),
-        (0.208, 0.698, 0.384),
-        (0.208, 0.698, 0.522),
-        (0.208, 0.698, 0.608),
-        (0.208, 0.698, 0.651),
-        (0.208, 0.698, 0.698),
-        (0.208, 0.651, 0.698),
-        (0.208, 0.608, 0.698),
-        (0.208, 0.522, 0.698),
-        (0.208, 0.384, 0.698),
-        (1.000, 1.000, 1.000),
-        (1.000, 1.000, 1.000),
-        (0.008, 0.008, 1.000),
-        (0.122, 0.008, 1.000),
-        (0.247, 0.008, 1.000),
-        (0.373, 0.008, 1.000),
-        (0.498, 0.008, 1.000),
-        (0.624, 0.008, 1.000),
-        (0.749, 0.008, 1.000),
-        (0.875, 0.008, 1.000),
-        (1.000, 0.008, 1.000),
-        (1.000, 0.008, 0.875),
-        (1.000, 0.008, 0.749),
-        (1.000, 0.008, 0.624),
-        (1.000, 0.008, 0.498),
-        (1.000, 0.008, 0.373),
-        (1.000, 0.008, 0.247),
-        (1.000, 0.008, 0.122),
-        (1.000, 0.008, 0.008),
-        (1.000, 0.122, 0.008),
-        (1.000, 0.247, 0.008),
-        (1.000, 0.373, 0.008),
-        (1.000, 0.498, 0.008),
-        (1.000, 0.624, 0.008),
-        (1.000, 0.749, 0.008),
-        (1.000, 0.875, 0.008),
-        (1.000, 1.000, 0.008),
-        (0.875, 1.000, 0.008),
-        (0.749, 1.000, 0.008),
-        (0.624, 1.000, 0.008),
-        (0.498, 1.000, 0.008),
-        (0.373, 1.000, 0.008),
-        (0.247, 1.000, 0.008),
-        (0.122, 1.000, 0.008),
-        (0.008, 1.000, 0.008),
-        (0.008, 1.000, 0.122),
-        (0.008, 1.000, 0.247),
-        (0.008, 1.000, 0.373),
-        (0.008, 1.000, 0.498),
-        (0.008, 1.000, 0.624),
-        (0.008, 1.000, 0.749),
-        (0.008, 1.000, 0.875),
-        (0.008, 1.000, 1.000),
-        (0.008, 0.875, 1.000),
-        (0.008, 0.749, 1.000),
-        (0.008, 0.624, 1.000),
-        (0.008, 0.498, 1.000),
-        (0.008, 0.373, 1.000),
-        (0.008, 0.247, 1.000),
-        (0.008, 0.122, 1.000),
-        (0.000, 1.000, 0.000),
-        (0.000, 1.000, 0.039),
-        (0.000, 1.000, 0.078),
-        (0.000, 1.000, 0.118),
-        (0.000, 1.000, 0.157),
-        (0.000, 1.000, 0.196),
-        (0.000, 1.000, 0.235),
-        (0.000, 1.000, 0.275),
-        (0.000, 1.000, 0.314),
-        (0.000, 1.000, 0.353),
-        (0.000, 1.000, 0.392),
-        (0.000, 1.000, 0.431),
-        (0.000, 1.000, 0.471),
-        (0.000, 1.000, 0.510),
-        (0.000, 1.000, 0.549),
-        (0.000, 1.000, 0.588),
-        (0.000, 1.000, 0.627),
-        (0.000, 1.000, 0.667),
-        (0.000, 1.000, 0.706),
-        (0.000, 1.000, 0.745),
-        (0.000, 1.000, 0.784),
-        (0.000, 1.000, 0.824),
-        (1.000, 0.000, 1.000),
-        (1.000, 0.039, 1.000),
-        (1.000, 0.078, 1.000),
-        (1.000, 0.118, 1.000),
-        (1.000, 0.157, 1.000),
-        (1.000, 0.196, 1.000),
-        (1.000, 0.235, 1.000),
-        (1.000, 0.275, 1.000),
-        (1.000, 0.314, 1.000),
-        (1.000, 0.353, 1.000),
-        (1.000, 0.392, 1.000),
-        (1.000, 0.431, 1.000),
-        (1.000, 0.471, 1.000),
-        (1.000, 0.510, 1.000),
-        (1.000, 0.549, 1.000),
-        (1.000, 0.588, 1.000),
-        (1.000, 0.627, 1.000),
-        (1.000, 0.667, 1.000),
-        (1.000, 0.706, 1.000),
-        (1.000, 0.745, 1.000),
-        (1.000, 0.784, 1.000),
-        (1.000, 0.824, 1.000),
-        (1.000, 0.863, 1.000),
-        (1.000, 1.000, 1.000),
-        (1.000, 1.000, 1.000),
-    ]
-
-    return ctable
-
-
-def colorsfromfile(fname, fformat="rms"):
-    """Read a color table from a file."""
-
-    logger.info("Default style is %s", fformat)
-    ctable = []
-
-    if not os.path.isfile(fname):
-        raise IOError(f"Color file not found: {fname}")
-
-    with open(fname) as fc:
-        for line in fc:
-            if line.startswith("ColorMap.color("):
-                _key, _eq, rgb1, rgb2, rgb3 = line.split()
-                ctable.append((float(rgb1), float(rgb2), float(rgb3)))
-
-    return ctable
+# -*- coding: utf-8 -*-
+"""Module for color tables."""
+
+import os
+import random
+
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def random40():
+    """Random but fixed color table with 40 entries, mostly for testing."""
+
+    ctable = [
+        (0.83345071168299956, 0.2382011018575051, 0.6457693086399422),
+        (0.541018453798349, 0.48038587236806063, 0.9151873938263322),
+        (0.10882072036344093, 0.8838633180878654, 0.5133890790690832),
+        (0.0021118859288566, 0.23207523745480785, 0.87105494758907984),
+        (0.9167955510272826, 0.06636986706409276, 0.2085761696435795),
+        (0.95445330127180916, 0.92537092140778787, 0.5643674283342163),
+        (0.1118142610865344, 0.8753812704140146, 0.882396282596847),
+        (0.9169300859386396, 0.94501442557231312, 0.01407158657305927),
+        (0.878693250908883, 0.11222139418674728, 0.6167801456569036),
+        (0.5256939400784498, 0.11435373675297966, 0.48354476358995946),
+        (0.48340383516545826, 0.6410572769763058, 0.29403657486119683),
+        (0.311139625063827, 0.9263058894491734, 0.7287039866021682),
+        (0.8598753478111007, 0.9018735026443733, 0.6313058592961296),
+        (0.43793800204923883, 0.2921469762701936, 0.2098830453305408),
+        (0.7381272212655949, 0.1156423111704514, 0.2452716196308139),
+        (0.21717885649022806, 0.3046216797738889, 0.6714686237358712),
+        (0.0070595461262333, 0.714345667698068, 0.34723053474857124),
+        (0.9745984699708928, 0.4730239548162848, 0.15007860849878418),
+        (0.6833692521550699, 0.4228448482259841, 0.0044103854819418364),
+        (0.5691320165209793, 0.08796666338494852, 0.8035539289654177),
+        (0.34042801546149426, 0.47713419001045765, 0.20879105326096692),
+        (0.3085826005434883, 0.5079488187213768, 0.4608596310424611),
+        (0.9396543635576965, 0.0064011918231481335, 0.2812656314040958),
+        (0.5217398018516609, 0.2575024696206665, 0.4677374482174591),
+        (0.42127435507233524, 0.8568338738437583, 0.7638805644700677),
+        (0.09602707599420568, 0.3864069927590945, 0.6039576328011579),
+        (0.16897839817549498, 0.7065043632174942, 0.8423968298858847),
+        (0.2986608595521294, 0.6791125573730088, 0.5589263091038545),
+        (0.445032995863182, 0.3024296426041809, 0.887087647343199),
+        (0.40061174588633763, 0.8547027289892815, 0.822969921567935),
+        (0.6547824166299984, 0.9159072024952073, 0.5999250835064326),
+        (0.6234796486190842, 0.9312014083676132, 0.2705486337045906),
+        (0.46405899784500193, 0.5740384701517647, 0.6154775367548058),
+        (0.5479496115847091, 0.607922568141698, 0.48482790495317296),
+        (0.950508478466332, 0.15137736802206792, 0.277696533485587),
+        (0.27606819502763125, 0.7150787664103754, 0.9004516278146157),
+        (0.41462932348169335, 0.1546152675658281, 0.6024802500176701),
+        (0.26316035181248487, 0.9924296950259774, 0.012858089574406706),
+        (0.6058186938654891, 0.8263245802609571, 0.5632477188449058),
+        (0.18715791502874501, 0.8748052205404021, 0.43694647042971213),
+    ]
+
+    return ctable
+
+
+def randomc(nlen):
+    """Get a truly random color table with nlen entries."""
+
+    ctable = []
+    for _ic in range(nlen):
+        red = random.uniform(0, 1)
+        green = random.uniform(0, 1)
+        blue = random.uniform(0, 1)
+        ctable.append((red, green, blue))
+
+    return ctable
+
+
+def xtgeocolors():
+    """Get the XTGeo color table which is basic inherited from old IRAP."""
+
+    ctable = [
+        (0.500, 0.500, 0.500),  # CGM colour 0 is "dummy" grey
+        (0.000, 0.000, 0.000),
+        (1.000, 0.000, 0.000),
+        (0.000, 1.000, 0.000),
+        (0.000, 0.000, 1.000),
+        (0.000, 1.000, 1.000),
+        (1.000, 0.408, 0.729),
+        (1.000, 1.000, 0.000),
+        (0.800, 0.196, 0.600),
+        (1.000, 0.800, 0.498),
+        (1.000, 0.000, 1.000),
+        (0.576, 0.859, 0.439),
+        (0.498, 1.000, 0.000),
+        (0.800, 0.498, 0.196),
+        (0.498, 0.000, 1.000),
+        (1.000, 0.498, 0.000),
+        (0.918, 0.678, 0.918),
+        (0.000, 0.000, 0.000),
+        (0.059, 0.059, 0.059),
+        (0.129, 0.129, 0.129),
+        (0.200, 0.200, 0.200),
+        (0.259, 0.259, 0.259),
+        (0.322, 0.322, 0.322),
+        (0.388, 0.388, 0.388),
+        (0.471, 0.471, 0.471),
+        (0.541, 0.541, 0.541),
+        (0.612, 0.612, 0.612),
+        (0.671, 0.671, 0.671),
+        (0.729, 0.729, 0.729),
+        (0.788, 0.788, 0.788),
+        (0.851, 0.851, 0.851),
+        (0.929, 0.929, 0.929),
+        (1.000, 1.000, 1.000),
+        (0.000, 0.000, 0.000),
+        (0.141, 0.000, 0.000),
+        (0.282, 0.000, 0.000),
+        (0.424, 0.000, 0.000),
+        (0.565, 0.000, 0.000),
+        (0.706, 0.000, 0.000),
+        (0.847, 0.000, 0.000),
+        (0.988, 0.000, 0.000),
+        (0.988, 0.122, 0.122),
+        (0.988, 0.243, 0.243),
+        (0.988, 0.365, 0.365),
+        (0.988, 0.486, 0.486),
+        (0.988, 0.608, 0.608),
+        (0.988, 0.729, 0.729),
+        (0.988, 0.851, 0.851),
+        (0.988, 0.973, 0.973),
+        (0.000, 0.000, 0.000),
+        (0.000, 0.141, 0.000),
+        (0.000, 0.282, 0.000),
+        (0.000, 0.424, 0.000),
+        (0.000, 0.565, 0.000),
+        (0.000, 0.706, 0.000),
+        (0.000, 0.847, 0.000),
+        (0.000, 0.988, 0.000),
+        (0.122, 0.988, 0.122),
+        (0.243, 0.988, 0.243),
+        (0.365, 0.988, 0.365),
+        (0.486, 0.988, 0.486),
+        (0.608, 0.988, 0.608),
+        (0.729, 0.988, 0.729),
+        (0.851, 0.988, 0.851),
+        (0.973, 0.988, 0.973),
+        (0.000, 0.000, 0.000),
+        (0.000, 0.000, 0.141),
+        (0.000, 0.000, 0.282),
+        (0.000, 0.000, 0.424),
+        (0.000, 0.000, 0.565),
+        (0.000, 0.000, 0.706),
+        (0.000, 0.000, 0.847),
+        (0.000, 0.000, 0.988),
+        (0.122, 0.122, 0.988),
+        (0.243, 0.243, 0.988),
+        (0.365, 0.365, 0.988),
+        (0.486, 0.486, 0.988),
+        (0.608, 0.608, 0.988),
+        (0.729, 0.729, 0.988),
+        (0.851, 0.851, 0.988),
+        (0.973, 0.973, 0.988),
+        (0.000, 0.000, 0.000),
+        (0.141, 0.141, 0.000),
+        (0.282, 0.282, 0.000),
+        (0.424, 0.424, 0.000),
+        (0.565, 0.565, 0.000),
+        (0.706, 0.706, 0.000),
+        (0.847, 0.847, 0.000),
+        (0.988, 0.988, 0.000),
+        (0.988, 0.988, 0.122),
+        (0.988, 0.988, 0.243),
+        (0.988, 0.988, 0.365),
+        (0.988, 0.988, 0.486),
+        (0.988, 0.988, 0.608),
+        (0.988, 0.988, 0.729),
+        (0.988, 0.988, 0.851),
+        (0.988, 0.988, 0.973),
+        (0.298, 0.298, 1.000),
+        (0.549, 0.298, 1.000),
+        (0.749, 0.298, 1.000),
+        (0.867, 0.298, 1.000),
+        (0.929, 0.298, 1.000),
+        (1.000, 0.298, 1.000),
+        (1.000, 0.298, 0.929),
+        (1.000, 0.298, 0.867),
+        (1.000, 0.298, 0.749),
+        (1.000, 0.298, 0.549),
+        (1.000, 0.298, 0.298),
+        (1.000, 0.549, 0.298),
+        (1.000, 0.749, 0.298),
+        (1.000, 0.867, 0.298),
+        (1.000, 0.929, 0.298),
+        (1.000, 1.000, 0.298),
+        (0.929, 1.000, 0.298),
+        (0.867, 1.000, 0.298),
+        (0.749, 1.000, 0.298),
+        (0.549, 1.000, 0.298),
+        (0.298, 1.000, 0.298),
+        (0.298, 1.000, 0.549),
+        (0.298, 1.000, 0.749),
+        (0.298, 1.000, 0.867),
+        (0.298, 1.000, 0.929),
+        (0.298, 1.000, 1.000),
+        (0.298, 0.929, 1.000),
+        (0.298, 0.867, 1.000),
+        (0.298, 0.749, 1.000),
+        (0.298, 0.549, 1.000),
+        (1.000, 1.000, 1.000),
+        (1.000, 1.000, 1.000),
+        (0.208, 0.208, 0.698),
+        (0.384, 0.208, 0.698),
+        (0.522, 0.208, 0.698),
+        (0.608, 0.208, 0.698),
+        (0.651, 0.208, 0.698),
+        (0.698, 0.208, 0.698),
+        (0.698, 0.208, 0.651),
+        (0.698, 0.208, 0.608),
+        (0.698, 0.208, 0.522),
+        (0.698, 0.208, 0.384),
+        (0.698, 0.208, 0.208),
+        (0.698, 0.384, 0.208),
+        (0.698, 0.522, 0.208),
+        (0.698, 0.608, 0.208),
+        (0.698, 0.651, 0.208),
+        (0.698, 0.698, 0.208),
+        (0.651, 0.698, 0.208),
+        (0.608, 0.698, 0.208),
+        (0.522, 0.698, 0.208),
+        (0.384, 0.698, 0.208),
+        (0.208, 0.698, 0.208),
+        (0.208, 0.698, 0.384),
+        (0.208, 0.698, 0.522),
+        (0.208, 0.698, 0.608),
+        (0.208, 0.698, 0.651),
+        (0.208, 0.698, 0.698),
+        (0.208, 0.651, 0.698),
+        (0.208, 0.608, 0.698),
+        (0.208, 0.522, 0.698),
+        (0.208, 0.384, 0.698),
+        (1.000, 1.000, 1.000),
+        (1.000, 1.000, 1.000),
+        (0.008, 0.008, 1.000),
+        (0.122, 0.008, 1.000),
+        (0.247, 0.008, 1.000),
+        (0.373, 0.008, 1.000),
+        (0.498, 0.008, 1.000),
+        (0.624, 0.008, 1.000),
+        (0.749, 0.008, 1.000),
+        (0.875, 0.008, 1.000),
+        (1.000, 0.008, 1.000),
+        (1.000, 0.008, 0.875),
+        (1.000, 0.008, 0.749),
+        (1.000, 0.008, 0.624),
+        (1.000, 0.008, 0.498),
+        (1.000, 0.008, 0.373),
+        (1.000, 0.008, 0.247),
+        (1.000, 0.008, 0.122),
+        (1.000, 0.008, 0.008),
+        (1.000, 0.122, 0.008),
+        (1.000, 0.247, 0.008),
+        (1.000, 0.373, 0.008),
+        (1.000, 0.498, 0.008),
+        (1.000, 0.624, 0.008),
+        (1.000, 0.749, 0.008),
+        (1.000, 0.875, 0.008),
+        (1.000, 1.000, 0.008),
+        (0.875, 1.000, 0.008),
+        (0.749, 1.000, 0.008),
+        (0.624, 1.000, 0.008),
+        (0.498, 1.000, 0.008),
+        (0.373, 1.000, 0.008),
+        (0.247, 1.000, 0.008),
+        (0.122, 1.000, 0.008),
+        (0.008, 1.000, 0.008),
+        (0.008, 1.000, 0.122),
+        (0.008, 1.000, 0.247),
+        (0.008, 1.000, 0.373),
+        (0.008, 1.000, 0.498),
+        (0.008, 1.000, 0.624),
+        (0.008, 1.000, 0.749),
+        (0.008, 1.000, 0.875),
+        (0.008, 1.000, 1.000),
+        (0.008, 0.875, 1.000),
+        (0.008, 0.749, 1.000),
+        (0.008, 0.624, 1.000),
+        (0.008, 0.498, 1.000),
+        (0.008, 0.373, 1.000),
+        (0.008, 0.247, 1.000),
+        (0.008, 0.122, 1.000),
+        (0.000, 1.000, 0.000),
+        (0.000, 1.000, 0.039),
+        (0.000, 1.000, 0.078),
+        (0.000, 1.000, 0.118),
+        (0.000, 1.000, 0.157),
+        (0.000, 1.000, 0.196),
+        (0.000, 1.000, 0.235),
+        (0.000, 1.000, 0.275),
+        (0.000, 1.000, 0.314),
+        (0.000, 1.000, 0.353),
+        (0.000, 1.000, 0.392),
+        (0.000, 1.000, 0.431),
+        (0.000, 1.000, 0.471),
+        (0.000, 1.000, 0.510),
+        (0.000, 1.000, 0.549),
+        (0.000, 1.000, 0.588),
+        (0.000, 1.000, 0.627),
+        (0.000, 1.000, 0.667),
+        (0.000, 1.000, 0.706),
+        (0.000, 1.000, 0.745),
+        (0.000, 1.000, 0.784),
+        (0.000, 1.000, 0.824),
+        (1.000, 0.000, 1.000),
+        (1.000, 0.039, 1.000),
+        (1.000, 0.078, 1.000),
+        (1.000, 0.118, 1.000),
+        (1.000, 0.157, 1.000),
+        (1.000, 0.196, 1.000),
+        (1.000, 0.235, 1.000),
+        (1.000, 0.275, 1.000),
+        (1.000, 0.314, 1.000),
+        (1.000, 0.353, 1.000),
+        (1.000, 0.392, 1.000),
+        (1.000, 0.431, 1.000),
+        (1.000, 0.471, 1.000),
+        (1.000, 0.510, 1.000),
+        (1.000, 0.549, 1.000),
+        (1.000, 0.588, 1.000),
+        (1.000, 0.627, 1.000),
+        (1.000, 0.667, 1.000),
+        (1.000, 0.706, 1.000),
+        (1.000, 0.745, 1.000),
+        (1.000, 0.784, 1.000),
+        (1.000, 0.824, 1.000),
+        (1.000, 0.863, 1.000),
+        (1.000, 1.000, 1.000),
+        (1.000, 1.000, 1.000),
+    ]
+
+    return ctable
+
+
+def colorsfromfile(fname, fformat="rms"):
+    """Read a color table from a file."""
+
+    logger.info("Default style is %s", fformat)
+    ctable = []
+
+    if not os.path.isfile(fname):
+        raise IOError(f"Color file not found: {fname}")
+
+    with open(fname) as fc:
+        for line in fc:
+            if line.startswith("ColorMap.color("):
+                _key, _eq, rgb1, rgb2, rgb3 = line.split()
+                ctable.append((float(rgb1), float(rgb2), float(rgb3)))
+
+    return ctable
```

## xtgeo/plot/baseplot.py

 * *Ordering differences only*

```diff
@@ -1,256 +1,256 @@
-"""The baseplot module."""
-import matplotlib as mpl
-import matplotlib.pyplot as plt
-from matplotlib.colors import LinearSegmentedColormap
-from packaging.version import parse as versionparse
-
-from xtgeo.common import XTGeoDialog
-
-from . import _colortables as _ctable
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def _get_colormap(name):
-    """For matplotlib compatibility."""
-    if versionparse(mpl.__version__) < versionparse("3.6"):
-        return plt.cm.get_cmap(name)
-    else:
-        return mpl.colormaps[name]
-
-
-class BasePlot(object):
-    """Base class for plots, providing some functions to share."""
-
-    def __init__(self):
-        """Init method."""
-        clsname = f"{type(self).__module__}.{type(self).__name__}"
-        logger.info(clsname)
-
-        self._contourlevels = 3
-        self._colormap = _get_colormap("viridis")
-
-        self._ax = None
-        self._tight = False
-        self._showok = True
-        self._fig = None
-        self._allfigs = []
-        self._pagesize = "A4"
-
-        logger.info("Ran __init__ ...")
-
-    @property
-    def contourlevels(self):
-        """Get or set the number of contour levels."""
-        return self._contourlevels
-
-    @contourlevels.setter
-    def contourlevels(self, num):
-        self._contourlevels = num
-
-    @property
-    def colormap(self):
-        """Get or set the color table as a matplot cmap object."""
-        return self._colormap
-
-    @colormap.setter
-    def colormap(self, cmap):
-        if isinstance(cmap, LinearSegmentedColormap):
-            self._colormap = cmap
-        elif isinstance(cmap, str):
-            logger.info("Definition of a colormap from string name: %s", cmap)
-            self.define_colormap(cmap)
-        else:
-            raise ValueError("Input incorrect")
-
-        logger.info("Colormap: %s", self._colormap)
-
-    @property
-    def pagesize(self):
-        """Returns page size."""
-        return self._pagesize
-
-    @staticmethod
-    def define_any_colormap(cfile, colorlist=None):
-        """Defines any color map from file or a predefined name.
-
-        This is a static method, which returns a matplotlib CM object.
-
-        Args:
-            cfile (str): File name (RMS format) or an alias for a predefined
-                map name, e.g. 'xtgeo', or one of matplotlibs numerous tables.
-            colorlist (list, int, optional): List of integers redefining
-                color entries per zone and/or well, which starts
-                from 0 index. Default is just keep the linear sequence as is.
-
-        """
-        valid_maps = sorted(m for m in plt.cm.datad)
-
-        logger.info("Valid color maps: %s", valid_maps)
-
-        colors = []
-
-        cmap = _get_colormap("rainbow")
-
-        if cfile is None:
-            cfile = "rainbow"
-            cmap = _get_colormap("rainbow")
-
-        elif cfile == "xtgeo":
-            colors = _ctable.xtgeocolors()
-            cmap = LinearSegmentedColormap.from_list(cfile, colors, N=len(colors))
-            cmap.name = "xtgeo"
-        elif cfile == "random40":
-            colors = _ctable.random40()
-            cmap = LinearSegmentedColormap.from_list(cfile, colors, N=len(colors))
-            cmap.name = "random40"
-
-        elif cfile == "randomc":
-            colors = _ctable.randomc(256)
-            cmap = LinearSegmentedColormap.from_list(cfile, colors, N=len(colors))
-            cmap.name = "randomc"
-
-        elif isinstance(cfile, str) and "rms" in cfile:
-            colors = _ctable.colorsfromfile(cfile)
-            cmap = LinearSegmentedColormap.from_list("rms", colors, N=len(colors))
-            cmap.name = cfile
-        elif cfile in valid_maps:
-            cmap = _get_colormap(cfile)
-            for i in range(cmap.N):
-                colors.append(cmap(i))
-        else:
-            xtg.warnuser(
-                "Trying to access as color map not installed in this version "
-                f"of matplotlib: <{cfile}>. Revert to <rainbow>"
-            )
-            cmap = _get_colormap("rainbow")
-            for i in range(cmap.N):
-                colors.append(cmap(i))
-
-        ctable = []
-
-        if colorlist:
-            for entry in colorlist:
-                if entry < len(colors):
-                    ctable.append(colors[entry])
-                else:
-                    logger.warning("Color list out of range")
-                    ctable.append(colors[0])
-
-            cmap = LinearSegmentedColormap.from_list(ctable, colors, N=len(colors))
-            cmap.name = "user"
-
-        return cmap
-
-    @staticmethod
-    def get_any_colormap_as_table(cmap):
-        """Returns the given color map cmap as a list of RGB tuples."""
-        cmaplist = [cmap(i) for i in range(cmap.N)]
-        return cmaplist
-
-    def get_colormap_as_table(self):
-        """Get the current color map as a list of RGB tuples."""
-        return self.get_any_colormap_as_table(self._colormap)
-
-    def define_colormap(self, cfile, colorlist=None):
-        """Defines a color map from file or a predefined name.
-
-        Args:
-            cfile (str): File name (RMS format) or an alias for a predefined
-                map name, e.g. 'xtgeo', or one of matplotlibs numerous tables.
-            colorlist (list, int, optional): List of integers redefining
-                color entries per zone and/or well, which starts
-                from 0 index. Default is just keep the linear sequence as is.
-
-        """
-        logger.info("Defining colormap")
-
-        cmap = self.define_any_colormap(cfile, colorlist=colorlist)
-
-        self.contourlevels = cmap.N
-        self._colormap = cmap
-
-    def canvas(self, title=None, subtitle=None, infotext=None, figscaling=1.0):
-        """Prepare the canvas to plot on, with title and subtitle.
-
-        Args:
-            title (str, optional): Title of plot.
-            subtitle (str, optional): Sub title of plot.
-            infotext (str, optional): Text to be written as info string.
-            figscaling (str, optional): Figure scaling, default is 1.0
-
-
-        """
-        # self._fig, (ax1, ax2) = plt.subplots(2, figsize=(11.69, 8.27))
-        self._fig, self._ax = plt.subplots(
-            figsize=(11.69 * figscaling, 8.27 * figscaling)
-        )
-        self._allfigs.append(self._fig)
-        if title is not None:
-            self._fig.suptitle(title, fontsize=18)
-        if subtitle is not None:
-            self._ax.set_title(subtitle, size=14)
-        if infotext is not None:
-            self._fig.text(0.01, 0.02, infotext, ha="left", va="center", fontsize=8)
-
-    def show(self):
-        """Call to matplotlib.pyplot show method.
-
-        Returns:
-            True of plotting is done; otherwise False
-        """
-        if self._tight:
-            self._fig.tight_layout()
-
-        if self._showok:
-            logger.info("Calling plt show method...")
-            plt.show()
-            return True
-
-        logger.warning("Nothing to plot (well outside Z range?)")
-        return False
-
-    def close(self):
-        """
-        Explicitly closes the plot, meaning that memory will be cleared.
-
-        After close is called, no more operations can be performed on the plot.
-
-        """
-        for fig in self._allfigs:
-            plt.close(fig)
-
-    def savefig(self, filename, fformat="png", last=True, **kwargs):
-        """Call to matplotlib.pyplot savefig method.
-
-        Args:
-            filename (str): File to plot to
-            fformat (str): Plot format, e.g. png (default), jpg, svg
-            last (bool): Default is true, calls close on the plot, let last
-                be False for all except the last plots.
-            kwargs: Additional keyword arguments that are passed
-                to matplotlib when saving the figure
-
-        Returns:
-            True of plotting is done; otherwise False
-
-        Example::
-
-            myplot.savefig('layerslice.svg', fformat='svg', last=False)
-            myplot.savefig('layerslice.png')
-
-        .. versionchanged:: 2.4 added kwargs option
-
-        """
-        if self._tight:
-            self._fig.tight_layout()
-
-        if self._showok:
-            plt.savefig(filename, format=fformat, **kwargs)
-            if last:
-                self.close()
-            return True
-
-        logger.warning("Nothing to plot (well outside Z range?)")
-        return False
+"""The baseplot module."""
+import matplotlib as mpl
+import matplotlib.pyplot as plt
+from matplotlib.colors import LinearSegmentedColormap
+from packaging.version import parse as versionparse
+
+from xtgeo.common import XTGeoDialog
+
+from . import _colortables as _ctable
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def _get_colormap(name):
+    """For matplotlib compatibility."""
+    if versionparse(mpl.__version__) < versionparse("3.6"):
+        return plt.cm.get_cmap(name)
+    else:
+        return mpl.colormaps[name]
+
+
+class BasePlot(object):
+    """Base class for plots, providing some functions to share."""
+
+    def __init__(self):
+        """Init method."""
+        clsname = f"{type(self).__module__}.{type(self).__name__}"
+        logger.info(clsname)
+
+        self._contourlevels = 3
+        self._colormap = _get_colormap("viridis")
+
+        self._ax = None
+        self._tight = False
+        self._showok = True
+        self._fig = None
+        self._allfigs = []
+        self._pagesize = "A4"
+
+        logger.info("Ran __init__ ...")
+
+    @property
+    def contourlevels(self):
+        """Get or set the number of contour levels."""
+        return self._contourlevels
+
+    @contourlevels.setter
+    def contourlevels(self, num):
+        self._contourlevels = num
+
+    @property
+    def colormap(self):
+        """Get or set the color table as a matplot cmap object."""
+        return self._colormap
+
+    @colormap.setter
+    def colormap(self, cmap):
+        if isinstance(cmap, LinearSegmentedColormap):
+            self._colormap = cmap
+        elif isinstance(cmap, str):
+            logger.info("Definition of a colormap from string name: %s", cmap)
+            self.define_colormap(cmap)
+        else:
+            raise ValueError("Input incorrect")
+
+        logger.info("Colormap: %s", self._colormap)
+
+    @property
+    def pagesize(self):
+        """Returns page size."""
+        return self._pagesize
+
+    @staticmethod
+    def define_any_colormap(cfile, colorlist=None):
+        """Defines any color map from file or a predefined name.
+
+        This is a static method, which returns a matplotlib CM object.
+
+        Args:
+            cfile (str): File name (RMS format) or an alias for a predefined
+                map name, e.g. 'xtgeo', or one of matplotlibs numerous tables.
+            colorlist (list, int, optional): List of integers redefining
+                color entries per zone and/or well, which starts
+                from 0 index. Default is just keep the linear sequence as is.
+
+        """
+        valid_maps = sorted(m for m in plt.cm.datad)
+
+        logger.info("Valid color maps: %s", valid_maps)
+
+        colors = []
+
+        cmap = _get_colormap("rainbow")
+
+        if cfile is None:
+            cfile = "rainbow"
+            cmap = _get_colormap("rainbow")
+
+        elif cfile == "xtgeo":
+            colors = _ctable.xtgeocolors()
+            cmap = LinearSegmentedColormap.from_list(cfile, colors, N=len(colors))
+            cmap.name = "xtgeo"
+        elif cfile == "random40":
+            colors = _ctable.random40()
+            cmap = LinearSegmentedColormap.from_list(cfile, colors, N=len(colors))
+            cmap.name = "random40"
+
+        elif cfile == "randomc":
+            colors = _ctable.randomc(256)
+            cmap = LinearSegmentedColormap.from_list(cfile, colors, N=len(colors))
+            cmap.name = "randomc"
+
+        elif isinstance(cfile, str) and "rms" in cfile:
+            colors = _ctable.colorsfromfile(cfile)
+            cmap = LinearSegmentedColormap.from_list("rms", colors, N=len(colors))
+            cmap.name = cfile
+        elif cfile in valid_maps:
+            cmap = _get_colormap(cfile)
+            for i in range(cmap.N):
+                colors.append(cmap(i))
+        else:
+            xtg.warnuser(
+                "Trying to access as color map not installed in this version "
+                f"of matplotlib: <{cfile}>. Revert to <rainbow>"
+            )
+            cmap = _get_colormap("rainbow")
+            for i in range(cmap.N):
+                colors.append(cmap(i))
+
+        ctable = []
+
+        if colorlist:
+            for entry in colorlist:
+                if entry < len(colors):
+                    ctable.append(colors[entry])
+                else:
+                    logger.warning("Color list out of range")
+                    ctable.append(colors[0])
+
+            cmap = LinearSegmentedColormap.from_list(ctable, colors, N=len(colors))
+            cmap.name = "user"
+
+        return cmap
+
+    @staticmethod
+    def get_any_colormap_as_table(cmap):
+        """Returns the given color map cmap as a list of RGB tuples."""
+        cmaplist = [cmap(i) for i in range(cmap.N)]
+        return cmaplist
+
+    def get_colormap_as_table(self):
+        """Get the current color map as a list of RGB tuples."""
+        return self.get_any_colormap_as_table(self._colormap)
+
+    def define_colormap(self, cfile, colorlist=None):
+        """Defines a color map from file or a predefined name.
+
+        Args:
+            cfile (str): File name (RMS format) or an alias for a predefined
+                map name, e.g. 'xtgeo', or one of matplotlibs numerous tables.
+            colorlist (list, int, optional): List of integers redefining
+                color entries per zone and/or well, which starts
+                from 0 index. Default is just keep the linear sequence as is.
+
+        """
+        logger.info("Defining colormap")
+
+        cmap = self.define_any_colormap(cfile, colorlist=colorlist)
+
+        self.contourlevels = cmap.N
+        self._colormap = cmap
+
+    def canvas(self, title=None, subtitle=None, infotext=None, figscaling=1.0):
+        """Prepare the canvas to plot on, with title and subtitle.
+
+        Args:
+            title (str, optional): Title of plot.
+            subtitle (str, optional): Sub title of plot.
+            infotext (str, optional): Text to be written as info string.
+            figscaling (str, optional): Figure scaling, default is 1.0
+
+
+        """
+        # self._fig, (ax1, ax2) = plt.subplots(2, figsize=(11.69, 8.27))
+        self._fig, self._ax = plt.subplots(
+            figsize=(11.69 * figscaling, 8.27 * figscaling)
+        )
+        self._allfigs.append(self._fig)
+        if title is not None:
+            self._fig.suptitle(title, fontsize=18)
+        if subtitle is not None:
+            self._ax.set_title(subtitle, size=14)
+        if infotext is not None:
+            self._fig.text(0.01, 0.02, infotext, ha="left", va="center", fontsize=8)
+
+    def show(self):
+        """Call to matplotlib.pyplot show method.
+
+        Returns:
+            True of plotting is done; otherwise False
+        """
+        if self._tight:
+            self._fig.tight_layout()
+
+        if self._showok:
+            logger.info("Calling plt show method...")
+            plt.show()
+            return True
+
+        logger.warning("Nothing to plot (well outside Z range?)")
+        return False
+
+    def close(self):
+        """
+        Explicitly closes the plot, meaning that memory will be cleared.
+
+        After close is called, no more operations can be performed on the plot.
+
+        """
+        for fig in self._allfigs:
+            plt.close(fig)
+
+    def savefig(self, filename, fformat="png", last=True, **kwargs):
+        """Call to matplotlib.pyplot savefig method.
+
+        Args:
+            filename (str): File to plot to
+            fformat (str): Plot format, e.g. png (default), jpg, svg
+            last (bool): Default is true, calls close on the plot, let last
+                be False for all except the last plots.
+            kwargs: Additional keyword arguments that are passed
+                to matplotlib when saving the figure
+
+        Returns:
+            True of plotting is done; otherwise False
+
+        Example::
+
+            myplot.savefig('layerslice.svg', fformat='svg', last=False)
+            myplot.savefig('layerslice.png')
+
+        .. versionchanged:: 2.4 added kwargs option
+
+        """
+        if self._tight:
+            self._fig.tight_layout()
+
+        if self._showok:
+            plt.savefig(filename, format=fformat, **kwargs)
+            if last:
+                self.close()
+            return True
+
+        logger.warning("Nothing to plot (well outside Z range?)")
+        return False
```

## xtgeo/plot/grid3d_slice.py

 * *Ordering differences only*

```diff
@@ -1,207 +1,207 @@
-"""Module for 3D Grid slice plots, using matplotlib."""
-
-
-import matplotlib.pyplot as plt
-from matplotlib.patches import Polygon
-from matplotlib.collections import PatchCollection
-
-from xtgeo.common import XTGeoDialog
-from xtgeo.plot.baseplot import BasePlot
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class Grid3DSlice(BasePlot):
-    """Class for plotting a row, a column, or a layer, using matplotlib."""
-
-    def __init__(self):
-        """Construct an instance for a Grid3DSlice object."""
-        super().__init__()
-
-        self._wells = None
-        self._surface = None
-        self._tight = False
-
-        self._wfence = None
-        self._legendtitle = "Map"
-
-        self._colormap = "rainbow"
-        self._linecolor = "black"
-        self._clist = None
-        self._prop = None
-        self._grid = None
-        self._geomlist = None
-        self._index = 1
-        self._actnum = None
-        self._window = None
-        self._active = True
-        self._minvalue = None
-        self._maxvalue = None
-
-    # ==================================================================================
-    # Functions methods (public)
-    # Beware general methods in base class also!
-    # ==================================================================================
-
-    def plot_gridslice(
-        self,
-        grid,
-        prop=None,
-        mode="layer",
-        minvalue=None,
-        maxvalue=None,
-        colormap=None,
-        linecolor="black",
-        index=1,
-        window=None,
-        activeonly=True,
-    ):
-        """Plot a row slice, column slice or layer slice of a grid.
-
-        Args:
-            grid (Grid): The XTGeo grid object
-            prop (GridProperty, optional): The XTGeo grid property object
-            mode (str): Choose between 'column', 'row', 'layer' (default)
-            minvalue (float): Minimum level color scale (default: from data)
-            maxvalue (float): Maximum level color scale (default: from data)
-            index (int): Index to plot e.g layer number if layer slice (first=1)
-            colormap: Color map to use for cells, e.g. 'rainbow' or an rmscol file
-            linecolor (str or tuple): Color of grid lines (black/white/grey
-                or a tuple with 4 numbers on valid matplotlib format)
-            activeonly (bool): If only use active cells
-            window (str): Some window
-
-        """
-        self._index = index
-        if colormap is not None:
-            self._colormap = colormap
-
-        self._linecolor = linecolor
-        if not isinstance(linecolor, tuple) and linecolor not in (
-            "black",
-            "grey",
-            "white",
-        ):
-            raise ValueError("Value of linecolor is invalid")
-
-        self._clist = grid.get_xyz_corners()  # get XYZ for each corner, 24 arrays
-        self._grid = grid
-        self._prop = prop
-        self._window = window
-
-        if self._geomlist is None:
-            # returns (xori, yori, zori, xmin, xmax, ymin, ymax, zmin, zmax,...)
-            self._geomlist = grid.get_geometrics(allcells=True, cellcenter=False)
-
-        self._active = activeonly
-
-        self._minvalue = minvalue
-        self._maxvalue = maxvalue
-
-        if mode == "column":
-            pass  # self._plot_row()
-        elif mode == "row":
-            pass  # self._plot_row()
-        else:
-            self._plot_layer()
-
-    # def _plot_row(self):
-
-    #     geomlist = self._geomlist
-
-    #     if self._window is None:
-    #         xmin = geomlist[3] - 0.05 * (abs(geomlist[4] - geomlist[3]))
-    #         xmax = geomlist[4] + 0.05 * (abs(geomlist[4] - geomlist[3]))
-    #         zmin = geomlist[7] - 0.05 * (abs(geomlist[8] - geomlist[7]))
-    #         zmax = geomlist[8] + 0.05 * (abs(geomlist[8] - geomlist[7]))
-    #     else:
-    #         xmin, xmax, zmin, zmax = self._window
-
-    #     # now some numpy operations, numbering is intended
-    #     clist = self._clist
-    #     xz0 = np.column_stack((clist[0].values1d, clist[2].values1d))
-    #     xz1 = np.column_stack((clist[3].values1d, clist[5].values1d))
-    #     xz2 = np.column_stack((clist[15].values1d, clist[17].values1d))
-    #     xz3 = np.column_stack((clist[12].values1d, clist[14].values1d))
-
-    #     xyc = np.column_stack((xz0, xz1, xz2, xz3))
-    #     xyc = xyc.reshape(self._grid.nlay, self._grid.ncol * self._grid.nrow, 4, 2)
-
-    #     patches = []
-
-    #     for pos in range(self._grid.nrow * self._grid.nlay):
-    #         nppol = xyc[self._index - 1, pos, :, :]
-    #         if nppol.mean() > 0.0:
-    #             polygon = Polygon(nppol, True)
-    #             patches.append(polygon)
-
-    #     black = (0, 0, 0, 1)
-    #     patchcoll = PatchCollection(patches, edgecolors=(black,), cmap=self.colormap)
-
-    #     # patchcoll.set_array(np.array(pvalues))
-
-    #     # patchcoll.set_clim([minvalue, maxvalue])
-
-    #     im = self._ax.add_collection(patchcoll)
-    #     self._ax.set_xlim((xmin, xmax))
-    #     self._ax.set_ylim((zmin, zmax))
-    #     self._ax.invert_yaxis()
-    #     self._fig.colorbar(im)
-
-    #     # plt.gca().set_aspect("equal", adjustable="box")
-
-    def _plot_layer(self):
-        xyc, ibn = self._grid.get_layer_slice(self._index, activeonly=self._active)
-
-        xval = xyc[:, :, 0]
-        yval = xyc[:, :, 1]
-
-        xvmin = xval.min()
-        xvmax = xval.max()
-        yvmin = yval.min()
-        yvmax = yval.max()
-
-        if self._window is None:
-            xmin = xvmin - 0.05 * (abs(xvmax - xvmin))
-            xmax = xvmax + 0.05 * (abs(xvmax - xvmin))
-            ymin = yvmin - 0.05 * (abs(yvmax - yvmin))
-            ymax = yvmax + 0.05 * (abs(yvmax - yvmin))
-        else:
-            xmin, xmax, ymin, ymax = self._window
-
-        patches = []
-
-        for pos in range(len(ibn)):
-            nppol = xyc[pos, :, :]
-            if nppol.mean() > 0.0:
-                polygon = Polygon(nppol, True)
-                patches.append(polygon)
-
-        patchcoll = PatchCollection(
-            patches, edgecolors=(self._linecolor,), cmap=self.colormap
-        )
-
-        if self._prop:
-            pvalues = self._prop.values
-            pvalues = pvalues[:, :, self._index - 1]
-            pvalues = pvalues[~pvalues.mask]
-
-            patchcoll.set_array(pvalues)
-
-            pmin = self._minvalue
-            if self._minvalue is None:
-                pmin = pvalues.min()
-
-            pmax = self._maxvalue
-            if self._maxvalue is None:
-                pmax = pvalues.max()
-
-            patchcoll.set_clim([pmin, pmax])
-
-        im = self._ax.add_collection(patchcoll)
-        self._ax.set_xlim((xmin, xmax))
-        self._ax.set_ylim((ymin, ymax))
-        self._fig.colorbar(im)
-
-        plt.gca().set_aspect("equal", adjustable="box")
+"""Module for 3D Grid slice plots, using matplotlib."""
+
+
+import matplotlib.pyplot as plt
+from matplotlib.patches import Polygon
+from matplotlib.collections import PatchCollection
+
+from xtgeo.common import XTGeoDialog
+from xtgeo.plot.baseplot import BasePlot
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class Grid3DSlice(BasePlot):
+    """Class for plotting a row, a column, or a layer, using matplotlib."""
+
+    def __init__(self):
+        """Construct an instance for a Grid3DSlice object."""
+        super().__init__()
+
+        self._wells = None
+        self._surface = None
+        self._tight = False
+
+        self._wfence = None
+        self._legendtitle = "Map"
+
+        self._colormap = "rainbow"
+        self._linecolor = "black"
+        self._clist = None
+        self._prop = None
+        self._grid = None
+        self._geomlist = None
+        self._index = 1
+        self._actnum = None
+        self._window = None
+        self._active = True
+        self._minvalue = None
+        self._maxvalue = None
+
+    # ==================================================================================
+    # Functions methods (public)
+    # Beware general methods in base class also!
+    # ==================================================================================
+
+    def plot_gridslice(
+        self,
+        grid,
+        prop=None,
+        mode="layer",
+        minvalue=None,
+        maxvalue=None,
+        colormap=None,
+        linecolor="black",
+        index=1,
+        window=None,
+        activeonly=True,
+    ):
+        """Plot a row slice, column slice or layer slice of a grid.
+
+        Args:
+            grid (Grid): The XTGeo grid object
+            prop (GridProperty, optional): The XTGeo grid property object
+            mode (str): Choose between 'column', 'row', 'layer' (default)
+            minvalue (float): Minimum level color scale (default: from data)
+            maxvalue (float): Maximum level color scale (default: from data)
+            index (int): Index to plot e.g layer number if layer slice (first=1)
+            colormap: Color map to use for cells, e.g. 'rainbow' or an rmscol file
+            linecolor (str or tuple): Color of grid lines (black/white/grey
+                or a tuple with 4 numbers on valid matplotlib format)
+            activeonly (bool): If only use active cells
+            window (str): Some window
+
+        """
+        self._index = index
+        if colormap is not None:
+            self._colormap = colormap
+
+        self._linecolor = linecolor
+        if not isinstance(linecolor, tuple) and linecolor not in (
+            "black",
+            "grey",
+            "white",
+        ):
+            raise ValueError("Value of linecolor is invalid")
+
+        self._clist = grid.get_xyz_corners()  # get XYZ for each corner, 24 arrays
+        self._grid = grid
+        self._prop = prop
+        self._window = window
+
+        if self._geomlist is None:
+            # returns (xori, yori, zori, xmin, xmax, ymin, ymax, zmin, zmax,...)
+            self._geomlist = grid.get_geometrics(allcells=True, cellcenter=False)
+
+        self._active = activeonly
+
+        self._minvalue = minvalue
+        self._maxvalue = maxvalue
+
+        if mode == "column":
+            pass  # self._plot_row()
+        elif mode == "row":
+            pass  # self._plot_row()
+        else:
+            self._plot_layer()
+
+    # def _plot_row(self):
+
+    #     geomlist = self._geomlist
+
+    #     if self._window is None:
+    #         xmin = geomlist[3] - 0.05 * (abs(geomlist[4] - geomlist[3]))
+    #         xmax = geomlist[4] + 0.05 * (abs(geomlist[4] - geomlist[3]))
+    #         zmin = geomlist[7] - 0.05 * (abs(geomlist[8] - geomlist[7]))
+    #         zmax = geomlist[8] + 0.05 * (abs(geomlist[8] - geomlist[7]))
+    #     else:
+    #         xmin, xmax, zmin, zmax = self._window
+
+    #     # now some numpy operations, numbering is intended
+    #     clist = self._clist
+    #     xz0 = np.column_stack((clist[0].values1d, clist[2].values1d))
+    #     xz1 = np.column_stack((clist[3].values1d, clist[5].values1d))
+    #     xz2 = np.column_stack((clist[15].values1d, clist[17].values1d))
+    #     xz3 = np.column_stack((clist[12].values1d, clist[14].values1d))
+
+    #     xyc = np.column_stack((xz0, xz1, xz2, xz3))
+    #     xyc = xyc.reshape(self._grid.nlay, self._grid.ncol * self._grid.nrow, 4, 2)
+
+    #     patches = []
+
+    #     for pos in range(self._grid.nrow * self._grid.nlay):
+    #         nppol = xyc[self._index - 1, pos, :, :]
+    #         if nppol.mean() > 0.0:
+    #             polygon = Polygon(nppol, True)
+    #             patches.append(polygon)
+
+    #     black = (0, 0, 0, 1)
+    #     patchcoll = PatchCollection(patches, edgecolors=(black,), cmap=self.colormap)
+
+    #     # patchcoll.set_array(np.array(pvalues))
+
+    #     # patchcoll.set_clim([minvalue, maxvalue])
+
+    #     im = self._ax.add_collection(patchcoll)
+    #     self._ax.set_xlim((xmin, xmax))
+    #     self._ax.set_ylim((zmin, zmax))
+    #     self._ax.invert_yaxis()
+    #     self._fig.colorbar(im)
+
+    #     # plt.gca().set_aspect("equal", adjustable="box")
+
+    def _plot_layer(self):
+        xyc, ibn = self._grid.get_layer_slice(self._index, activeonly=self._active)
+
+        xval = xyc[:, :, 0]
+        yval = xyc[:, :, 1]
+
+        xvmin = xval.min()
+        xvmax = xval.max()
+        yvmin = yval.min()
+        yvmax = yval.max()
+
+        if self._window is None:
+            xmin = xvmin - 0.05 * (abs(xvmax - xvmin))
+            xmax = xvmax + 0.05 * (abs(xvmax - xvmin))
+            ymin = yvmin - 0.05 * (abs(yvmax - yvmin))
+            ymax = yvmax + 0.05 * (abs(yvmax - yvmin))
+        else:
+            xmin, xmax, ymin, ymax = self._window
+
+        patches = []
+
+        for pos in range(len(ibn)):
+            nppol = xyc[pos, :, :]
+            if nppol.mean() > 0.0:
+                polygon = Polygon(nppol, True)
+                patches.append(polygon)
+
+        patchcoll = PatchCollection(
+            patches, edgecolors=(self._linecolor,), cmap=self.colormap
+        )
+
+        if self._prop:
+            pvalues = self._prop.values
+            pvalues = pvalues[:, :, self._index - 1]
+            pvalues = pvalues[~pvalues.mask]
+
+            patchcoll.set_array(pvalues)
+
+            pmin = self._minvalue
+            if self._minvalue is None:
+                pmin = pvalues.min()
+
+            pmax = self._maxvalue
+            if self._maxvalue is None:
+                pmax = pvalues.max()
+
+            patchcoll.set_clim([pmin, pmax])
+
+        im = self._ax.add_collection(patchcoll)
+        self._ax.set_xlim((xmin, xmax))
+        self._ax.set_ylim((ymin, ymax))
+        self._fig.colorbar(im)
+
+        plt.gca().set_aspect("equal", adjustable="box")
```

## xtgeo/plot/xsection.py

 * *Ordering differences only*

```diff
@@ -1,1204 +1,1204 @@
-"""Module for fast XSection plots of wells/surfaces etc, using matplotlib."""
-
-
-import math
-import warnings
-from collections import OrderedDict
-
-import matplotlib.pyplot as plt
-import numpy as np
-import numpy.ma as ma
-import pandas as pd
-from matplotlib import collections as mc
-from matplotlib.lines import Line2D
-from scipy.ndimage import gaussian_filter
-
-from xtgeo.common import XTGeoDialog
-from xtgeo.well import Well
-from xtgeo.xyz import Polygons
-
-from .baseplot import BasePlot, _get_colormap
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class XSection(BasePlot):
-    """Class for plotting a cross-section of a well.
-
-    Args:
-        zmin (float): Upper level of the plot (top Y axis).
-        zmax (float): Lower level of the plot (bottom Y axis).
-        well (Well): XTGeo well object.
-        surfaces (list): List of XTGeo RegularSurface objects
-        surfacenames (list): List of surface names (str) for legend
-        cube (Cube): A XTGeo Cube instance
-        grid (Grid): A XTGeo Grid instance
-        gridproperty (GridProperty): A XTGeo GridProperty instance
-        colormap (str): Name of colormap, e.g. 'Set1'. Default is 'xtgeo'
-        outline (obj): XTGeo Polygons object
-
-    """
-
-    # pylint: disable=too-many-instance-attributes
-
-    def __init__(
-        self,
-        zmin=0,
-        zmax=9999,
-        well=None,
-        surfaces=None,
-        sampling=20,
-        nextend=5,
-        colormap=None,
-        zonelogshift=0,
-        surfacenames=None,
-        cube=None,
-        grid=None,
-        gridproperty=None,
-        outline=None,
-    ):
-        """Init method."""
-        super().__init__()
-
-        self._zmin = zmin
-        self._zmax = zmax
-        self._well = well
-        self._nextend = nextend
-        self._sampling = sampling
-        self._surfaces = surfaces
-        self._surfacenames = surfacenames
-        self._cube = cube
-        self._grid = grid
-        self._gridproperty = gridproperty
-        self._zonelogshift = zonelogshift
-        self._outline = outline
-
-        self._has_axes = True
-        self._has_legend = True
-
-        self._pagesize = "A4"
-        self._fence = None
-        self._legendtitle = "Zones"
-        self._legendsize = 5
-
-        self._ax1 = None
-        self._ax2 = None
-        self._ax3 = None
-        self._allfigs = []
-
-        self._colormap_cube = None
-        self._colorlegend_cube = False
-
-        self._colormap_grid = None
-        self._colorlegend_grid = False
-
-        if colormap is None:
-            self._colormap = _get_colormap("viridis")
-
-        else:
-            self.define_colormap(colormap)
-
-        self._colormap_facies = self.define_any_colormap("xtgeo")
-        self._colormap_facies_dict = {idx: idx for idx in range(100)}
-
-        self._colormap_perf = self.define_any_colormap("xtgeo")
-        self._colormap_perf_dict = {idx: idx for idx in range(100)}
-
-        self._colormap_zonelog = None
-        self._colormap_zonelog_dict = {idx: idx for idx in range(100)}
-
-        logger.info("Ran __init__ ...")
-        logger.debug("Colormap is %s", self._colormap)
-
-    # ==================================================================================
-    # Properties
-    # Notice additonal props in base class
-    # ==================================================================================
-    @property
-    def pagesize(self):
-        """Returns page size."""
-        return self._pagesize
-
-    @property
-    def legendsize(self):
-        """Returns or set the legend size."""
-        return self._legendsize
-
-    @legendsize.setter
-    def legendsize(self, lsize):
-        self._legendsize = lsize
-
-    @property
-    def has_legend(self):
-        """Returns or set the legends."""
-        return self._has_legend
-
-    @has_legend.setter
-    def has_legend(self, value):
-        if not isinstance(value, bool):
-            raise ValueError("Input is not a bool")
-
-        self._has_legend = value
-
-    @property
-    def has_axes(self):
-        """Returns or set the axes status."""
-        return self._has_axes
-
-    @has_axes.setter
-    def has_axes(self, value):
-        if not isinstance(value, bool):
-            raise ValueError("Input is not a bool")
-
-        self._has_axes = value
-
-    @property
-    def colormap_facies(self):
-        """Set or get the facies colormap."""
-        return self._colormap_facies
-
-    @colormap_facies.setter
-    def colormap_facies(self, cmap):
-        self._colormap_facies = self.define_any_colormap(cmap)
-
-    @property
-    def colormap_zonelog(self):
-        """Set or get the zonelog colormap."""
-        return self._colormap_zonelog
-
-    @colormap_zonelog.setter
-    def colormap_zonelog(self, cmap):
-        self._colormap_zonelog = self.define_any_colormap(cmap)
-
-    @property
-    def colormap_perf(self):
-        """Set or get the perforations colormap."""
-        return self._colormap_perf
-
-    @colormap_perf.setter
-    def colormap_perf(self, cmap):
-        self._colormap_perf = self.define_any_colormap(cmap)
-
-    @property
-    def colormap_facies_dict(self):
-        """Set or get the facies colormap actual dict table."""
-        return self._colormap_facies_dict
-
-    @colormap_facies_dict.setter
-    def colormap_facies_dict(self, xdict):
-        if not isinstance(xdict, dict):
-            raise ValueError("Input is not a dict")
-
-        # if not all(isinstance(item, int) for item in list(xdict.values)):
-        #     raise ValueError('Dict values is a list, but some elems are '
-        #                      'not ints!')
-
-        self._colormap_facies_dict = xdict
-
-    @property
-    def colormap_perf_dict(self):
-        """Set or get the perf colormap actual dict table."""
-        return self._colormap_perf_dict
-
-    @colormap_perf_dict.setter
-    def colormap_perf_dict(self, xdict):
-        if not isinstance(xdict, dict):
-            raise ValueError("Input is not a dict")
-
-        # if not all(isinstance(item, int) for item in list(xdict.values)):
-        #     raise ValueError('Dict values is a list, but some elems are '
-        #                      'not ints!')
-
-        self._colormap_perf_dict = xdict
-
-    @property
-    def colormap_zonelog_dict(self):
-        """Set or get the zonelog colormap actual dict table."""
-        return self._colormap_zonelog_dict
-
-    @colormap_zonelog_dict.setter
-    def colormap_zonelog_dict(self, xdict):
-        if not isinstance(xdict, dict):
-            raise ValueError("Input is not a dict")
-
-        # if not all(isinstance(item, int) for item in list(xdict.values)):
-        #     raise ValueError('Dict values is a list, but some elems are '
-        #                      'not ints!')
-
-        self._colormap_zonelog_dict = xdict
-
-    @property
-    def fence(self):
-        """Set or get the fence spesification."""
-        if self._fence is None:
-            if self._well is not None:
-                wfence = self._well.get_fence_polyline(
-                    sampling=self._sampling, nextend=self._nextend, tvdmin=self._zmin
-                )
-                self._fence = wfence
-
-                if wfence is False:
-                    self._fence = None
-            else:
-                raise ValueError("Input well is None")  # should be more flexible
-        return self._fence
-
-    @fence.setter
-    def fence(self, myfence):
-        # this can be extended with checks and various types of input...
-        self._fence = myfence
-
-    # ==================================================================================
-    # Functions methods (public)
-    # ==================================================================================
-
-    def canvas(self, title=None, subtitle=None, infotext=None, figscaling=1.0):
-        """Prepare the canvas to plot on, with title and subtitle.
-
-        Args:
-            title (str, optional): Title of plot.
-            subtitle (str, optional): Sub title of plot.
-            infotext (str, optional): Text to be written as info string.
-            figscaling (str, optional): Figure scaling, default is 1.0
-
-
-        """
-        # overriding the base class canvas
-
-        plt.rcParams["axes.xmargin"] = 0  # fill the plot margins
-
-        if not self._has_axes:
-            plt.rcParams["axes.titlecolor"] = (0, 0, 0, 0)
-            plt.rcParams["axes.edgecolor"] = (0, 0, 0, 0)
-            plt.rcParams["axes.labelcolor"] = (0, 0, 0, 0)
-            plt.rcParams["axes.titlecolor"] = (0, 0, 0, 0)
-            plt.rcParams["xtick.color"] = (0, 0, 0, 0)
-            plt.rcParams["ytick.color"] = (0, 0, 0, 0)
-
-        # self._fig, (ax1, ax2) = plt.subplots(2, figsize=(11.69, 8.27))
-        self._fig, _ = plt.subplots(figsize=(11.69 * figscaling, 8.27 * figscaling))
-        self._allfigs.append(self._fig)
-        ax1 = OrderedDict()
-
-        # since the plan is to at some point remove plotting from xtgeo:
-        warnings.filterwarnings("ignore", category=DeprecationWarning)
-        ax1["main"] = plt.subplot2grid((20, 28), (0, 0), rowspan=20, colspan=23)
-
-        ax2 = plt.subplot2grid(
-            (20, 28), (10, 23), rowspan=5, colspan=5, frame_on=self._has_legend
-        )
-
-        ax3 = plt.subplot2grid(
-            (20, 28), (15, 23), rowspan=5, colspan=5, frame_on=self._has_legend
-        )
-
-        if self._has_legend:
-            # indicate A to B
-            plt.text(
-                0.02,
-                0.98,
-                "A",
-                ha="left",
-                va="top",
-                transform=ax1["main"].transAxes,
-                fontsize=8,
-            )
-            plt.text(
-                0.98,
-                0.98,
-                "B",
-                ha="right",
-                va="top",
-                transform=ax1["main"].transAxes,
-                fontsize=8,
-            )
-
-        # title here:
-        if title is not None:
-            plt.text(
-                0.5,
-                1.09,
-                title,
-                ha="center",
-                va="center",
-                transform=ax1["main"].transAxes,
-                fontsize=18,
-            )
-
-        if subtitle is not None:
-            ax1["main"].set_title(subtitle, size=14)
-
-        if infotext is not None:
-            plt.text(
-                -0.11,
-                -0.11,
-                infotext,
-                ha="left",
-                va="center",
-                transform=ax1["main"].transAxes,
-                fontsize=6,
-            )
-
-        ax1["main"].set_ylabel("Depth", fontsize=12.0)
-        ax1["main"].set_xlabel("Length along well", fontsize=12)
-
-        ax2.tick_params(
-            axis="both",
-            which="both",
-            bottom=False,
-            top=False,
-            right=False,
-            left=False,
-            labelbottom=False,
-            labeltop=False,
-            labelright=False,
-            labelleft=False,
-        )
-
-        ax3.tick_params(
-            axis="both",
-            which="both",
-            bottom=False,
-            top=False,
-            right=False,
-            left=False,
-            labelbottom=False,
-            labeltop=False,
-            labelright=False,
-            labelleft=False,
-        )
-
-        # need these also, a bug in functions above?
-        ax2.xaxis.set_major_formatter(plt.NullFormatter())
-        ax2.yaxis.set_major_formatter(plt.NullFormatter())
-        ax3.xaxis.set_major_formatter(plt.NullFormatter())
-        ax3.yaxis.set_major_formatter(plt.NullFormatter())
-
-        self._ax1 = ax1
-        self._ax2 = ax2
-        self._ax3 = ax3
-
-    def plot_well(
-        self,
-        zonelogname="ZONELOG",
-        facieslogname=None,
-        perflogname=None,
-        wellcrossings=None,
-        wellcrossingnames=True,
-        wellcrossingyears=False,
-        welltrajcolor="black",
-        welltrajwidth=6,
-    ):
-        """Input an XTGeo Well object and plot it."""
-        if self.fence is None:
-            return
-
-        wo = self._well
-
-        # reduce the well data by Pandas operations
-        dfr = wo.dataframe
-        wo.dataframe = dfr[dfr["Z_TVDSS"] > self._zmin]
-
-        # Create a relative XYLENGTH vector (0.0 where well starts)
-        wo.create_relative_hlen()
-
-        dfr = wo.dataframe
-        if dfr.empty:
-            self._showok = False
-            return
-
-        # get the well trajectory (numpies) as copy
-        zv = dfr["Z_TVDSS"].values.copy()
-        hv = dfr["R_HLEN"].values.copy()
-
-        # plot the perflog, if any, first
-        if perflogname:
-            ax, bba = self._currentax(axisname="perf")
-            self._plot_well_perflog(dfr, ax, bba, perflogname, legend=self._has_legend)
-
-        # plot the facies, if any, behind the trajectory; ie. first or second
-        if facieslogname:
-            ax, bba = self._currentax(axisname="facies")
-            self._plot_well_faclog(dfr, ax, bba, facieslogname, legend=self._has_legend)
-
-        axx, _bbxa = self._currentax(axisname="well")
-        self._plot_well_traj(
-            axx, zv, hv, welltrajcolor=welltrajcolor, linewidth=welltrajwidth
-        )
-
-        if zonelogname:
-            ax, bba = self._currentax(axisname="main")
-            self._plot_well_zlog(dfr, axx, bba, zonelogname, legend=self._has_legend)
-
-        if wellcrossings is not None and wellcrossings.empty:
-            wellcrossings = None
-
-        if wellcrossings is not None:
-            self._plot_well_crossings(
-                dfr, axx, wellcrossings, wellcrossingnames, wellcrossingyears
-            )
-
-    def set_xaxis_md(self, gridlines=False):
-        """Set x-axis labels to measured depth."""
-        md_start = self._well.dataframe["MDEPTH"].iloc[0]
-        md_start_round = int(math.floor(md_start / 100.0)) * 100
-        md_start_delta = md_start - md_start_round
-
-        auto_ticks = plt.xticks()
-        auto_ticks_delta = auto_ticks[0][1] - auto_ticks[0][0]
-
-        ax, _ = self._currentax(axisname="main")
-        lim = ax.get_xlim()
-
-        new_ticks = []
-        new_tick_labels = []
-        delta = 0
-        for tick in auto_ticks[0]:
-            new_ticks.append(int(float(tick) - md_start_delta))
-            new_tick_labels.append(int(md_start_round + delta))
-            delta += auto_ticks_delta
-
-        # Set new xticks and labels
-        plt.xticks(new_ticks, new_tick_labels)
-
-        if gridlines:
-            ax.tick_params(axis="y", direction="in", which="both")
-            ax.minorticks_on()
-            ax.grid(color="black", linewidth=0.8, which="major", linestyle="-")
-            ax.grid(color="black", linewidth=0.5, which="minor", linestyle="--")
-
-        # Restore xaxis limits and set axis title
-        ax.set_xlim(lim)
-        ax.set_ylabel("TVD MSL [m]", fontsize=12.0)
-        ax.set_xlabel("Measured Depth [m]", fontsize=12)
-
-    def _plot_well_traj(self, ax, zv, hv, welltrajcolor, linewidth):
-        """Plot the trajectory as a black line."""
-        zv_copy = ma.masked_where(zv < self._zmin, zv)
-        hv_copy = ma.masked_where(zv < self._zmin, hv)
-
-        ax.plot(hv_copy, zv_copy, linewidth=linewidth, c=welltrajcolor)
-
-    @staticmethod
-    def _line_segments_colors(df, idx, ctable, logname, fillnavalue):
-        """Get segment and color array for plotting matplotlib lineCollection."""
-        df_idx = pd.DataFrame(
-            {"idx_log": list(idx.keys()), "idx_color": list(idx.values())}
-        )
-
-        df_ctable = df_idx.merge(
-            pd.DataFrame({"ctable": ctable}),
-            how="left",
-            left_on="idx_color",
-            right_index=True,
-        )
-
-        dff = df.merge(df_ctable, how="left", left_on=logname, right_on="idx_log")
-
-        dff["point"] = list(zip(dff["R_HLEN"], dff["Z_TVDSS"]))
-
-        # find line segments
-        segments = []
-        segments_i = -1
-        colorlist = []
-        previous_color = None
-
-        for point, color in zip(dff["point"], dff["ctable"]):
-            if np.any(np.isnan(color)):
-                color = fillnavalue
-
-            if color == previous_color:
-                segments[segments_i].append(point)
-                previous_color = color
-            else:
-                # add endpoint to current segment
-                if segments_i > 0:
-                    segments[segments_i].append(point)
-
-                # start new segment
-                segments.append([point])
-                colorlist.append(color)
-
-                previous_color = color
-                segments_i += 1
-
-        colorlist = np.asarray(colorlist, dtype=object)
-
-        return segments, colorlist
-
-    def _plot_well_zlog(self, df, ax, bba, zonelogname, logwidth=4, legend=False):
-        """Plot the zone log as colored segments."""
-        if zonelogname not in df.columns:
-            return
-
-        zomin = 0
-        zomax = 0
-
-        try:
-            zomin = int(df[zonelogname].min())
-            zomax = int(df[zonelogname].max())
-        except ValueError:
-            self._showok = False
-            return
-
-        logger.info("ZONELOG min - max is %s - %s", zomin, zomax)
-
-        zshift = 0
-        if self._zonelogshift != 0:
-            zshift = self._zonelogshift
-
-        if self.colormap_zonelog is not None:
-            cmap = self.colormap_zonelog
-            ctable = self.get_any_colormap_as_table(cmap)
-        else:
-            ctable = self.get_colormap_as_table()
-
-        idx = self.colormap_zonelog_dict
-
-        # adjust for zoneshift.
-        idx_zshift = dict()
-        for key in idx:
-            idx_zshift[key - zshift + 1] = idx[key]
-
-        fillnavalue = (0.9, 0.9, 0.9)
-        segments, segments_colors = self._line_segments_colors(
-            df, idx_zshift, ctable, zonelogname, fillnavalue
-        )
-
-        lc = mc.LineCollection(
-            segments, colors=segments_colors, linewidth=logwidth, zorder=202
-        )
-
-        ax.add_collection(lc)
-
-        if legend:
-            zrecord = self._well.get_logrecord(zonelogname)
-            zrecord = {val: zname for val, zname in zrecord.items() if val >= 0}
-
-            zcolors = dict()
-            for zone in zrecord:
-                if isinstance(idx[zone], str):
-                    color = idx[zone]
-                else:
-                    color = ctable[idx[zone]]
-
-                zcolors[zrecord[zone]] = color
-
-            self._drawproxylegend(ax, bba, items=zcolors, title="Zonelog")
-
-    def _plot_well_faclog(self, df, ax, bba, facieslogname, logwidth=9, legend=True):
-        """Plot the facies log as colored segments.
-
-        Args:
-            df (dataframe): The Well dataframe.
-            ax (axes): The ax plot object.
-            bba: Bounding box
-            facieslogname (str): name of the facies log.
-            logwidth (int): Log linewidth.
-            legend (bool): Plot log legend?
-        """
-        if facieslogname not in df.columns:
-            return
-
-        cmap = self.colormap_facies
-        ctable = self.get_any_colormap_as_table(cmap)
-        idx = self.colormap_facies_dict
-
-        fillnavalue = (0, 0, 0, 0)  # transparent
-        segments, segments_colors = self._line_segments_colors(
-            df, idx, ctable, facieslogname, fillnavalue
-        )
-
-        lc = mc.LineCollection(
-            segments, colors=segments_colors, linewidth=logwidth, zorder=201
-        )
-
-        ax.add_collection(lc)
-
-        if legend:
-            frecord = self._well.get_logrecord(facieslogname)
-            frecord = {val: fname for val, fname in frecord.items() if val >= 0}
-
-            fcolors = dict()
-            for facies in frecord:
-                if isinstance(idx[facies], str):
-                    color = idx[facies]
-                else:
-                    color = ctable[idx[facies]]
-
-                fcolors[frecord[facies]] = color
-
-            self._drawproxylegend(ax, bba, items=fcolors, title="Facies")
-
-    def _plot_well_perflog(self, df, ax, bba, perflogname, logwidth=12, legend=True):
-        """Plot the perforation log as colored segments.
-
-        Args:
-            df (dataframe): The Well dataframe.
-            ax (axes): The ax plot object.
-            zv (ndarray): The numpy Z TVD array.
-            bba: Boundinng box
-            hv (ndarray): The numpy Length  array.
-            perflogname (str): name of the perforation log.
-            logwidth (int): Log linewidth.
-            legend (bool): Plot log legend?
-        """
-        if perflogname not in df.columns:
-            return
-
-        cmap = self.colormap_perf
-        ctable = self.get_any_colormap_as_table(cmap)
-        idx = self.colormap_perf_dict
-
-        fillnavalue = (0, 0, 0, 0)  # transparent
-        segments, segments_colors = self._line_segments_colors(
-            df, idx, ctable, perflogname, fillnavalue
-        )
-
-        lc = mc.LineCollection(
-            segments, colors=segments_colors, linewidth=logwidth, zorder=200
-        )
-
-        ax.add_collection(lc)
-
-        if legend:
-            precord = self._well.get_logrecord(perflogname)
-            precord = {val: pname for val, pname in precord.items() if val >= 0}
-
-            pcolors = dict()
-            for perf in precord:
-                if isinstance(idx[perf], str):
-                    color = idx[perf]
-                else:
-                    color = ctable[idx[perf]]
-
-                pcolors[precord[perf]] = color
-
-            self._drawproxylegend(ax, bba, items=pcolors, title="Perforations")
-
-    @staticmethod
-    def _plot_well_crossings(dfr, ax, wcross, names=True, years=False):
-        """Plot well crossing based on dataframe (wcross).
-
-        The well crossing coordinates are identified for this well,
-        and then it is looking for the closest coordinate. Given this
-        coordinate, a position is chosen.
-
-        The pandas dataframe wcross shall have the following columns:
-
-        * Name of crossing wells named CWELL
-        * Coordinate X named X_UTME
-        * Coordinate Y named Y_UTMN
-        * Coordinate Z named Z_TVDSS
-
-        Optional column:
-        * Drilled year of crossing well named CYEAR
-
-        Args:
-            dfr: Well dataframe
-            ax: current axis
-            wcross: A pandas dataframe with precomputed well crossings
-            names: Display the names of the crossed wells
-            years: Display the drilled year of the crossed wells
-        """
-        placings = {
-            0: (40, 40),
-            1: (40, -20),
-            2: (-30, 30),
-            3: (30, 20),
-            4: (-40, 30),
-            5: (-20, 40),
-        }
-
-        for index, row in wcross.iterrows():
-            xcoord = row.X_UTME
-            ycoord = row.Y_UTMN
-
-            dfrc = dfr.copy()
-
-            dfrc["DLEN"] = pow(
-                pow(dfrc.X_UTME - xcoord, 2) + pow(dfrc.Y_UTMN - ycoord, 2), 0.5
-            )
-
-            minindx = dfrc.DLEN.idxmin()
-
-            ax.scatter(
-                dfrc.R_HLEN[minindx],
-                row.Z_TVDSS,
-                marker="o",
-                color="black",
-                s=70,
-                zorder=300,
-            )
-            ax.scatter(
-                dfrc.R_HLEN[minindx],
-                row.Z_TVDSS,
-                marker="o",
-                color="orange",
-                s=38,
-                zorder=302,
-            )
-
-            modulo = index % 5
-
-            text = ""
-            if names:
-                text = Well.get_short_wellname(row.CWELL)
-
-            if years:
-                if names:
-                    text = text + "\n" + row.CYEAR
-                else:
-                    text = row.CYEAR
-
-            if names or years:
-                ax.annotate(
-                    text,
-                    size=6,
-                    xy=(dfrc.R_HLEN[minindx], row.Z_TVDSS),
-                    xytext=placings[modulo],
-                    textcoords="offset points",
-                    arrowprops=dict(
-                        arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=90"
-                    ),
-                    color="black",
-                )
-
-    def _drawproxylegend(self, ax, bba, items, title=None):
-        proxies = []
-        labels = []
-
-        for item in items:
-            color = items[item]
-            proxies.append(Line2D([0, 1], [0, 1], color=color, linewidth=5))
-            labels.append(item)
-
-        ax.legend(
-            proxies,
-            labels,
-            loc="upper left",
-            bbox_to_anchor=bba,
-            prop={"size": self._legendsize},
-            title=title,
-            handlelength=2,
-        )
-
-    def _drawlegend(self, ax, bba, title=None):
-        leg = ax.legend(
-            loc="upper left",
-            bbox_to_anchor=bba,
-            prop={"size": self._legendsize},
-            title=title,
-            handlelength=2,
-        )
-
-        for myleg in leg.get_lines():
-            myleg.set_linewidth(5)
-
-    def _currentax(self, axisname="main"):
-        """Keep track of current axis; is needed as one new legend need one new axis."""
-        # for multiple legends, bba is dynamic
-        bbapos = {
-            "main": (1.22, 1.12, 1, 0),
-            "contacts": (1.01, 1.12),
-            "second": (1.22, 0.50),
-            "facies": (1.01, 1.00),
-            "perf": (1.22, 0.45),
-        }
-
-        ax1 = self._ax1
-
-        if axisname != "main":
-            ax1[axisname] = self._ax1["main"].twinx()
-
-            # invert min,max to invert the Y axis
-            ax1[axisname].set_ylim([self._zmax, self._zmin])
-
-            ax1[axisname].set_yticklabels([])
-            ax1[axisname].tick_params(axis="y", direction="in")
-
-        ax = self._ax1[axisname]
-
-        bba = bbapos.get(axisname, (1.22, 0.5))
-
-        return ax, bba
-
-    def plot_cube(
-        self,
-        colormap="seismic",
-        vmin=None,
-        vmax=None,
-        alpha=0.7,
-        interpolation="gaussian",
-        sampling="nearest",
-    ):
-        """Plot a cube backdrop.
-
-        Args:
-            colormap (ColorMap): Name of color map (default 'seismic')
-            vmin (float): Minimum value in plot.
-            vmax (float); Maximum value in plot
-            alpha (float): Alpah blending number beween 0 and 1.
-            interpolation (str): Interpolation for plotting, cf. matplotlib
-                documentation on this. Also gaussianN is allowed, where
-                N = 1..9.
-            sampling (str): 'nearest' (default) or 'trilinear' (more precise)
-
-        Raises:
-            ValueError: No cube is loaded
-
-        """
-        if self.fence is None:
-            return
-
-        if self._cube is None:
-            raise ValueError("Ask for plot cube, but noe cube is loaded")
-
-        ax, _ = self._currentax(axisname="main")
-
-        zinc = self._cube.zinc / 2.0
-
-        zvv = self._cube.get_randomline(
-            self.fence,
-            zmin=self._zmin,
-            zmax=self._zmax,
-            zincrement=zinc,
-            sampling=sampling,
-        )
-
-        h1, h2, v1, v2, arr = zvv
-
-        # if vmin is not None or vmax is not None:
-        #     arr = np.clip(arr, vmin, vmax)
-
-        if self._colormap_cube is None:
-            if colormap is None:
-                colormap = "seismic"
-            self._colormap_cube = self.define_any_colormap(colormap)
-
-        if "gaussian" in interpolation:  # allow gaussian3 etc
-            nnv = interpolation[-1]
-            try:
-                nnv = int(nnv)
-                arr = gaussian_filter(arr, nnv)
-                interpolation = "none"
-            except ValueError:
-                interpolation = "gaussian"
-
-        img = ax.imshow(
-            arr,
-            cmap=self._colormap_cube,
-            interpolation=interpolation,
-            vmin=vmin,
-            vmax=vmax,
-            extent=(h1, h2, v2, v1),
-            aspect="auto",
-            alpha=alpha,
-        )
-
-        logger.info("Actual VMIN and VMAX: %s", img.get_clim())
-        # steer this?
-        if self._colorlegend_cube:
-            self._fig.colorbar(img, ax=ax)
-
-    def plot_grid3d(
-        self,
-        colormap="rainbow",
-        vmin=None,
-        vmax=None,
-        alpha=0.7,
-        zinc=0.5,
-        interpolation="auto",
-    ):
-        """Plot a sampled grid with gridproperty backdrop.
-
-        Args:
-            colormap (ColorMap): Name of color map (default 'rainbow')
-            vmin (float): Minimum value in plot.
-            vmax (float); Maximum value in plot
-            alpha (float): Alpha blending number beween 0 and 1.
-            zinc (float): Sampling vertically, default is 0.5
-            interpolation (str): Interpolation for plotting, cf. matplotlib
-                documentation on this. "auto" uses "nearest" for discrete
-                parameters and "antialiased" for floats.
-
-        Raises:
-            ValueError: No grid or gridproperty is loaded
-
-        """
-        if self.fence is None:
-            return
-
-        if self._grid is None or self._gridproperty is None:
-            raise ValueError("Ask for plot of grid, but no grid is loaded")
-
-        ax, _bba = self._currentax(axisname="main")
-
-        zvv = self._grid.get_randomline(
-            self.fence,
-            self._gridproperty,
-            zmin=self._zmin,
-            zmax=self._zmax,
-            zincrement=zinc,
-        )
-
-        h1, h2, v1, v2, arr = zvv
-
-        # if vmin is not None or vmax is not None:
-        #     arr = np.clip(arr, vmin, vmax)
-
-        if self._colormap_grid is None:
-            if colormap is None:
-                colormap = "rainbow"
-            self._colormap_grid = self.define_any_colormap(colormap)
-
-        if interpolation == "auto":
-            if self._gridproperty.isdiscrete:
-                interpolation = "nearest"
-            else:
-                interpolation = "antialiased"
-
-        img = ax.imshow(
-            arr,
-            cmap=self._colormap_grid,
-            vmin=vmin,
-            vmax=vmax,
-            extent=(h1, h2, v2, v1),
-            aspect="auto",
-            alpha=alpha,
-            interpolation=interpolation,
-        )
-
-        logger.info("Actual VMIN and VMAX: %s", img.get_clim())
-        # steer this?
-        if self._colorlegend_grid:
-            self._fig.colorbar(img, ax=ax)
-
-    def plot_surfaces(
-        self,
-        fill=False,
-        surfaces=None,
-        surfacenames=None,
-        colormap=None,
-        onecolor=None,
-        linewidth=1.0,
-        linestyle="-",
-        legend=True,
-        legendtitle=None,
-        fancyline=False,
-        axisname="main",
-        gridlines=False,
-    ):  # pylint: disable=too-many-branches, too-many-statements
-        """Input a surface list (ordered from top to base) , and plot them."""
-        if self.fence is None:
-            return
-
-        ax, bba = self._currentax(axisname=axisname)
-
-        # either use surfaces from __init__, or override with surfaces
-        # speciefied here
-        if surfaces is None:
-            surfaces = self._surfaces
-            surfacenames = self._surfacenames
-
-        surfacenames = [surf.name for surf in surfaces]
-
-        if legendtitle is None:
-            legendtitle = self._legendtitle
-
-        if colormap is None:
-            colormap = self._colormap
-        else:
-            self.define_colormap(colormap)
-
-        nlen = len(surfaces)
-
-        # legend
-        slegend = []
-        if surfacenames is None:
-            for i in range(nlen):
-                slegend.append(f"Surf {i}")
-
-        else:
-            # do a check
-            if len(surfacenames) != nlen:
-                msg = (
-                    "Wrong number of entries in surfacenames! Number of names "
-                    f"is {len(surfacenames)} while number of files is {nlen}"
-                )
-                raise ValueError(msg)
-
-            slegend = surfacenames
-
-        if self._colormap.N < nlen:
-            raise ValueError(
-                f"Too few colors in color table ({self._colormap.N}) "
-                f"vs number of surfaces ({nlen})"
-            )
-
-        # sample the horizon to the fence:
-        colortable = self.get_colormap_as_table()
-        for i in range(nlen):
-            usecolor = colortable[i]
-            if onecolor:
-                usecolor = onecolor
-            if not fill:
-                hfence = surfaces[i].get_randomline(self.fence)
-                xcol = "white"
-                if fancyline:
-                    cxx = usecolor
-                    if cxx[0] + cxx[1] + cxx[2] > 1.5:
-                        xcol = "black"
-                    ax.plot(
-                        hfence[:, 0], hfence[:, 1], linewidth=1.2 * linewidth, c=xcol
-                    )
-                ax.plot(
-                    hfence[:, 0],
-                    hfence[:, 1],
-                    linewidth=linewidth,
-                    c=usecolor,
-                    label=slegend[i],
-                    linestyle=linestyle,
-                )
-                if fancyline:
-                    ax.plot(
-                        hfence[:, 0], hfence[:, 1], linewidth=0.3 * linewidth, c=xcol
-                    )
-            else:
-                # need copy() .. why?? found by debugging...
-                hfence1 = surfaces[i].get_randomline(self.fence).copy()
-                x1 = hfence1[:, 0]
-                y1 = hfence1[:, 1]
-                if i < (nlen - 1):
-                    hfence2 = surfaces[i + 1].get_randomline(self.fence).copy()
-                    y2 = hfence2[:, 1]
-                else:
-                    y2 = y1.copy()
-
-                ax.plot(
-                    x1, y1, linewidth=0.1 * linewidth, linestyle=linestyle, c="black"
-                )
-                ax.fill_between(x1, y1, y2, facecolor=colortable[i], label=slegend[i])
-
-        # invert min,max to invert the Y axis
-        ax.set_ylim([self._zmax, self._zmin])
-
-        if legend:
-            self._drawlegend(ax, bba, title=legendtitle)
-
-        if axisname != "main":
-            ax.set_yticklabels([])
-
-        ax.tick_params(axis="y", direction="in")
-
-        if axisname == "main" and gridlines:
-            ax.grid(color="grey", linewidth=0.2)
-
-    def plot_md_data(
-        self,
-        data=None,
-        markersize=10,
-        color="red",
-        linestyle="",
-        label=False,
-        zorder=350,
-        **kwargs,
-    ):
-        """Plot MD vs TVD data as lines and/or markers.
-
-        The input pandas dataframe points shall have the following columns:
-        * Name of well(s) named WELL
-        * Coordinate X named MDEPTH
-        * Coordinate Y named Z_TVDSS
-        """
-        ax, _ = self._currentax(axisname="main")
-
-        well = self._well
-        data_well = data.copy()
-        data_well = data_well.loc[data_well["WELL"] == well.xwellname]
-        del data_well["WELL"]
-
-        md_start = well.dataframe["MDEPTH"].iloc[0]
-        data_well["R_HLEN"] = data_well["MDEPTH"]
-        data_well["R_HLEN"] = data_well["R_HLEN"].subtract(md_start)
-
-        data_well.plot(
-            ax=ax,
-            x="R_HLEN",
-            y="Z_TVDSS",
-            legend=None,
-            linestyle=linestyle,
-            markersize=markersize,
-            color=color,
-            label=label,
-            zorder=zorder,
-            **kwargs,
-        )
-
-    def plot_wellmap(self, otherwells=None, expand=1):
-        """Plot well map as local view, optionally with nearby wells.
-
-        Args:
-            otherwells (list of Polygons): List of surrounding wells to plot,
-                these wells are repr as Polygons instances, one per well.
-            expand (float): Plot axis expand factor (default is 1); larger
-                values may be used if other wells are plotted.
-
-
-        """
-        ax = self._ax2
-
-        if self.fence is not None:
-            xwellarray = self._well.dataframe["X_UTME"].values
-            ywellarray = self._well.dataframe["Y_UTMN"].values
-
-            ax.plot(xwellarray, ywellarray, linewidth=4, c="cyan")
-
-            ax.plot(self.fence[:, 0], self.fence[:, 1], linewidth=1, c="black")
-            ax.annotate("A", xy=(self.fence[0, 0], self.fence[0, 1]), fontsize=8)
-            ax.annotate("B", xy=(self.fence[-1, 0], self.fence[-1, 1]), fontsize=8)
-            ax.set_aspect("equal", "datalim")
-
-            left, right = ax.get_xlim()
-            xdiff = right - left
-            bottom, top = ax.get_ylim()
-            ydiff = top - bottom
-
-            ax.set_xlim(left - (expand - 1.0) * xdiff, right + (expand - 1.0) * xdiff)
-            ax.set_ylim(bottom - (expand - 1.0) * ydiff, top + (expand - 1.0) * ydiff)
-        if otherwells:
-            for poly in otherwells:
-                if not isinstance(poly, Polygons):
-                    xtg.warn(f"<otherw> not a Polygons instance, but a {type(poly)}")
-                    continue
-                if poly.name == self._well.xwellname:
-                    continue
-                xwp = poly.dataframe[poly.xname].values
-                ywp = poly.dataframe[poly.yname].values
-                ax.plot(xwp, ywp, linewidth=1, c="grey")
-                ax.annotate(poly.name, xy=(xwp[-1], ywp[-1]), color="grey", size=5)
-
-    def plot_map(self):
-        """Plot well location map as an overall view (with field outline)."""
-        if self.fence is None:
-            return
-
-        if not self._outline:
-            return
-
-        ax = self._ax3
-        if self.fence is not None:
-            xp = self._outline.dataframe["X_UTME"].values
-            yp = self._outline.dataframe["Y_UTMN"].values
-            ip = self._outline.dataframe["POLY_ID"].values
-
-            ax.plot(self._fence[:, 0], self._fence[:, 1], linewidth=3, c="red")
-
-            for i in range(int(ip.min()), int(ip.max()) + 1):
-                xpc = xp.copy()[ip == i]
-                ypc = yp.copy()[ip == i]
-                if len(xpc) > 1:
-                    ax.plot(xpc, ypc, linewidth=0.3, c="black")
-
-            ax.set_aspect("equal", "datalim")
+"""Module for fast XSection plots of wells/surfaces etc, using matplotlib."""
+
+
+import math
+import warnings
+from collections import OrderedDict
+
+import matplotlib.pyplot as plt
+import numpy as np
+import numpy.ma as ma
+import pandas as pd
+from matplotlib import collections as mc
+from matplotlib.lines import Line2D
+from scipy.ndimage import gaussian_filter
+
+from xtgeo.common import XTGeoDialog
+from xtgeo.well import Well
+from xtgeo.xyz import Polygons
+
+from .baseplot import BasePlot, _get_colormap
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class XSection(BasePlot):
+    """Class for plotting a cross-section of a well.
+
+    Args:
+        zmin (float): Upper level of the plot (top Y axis).
+        zmax (float): Lower level of the plot (bottom Y axis).
+        well (Well): XTGeo well object.
+        surfaces (list): List of XTGeo RegularSurface objects
+        surfacenames (list): List of surface names (str) for legend
+        cube (Cube): A XTGeo Cube instance
+        grid (Grid): A XTGeo Grid instance
+        gridproperty (GridProperty): A XTGeo GridProperty instance
+        colormap (str): Name of colormap, e.g. 'Set1'. Default is 'xtgeo'
+        outline (obj): XTGeo Polygons object
+
+    """
+
+    # pylint: disable=too-many-instance-attributes
+
+    def __init__(
+        self,
+        zmin=0,
+        zmax=9999,
+        well=None,
+        surfaces=None,
+        sampling=20,
+        nextend=5,
+        colormap=None,
+        zonelogshift=0,
+        surfacenames=None,
+        cube=None,
+        grid=None,
+        gridproperty=None,
+        outline=None,
+    ):
+        """Init method."""
+        super().__init__()
+
+        self._zmin = zmin
+        self._zmax = zmax
+        self._well = well
+        self._nextend = nextend
+        self._sampling = sampling
+        self._surfaces = surfaces
+        self._surfacenames = surfacenames
+        self._cube = cube
+        self._grid = grid
+        self._gridproperty = gridproperty
+        self._zonelogshift = zonelogshift
+        self._outline = outline
+
+        self._has_axes = True
+        self._has_legend = True
+
+        self._pagesize = "A4"
+        self._fence = None
+        self._legendtitle = "Zones"
+        self._legendsize = 5
+
+        self._ax1 = None
+        self._ax2 = None
+        self._ax3 = None
+        self._allfigs = []
+
+        self._colormap_cube = None
+        self._colorlegend_cube = False
+
+        self._colormap_grid = None
+        self._colorlegend_grid = False
+
+        if colormap is None:
+            self._colormap = _get_colormap("viridis")
+
+        else:
+            self.define_colormap(colormap)
+
+        self._colormap_facies = self.define_any_colormap("xtgeo")
+        self._colormap_facies_dict = {idx: idx for idx in range(100)}
+
+        self._colormap_perf = self.define_any_colormap("xtgeo")
+        self._colormap_perf_dict = {idx: idx for idx in range(100)}
+
+        self._colormap_zonelog = None
+        self._colormap_zonelog_dict = {idx: idx for idx in range(100)}
+
+        logger.info("Ran __init__ ...")
+        logger.debug("Colormap is %s", self._colormap)
+
+    # ==================================================================================
+    # Properties
+    # Notice additonal props in base class
+    # ==================================================================================
+    @property
+    def pagesize(self):
+        """Returns page size."""
+        return self._pagesize
+
+    @property
+    def legendsize(self):
+        """Returns or set the legend size."""
+        return self._legendsize
+
+    @legendsize.setter
+    def legendsize(self, lsize):
+        self._legendsize = lsize
+
+    @property
+    def has_legend(self):
+        """Returns or set the legends."""
+        return self._has_legend
+
+    @has_legend.setter
+    def has_legend(self, value):
+        if not isinstance(value, bool):
+            raise ValueError("Input is not a bool")
+
+        self._has_legend = value
+
+    @property
+    def has_axes(self):
+        """Returns or set the axes status."""
+        return self._has_axes
+
+    @has_axes.setter
+    def has_axes(self, value):
+        if not isinstance(value, bool):
+            raise ValueError("Input is not a bool")
+
+        self._has_axes = value
+
+    @property
+    def colormap_facies(self):
+        """Set or get the facies colormap."""
+        return self._colormap_facies
+
+    @colormap_facies.setter
+    def colormap_facies(self, cmap):
+        self._colormap_facies = self.define_any_colormap(cmap)
+
+    @property
+    def colormap_zonelog(self):
+        """Set or get the zonelog colormap."""
+        return self._colormap_zonelog
+
+    @colormap_zonelog.setter
+    def colormap_zonelog(self, cmap):
+        self._colormap_zonelog = self.define_any_colormap(cmap)
+
+    @property
+    def colormap_perf(self):
+        """Set or get the perforations colormap."""
+        return self._colormap_perf
+
+    @colormap_perf.setter
+    def colormap_perf(self, cmap):
+        self._colormap_perf = self.define_any_colormap(cmap)
+
+    @property
+    def colormap_facies_dict(self):
+        """Set or get the facies colormap actual dict table."""
+        return self._colormap_facies_dict
+
+    @colormap_facies_dict.setter
+    def colormap_facies_dict(self, xdict):
+        if not isinstance(xdict, dict):
+            raise ValueError("Input is not a dict")
+
+        # if not all(isinstance(item, int) for item in list(xdict.values)):
+        #     raise ValueError('Dict values is a list, but some elems are '
+        #                      'not ints!')
+
+        self._colormap_facies_dict = xdict
+
+    @property
+    def colormap_perf_dict(self):
+        """Set or get the perf colormap actual dict table."""
+        return self._colormap_perf_dict
+
+    @colormap_perf_dict.setter
+    def colormap_perf_dict(self, xdict):
+        if not isinstance(xdict, dict):
+            raise ValueError("Input is not a dict")
+
+        # if not all(isinstance(item, int) for item in list(xdict.values)):
+        #     raise ValueError('Dict values is a list, but some elems are '
+        #                      'not ints!')
+
+        self._colormap_perf_dict = xdict
+
+    @property
+    def colormap_zonelog_dict(self):
+        """Set or get the zonelog colormap actual dict table."""
+        return self._colormap_zonelog_dict
+
+    @colormap_zonelog_dict.setter
+    def colormap_zonelog_dict(self, xdict):
+        if not isinstance(xdict, dict):
+            raise ValueError("Input is not a dict")
+
+        # if not all(isinstance(item, int) for item in list(xdict.values)):
+        #     raise ValueError('Dict values is a list, but some elems are '
+        #                      'not ints!')
+
+        self._colormap_zonelog_dict = xdict
+
+    @property
+    def fence(self):
+        """Set or get the fence spesification."""
+        if self._fence is None:
+            if self._well is not None:
+                wfence = self._well.get_fence_polyline(
+                    sampling=self._sampling, nextend=self._nextend, tvdmin=self._zmin
+                )
+                self._fence = wfence
+
+                if wfence is False:
+                    self._fence = None
+            else:
+                raise ValueError("Input well is None")  # should be more flexible
+        return self._fence
+
+    @fence.setter
+    def fence(self, myfence):
+        # this can be extended with checks and various types of input...
+        self._fence = myfence
+
+    # ==================================================================================
+    # Functions methods (public)
+    # ==================================================================================
+
+    def canvas(self, title=None, subtitle=None, infotext=None, figscaling=1.0):
+        """Prepare the canvas to plot on, with title and subtitle.
+
+        Args:
+            title (str, optional): Title of plot.
+            subtitle (str, optional): Sub title of plot.
+            infotext (str, optional): Text to be written as info string.
+            figscaling (str, optional): Figure scaling, default is 1.0
+
+
+        """
+        # overriding the base class canvas
+
+        plt.rcParams["axes.xmargin"] = 0  # fill the plot margins
+
+        if not self._has_axes:
+            plt.rcParams["axes.titlecolor"] = (0, 0, 0, 0)
+            plt.rcParams["axes.edgecolor"] = (0, 0, 0, 0)
+            plt.rcParams["axes.labelcolor"] = (0, 0, 0, 0)
+            plt.rcParams["axes.titlecolor"] = (0, 0, 0, 0)
+            plt.rcParams["xtick.color"] = (0, 0, 0, 0)
+            plt.rcParams["ytick.color"] = (0, 0, 0, 0)
+
+        # self._fig, (ax1, ax2) = plt.subplots(2, figsize=(11.69, 8.27))
+        self._fig, _ = plt.subplots(figsize=(11.69 * figscaling, 8.27 * figscaling))
+        self._allfigs.append(self._fig)
+        ax1 = OrderedDict()
+
+        # since the plan is to at some point remove plotting from xtgeo:
+        warnings.filterwarnings("ignore", category=DeprecationWarning)
+        ax1["main"] = plt.subplot2grid((20, 28), (0, 0), rowspan=20, colspan=23)
+
+        ax2 = plt.subplot2grid(
+            (20, 28), (10, 23), rowspan=5, colspan=5, frame_on=self._has_legend
+        )
+
+        ax3 = plt.subplot2grid(
+            (20, 28), (15, 23), rowspan=5, colspan=5, frame_on=self._has_legend
+        )
+
+        if self._has_legend:
+            # indicate A to B
+            plt.text(
+                0.02,
+                0.98,
+                "A",
+                ha="left",
+                va="top",
+                transform=ax1["main"].transAxes,
+                fontsize=8,
+            )
+            plt.text(
+                0.98,
+                0.98,
+                "B",
+                ha="right",
+                va="top",
+                transform=ax1["main"].transAxes,
+                fontsize=8,
+            )
+
+        # title here:
+        if title is not None:
+            plt.text(
+                0.5,
+                1.09,
+                title,
+                ha="center",
+                va="center",
+                transform=ax1["main"].transAxes,
+                fontsize=18,
+            )
+
+        if subtitle is not None:
+            ax1["main"].set_title(subtitle, size=14)
+
+        if infotext is not None:
+            plt.text(
+                -0.11,
+                -0.11,
+                infotext,
+                ha="left",
+                va="center",
+                transform=ax1["main"].transAxes,
+                fontsize=6,
+            )
+
+        ax1["main"].set_ylabel("Depth", fontsize=12.0)
+        ax1["main"].set_xlabel("Length along well", fontsize=12)
+
+        ax2.tick_params(
+            axis="both",
+            which="both",
+            bottom=False,
+            top=False,
+            right=False,
+            left=False,
+            labelbottom=False,
+            labeltop=False,
+            labelright=False,
+            labelleft=False,
+        )
+
+        ax3.tick_params(
+            axis="both",
+            which="both",
+            bottom=False,
+            top=False,
+            right=False,
+            left=False,
+            labelbottom=False,
+            labeltop=False,
+            labelright=False,
+            labelleft=False,
+        )
+
+        # need these also, a bug in functions above?
+        ax2.xaxis.set_major_formatter(plt.NullFormatter())
+        ax2.yaxis.set_major_formatter(plt.NullFormatter())
+        ax3.xaxis.set_major_formatter(plt.NullFormatter())
+        ax3.yaxis.set_major_formatter(plt.NullFormatter())
+
+        self._ax1 = ax1
+        self._ax2 = ax2
+        self._ax3 = ax3
+
+    def plot_well(
+        self,
+        zonelogname="ZONELOG",
+        facieslogname=None,
+        perflogname=None,
+        wellcrossings=None,
+        wellcrossingnames=True,
+        wellcrossingyears=False,
+        welltrajcolor="black",
+        welltrajwidth=6,
+    ):
+        """Input an XTGeo Well object and plot it."""
+        if self.fence is None:
+            return
+
+        wo = self._well
+
+        # reduce the well data by Pandas operations
+        dfr = wo.dataframe
+        wo.dataframe = dfr[dfr["Z_TVDSS"] > self._zmin]
+
+        # Create a relative XYLENGTH vector (0.0 where well starts)
+        wo.create_relative_hlen()
+
+        dfr = wo.dataframe
+        if dfr.empty:
+            self._showok = False
+            return
+
+        # get the well trajectory (numpies) as copy
+        zv = dfr["Z_TVDSS"].values.copy()
+        hv = dfr["R_HLEN"].values.copy()
+
+        # plot the perflog, if any, first
+        if perflogname:
+            ax, bba = self._currentax(axisname="perf")
+            self._plot_well_perflog(dfr, ax, bba, perflogname, legend=self._has_legend)
+
+        # plot the facies, if any, behind the trajectory; ie. first or second
+        if facieslogname:
+            ax, bba = self._currentax(axisname="facies")
+            self._plot_well_faclog(dfr, ax, bba, facieslogname, legend=self._has_legend)
+
+        axx, _bbxa = self._currentax(axisname="well")
+        self._plot_well_traj(
+            axx, zv, hv, welltrajcolor=welltrajcolor, linewidth=welltrajwidth
+        )
+
+        if zonelogname:
+            ax, bba = self._currentax(axisname="main")
+            self._plot_well_zlog(dfr, axx, bba, zonelogname, legend=self._has_legend)
+
+        if wellcrossings is not None and wellcrossings.empty:
+            wellcrossings = None
+
+        if wellcrossings is not None:
+            self._plot_well_crossings(
+                dfr, axx, wellcrossings, wellcrossingnames, wellcrossingyears
+            )
+
+    def set_xaxis_md(self, gridlines=False):
+        """Set x-axis labels to measured depth."""
+        md_start = self._well.dataframe["MDEPTH"].iloc[0]
+        md_start_round = int(math.floor(md_start / 100.0)) * 100
+        md_start_delta = md_start - md_start_round
+
+        auto_ticks = plt.xticks()
+        auto_ticks_delta = auto_ticks[0][1] - auto_ticks[0][0]
+
+        ax, _ = self._currentax(axisname="main")
+        lim = ax.get_xlim()
+
+        new_ticks = []
+        new_tick_labels = []
+        delta = 0
+        for tick in auto_ticks[0]:
+            new_ticks.append(int(float(tick) - md_start_delta))
+            new_tick_labels.append(int(md_start_round + delta))
+            delta += auto_ticks_delta
+
+        # Set new xticks and labels
+        plt.xticks(new_ticks, new_tick_labels)
+
+        if gridlines:
+            ax.tick_params(axis="y", direction="in", which="both")
+            ax.minorticks_on()
+            ax.grid(color="black", linewidth=0.8, which="major", linestyle="-")
+            ax.grid(color="black", linewidth=0.5, which="minor", linestyle="--")
+
+        # Restore xaxis limits and set axis title
+        ax.set_xlim(lim)
+        ax.set_ylabel("TVD MSL [m]", fontsize=12.0)
+        ax.set_xlabel("Measured Depth [m]", fontsize=12)
+
+    def _plot_well_traj(self, ax, zv, hv, welltrajcolor, linewidth):
+        """Plot the trajectory as a black line."""
+        zv_copy = ma.masked_where(zv < self._zmin, zv)
+        hv_copy = ma.masked_where(zv < self._zmin, hv)
+
+        ax.plot(hv_copy, zv_copy, linewidth=linewidth, c=welltrajcolor)
+
+    @staticmethod
+    def _line_segments_colors(df, idx, ctable, logname, fillnavalue):
+        """Get segment and color array for plotting matplotlib lineCollection."""
+        df_idx = pd.DataFrame(
+            {"idx_log": list(idx.keys()), "idx_color": list(idx.values())}
+        )
+
+        df_ctable = df_idx.merge(
+            pd.DataFrame({"ctable": ctable}),
+            how="left",
+            left_on="idx_color",
+            right_index=True,
+        )
+
+        dff = df.merge(df_ctable, how="left", left_on=logname, right_on="idx_log")
+
+        dff["point"] = list(zip(dff["R_HLEN"], dff["Z_TVDSS"]))
+
+        # find line segments
+        segments = []
+        segments_i = -1
+        colorlist = []
+        previous_color = None
+
+        for point, color in zip(dff["point"], dff["ctable"]):
+            if np.any(np.isnan(color)):
+                color = fillnavalue
+
+            if color == previous_color:
+                segments[segments_i].append(point)
+                previous_color = color
+            else:
+                # add endpoint to current segment
+                if segments_i > 0:
+                    segments[segments_i].append(point)
+
+                # start new segment
+                segments.append([point])
+                colorlist.append(color)
+
+                previous_color = color
+                segments_i += 1
+
+        colorlist = np.asarray(colorlist, dtype=object)
+
+        return segments, colorlist
+
+    def _plot_well_zlog(self, df, ax, bba, zonelogname, logwidth=4, legend=False):
+        """Plot the zone log as colored segments."""
+        if zonelogname not in df.columns:
+            return
+
+        zomin = 0
+        zomax = 0
+
+        try:
+            zomin = int(df[zonelogname].min())
+            zomax = int(df[zonelogname].max())
+        except ValueError:
+            self._showok = False
+            return
+
+        logger.info("ZONELOG min - max is %s - %s", zomin, zomax)
+
+        zshift = 0
+        if self._zonelogshift != 0:
+            zshift = self._zonelogshift
+
+        if self.colormap_zonelog is not None:
+            cmap = self.colormap_zonelog
+            ctable = self.get_any_colormap_as_table(cmap)
+        else:
+            ctable = self.get_colormap_as_table()
+
+        idx = self.colormap_zonelog_dict
+
+        # adjust for zoneshift.
+        idx_zshift = dict()
+        for key in idx:
+            idx_zshift[key - zshift + 1] = idx[key]
+
+        fillnavalue = (0.9, 0.9, 0.9)
+        segments, segments_colors = self._line_segments_colors(
+            df, idx_zshift, ctable, zonelogname, fillnavalue
+        )
+
+        lc = mc.LineCollection(
+            segments, colors=segments_colors, linewidth=logwidth, zorder=202
+        )
+
+        ax.add_collection(lc)
+
+        if legend:
+            zrecord = self._well.get_logrecord(zonelogname)
+            zrecord = {val: zname for val, zname in zrecord.items() if val >= 0}
+
+            zcolors = dict()
+            for zone in zrecord:
+                if isinstance(idx[zone], str):
+                    color = idx[zone]
+                else:
+                    color = ctable[idx[zone]]
+
+                zcolors[zrecord[zone]] = color
+
+            self._drawproxylegend(ax, bba, items=zcolors, title="Zonelog")
+
+    def _plot_well_faclog(self, df, ax, bba, facieslogname, logwidth=9, legend=True):
+        """Plot the facies log as colored segments.
+
+        Args:
+            df (dataframe): The Well dataframe.
+            ax (axes): The ax plot object.
+            bba: Bounding box
+            facieslogname (str): name of the facies log.
+            logwidth (int): Log linewidth.
+            legend (bool): Plot log legend?
+        """
+        if facieslogname not in df.columns:
+            return
+
+        cmap = self.colormap_facies
+        ctable = self.get_any_colormap_as_table(cmap)
+        idx = self.colormap_facies_dict
+
+        fillnavalue = (0, 0, 0, 0)  # transparent
+        segments, segments_colors = self._line_segments_colors(
+            df, idx, ctable, facieslogname, fillnavalue
+        )
+
+        lc = mc.LineCollection(
+            segments, colors=segments_colors, linewidth=logwidth, zorder=201
+        )
+
+        ax.add_collection(lc)
+
+        if legend:
+            frecord = self._well.get_logrecord(facieslogname)
+            frecord = {val: fname for val, fname in frecord.items() if val >= 0}
+
+            fcolors = dict()
+            for facies in frecord:
+                if isinstance(idx[facies], str):
+                    color = idx[facies]
+                else:
+                    color = ctable[idx[facies]]
+
+                fcolors[frecord[facies]] = color
+
+            self._drawproxylegend(ax, bba, items=fcolors, title="Facies")
+
+    def _plot_well_perflog(self, df, ax, bba, perflogname, logwidth=12, legend=True):
+        """Plot the perforation log as colored segments.
+
+        Args:
+            df (dataframe): The Well dataframe.
+            ax (axes): The ax plot object.
+            zv (ndarray): The numpy Z TVD array.
+            bba: Boundinng box
+            hv (ndarray): The numpy Length  array.
+            perflogname (str): name of the perforation log.
+            logwidth (int): Log linewidth.
+            legend (bool): Plot log legend?
+        """
+        if perflogname not in df.columns:
+            return
+
+        cmap = self.colormap_perf
+        ctable = self.get_any_colormap_as_table(cmap)
+        idx = self.colormap_perf_dict
+
+        fillnavalue = (0, 0, 0, 0)  # transparent
+        segments, segments_colors = self._line_segments_colors(
+            df, idx, ctable, perflogname, fillnavalue
+        )
+
+        lc = mc.LineCollection(
+            segments, colors=segments_colors, linewidth=logwidth, zorder=200
+        )
+
+        ax.add_collection(lc)
+
+        if legend:
+            precord = self._well.get_logrecord(perflogname)
+            precord = {val: pname for val, pname in precord.items() if val >= 0}
+
+            pcolors = dict()
+            for perf in precord:
+                if isinstance(idx[perf], str):
+                    color = idx[perf]
+                else:
+                    color = ctable[idx[perf]]
+
+                pcolors[precord[perf]] = color
+
+            self._drawproxylegend(ax, bba, items=pcolors, title="Perforations")
+
+    @staticmethod
+    def _plot_well_crossings(dfr, ax, wcross, names=True, years=False):
+        """Plot well crossing based on dataframe (wcross).
+
+        The well crossing coordinates are identified for this well,
+        and then it is looking for the closest coordinate. Given this
+        coordinate, a position is chosen.
+
+        The pandas dataframe wcross shall have the following columns:
+
+        * Name of crossing wells named CWELL
+        * Coordinate X named X_UTME
+        * Coordinate Y named Y_UTMN
+        * Coordinate Z named Z_TVDSS
+
+        Optional column:
+        * Drilled year of crossing well named CYEAR
+
+        Args:
+            dfr: Well dataframe
+            ax: current axis
+            wcross: A pandas dataframe with precomputed well crossings
+            names: Display the names of the crossed wells
+            years: Display the drilled year of the crossed wells
+        """
+        placings = {
+            0: (40, 40),
+            1: (40, -20),
+            2: (-30, 30),
+            3: (30, 20),
+            4: (-40, 30),
+            5: (-20, 40),
+        }
+
+        for index, row in wcross.iterrows():
+            xcoord = row.X_UTME
+            ycoord = row.Y_UTMN
+
+            dfrc = dfr.copy()
+
+            dfrc["DLEN"] = pow(
+                pow(dfrc.X_UTME - xcoord, 2) + pow(dfrc.Y_UTMN - ycoord, 2), 0.5
+            )
+
+            minindx = dfrc.DLEN.idxmin()
+
+            ax.scatter(
+                dfrc.R_HLEN[minindx],
+                row.Z_TVDSS,
+                marker="o",
+                color="black",
+                s=70,
+                zorder=300,
+            )
+            ax.scatter(
+                dfrc.R_HLEN[minindx],
+                row.Z_TVDSS,
+                marker="o",
+                color="orange",
+                s=38,
+                zorder=302,
+            )
+
+            modulo = index % 5
+
+            text = ""
+            if names:
+                text = Well.get_short_wellname(row.CWELL)
+
+            if years:
+                if names:
+                    text = text + "\n" + row.CYEAR
+                else:
+                    text = row.CYEAR
+
+            if names or years:
+                ax.annotate(
+                    text,
+                    size=6,
+                    xy=(dfrc.R_HLEN[minindx], row.Z_TVDSS),
+                    xytext=placings[modulo],
+                    textcoords="offset points",
+                    arrowprops=dict(
+                        arrowstyle="->", connectionstyle="angle3,angleA=0,angleB=90"
+                    ),
+                    color="black",
+                )
+
+    def _drawproxylegend(self, ax, bba, items, title=None):
+        proxies = []
+        labels = []
+
+        for item in items:
+            color = items[item]
+            proxies.append(Line2D([0, 1], [0, 1], color=color, linewidth=5))
+            labels.append(item)
+
+        ax.legend(
+            proxies,
+            labels,
+            loc="upper left",
+            bbox_to_anchor=bba,
+            prop={"size": self._legendsize},
+            title=title,
+            handlelength=2,
+        )
+
+    def _drawlegend(self, ax, bba, title=None):
+        leg = ax.legend(
+            loc="upper left",
+            bbox_to_anchor=bba,
+            prop={"size": self._legendsize},
+            title=title,
+            handlelength=2,
+        )
+
+        for myleg in leg.get_lines():
+            myleg.set_linewidth(5)
+
+    def _currentax(self, axisname="main"):
+        """Keep track of current axis; is needed as one new legend need one new axis."""
+        # for multiple legends, bba is dynamic
+        bbapos = {
+            "main": (1.22, 1.12, 1, 0),
+            "contacts": (1.01, 1.12),
+            "second": (1.22, 0.50),
+            "facies": (1.01, 1.00),
+            "perf": (1.22, 0.45),
+        }
+
+        ax1 = self._ax1
+
+        if axisname != "main":
+            ax1[axisname] = self._ax1["main"].twinx()
+
+            # invert min,max to invert the Y axis
+            ax1[axisname].set_ylim([self._zmax, self._zmin])
+
+            ax1[axisname].set_yticklabels([])
+            ax1[axisname].tick_params(axis="y", direction="in")
+
+        ax = self._ax1[axisname]
+
+        bba = bbapos.get(axisname, (1.22, 0.5))
+
+        return ax, bba
+
+    def plot_cube(
+        self,
+        colormap="seismic",
+        vmin=None,
+        vmax=None,
+        alpha=0.7,
+        interpolation="gaussian",
+        sampling="nearest",
+    ):
+        """Plot a cube backdrop.
+
+        Args:
+            colormap (ColorMap): Name of color map (default 'seismic')
+            vmin (float): Minimum value in plot.
+            vmax (float); Maximum value in plot
+            alpha (float): Alpah blending number beween 0 and 1.
+            interpolation (str): Interpolation for plotting, cf. matplotlib
+                documentation on this. Also gaussianN is allowed, where
+                N = 1..9.
+            sampling (str): 'nearest' (default) or 'trilinear' (more precise)
+
+        Raises:
+            ValueError: No cube is loaded
+
+        """
+        if self.fence is None:
+            return
+
+        if self._cube is None:
+            raise ValueError("Ask for plot cube, but noe cube is loaded")
+
+        ax, _ = self._currentax(axisname="main")
+
+        zinc = self._cube.zinc / 2.0
+
+        zvv = self._cube.get_randomline(
+            self.fence,
+            zmin=self._zmin,
+            zmax=self._zmax,
+            zincrement=zinc,
+            sampling=sampling,
+        )
+
+        h1, h2, v1, v2, arr = zvv
+
+        # if vmin is not None or vmax is not None:
+        #     arr = np.clip(arr, vmin, vmax)
+
+        if self._colormap_cube is None:
+            if colormap is None:
+                colormap = "seismic"
+            self._colormap_cube = self.define_any_colormap(colormap)
+
+        if "gaussian" in interpolation:  # allow gaussian3 etc
+            nnv = interpolation[-1]
+            try:
+                nnv = int(nnv)
+                arr = gaussian_filter(arr, nnv)
+                interpolation = "none"
+            except ValueError:
+                interpolation = "gaussian"
+
+        img = ax.imshow(
+            arr,
+            cmap=self._colormap_cube,
+            interpolation=interpolation,
+            vmin=vmin,
+            vmax=vmax,
+            extent=(h1, h2, v2, v1),
+            aspect="auto",
+            alpha=alpha,
+        )
+
+        logger.info("Actual VMIN and VMAX: %s", img.get_clim())
+        # steer this?
+        if self._colorlegend_cube:
+            self._fig.colorbar(img, ax=ax)
+
+    def plot_grid3d(
+        self,
+        colormap="rainbow",
+        vmin=None,
+        vmax=None,
+        alpha=0.7,
+        zinc=0.5,
+        interpolation="auto",
+    ):
+        """Plot a sampled grid with gridproperty backdrop.
+
+        Args:
+            colormap (ColorMap): Name of color map (default 'rainbow')
+            vmin (float): Minimum value in plot.
+            vmax (float); Maximum value in plot
+            alpha (float): Alpha blending number beween 0 and 1.
+            zinc (float): Sampling vertically, default is 0.5
+            interpolation (str): Interpolation for plotting, cf. matplotlib
+                documentation on this. "auto" uses "nearest" for discrete
+                parameters and "antialiased" for floats.
+
+        Raises:
+            ValueError: No grid or gridproperty is loaded
+
+        """
+        if self.fence is None:
+            return
+
+        if self._grid is None or self._gridproperty is None:
+            raise ValueError("Ask for plot of grid, but no grid is loaded")
+
+        ax, _bba = self._currentax(axisname="main")
+
+        zvv = self._grid.get_randomline(
+            self.fence,
+            self._gridproperty,
+            zmin=self._zmin,
+            zmax=self._zmax,
+            zincrement=zinc,
+        )
+
+        h1, h2, v1, v2, arr = zvv
+
+        # if vmin is not None or vmax is not None:
+        #     arr = np.clip(arr, vmin, vmax)
+
+        if self._colormap_grid is None:
+            if colormap is None:
+                colormap = "rainbow"
+            self._colormap_grid = self.define_any_colormap(colormap)
+
+        if interpolation == "auto":
+            if self._gridproperty.isdiscrete:
+                interpolation = "nearest"
+            else:
+                interpolation = "antialiased"
+
+        img = ax.imshow(
+            arr,
+            cmap=self._colormap_grid,
+            vmin=vmin,
+            vmax=vmax,
+            extent=(h1, h2, v2, v1),
+            aspect="auto",
+            alpha=alpha,
+            interpolation=interpolation,
+        )
+
+        logger.info("Actual VMIN and VMAX: %s", img.get_clim())
+        # steer this?
+        if self._colorlegend_grid:
+            self._fig.colorbar(img, ax=ax)
+
+    def plot_surfaces(
+        self,
+        fill=False,
+        surfaces=None,
+        surfacenames=None,
+        colormap=None,
+        onecolor=None,
+        linewidth=1.0,
+        linestyle="-",
+        legend=True,
+        legendtitle=None,
+        fancyline=False,
+        axisname="main",
+        gridlines=False,
+    ):  # pylint: disable=too-many-branches, too-many-statements
+        """Input a surface list (ordered from top to base) , and plot them."""
+        if self.fence is None:
+            return
+
+        ax, bba = self._currentax(axisname=axisname)
+
+        # either use surfaces from __init__, or override with surfaces
+        # speciefied here
+        if surfaces is None:
+            surfaces = self._surfaces
+            surfacenames = self._surfacenames
+
+        surfacenames = [surf.name for surf in surfaces]
+
+        if legendtitle is None:
+            legendtitle = self._legendtitle
+
+        if colormap is None:
+            colormap = self._colormap
+        else:
+            self.define_colormap(colormap)
+
+        nlen = len(surfaces)
+
+        # legend
+        slegend = []
+        if surfacenames is None:
+            for i in range(nlen):
+                slegend.append(f"Surf {i}")
+
+        else:
+            # do a check
+            if len(surfacenames) != nlen:
+                msg = (
+                    "Wrong number of entries in surfacenames! Number of names "
+                    f"is {len(surfacenames)} while number of files is {nlen}"
+                )
+                raise ValueError(msg)
+
+            slegend = surfacenames
+
+        if self._colormap.N < nlen:
+            raise ValueError(
+                f"Too few colors in color table ({self._colormap.N}) "
+                f"vs number of surfaces ({nlen})"
+            )
+
+        # sample the horizon to the fence:
+        colortable = self.get_colormap_as_table()
+        for i in range(nlen):
+            usecolor = colortable[i]
+            if onecolor:
+                usecolor = onecolor
+            if not fill:
+                hfence = surfaces[i].get_randomline(self.fence)
+                xcol = "white"
+                if fancyline:
+                    cxx = usecolor
+                    if cxx[0] + cxx[1] + cxx[2] > 1.5:
+                        xcol = "black"
+                    ax.plot(
+                        hfence[:, 0], hfence[:, 1], linewidth=1.2 * linewidth, c=xcol
+                    )
+                ax.plot(
+                    hfence[:, 0],
+                    hfence[:, 1],
+                    linewidth=linewidth,
+                    c=usecolor,
+                    label=slegend[i],
+                    linestyle=linestyle,
+                )
+                if fancyline:
+                    ax.plot(
+                        hfence[:, 0], hfence[:, 1], linewidth=0.3 * linewidth, c=xcol
+                    )
+            else:
+                # need copy() .. why?? found by debugging...
+                hfence1 = surfaces[i].get_randomline(self.fence).copy()
+                x1 = hfence1[:, 0]
+                y1 = hfence1[:, 1]
+                if i < (nlen - 1):
+                    hfence2 = surfaces[i + 1].get_randomline(self.fence).copy()
+                    y2 = hfence2[:, 1]
+                else:
+                    y2 = y1.copy()
+
+                ax.plot(
+                    x1, y1, linewidth=0.1 * linewidth, linestyle=linestyle, c="black"
+                )
+                ax.fill_between(x1, y1, y2, facecolor=colortable[i], label=slegend[i])
+
+        # invert min,max to invert the Y axis
+        ax.set_ylim([self._zmax, self._zmin])
+
+        if legend:
+            self._drawlegend(ax, bba, title=legendtitle)
+
+        if axisname != "main":
+            ax.set_yticklabels([])
+
+        ax.tick_params(axis="y", direction="in")
+
+        if axisname == "main" and gridlines:
+            ax.grid(color="grey", linewidth=0.2)
+
+    def plot_md_data(
+        self,
+        data=None,
+        markersize=10,
+        color="red",
+        linestyle="",
+        label=False,
+        zorder=350,
+        **kwargs,
+    ):
+        """Plot MD vs TVD data as lines and/or markers.
+
+        The input pandas dataframe points shall have the following columns:
+        * Name of well(s) named WELL
+        * Coordinate X named MDEPTH
+        * Coordinate Y named Z_TVDSS
+        """
+        ax, _ = self._currentax(axisname="main")
+
+        well = self._well
+        data_well = data.copy()
+        data_well = data_well.loc[data_well["WELL"] == well.xwellname]
+        del data_well["WELL"]
+
+        md_start = well.dataframe["MDEPTH"].iloc[0]
+        data_well["R_HLEN"] = data_well["MDEPTH"]
+        data_well["R_HLEN"] = data_well["R_HLEN"].subtract(md_start)
+
+        data_well.plot(
+            ax=ax,
+            x="R_HLEN",
+            y="Z_TVDSS",
+            legend=None,
+            linestyle=linestyle,
+            markersize=markersize,
+            color=color,
+            label=label,
+            zorder=zorder,
+            **kwargs,
+        )
+
+    def plot_wellmap(self, otherwells=None, expand=1):
+        """Plot well map as local view, optionally with nearby wells.
+
+        Args:
+            otherwells (list of Polygons): List of surrounding wells to plot,
+                these wells are repr as Polygons instances, one per well.
+            expand (float): Plot axis expand factor (default is 1); larger
+                values may be used if other wells are plotted.
+
+
+        """
+        ax = self._ax2
+
+        if self.fence is not None:
+            xwellarray = self._well.dataframe["X_UTME"].values
+            ywellarray = self._well.dataframe["Y_UTMN"].values
+
+            ax.plot(xwellarray, ywellarray, linewidth=4, c="cyan")
+
+            ax.plot(self.fence[:, 0], self.fence[:, 1], linewidth=1, c="black")
+            ax.annotate("A", xy=(self.fence[0, 0], self.fence[0, 1]), fontsize=8)
+            ax.annotate("B", xy=(self.fence[-1, 0], self.fence[-1, 1]), fontsize=8)
+            ax.set_aspect("equal", "datalim")
+
+            left, right = ax.get_xlim()
+            xdiff = right - left
+            bottom, top = ax.get_ylim()
+            ydiff = top - bottom
+
+            ax.set_xlim(left - (expand - 1.0) * xdiff, right + (expand - 1.0) * xdiff)
+            ax.set_ylim(bottom - (expand - 1.0) * ydiff, top + (expand - 1.0) * ydiff)
+        if otherwells:
+            for poly in otherwells:
+                if not isinstance(poly, Polygons):
+                    xtg.warn(f"<otherw> not a Polygons instance, but a {type(poly)}")
+                    continue
+                if poly.name == self._well.xwellname:
+                    continue
+                xwp = poly.dataframe[poly.xname].values
+                ywp = poly.dataframe[poly.yname].values
+                ax.plot(xwp, ywp, linewidth=1, c="grey")
+                ax.annotate(poly.name, xy=(xwp[-1], ywp[-1]), color="grey", size=5)
+
+    def plot_map(self):
+        """Plot well location map as an overall view (with field outline)."""
+        if self.fence is None:
+            return
+
+        if not self._outline:
+            return
+
+        ax = self._ax3
+        if self.fence is not None:
+            xp = self._outline.dataframe["X_UTME"].values
+            yp = self._outline.dataframe["Y_UTMN"].values
+            ip = self._outline.dataframe["POLY_ID"].values
+
+            ax.plot(self._fence[:, 0], self._fence[:, 1], linewidth=3, c="red")
+
+            for i in range(int(ip.min()), int(ip.max()) + 1):
+                xpc = xp.copy()[ip == i]
+                ypc = yp.copy()[ip == i]
+                if len(xpc) > 1:
+                    ax.plot(xpc, ypc, linewidth=0.3, c="black")
+
+            ax.set_aspect("equal", "datalim")
```

## xtgeo/plot/xtmap.py

 * *Ordering differences only*

```diff
@@ -1,250 +1,250 @@
-"""Module for map plots of surfaces, using matplotlib."""
-
-
-import matplotlib.pyplot as plt
-import matplotlib.patches as mplp
-from matplotlib import ticker
-import numpy as np
-import numpy.ma as ma
-
-from xtgeo.common import XTGeoDialog
-from .baseplot import BasePlot
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class Map(BasePlot):
-    """Class for plotting a map, using matplotlib."""
-
-    def __init__(self):
-        """The constructor method for a Map object."""
-        super().__init__()
-
-        clsname = f"{type(self).__module__}.{type(self).__name__}"
-        logger.info(clsname)
-
-        self._wells = None
-        self._surface = None
-        self._tight = False
-
-        self._wfence = None
-        self._showok = True  # to indicate if plot is OK to show
-        self._legendtitle = "Map"
-
-    # =========================================================================
-    # Properties
-    # =========================================================================
-    @property
-    def pagesize(self):
-        """Returns page size."""
-        return self._pagesize
-
-    # =========================================================================
-    # Functions methods (public)
-    # =========================================================================
-
-    def plot_surface(
-        self,
-        surf,
-        minvalue=None,
-        maxvalue=None,
-        contourlevels=None,
-        xlabelrotation=None,
-        colormap=None,
-        logarithmic=False,
-    ):  # pylint: disable=too-many-statements
-        """Input a surface and plot it."""
-        # need a deep copy to avoid changes in the original surf
-
-        logger.info("The key contourlevels %s is not in use", contourlevels)
-
-        usesurf = surf.copy()
-        if usesurf.yflip < 0:
-            usesurf.swapaxes()
-
-        if abs(surf.rotation) > 0.001:
-            usesurf.unrotate()
-
-        xi, yi, zi = usesurf.get_xyz_values()
-
-        zimask = ma.getmaskarray(zi).copy()  # yes need a copy!
-
-        legendticks = None
-        if minvalue is not None and maxvalue is not None:
-            minv = float(minvalue)
-            maxv = float(maxvalue)
-
-            step = (maxv - minv) / 10.0
-            legendticks = []
-            for i in range(10 + 1):
-                llabel = float(f"{minv + step * i:9.4f}")
-                legendticks.append(llabel)
-
-            zi.unshare_mask()
-            zi[zi < minv] = minv
-            zi[zi > maxv] = maxv
-
-            # need to restore the mask:
-            zi.mask = zimask
-
-            # note use surf.min, not usesurf.min here ...
-            notetxt = (
-                "Note: map values are truncated from ["
-                + str(surf.values.min())
-                + ", "
-                + str(surf.values.max())
-                + "] "
-                + "to interval ["
-                + str(minvalue)
-                + ", "
-                + str(maxvalue)
-                + "]"
-            )
-
-            self._fig.text(0.99, 0.02, notetxt, ha="right", va="center", fontsize=8)
-
-        logger.info("Legendticks: %s", legendticks)
-
-        if minvalue is None:
-            minvalue = usesurf.values.min()
-
-        if maxvalue is None:
-            maxvalue = usesurf.values.max()
-
-        # this will override current instance colormap locally, and is
-        # therefore reset afterwards
-        keepcolor = self.colormap
-        if colormap is not None:
-            self.colormap = colormap
-
-        levels = np.linspace(minvalue, maxvalue, self.contourlevels)
-        logger.debug("Number of contour levels: %s", levels)
-
-        plt.setp(self._ax.xaxis.get_majorticklabels(), rotation=xlabelrotation)
-
-        # zi = ma.masked_where(zimask, zi)
-        # zi = ma.masked_greater(zi, xtgeo.UNDEF_LIMIT)
-        logger.info("Current colormap is %s, requested is %s", self.colormap, colormap)
-        logger.info("Current colormap name is %s", self.colormap.name)
-
-        if ma.std(zi) > 1e-07:
-            uselevels = levels
-        else:
-            uselevels = 1
-
-        try:
-            if logarithmic is False:
-                locator = None
-                ticks = legendticks
-                im = self._ax.contourf(
-                    xi, yi, zi, uselevels, locator=locator, cmap=self.colormap
-                )
-
-            else:
-                logger.info("use LogLocator")
-                locator = ticker.LogLocator()
-                ticks = None
-                uselevels = None
-                im = self._ax.contourf(xi, yi, zi, locator=locator, cmap=self.colormap)
-
-            self._fig.colorbar(im, ticks=ticks)
-        except ValueError as err:
-            logger.warning("Could not make plot: %s", err)
-
-        plt.gca().set_aspect("equal", adjustable="box")
-        self.colormap = keepcolor
-
-    def plot_faults(
-        self,
-        fpoly,
-        idname="POLY_ID",
-        color="k",
-        edgecolor="k",
-        alpha=0.7,
-        linewidth=0.8,
-    ):
-        """Plot the faults.
-
-        Args:
-            fpoly (object): A XTGeo Polygons object
-            idname (str): Name of column which has the faults ID
-            color (c): Fill color model c according to Matplotlib_
-            edgecolor (c): Edge color according to Matplotlib_
-            alpha (float): Degree of opacity
-            linewidth (float): Line width
-
-        .. _Matplotlib: http://matplotlib.org/api/colors_api.html
-        """
-        aff = fpoly.dataframe.groupby(idname)
-
-        for name, _group in aff:
-            # make a dataframe sorted on faults (groupname)
-            myfault = aff.get_group(name)
-
-            # make a list [(X,Y) ...];
-            af = list(zip(myfault["X_UTME"].values, myfault["Y_UTMN"].values))
-
-            px = mplp.Polygon(af, alpha=alpha, color=color, ec=edgecolor, lw=linewidth)
-
-            if px.get_closed():
-                self._ax.add_artist(px)
-            else:
-                IOError(f"A polygon is not closed: {px}")
-
-    def plot_polygons(self, fpoly, idname="POLY_ID", color="k", linewidth=0.8):
-        """Plot a polygons instance.
-
-        Args:
-            fpoly (object): A XTGeo Polygons object
-            idname (str): Name of column which has the faults ID
-            color (c): Line color model c according to Matplotlib_
-            linewidth (float): Line width
-
-        .. _Matplotlib: http://matplotlib.org/api/colors_api.html
-        """
-        aff = fpoly.dataframe.groupby(idname)
-
-        for _name, group in aff:
-            # make a dataframe sorted on groupname
-            pname = fpoly.name
-
-            xarr = group[fpoly.xname].values
-            yarr = group[fpoly.yname].values
-
-            self._ax.plot(xarr, yarr, label=pname, lw=linewidth, color=color)
-            self._ax.legend()
-
-    def plot_points(self, points):
-        """Plot a points set on the map.
-
-        This can be be useful e.g. for plotting the underlying point set
-        that makes a gridded map.
-
-        Args:
-            points (Points): A XTGeo Points object X Y VALUE
-
-        """
-        # This function is "in prep"
-
-        dataframe = points.dataframe
-
-        self._ax.scatter(
-            dataframe["X_UTME"].values, dataframe["Y_UTMN"].values, marker="x"
-        )
-
-    def plot_wells(self, wells):
-        """Plot wells on the map.
-
-        Args:
-            wells (Wells): A XTGeo Wells object (contains a number of Well
-                instances).
-
-        """
-        for well in wells.wells:
-            dataframe = well.dataframe
-
-            xval = dataframe["X_UTME"].values
-            yval = dataframe["Y_UTMN"].values
-            self._ax.plot(xval, yval)
-            self._ax.annotate(well.name, xy=(xval[-1], yval[-1]))
+"""Module for map plots of surfaces, using matplotlib."""
+
+
+import matplotlib.pyplot as plt
+import matplotlib.patches as mplp
+from matplotlib import ticker
+import numpy as np
+import numpy.ma as ma
+
+from xtgeo.common import XTGeoDialog
+from .baseplot import BasePlot
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class Map(BasePlot):
+    """Class for plotting a map, using matplotlib."""
+
+    def __init__(self):
+        """The constructor method for a Map object."""
+        super().__init__()
+
+        clsname = f"{type(self).__module__}.{type(self).__name__}"
+        logger.info(clsname)
+
+        self._wells = None
+        self._surface = None
+        self._tight = False
+
+        self._wfence = None
+        self._showok = True  # to indicate if plot is OK to show
+        self._legendtitle = "Map"
+
+    # =========================================================================
+    # Properties
+    # =========================================================================
+    @property
+    def pagesize(self):
+        """Returns page size."""
+        return self._pagesize
+
+    # =========================================================================
+    # Functions methods (public)
+    # =========================================================================
+
+    def plot_surface(
+        self,
+        surf,
+        minvalue=None,
+        maxvalue=None,
+        contourlevels=None,
+        xlabelrotation=None,
+        colormap=None,
+        logarithmic=False,
+    ):  # pylint: disable=too-many-statements
+        """Input a surface and plot it."""
+        # need a deep copy to avoid changes in the original surf
+
+        logger.info("The key contourlevels %s is not in use", contourlevels)
+
+        usesurf = surf.copy()
+        if usesurf.yflip < 0:
+            usesurf.swapaxes()
+
+        if abs(surf.rotation) > 0.001:
+            usesurf.unrotate()
+
+        xi, yi, zi = usesurf.get_xyz_values()
+
+        zimask = ma.getmaskarray(zi).copy()  # yes need a copy!
+
+        legendticks = None
+        if minvalue is not None and maxvalue is not None:
+            minv = float(minvalue)
+            maxv = float(maxvalue)
+
+            step = (maxv - minv) / 10.0
+            legendticks = []
+            for i in range(10 + 1):
+                llabel = float(f"{minv + step * i:9.4f}")
+                legendticks.append(llabel)
+
+            zi.unshare_mask()
+            zi[zi < minv] = minv
+            zi[zi > maxv] = maxv
+
+            # need to restore the mask:
+            zi.mask = zimask
+
+            # note use surf.min, not usesurf.min here ...
+            notetxt = (
+                "Note: map values are truncated from ["
+                + str(surf.values.min())
+                + ", "
+                + str(surf.values.max())
+                + "] "
+                + "to interval ["
+                + str(minvalue)
+                + ", "
+                + str(maxvalue)
+                + "]"
+            )
+
+            self._fig.text(0.99, 0.02, notetxt, ha="right", va="center", fontsize=8)
+
+        logger.info("Legendticks: %s", legendticks)
+
+        if minvalue is None:
+            minvalue = usesurf.values.min()
+
+        if maxvalue is None:
+            maxvalue = usesurf.values.max()
+
+        # this will override current instance colormap locally, and is
+        # therefore reset afterwards
+        keepcolor = self.colormap
+        if colormap is not None:
+            self.colormap = colormap
+
+        levels = np.linspace(minvalue, maxvalue, self.contourlevels)
+        logger.debug("Number of contour levels: %s", levels)
+
+        plt.setp(self._ax.xaxis.get_majorticklabels(), rotation=xlabelrotation)
+
+        # zi = ma.masked_where(zimask, zi)
+        # zi = ma.masked_greater(zi, xtgeo.UNDEF_LIMIT)
+        logger.info("Current colormap is %s, requested is %s", self.colormap, colormap)
+        logger.info("Current colormap name is %s", self.colormap.name)
+
+        if ma.std(zi) > 1e-07:
+            uselevels = levels
+        else:
+            uselevels = 1
+
+        try:
+            if logarithmic is False:
+                locator = None
+                ticks = legendticks
+                im = self._ax.contourf(
+                    xi, yi, zi, uselevels, locator=locator, cmap=self.colormap
+                )
+
+            else:
+                logger.info("use LogLocator")
+                locator = ticker.LogLocator()
+                ticks = None
+                uselevels = None
+                im = self._ax.contourf(xi, yi, zi, locator=locator, cmap=self.colormap)
+
+            self._fig.colorbar(im, ticks=ticks)
+        except ValueError as err:
+            logger.warning("Could not make plot: %s", err)
+
+        plt.gca().set_aspect("equal", adjustable="box")
+        self.colormap = keepcolor
+
+    def plot_faults(
+        self,
+        fpoly,
+        idname="POLY_ID",
+        color="k",
+        edgecolor="k",
+        alpha=0.7,
+        linewidth=0.8,
+    ):
+        """Plot the faults.
+
+        Args:
+            fpoly (object): A XTGeo Polygons object
+            idname (str): Name of column which has the faults ID
+            color (c): Fill color model c according to Matplotlib_
+            edgecolor (c): Edge color according to Matplotlib_
+            alpha (float): Degree of opacity
+            linewidth (float): Line width
+
+        .. _Matplotlib: http://matplotlib.org/api/colors_api.html
+        """
+        aff = fpoly.dataframe.groupby(idname)
+
+        for name, _group in aff:
+            # make a dataframe sorted on faults (groupname)
+            myfault = aff.get_group(name)
+
+            # make a list [(X,Y) ...];
+            af = list(zip(myfault["X_UTME"].values, myfault["Y_UTMN"].values))
+
+            px = mplp.Polygon(af, alpha=alpha, color=color, ec=edgecolor, lw=linewidth)
+
+            if px.get_closed():
+                self._ax.add_artist(px)
+            else:
+                IOError(f"A polygon is not closed: {px}")
+
+    def plot_polygons(self, fpoly, idname="POLY_ID", color="k", linewidth=0.8):
+        """Plot a polygons instance.
+
+        Args:
+            fpoly (object): A XTGeo Polygons object
+            idname (str): Name of column which has the faults ID
+            color (c): Line color model c according to Matplotlib_
+            linewidth (float): Line width
+
+        .. _Matplotlib: http://matplotlib.org/api/colors_api.html
+        """
+        aff = fpoly.dataframe.groupby(idname)
+
+        for _name, group in aff:
+            # make a dataframe sorted on groupname
+            pname = fpoly.name
+
+            xarr = group[fpoly.xname].values
+            yarr = group[fpoly.yname].values
+
+            self._ax.plot(xarr, yarr, label=pname, lw=linewidth, color=color)
+            self._ax.legend()
+
+    def plot_points(self, points):
+        """Plot a points set on the map.
+
+        This can be be useful e.g. for plotting the underlying point set
+        that makes a gridded map.
+
+        Args:
+            points (Points): A XTGeo Points object X Y VALUE
+
+        """
+        # This function is "in prep"
+
+        dataframe = points.dataframe
+
+        self._ax.scatter(
+            dataframe["X_UTME"].values, dataframe["Y_UTMN"].values, marker="x"
+        )
+
+    def plot_wells(self, wells):
+        """Plot wells on the map.
+
+        Args:
+            wells (Wells): A XTGeo Wells object (contains a number of Well
+                instances).
+
+        """
+        for well in wells.wells:
+            dataframe = well.dataframe
+
+            xval = dataframe["X_UTME"].values
+            yval = dataframe["Y_UTMN"].values
+            self._ax.plot(xval, yval)
+            self._ax.annotate(well.name, xy=(xval[-1], yval[-1]))
```

## xtgeo/roxutils/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-# -*- coding: utf-8 -*-
-# flake8: noqa
-"""XTGeo roxutils package"""
-
-
-from xtgeo.roxutils.roxutils import RoxUtils
+# -*- coding: utf-8 -*-
+# flake8: noqa
+"""XTGeo roxutils package"""
+
+
+from xtgeo.roxutils.roxutils import RoxUtils
```

## xtgeo/roxutils/_roxutils_etc.py

 * *Ordering differences only*

```diff
@@ -1,133 +1,133 @@
-# -*- coding: utf-8 -*-
-"""Private module for etc functions"""
-
-
-try:
-    import roxar
-except ImportError:
-    pass
-
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def create_whatever_category(
-    self, category, stype="horizons", domain="depth", htype="surface"
-):
-    """Create one or more a Horizons/Zones... category entries.
-
-    Args:
-        category (str or list): Name(s) of category to make, either as
-             a simple string or a list of strings.
-        stype (str): 'Super type' in RMS (horizons or zones).
-            Default is horizons
-        domain (str): 'depth' (default) or 'time'
-        htype (str): Horizon type: surface/lines/points
-    """
-
-    project = self.project
-    categories = []
-
-    if isinstance(category, str):
-        categories.append(category)
-    else:
-        categories.extend(category)
-
-    for catg in categories:
-        geom = roxar.GeometryType.surface
-        if htype.lower() == "lines":
-            geom = roxar.GeometryType.lines
-        elif htype.lower() == "points":
-            geom = roxar.GeometryType.points
-
-        dom = roxar.VerticalDomain.depth
-        if domain.lower() == "time":
-            dom = roxar.GeometryType.lines
-
-        if stype.lower() == "horizons":
-            if catg not in project.horizons.representations:
-                try:
-                    project.horizons.representations.create(catg, geom, dom)
-                except Exception as exmsg:  # pylint: disable=broad-except
-                    print(f"Error: {exmsg}")
-            else:
-                print(f"Category <{catg}> already exists")
-
-        elif stype.lower() == "zones":
-            if catg not in project.zones.representations:
-                try:
-                    project.zones.representations.create(catg, geom, dom)
-                except Exception as exmsg:  # pylint: disable=broad-except
-                    print(f"Error: {exmsg}")
-            else:
-                print(f"Category <{catg}> already exists")
-
-
-def delete_whatever_category(self, category, stype="horizons"):
-    """Delete one or more horizons or zones categories.
-
-    Args:
-        category (str or list): Name(s) of category to make, either
-            as a simple string or a list of strings.
-        stype (str): 'Super type', in RMS ('horizons' or 'zones').
-            Default is 'horizons'
-    """
-
-    project = self.project
-    categories = []
-
-    if isinstance(category, str):
-        categories.append(category)
-    else:
-        categories.extend(category)
-
-    for catg in categories:
-        if stype.lower() == "horizons":
-            try:
-                del project.horizons.representations[catg]
-            except KeyError as kerr:
-                if kerr == catg:
-                    print(f"Cannot delete {kerr}, does not exist")
-        elif stype.lower() == "zones":
-            try:
-                del project.horizons.representations[catg]
-            except KeyError as kerr:
-                if kerr == catg:
-                    print(f"Cannot delete {kerr}, does not exist")
-        else:
-            raise ValueError("Wrong stype applied")
-
-
-def clear_whatever_category(self, category, stype="horizons"):
-    """Clear (or make empty) the content of one or more horizon/zones... categories.
-
-    Args:
-        category (str or list): Name(s) of category to empty, either as
-             a simple string or a list of strings.
-        stype (str): 'Super type' in RMS (horizons or zones).
-            Default is horizons
-
-    .. versionadded:: 2.1
-    """
-
-    project = self.project
-
-    categories = []
-    if isinstance(category, str):
-        categories.append(category)
-    else:
-        categories.extend(category)
-
-    xtype = project.horizons
-    if stype.lower() == "zones":
-        xtype = project.zones
-
-    for catg in categories:
-        for xitem in xtype:
-            try:
-                item = xtype[xitem.name][catg]
-                item.set_empty()
-            except KeyError as kmsg:
-                print(kmsg)
+# -*- coding: utf-8 -*-
+"""Private module for etc functions"""
+
+
+try:
+    import roxar
+except ImportError:
+    pass
+
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def create_whatever_category(
+    self, category, stype="horizons", domain="depth", htype="surface"
+):
+    """Create one or more a Horizons/Zones... category entries.
+
+    Args:
+        category (str or list): Name(s) of category to make, either as
+             a simple string or a list of strings.
+        stype (str): 'Super type' in RMS (horizons or zones).
+            Default is horizons
+        domain (str): 'depth' (default) or 'time'
+        htype (str): Horizon type: surface/lines/points
+    """
+
+    project = self.project
+    categories = []
+
+    if isinstance(category, str):
+        categories.append(category)
+    else:
+        categories.extend(category)
+
+    for catg in categories:
+        geom = roxar.GeometryType.surface
+        if htype.lower() == "lines":
+            geom = roxar.GeometryType.lines
+        elif htype.lower() == "points":
+            geom = roxar.GeometryType.points
+
+        dom = roxar.VerticalDomain.depth
+        if domain.lower() == "time":
+            dom = roxar.GeometryType.lines
+
+        if stype.lower() == "horizons":
+            if catg not in project.horizons.representations:
+                try:
+                    project.horizons.representations.create(catg, geom, dom)
+                except Exception as exmsg:  # pylint: disable=broad-except
+                    print(f"Error: {exmsg}")
+            else:
+                print(f"Category <{catg}> already exists")
+
+        elif stype.lower() == "zones":
+            if catg not in project.zones.representations:
+                try:
+                    project.zones.representations.create(catg, geom, dom)
+                except Exception as exmsg:  # pylint: disable=broad-except
+                    print(f"Error: {exmsg}")
+            else:
+                print(f"Category <{catg}> already exists")
+
+
+def delete_whatever_category(self, category, stype="horizons"):
+    """Delete one or more horizons or zones categories.
+
+    Args:
+        category (str or list): Name(s) of category to make, either
+            as a simple string or a list of strings.
+        stype (str): 'Super type', in RMS ('horizons' or 'zones').
+            Default is 'horizons'
+    """
+
+    project = self.project
+    categories = []
+
+    if isinstance(category, str):
+        categories.append(category)
+    else:
+        categories.extend(category)
+
+    for catg in categories:
+        if stype.lower() == "horizons":
+            try:
+                del project.horizons.representations[catg]
+            except KeyError as kerr:
+                if kerr == catg:
+                    print(f"Cannot delete {kerr}, does not exist")
+        elif stype.lower() == "zones":
+            try:
+                del project.horizons.representations[catg]
+            except KeyError as kerr:
+                if kerr == catg:
+                    print(f"Cannot delete {kerr}, does not exist")
+        else:
+            raise ValueError("Wrong stype applied")
+
+
+def clear_whatever_category(self, category, stype="horizons"):
+    """Clear (or make empty) the content of one or more horizon/zones... categories.
+
+    Args:
+        category (str or list): Name(s) of category to empty, either as
+             a simple string or a list of strings.
+        stype (str): 'Super type' in RMS (horizons or zones).
+            Default is horizons
+
+    .. versionadded:: 2.1
+    """
+
+    project = self.project
+
+    categories = []
+    if isinstance(category, str):
+        categories.append(category)
+    else:
+        categories.extend(category)
+
+    xtype = project.horizons
+    if stype.lower() == "zones":
+        xtype = project.zones
+
+    for catg in categories:
+        for xitem in xtype:
+            try:
+                item = xtype[xitem.name][catg]
+                item.set_empty()
+            except KeyError as kmsg:
+                print(kmsg)
```

## xtgeo/roxutils/roxutils.py

```diff
@@ -1,219 +1,218 @@
-# -*- coding: utf-8 -*-
-"""Module for simplifying various operation in the Roxar python interface."""
-
-
-from packaging.version import parse as versionparse
-
-# from pkg_resources import parse_version as pver (ALT)
-
-try:
-    import _roxar
-    import roxar
-except ImportError:
-    pass
-
-from xtgeo.common import XTGeoDialog
-
-from . import _roxutils_etc
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class RoxUtils(object):
-
-    """Class RoxUtils, for accessing project level methods::
-
-     import xtgeo
-
-     xr = xtgeo.RoxUtils(project)
-     xr.create_horizon_category('DS_extracted_run3')
-     xr.delete_horizon_category('DS_extracted_run2')
-
-    The project itself can be a reference to an existing project, typically
-    the magic ``project`` wording inside RMS python,
-    or a file path to a RMS project (for external access).
-
-    Args:
-        project (_roxar.Project or str): Reference to a RMS project
-            either an existing instance or a RMS project folder path.
-        readonly (bool). Default is False. If readonly, then it cannot be
-            saved to this project (which is the case for "secondary" projects).
-
-    Examples::
-
-        import xgeo
-        path = '/some/path/to/rmsprject.rmsx'
-
-        ext = xtgeo.RoxUtils(path, readonly=True)
-        # ...do something
-        ext.safe_close()
-
-    """
-
-    def __init__(self, project, readonly=False):
-        self._project = None
-
-        self._version = roxar.__version__
-
-        if versionparse(self._version) < versionparse("1.5"):
-            raise RuntimeError("XTGeo >= 3.0 requires Roxar API >= 1.5")
-
-        self._roxexternal = True
-
-        self._versions = {
-            "1.0": ["10.0.x"],
-            "1.1": ["10.1.0", "10.1.1", "10.1.2"],
-            "1.1.1": ["10.1.3"],
-            "1.2": ["11.0.0"],
-            "1.2.1": ["11.0.1"],
-            "1.3": ["11.1.0", "11.1.1", "11.1.2"],
-            "1.4": ["12.0.0", "12.0.1", "12.0.2"],
-            "1.5": ["12.1"],
-            "1.6": ["13.0"],
-            "1.7": ["13.1"],
-        }
-
-        if project is not None and isinstance(project, str):
-            projectname = project
-            if readonly:
-                self._project = roxar.Project.open_import(projectname)
-            else:
-                self._project = roxar.Project.open(projectname)
-            logger.info("Open RMS project from %s", projectname)
-
-        elif isinstance(project, _roxar.Project):
-            # this will happen for _current_ project inside RMS or if
-            # project is opened already e.g. by roxar.Project.open(). In the latter
-            # case, the user should also close the project by project.close() as
-            # an explicit action.
-
-            self._roxexternal = False
-            self._project = project
-            logger.info("RMS project instance is already open as <%s>", project)
-        else:
-            raise RuntimeError("Project is not valid")
-
-    @property
-    def roxversion(self):
-        """Roxar API version (read only)"""
-        return self._version
-
-    @property
-    def project(self):
-        """The Roxar project instance (read only)"""
-        return self._project
-
-    def safe_close(self):
-        """Close the project but only if roxarapps (external) mode, i.e.
-        not current RMS project
-
-        In case roxar.Project.open() is done explicitly, safe_close() will do nothing.
-
-        """
-        if self._roxexternal:
-            try:
-                self._project.close()
-                logger.info("RMS project instance is closed")
-            except TypeError as msg:
-                xtg.warn(msg)
-        else:
-            logger.info("Close request, but skip for good reasons...")
-            logger.debug("... either in RMS GUI or in a sequence of running roxarapps")
-
-    def version_required(self, targetversion):
-        """Defines a minimum ROXAPI version for some feature (True or False).
-
-        Args:
-            targetversion (str): Minimum version to compare with.
-
-        Example::
-
-            rox = RoxUtils(project)
-            if rox.version_required('1.5'):
-                somefunction()
-            else:
-                print('Not supported in this version')
-
-        """
-        return versionparse(self._version) >= versionparse(targetversion)
-
-    def rmsversion(self, apiversion):
-        """Get the actual RMS version(s) given an API version.
-
-        Args:
-            apiversion (str): ROXAPI version to ask for
-
-        Returns:
-            A list of RMS version(s) for the given API version, None if
-                not any match.
-
-        Example::
-
-            rox = RoxUtils(project)
-            rmsver = rox.rmsversion('1.5')
-            print('The supported RMS version are {}'.format(rmsver))
-
-        """
-
-        return self._versions.get(apiversion, None)
-
-    def create_horizons_category(self, category, domain="depth", htype="surface"):
-        """Create one or more a Horizons category entries.
-
-        Args:
-            category (str or list): Name(s) of category to make, either as
-                a simple string or a list of strings.
-            domain (str): 'depth' (default) or 'time'
-            htype (str): Horizon type: surface/lines/points
-        """
-
-        _roxutils_etc.create_whatever_category(
-            self, category, stype="horizons", domain=domain, htype=htype
-        )
-
-    def create_zones_category(self, category, domain="thickness", htype="surface"):
-        """Create one or more a Horizons category entries.
-
-        Args:
-            category (str or list): Name(s) of category to make, either as
-                a simple string or a list of strings.
-            domain (str): 'thickness' (default) or ...?
-            htype (str): Horizon type: surface/lines/points
-        """
-
-        _roxutils_etc.create_whatever_category(
-            self, category, stype="zones", domain=domain, htype=htype
-        )
-
-    def delete_horizons_category(self, category):
-        """Delete on or more horizons or zones categories"""
-
-        _roxutils_etc.delete_whatever_category(self, category, stype="horizons")
-
-    def delete_zones_category(self, category):
-        """Delete on or more horizons or zones categories. See previous"""
-
-        _roxutils_etc.delete_whatever_category(self, category, stype="zones")
-
-    def clear_horizon_category(self, category):
-        """Clear (or make empty) the content of one or more horizon categories.
-
-        Args:
-            category (str or list): Name(s) of category to empty, either as
-                 a simple string or a list of strings.
-
-        .. versionadded:: 2.1
-        """
-        _roxutils_etc.clear_whatever_category(self, category, stype="horizons")
-
-    def clear_zone_category(self, category):
-        """Clear (or make empty) the content of one or more zone categories.
-
-        Args:
-            category (str or list): Name(s) of category to empty, either as
-                 a simple string or a list of strings.
-
-        .. versionadded:: 2.1
-        """
-        _roxutils_etc.clear_whatever_category(self, category, stype="zones")
+# -*- coding: utf-8 -*-
+"""Module for simplifying various operation in the Roxar python interface."""
+
+
+from packaging.version import parse as versionparse
+
+# from pkg_resources import parse_version as pver (ALT)
+
+try:
+    import roxar
+except ImportError:
+    pass
+
+from xtgeo.common import XTGeoDialog
+
+from . import _roxutils_etc
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class RoxUtils(object):
+
+    """Class RoxUtils, for accessing project level methods::
+
+     import xtgeo
+
+     xr = xtgeo.RoxUtils(project)
+     xr.create_horizon_category('DS_extracted_run3')
+     xr.delete_horizon_category('DS_extracted_run2')
+
+    The project itself can be a reference to an existing project, typically
+    the magic ``project`` wording inside RMS python,
+    or a file path to a RMS project (for external access).
+
+    Args:
+        project (roxar.Project or str): Reference to a RMS project
+            either an existing instance or a RMS project folder path.
+        readonly (bool). Default is False. If readonly, then it cannot be
+            saved to this project (which is the case for "secondary" projects).
+
+    Examples::
+
+        import xgeo
+        path = '/some/path/to/rmsprject.rmsx'
+
+        ext = xtgeo.RoxUtils(path, readonly=True)
+        # ...do something
+        ext.safe_close()
+
+    """
+
+    def __init__(self, project, readonly=False):
+        self._project = None
+
+        self._version = roxar.__version__
+
+        if versionparse(self._version) < versionparse("1.5"):
+            raise RuntimeError("XTGeo >= 3.0 requires Roxar API >= 1.5")
+
+        self._roxexternal = True
+
+        self._versions = {
+            "1.0": ["10.0.x"],
+            "1.1": ["10.1.0", "10.1.1", "10.1.2"],
+            "1.1.1": ["10.1.3"],
+            "1.2": ["11.0.0"],
+            "1.2.1": ["11.0.1"],
+            "1.3": ["11.1.0", "11.1.1", "11.1.2"],
+            "1.4": ["12.0.0", "12.0.1", "12.0.2"],
+            "1.5": ["12.1"],
+            "1.6": ["13.0"],
+            "1.7": ["13.1"],
+        }
+
+        if project is not None and isinstance(project, str):
+            projectname = project
+            if readonly:
+                self._project = roxar.Project.open_import(projectname)
+            else:
+                self._project = roxar.Project.open(projectname)
+            logger.info("Open RMS project from %s", projectname)
+
+        elif isinstance(project, roxar.Project):
+            # this will happen for _current_ project inside RMS or if
+            # project is opened already e.g. by roxar.Project.open(). In the latter
+            # case, the user should also close the project by project.close() as
+            # an explicit action.
+
+            self._roxexternal = False
+            self._project = project
+            logger.info("RMS project instance is already open as <%s>", project)
+        else:
+            raise RuntimeError("Project is not valid")
+
+    @property
+    def roxversion(self):
+        """Roxar API version (read only)"""
+        return self._version
+
+    @property
+    def project(self):
+        """The Roxar project instance (read only)"""
+        return self._project
+
+    def safe_close(self):
+        """Close the project but only if roxarapps (external) mode, i.e.
+        not current RMS project
+
+        In case roxar.Project.open() is done explicitly, safe_close() will do nothing.
+
+        """
+        if self._roxexternal:
+            try:
+                self._project.close()
+                logger.info("RMS project instance is closed")
+            except TypeError as msg:
+                xtg.warn(msg)
+        else:
+            logger.info("Close request, but skip for good reasons...")
+            logger.debug("... either in RMS GUI or in a sequence of running roxarapps")
+
+    def version_required(self, targetversion):
+        """Defines a minimum ROXAPI version for some feature (True or False).
+
+        Args:
+            targetversion (str): Minimum version to compare with.
+
+        Example::
+
+            rox = RoxUtils(project)
+            if rox.version_required('1.5'):
+                somefunction()
+            else:
+                print('Not supported in this version')
+
+        """
+        return versionparse(self._version) >= versionparse(targetversion)
+
+    def rmsversion(self, apiversion):
+        """Get the actual RMS version(s) given an API version.
+
+        Args:
+            apiversion (str): ROXAPI version to ask for
+
+        Returns:
+            A list of RMS version(s) for the given API version, None if
+                not any match.
+
+        Example::
+
+            rox = RoxUtils(project)
+            rmsver = rox.rmsversion('1.5')
+            print('The supported RMS version are {}'.format(rmsver))
+
+        """
+
+        return self._versions.get(apiversion, None)
+
+    def create_horizons_category(self, category, domain="depth", htype="surface"):
+        """Create one or more a Horizons category entries.
+
+        Args:
+            category (str or list): Name(s) of category to make, either as
+                a simple string or a list of strings.
+            domain (str): 'depth' (default) or 'time'
+            htype (str): Horizon type: surface/lines/points
+        """
+
+        _roxutils_etc.create_whatever_category(
+            self, category, stype="horizons", domain=domain, htype=htype
+        )
+
+    def create_zones_category(self, category, domain="thickness", htype="surface"):
+        """Create one or more a Horizons category entries.
+
+        Args:
+            category (str or list): Name(s) of category to make, either as
+                a simple string or a list of strings.
+            domain (str): 'thickness' (default) or ...?
+            htype (str): Horizon type: surface/lines/points
+        """
+
+        _roxutils_etc.create_whatever_category(
+            self, category, stype="zones", domain=domain, htype=htype
+        )
+
+    def delete_horizons_category(self, category):
+        """Delete on or more horizons or zones categories"""
+
+        _roxutils_etc.delete_whatever_category(self, category, stype="horizons")
+
+    def delete_zones_category(self, category):
+        """Delete on or more horizons or zones categories. See previous"""
+
+        _roxutils_etc.delete_whatever_category(self, category, stype="zones")
+
+    def clear_horizon_category(self, category):
+        """Clear (or make empty) the content of one or more horizon categories.
+
+        Args:
+            category (str or list): Name(s) of category to empty, either as
+                 a simple string or a list of strings.
+
+        .. versionadded:: 2.1
+        """
+        _roxutils_etc.clear_whatever_category(self, category, stype="horizons")
+
+    def clear_zone_category(self, category):
+        """Clear (or make empty) the content of one or more zone categories.
+
+        Args:
+            category (str or list): Name(s) of category to empty, either as
+                 a simple string or a list of strings.
+
+        .. versionadded:: 2.1
+        """
+        _roxutils_etc.clear_whatever_category(self, category, stype="zones")
```

## xtgeo/surface/__init__.py

 * *Ordering differences only*

```diff
@@ -1,6 +1,6 @@
-# -*- coding: utf-8 -*-
-# flake8: noqa
-"""XTGeo surface package"""
-
-
-from xtgeo.surface.regular_surface import RegularSurface
+# -*- coding: utf-8 -*-
+# flake8: noqa
+"""XTGeo surface package"""
+
+
+from xtgeo.surface.regular_surface import RegularSurface
```

## xtgeo/surface/_regsurf_boundary.py

 * *Ordering differences only*

```diff
@@ -1,28 +1,28 @@
-import math
-
-import xtgeo
-
-
-def create_boundary(self, alpha_factor, is_convex, simplify):
-    """Create boundary polygons for a surface."""
-
-    # make into points
-    points = xtgeo.points_from_surface(self)
-
-    # compute minimum alpha based on surface resolution
-    alpha = math.ceil(math.sqrt(self.xinc**2 + self.yinc**2) / 2)
-
-    pol = xtgeo.Polygons.boundary_from_points(points, alpha_factor, alpha, is_convex)
-
-    if pol is None:
-        raise RuntimeError("Could not create a valid Polygons instance for boundary.")
-
-    if simplify:
-        if isinstance(simplify, bool):
-            pol.simplify(tolerance=0.1)
-        elif isinstance(simplify, dict) and "tolerance" in simplify:
-            pol.simplify(**simplify)
-        else:
-            raise ValueError("Invalid values for simplify keyword")
-
-    return pol
+import math
+
+import xtgeo
+
+
+def create_boundary(self, alpha_factor, is_convex, simplify):
+    """Create boundary polygons for a surface."""
+
+    # make into points
+    points = xtgeo.points_from_surface(self)
+
+    # compute minimum alpha based on surface resolution
+    alpha = math.ceil(math.sqrt(self.xinc**2 + self.yinc**2) / 2)
+
+    pol = xtgeo.Polygons.boundary_from_points(points, alpha_factor, alpha, is_convex)
+
+    if pol is None:
+        raise RuntimeError("Could not create a valid Polygons instance for boundary.")
+
+    if simplify:
+        if isinstance(simplify, bool):
+            pol.simplify(tolerance=0.1)
+        elif isinstance(simplify, dict) and "tolerance" in simplify:
+            pol.simplify(**simplify)
+        else:
+            raise ValueError("Invalid values for simplify keyword")
+
+    return pol
```

## xtgeo/surface/_regsurf_cube.py

 * *Ordering differences only*

```diff
@@ -1,221 +1,221 @@
-# -*- coding: utf-8 -*-
-"""Regular surface vs Cube"""
-
-
-import numpy as np
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def slice_cube(
-    self,
-    cube,
-    zsurf=None,
-    sampling="nearest",
-    mask=True,
-    snapxy=False,
-    deadtraces=True,
-    algorithm=1,
-):
-    """Slicing a cube, using different algorithms"""
-
-    if algorithm == 2:
-        if snapxy:
-            return _slice_cube_v2(
-                self,
-                cube,
-                zsurf=zsurf,
-                sampling=sampling,
-                mask=mask,
-                deadtraces=deadtraces,
-            )
-        return _slice_cube_v2_resample(
-            self,
-            cube,
-            zsurf=zsurf,
-            sampling=sampling,
-            mask=mask,
-            deadtraces=deadtraces,
-        )
-
-    # legacy version:
-    return _slice_cube_v1(
-        self,
-        cube,
-        zsurf=zsurf,
-        sampling=sampling,
-        mask=mask,
-        snapxy=snapxy,
-        deadtraces=deadtraces,
-    )
-
-
-def _slice_cube_v1(
-    self, cube, zsurf=None, sampling="nearest", mask=True, snapxy=False, deadtraces=True
-):
-    """
-    Private function for the Cube slicing. This is the legacy version up to version
-    2.8.x but may remain default for a while
-    """
-
-    logger.info("Slice cube algorithm 1")
-    if zsurf is not None:
-        other = zsurf
-    else:
-        logger.info("The current surface is copied as 'other'")
-        other = self.copy()
-
-    if not self.compare_topology(other, strict=False):
-        raise RuntimeError("Topology of maps differ. Stop!")
-
-    if mask:
-        opt2 = 0
-    else:
-        opt2 = 1
-
-    if deadtraces:
-        # set dead traces to cxtgeo UNDEF -> special treatment in the C code
-        olddead = cube.values_dead_traces(xtgeo.UNDEF)
-
-    cubeval1d = np.ravel(cube.values, order="C")
-
-    nsurf = self.ncol * self.nrow
-
-    usesampling = 0
-    if sampling == "trilinear":
-        usesampling = 1
-        if snapxy:
-            usesampling = 2
-
-    logger.debug("Running method from C... (using typemaps for numpies!:")
-    istat, v1d = _cxtgeo.surf_slice_cube(
-        cube.ncol,
-        cube.nrow,
-        cube.nlay,
-        cube.xori,
-        cube.xinc,
-        cube.yori,
-        cube.yinc,
-        cube.zori,
-        cube.zinc,
-        cube.rotation,
-        cube.yflip,
-        cubeval1d,
-        self.ncol,
-        self.nrow,
-        self.xori,
-        self.xinc,
-        self.yori,
-        self.yinc,
-        self.yflip,
-        self.rotation,
-        other.get_values1d(),
-        nsurf,
-        usesampling,
-        opt2,
-    )
-
-    self.set_values1d(v1d)
-
-    if deadtraces:
-        cube.values_dead_traces(olddead)  # reset value for dead traces
-
-    return istat
-
-
-def _slice_cube_v2(
-    self, cube, zsurf=None, sampling="nearest", mask=True, deadtraces=True
-):
-    """
-    This is the new version, optimised for the case where the surface has exact same
-    topology as the cube. This should both simplify and speed up calculations
-
-    From xtgeo 2.9
-    """
-
-    logger.info("Slice cube algorithm 2 with sampling %s", sampling)
-    if zsurf is not None:
-        other = zsurf
-    else:
-        other = self.copy()
-
-    if not self.compare_topology(other, strict=False):
-        raise RuntimeError("Topology of maps differ. Stop!")
-
-    optmask = 0
-    if mask:
-        optmask = 1
-
-    if deadtraces:
-        # set dead traces to cxtgeo UNDEF -> special treatment in the C code
-        olddead = cube.values_dead_traces(xtgeo.UNDEF)
-
-    optnearest = 1
-    if sampling == "trilinear":
-        optnearest = 0
-
-    # cube and surf shall share same topology, e.g. cube.col == surf.ncol etc
-    # print(self.values.mask)
-    istat = _cxtgeo.surf_slice_cube_v3(
-        cube.ncol,
-        cube.nrow,
-        cube.nlay,
-        cube.zori,
-        cube.zinc,
-        cube.values,
-        other.values.data,
-        self.values.data,
-        self.values.mask,
-        optnearest,
-        optmask,
-    )
-
-    if istat != 0:
-        logger.warning("Problem, ISTAT = %s", istat)
-
-    if deadtraces:
-        cube.values_dead_traces(olddead)  # reset value for dead traces
-
-    return istat
-
-
-def _slice_cube_v2_resample(
-    self, cube, zsurf=None, sampling="nearest", mask=True, deadtraces=True
-):
-    """Slicing with surfaces that not match the cube geometry, snapxy=False
-
-    The idea here is to resample the surface to the cube, then afterwards
-    do an inverse sampling
-    """
-
-    scube = xtgeo.surface_from_cube(cube, 0)
-
-    if self.compare_topology(scube, strict=False):
-        return _slice_cube_v2(self, cube, zsurf, sampling, mask, deadtraces)
-
-    scube.resample(self)
-
-    zcube = None
-    if zsurf:
-        zcube = scube.copy()
-        zcube.resample(zsurf)
-
-    istat = _slice_cube_v2(
-        scube,
-        cube=cube,
-        zsurf=zcube,
-        sampling=sampling,
-        mask=mask,
-        deadtraces=deadtraces,
-    )
-
-    # sample back
-    self.resample(scube, mask=mask)
-
-    return istat
+# -*- coding: utf-8 -*-
+"""Regular surface vs Cube"""
+
+
+import numpy as np
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def slice_cube(
+    self,
+    cube,
+    zsurf=None,
+    sampling="nearest",
+    mask=True,
+    snapxy=False,
+    deadtraces=True,
+    algorithm=1,
+):
+    """Slicing a cube, using different algorithms"""
+
+    if algorithm == 2:
+        if snapxy:
+            return _slice_cube_v2(
+                self,
+                cube,
+                zsurf=zsurf,
+                sampling=sampling,
+                mask=mask,
+                deadtraces=deadtraces,
+            )
+        return _slice_cube_v2_resample(
+            self,
+            cube,
+            zsurf=zsurf,
+            sampling=sampling,
+            mask=mask,
+            deadtraces=deadtraces,
+        )
+
+    # legacy version:
+    return _slice_cube_v1(
+        self,
+        cube,
+        zsurf=zsurf,
+        sampling=sampling,
+        mask=mask,
+        snapxy=snapxy,
+        deadtraces=deadtraces,
+    )
+
+
+def _slice_cube_v1(
+    self, cube, zsurf=None, sampling="nearest", mask=True, snapxy=False, deadtraces=True
+):
+    """
+    Private function for the Cube slicing. This is the legacy version up to version
+    2.8.x but may remain default for a while
+    """
+
+    logger.info("Slice cube algorithm 1")
+    if zsurf is not None:
+        other = zsurf
+    else:
+        logger.info("The current surface is copied as 'other'")
+        other = self.copy()
+
+    if not self.compare_topology(other, strict=False):
+        raise RuntimeError("Topology of maps differ. Stop!")
+
+    if mask:
+        opt2 = 0
+    else:
+        opt2 = 1
+
+    if deadtraces:
+        # set dead traces to cxtgeo UNDEF -> special treatment in the C code
+        olddead = cube.values_dead_traces(xtgeo.UNDEF)
+
+    cubeval1d = np.ravel(cube.values, order="C")
+
+    nsurf = self.ncol * self.nrow
+
+    usesampling = 0
+    if sampling == "trilinear":
+        usesampling = 1
+        if snapxy:
+            usesampling = 2
+
+    logger.debug("Running method from C... (using typemaps for numpies!:")
+    istat, v1d = _cxtgeo.surf_slice_cube(
+        cube.ncol,
+        cube.nrow,
+        cube.nlay,
+        cube.xori,
+        cube.xinc,
+        cube.yori,
+        cube.yinc,
+        cube.zori,
+        cube.zinc,
+        cube.rotation,
+        cube.yflip,
+        cubeval1d,
+        self.ncol,
+        self.nrow,
+        self.xori,
+        self.xinc,
+        self.yori,
+        self.yinc,
+        self.yflip,
+        self.rotation,
+        other.get_values1d(),
+        nsurf,
+        usesampling,
+        opt2,
+    )
+
+    self.set_values1d(v1d)
+
+    if deadtraces:
+        cube.values_dead_traces(olddead)  # reset value for dead traces
+
+    return istat
+
+
+def _slice_cube_v2(
+    self, cube, zsurf=None, sampling="nearest", mask=True, deadtraces=True
+):
+    """
+    This is the new version, optimised for the case where the surface has exact same
+    topology as the cube. This should both simplify and speed up calculations
+
+    From xtgeo 2.9
+    """
+
+    logger.info("Slice cube algorithm 2 with sampling %s", sampling)
+    if zsurf is not None:
+        other = zsurf
+    else:
+        other = self.copy()
+
+    if not self.compare_topology(other, strict=False):
+        raise RuntimeError("Topology of maps differ. Stop!")
+
+    optmask = 0
+    if mask:
+        optmask = 1
+
+    if deadtraces:
+        # set dead traces to cxtgeo UNDEF -> special treatment in the C code
+        olddead = cube.values_dead_traces(xtgeo.UNDEF)
+
+    optnearest = 1
+    if sampling == "trilinear":
+        optnearest = 0
+
+    # cube and surf shall share same topology, e.g. cube.col == surf.ncol etc
+    # print(self.values.mask)
+    istat = _cxtgeo.surf_slice_cube_v3(
+        cube.ncol,
+        cube.nrow,
+        cube.nlay,
+        cube.zori,
+        cube.zinc,
+        cube.values,
+        other.values.data,
+        self.values.data,
+        self.values.mask,
+        optnearest,
+        optmask,
+    )
+
+    if istat != 0:
+        logger.warning("Problem, ISTAT = %s", istat)
+
+    if deadtraces:
+        cube.values_dead_traces(olddead)  # reset value for dead traces
+
+    return istat
+
+
+def _slice_cube_v2_resample(
+    self, cube, zsurf=None, sampling="nearest", mask=True, deadtraces=True
+):
+    """Slicing with surfaces that not match the cube geometry, snapxy=False
+
+    The idea here is to resample the surface to the cube, then afterwards
+    do an inverse sampling
+    """
+
+    scube = xtgeo.surface_from_cube(cube, 0)
+
+    if self.compare_topology(scube, strict=False):
+        return _slice_cube_v2(self, cube, zsurf, sampling, mask, deadtraces)
+
+    scube.resample(self)
+
+    zcube = None
+    if zsurf:
+        zcube = scube.copy()
+        zcube.resample(zsurf)
+
+    istat = _slice_cube_v2(
+        scube,
+        cube=cube,
+        zsurf=zcube,
+        sampling=sampling,
+        mask=mask,
+        deadtraces=deadtraces,
+    )
+
+    # sample back
+    self.resample(scube, mask=mask)
+
+    return istat
```

## xtgeo/surface/_regsurf_cube_window.py

 * *Ordering differences only*

```diff
@@ -1,379 +1,379 @@
-# -*- coding: utf-8 -*-
-"""Regular surface vs Cube, slice a window interval"""
-
-
-import numpy as np
-import numpy.ma as ma
-from xtgeo.common import XTGeoDialog
-from xtgeo.common import XTGShowProgress
-from . import _regsurf_cube_window_v2 as cwv
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-ALLATTRS = [
-    "max",
-    "min",
-    "rms",
-    "mean",
-    "var",
-    "maxpos",
-    "maxneg",
-    "maxabs",
-    "sumpos",
-    "sumneg",
-    "sumabs",
-    "meanabs",
-    "meanpos",
-    "meanneg",
-]
-
-
-def slice_cube_window(
-    self,
-    cube,
-    zsurf=None,
-    other=None,
-    other_position="below",
-    sampling="nearest",
-    mask=True,
-    zrange=10,
-    ndiv=None,
-    attribute="max",
-    maskthreshold=0.1,
-    snapxy=False,
-    showprogress=False,
-    deadtraces=True,
-    algorithm=1,
-):
-    if algorithm == 1:
-        if sampling == "cube":
-            sampling = "nearest"
-
-        attrs = _slice_cube_window_v1(
-            self,
-            cube,
-            zsurf=zsurf,
-            other=other,
-            other_position=other_position,
-            sampling=sampling,
-            mask=mask,
-            zrange=zrange,
-            ndiv=ndiv,
-            attribute=attribute,
-            maskthreshold=maskthreshold,
-            snapxy=snapxy,
-            showprogress=showprogress,
-            deadtraces=deadtraces,
-        )
-
-    else:
-        attrs = cwv.slice_cube_window(
-            self,
-            cube,
-            zsurf=zsurf,
-            other=other,
-            other_position=other_position,
-            sampling=sampling,
-            mask=mask,
-            zrange=zrange,
-            ndiv=ndiv,
-            attribute=attribute,
-            maskthreshold=maskthreshold,
-            snapxy=snapxy,
-            showprogress=showprogress,
-            deadtraces=deadtraces,
-        )
-    return attrs
-
-
-def _slice_cube_window_v1(
-    self,
-    cube,
-    zsurf=None,
-    other=None,
-    other_position="below",
-    sampling="nearest",
-    mask=True,
-    zrange=10,
-    ndiv=None,
-    attribute="max",
-    maskthreshold=0.1,
-    snapxy=False,
-    showprogress=False,
-    deadtraces=True,
-):
-    """Slice Cube with a window and extract attribute(s)
-
-    This is legacy version algorithm 1 and will removed later.
-
-    The zrange is one-sided (on order to secure a centered input); hence
-    of zrange is 5 than the fill window is 10.
-
-    The maskthreshold is only valid for surfaces; if isochore is less than
-    given value then the result will be masked.
-
-    Note: attribute may be a scalar or a list. If a list, then a dict of
-    surfaces are returned.
-    """
-    logger.info("Slice cube window method")
-
-    qattr_is_string = True
-    if not isinstance(attribute, list):
-        if attribute == "all":
-            attrlist = ALLATTRS
-            qattr_is_string = False
-        else:
-            attrlist = [attribute]
-    else:
-        attrlist = attribute
-        qattr_is_string = False
-
-    if zsurf is not None:
-        this = zsurf
-    else:
-        this = self.copy()
-
-    if other is not None:
-        zdelta = np.absolute(this.values - other.values)
-        zrange = zdelta.max()
-
-    ndivmode = "user setting"
-    if ndiv is None:
-        ndivmode = "auto"
-        ndiv = int(2 * zrange / cube.zinc)
-        if ndiv < 1:
-            ndiv = 1
-            logger.warning("NDIV < 1; reset to 1")
-
-    logger.info("ZRANGE is %s", zrange)
-    logger.info("NDIV is set to %s (%s)", ndiv, ndivmode)
-
-    # This will run slice in a loop within a window. Then, numpy methods
-    # are applied to get the attributes
-
-    if other is None:
-        attvalues = _slice_constant_window(
-            this,
-            cube,
-            sampling,
-            zrange,
-            ndiv,
-            mask,
-            attrlist,
-            snapxy,
-            showprogress=showprogress,
-            deadtraces=deadtraces,
-        )
-    else:
-        attvalues = _slice_between_surfaces(
-            this,
-            cube,
-            sampling,
-            other,
-            other_position,
-            zrange,
-            ndiv,
-            mask,
-            attrlist,
-            maskthreshold,
-            snapxy,
-            showprogress=showprogress,
-            deadtraces=deadtraces,
-        )
-
-    results = dict()
-
-    for attr in attrlist:
-        scopy = self.copy()
-        scopy.values = attvalues[attr]
-        results[attr] = scopy
-
-    # for backward compatibility
-    if qattr_is_string:
-        self.values = attvalues[attrlist[0]]
-        return None
-
-    return results
-
-
-def _slice_constant_window(
-    this,
-    cube,
-    sampling,
-    zrange,
-    ndiv,
-    mask,
-    attrlist,
-    snapxy,
-    showprogress=False,
-    deadtraces=True,
-):
-    """Slice a window, (constant in vertical extent)."""
-    npcollect = []
-    zcenter = this.copy()
-
-    logger.info("Mean W of depth no MIDDLE slice is %s", zcenter.values.mean())
-    zcenter.slice_cube(
-        cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
-    )
-    logger.info("Mean of cube slice is %s", zcenter.values.mean())
-
-    npcollect.append(zcenter.values)
-
-    zincr = zrange / float(ndiv)
-
-    logger.info("ZINCR is %s", zincr)
-
-    # collect above the original surface
-    progress = XTGShowProgress(
-        ndiv * 2, show=showprogress, leadtext="progress: ", skip=1
-    )
-    for idv in range(ndiv):
-        progress.flush(idv)
-        ztmp = this.copy()
-        ztmp.values -= zincr * (idv + 1)
-        ztmp.slice_cube(
-            cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
-        )
-        npcollect.append(ztmp.values)
-    # collect below the original surface
-    for idv in range(ndiv):
-        progress.flush(ndiv + idv)
-        ztmp = this.copy()
-        ztmp.values += zincr * (idv + 1)
-        ztmp.slice_cube(
-            cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
-        )
-        npcollect.append(ztmp.values)
-
-    logger.info("Make a stack of the maps...")
-    stacked = ma.dstack(npcollect)
-    del npcollect
-
-    attvalues = dict()
-    for attr in attrlist:
-        logger.info("Running attribute %s", attr)
-        attvalues[attr] = _attvalues(attr, stacked)
-
-    progress.finished()
-    return attvalues  # this is dict with numpies, one per attribute
-
-
-def _slice_between_surfaces(
-    this,
-    cube,
-    sampling,
-    other,
-    other_position,
-    zrange,
-    ndiv,
-    mask,
-    attrlist,
-    mthreshold,
-    snapxy,
-    showprogress=False,
-    deadtraces=True,
-):
-    """Slice and find values between two surfaces."""
-
-    npcollect = []
-    zincr = zrange / float(ndiv)
-
-    zcenter = this.copy()
-    zcenter.slice_cube(
-        cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
-    )
-    npcollect.append(zcenter.values)
-
-    # collect below or above the original surface
-    if other_position == "above":
-        mul = -1
-    else:
-        mul = 1
-
-    # collect above the original surface
-    progress = XTGShowProgress(ndiv, show=showprogress, leadtext="progress: ")
-    for idv in range(ndiv):
-        progress.flush(idv)
-        ztmp = this.copy()
-        ztmp.values += zincr * (idv + 1) * mul
-        zvalues = ztmp.values.copy()
-
-        ztmp.slice_cube(
-            cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
-        )
-
-        diff = mul * (other.values - zvalues)
-
-        values = ztmp.values
-        values = ma.masked_where(diff < 0.0, values)
-
-        npcollect.append(values)
-
-    stacked = ma.dstack(npcollect)
-
-    del npcollect
-
-    # for cases with erosion, the two surfaces are equal
-    isovalues = mul * (other.values - this.values)
-
-    attvalues = dict()
-    for attr in attrlist:
-        attvaluestmp = _attvalues(attr, stacked)
-        attvalues[attr] = ma.masked_where(isovalues < mthreshold, attvaluestmp)
-
-    progress.finished()
-
-    return attvalues  # this is dict with numpies, one per attribute
-
-
-def _attvalues(attribute, stacked):  # pylint: disable=too-many-branches
-    """Attribute values computed in numpy.ma stack."""
-    if attribute == "max":
-        attvalues = ma.max(stacked, axis=2)
-    elif attribute == "min":
-        attvalues = ma.min(stacked, axis=2)
-    elif attribute == "rms":
-        attvalues = np.sqrt(ma.mean(np.square(stacked), axis=2))
-    elif attribute == "var":
-        attvalues = ma.var(stacked, axis=2)
-    elif attribute == "mean":
-        attvalues = ma.mean(stacked, axis=2)
-    elif attribute == "maxpos":
-        stacked = ma.masked_less(stacked, 0.0, copy=True)
-        attvalues = ma.max(stacked, axis=2)
-    elif attribute == "maxneg":  # ~ minimum of negative values?
-        stacked = ma.masked_greater_equal(stacked, 0.0, copy=True)
-        attvalues = ma.min(stacked, axis=2)
-    elif attribute == "maxabs":
-        attvalues = ma.max(abs(stacked), axis=2)
-    elif attribute == "sumpos":
-        stacked = ma.masked_less(stacked, 0.0, copy=True)
-        attvalues = ma.sum(stacked, axis=2)
-    elif attribute == "sumneg":
-        stacked = ma.masked_greater_equal(stacked, 0.0, copy=True)
-        attvalues = ma.sum(stacked, axis=2)
-    elif attribute == "sumabs":
-        attvalues = ma.sum(abs(stacked), axis=2)
-    elif attribute == "meanabs":
-        attvalues = ma.mean(abs(stacked), axis=2)
-    elif attribute == "meanpos":
-        stacked = ma.masked_less(stacked, 0.0, copy=True)
-        attvalues = ma.mean(stacked, axis=2)
-    elif attribute == "meanneg":
-        stacked = ma.masked_greater_equal(stacked, 0.0, copy=True)
-        attvalues = ma.mean(stacked, axis=2)
-    else:
-        etxt = f"Invalid attribute applied: {attribute}"
-        raise ValueError(etxt)
-
-    if not attvalues.flags["C_CONTIGUOUS"]:
-        mask = ma.getmaskarray(attvalues)
-        mask = np.asanyarray(mask, order="C")
-        attvalues = np.asanyarray(attvalues, order="C")
-        attvalues = ma.array(attvalues, mask=mask, order="C")
-
-    return attvalues
+# -*- coding: utf-8 -*-
+"""Regular surface vs Cube, slice a window interval"""
+
+
+import numpy as np
+import numpy.ma as ma
+from xtgeo.common import XTGeoDialog
+from xtgeo.common import XTGShowProgress
+from . import _regsurf_cube_window_v2 as cwv
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+ALLATTRS = [
+    "max",
+    "min",
+    "rms",
+    "mean",
+    "var",
+    "maxpos",
+    "maxneg",
+    "maxabs",
+    "sumpos",
+    "sumneg",
+    "sumabs",
+    "meanabs",
+    "meanpos",
+    "meanneg",
+]
+
+
+def slice_cube_window(
+    self,
+    cube,
+    zsurf=None,
+    other=None,
+    other_position="below",
+    sampling="nearest",
+    mask=True,
+    zrange=10,
+    ndiv=None,
+    attribute="max",
+    maskthreshold=0.1,
+    snapxy=False,
+    showprogress=False,
+    deadtraces=True,
+    algorithm=1,
+):
+    if algorithm == 1:
+        if sampling == "cube":
+            sampling = "nearest"
+
+        attrs = _slice_cube_window_v1(
+            self,
+            cube,
+            zsurf=zsurf,
+            other=other,
+            other_position=other_position,
+            sampling=sampling,
+            mask=mask,
+            zrange=zrange,
+            ndiv=ndiv,
+            attribute=attribute,
+            maskthreshold=maskthreshold,
+            snapxy=snapxy,
+            showprogress=showprogress,
+            deadtraces=deadtraces,
+        )
+
+    else:
+        attrs = cwv.slice_cube_window(
+            self,
+            cube,
+            zsurf=zsurf,
+            other=other,
+            other_position=other_position,
+            sampling=sampling,
+            mask=mask,
+            zrange=zrange,
+            ndiv=ndiv,
+            attribute=attribute,
+            maskthreshold=maskthreshold,
+            snapxy=snapxy,
+            showprogress=showprogress,
+            deadtraces=deadtraces,
+        )
+    return attrs
+
+
+def _slice_cube_window_v1(
+    self,
+    cube,
+    zsurf=None,
+    other=None,
+    other_position="below",
+    sampling="nearest",
+    mask=True,
+    zrange=10,
+    ndiv=None,
+    attribute="max",
+    maskthreshold=0.1,
+    snapxy=False,
+    showprogress=False,
+    deadtraces=True,
+):
+    """Slice Cube with a window and extract attribute(s)
+
+    This is legacy version algorithm 1 and will removed later.
+
+    The zrange is one-sided (on order to secure a centered input); hence
+    of zrange is 5 than the fill window is 10.
+
+    The maskthreshold is only valid for surfaces; if isochore is less than
+    given value then the result will be masked.
+
+    Note: attribute may be a scalar or a list. If a list, then a dict of
+    surfaces are returned.
+    """
+    logger.info("Slice cube window method")
+
+    qattr_is_string = True
+    if not isinstance(attribute, list):
+        if attribute == "all":
+            attrlist = ALLATTRS
+            qattr_is_string = False
+        else:
+            attrlist = [attribute]
+    else:
+        attrlist = attribute
+        qattr_is_string = False
+
+    if zsurf is not None:
+        this = zsurf
+    else:
+        this = self.copy()
+
+    if other is not None:
+        zdelta = np.absolute(this.values - other.values)
+        zrange = zdelta.max()
+
+    ndivmode = "user setting"
+    if ndiv is None:
+        ndivmode = "auto"
+        ndiv = int(2 * zrange / cube.zinc)
+        if ndiv < 1:
+            ndiv = 1
+            logger.warning("NDIV < 1; reset to 1")
+
+    logger.info("ZRANGE is %s", zrange)
+    logger.info("NDIV is set to %s (%s)", ndiv, ndivmode)
+
+    # This will run slice in a loop within a window. Then, numpy methods
+    # are applied to get the attributes
+
+    if other is None:
+        attvalues = _slice_constant_window(
+            this,
+            cube,
+            sampling,
+            zrange,
+            ndiv,
+            mask,
+            attrlist,
+            snapxy,
+            showprogress=showprogress,
+            deadtraces=deadtraces,
+        )
+    else:
+        attvalues = _slice_between_surfaces(
+            this,
+            cube,
+            sampling,
+            other,
+            other_position,
+            zrange,
+            ndiv,
+            mask,
+            attrlist,
+            maskthreshold,
+            snapxy,
+            showprogress=showprogress,
+            deadtraces=deadtraces,
+        )
+
+    results = dict()
+
+    for attr in attrlist:
+        scopy = self.copy()
+        scopy.values = attvalues[attr]
+        results[attr] = scopy
+
+    # for backward compatibility
+    if qattr_is_string:
+        self.values = attvalues[attrlist[0]]
+        return None
+
+    return results
+
+
+def _slice_constant_window(
+    this,
+    cube,
+    sampling,
+    zrange,
+    ndiv,
+    mask,
+    attrlist,
+    snapxy,
+    showprogress=False,
+    deadtraces=True,
+):
+    """Slice a window, (constant in vertical extent)."""
+    npcollect = []
+    zcenter = this.copy()
+
+    logger.info("Mean W of depth no MIDDLE slice is %s", zcenter.values.mean())
+    zcenter.slice_cube(
+        cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
+    )
+    logger.info("Mean of cube slice is %s", zcenter.values.mean())
+
+    npcollect.append(zcenter.values)
+
+    zincr = zrange / float(ndiv)
+
+    logger.info("ZINCR is %s", zincr)
+
+    # collect above the original surface
+    progress = XTGShowProgress(
+        ndiv * 2, show=showprogress, leadtext="progress: ", skip=1
+    )
+    for idv in range(ndiv):
+        progress.flush(idv)
+        ztmp = this.copy()
+        ztmp.values -= zincr * (idv + 1)
+        ztmp.slice_cube(
+            cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
+        )
+        npcollect.append(ztmp.values)
+    # collect below the original surface
+    for idv in range(ndiv):
+        progress.flush(ndiv + idv)
+        ztmp = this.copy()
+        ztmp.values += zincr * (idv + 1)
+        ztmp.slice_cube(
+            cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
+        )
+        npcollect.append(ztmp.values)
+
+    logger.info("Make a stack of the maps...")
+    stacked = ma.dstack(npcollect)
+    del npcollect
+
+    attvalues = dict()
+    for attr in attrlist:
+        logger.info("Running attribute %s", attr)
+        attvalues[attr] = _attvalues(attr, stacked)
+
+    progress.finished()
+    return attvalues  # this is dict with numpies, one per attribute
+
+
+def _slice_between_surfaces(
+    this,
+    cube,
+    sampling,
+    other,
+    other_position,
+    zrange,
+    ndiv,
+    mask,
+    attrlist,
+    mthreshold,
+    snapxy,
+    showprogress=False,
+    deadtraces=True,
+):
+    """Slice and find values between two surfaces."""
+
+    npcollect = []
+    zincr = zrange / float(ndiv)
+
+    zcenter = this.copy()
+    zcenter.slice_cube(
+        cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
+    )
+    npcollect.append(zcenter.values)
+
+    # collect below or above the original surface
+    if other_position == "above":
+        mul = -1
+    else:
+        mul = 1
+
+    # collect above the original surface
+    progress = XTGShowProgress(ndiv, show=showprogress, leadtext="progress: ")
+    for idv in range(ndiv):
+        progress.flush(idv)
+        ztmp = this.copy()
+        ztmp.values += zincr * (idv + 1) * mul
+        zvalues = ztmp.values.copy()
+
+        ztmp.slice_cube(
+            cube, sampling=sampling, mask=mask, snapxy=snapxy, deadtraces=deadtraces
+        )
+
+        diff = mul * (other.values - zvalues)
+
+        values = ztmp.values
+        values = ma.masked_where(diff < 0.0, values)
+
+        npcollect.append(values)
+
+    stacked = ma.dstack(npcollect)
+
+    del npcollect
+
+    # for cases with erosion, the two surfaces are equal
+    isovalues = mul * (other.values - this.values)
+
+    attvalues = dict()
+    for attr in attrlist:
+        attvaluestmp = _attvalues(attr, stacked)
+        attvalues[attr] = ma.masked_where(isovalues < mthreshold, attvaluestmp)
+
+    progress.finished()
+
+    return attvalues  # this is dict with numpies, one per attribute
+
+
+def _attvalues(attribute, stacked):  # pylint: disable=too-many-branches
+    """Attribute values computed in numpy.ma stack."""
+    if attribute == "max":
+        attvalues = ma.max(stacked, axis=2)
+    elif attribute == "min":
+        attvalues = ma.min(stacked, axis=2)
+    elif attribute == "rms":
+        attvalues = np.sqrt(ma.mean(np.square(stacked), axis=2))
+    elif attribute == "var":
+        attvalues = ma.var(stacked, axis=2)
+    elif attribute == "mean":
+        attvalues = ma.mean(stacked, axis=2)
+    elif attribute == "maxpos":
+        stacked = ma.masked_less(stacked, 0.0, copy=True)
+        attvalues = ma.max(stacked, axis=2)
+    elif attribute == "maxneg":  # ~ minimum of negative values?
+        stacked = ma.masked_greater_equal(stacked, 0.0, copy=True)
+        attvalues = ma.min(stacked, axis=2)
+    elif attribute == "maxabs":
+        attvalues = ma.max(abs(stacked), axis=2)
+    elif attribute == "sumpos":
+        stacked = ma.masked_less(stacked, 0.0, copy=True)
+        attvalues = ma.sum(stacked, axis=2)
+    elif attribute == "sumneg":
+        stacked = ma.masked_greater_equal(stacked, 0.0, copy=True)
+        attvalues = ma.sum(stacked, axis=2)
+    elif attribute == "sumabs":
+        attvalues = ma.sum(abs(stacked), axis=2)
+    elif attribute == "meanabs":
+        attvalues = ma.mean(abs(stacked), axis=2)
+    elif attribute == "meanpos":
+        stacked = ma.masked_less(stacked, 0.0, copy=True)
+        attvalues = ma.mean(stacked, axis=2)
+    elif attribute == "meanneg":
+        stacked = ma.masked_greater_equal(stacked, 0.0, copy=True)
+        attvalues = ma.mean(stacked, axis=2)
+    else:
+        etxt = f"Invalid attribute applied: {attribute}"
+        raise ValueError(etxt)
+
+    if not attvalues.flags["C_CONTIGUOUS"]:
+        mask = ma.getmaskarray(attvalues)
+        mask = np.asanyarray(mask, order="C")
+        attvalues = np.asanyarray(attvalues, order="C")
+        attvalues = ma.array(attvalues, mask=mask, order="C")
+
+    return attvalues
```

## xtgeo/surface/_regsurf_cube_window_v2.py

 * *Ordering differences only*

```diff
@@ -1,307 +1,307 @@
-# -*- coding: utf-8 -*-
-"""Regular surface vs Cube, slice a window interval v2"""
-
-
-import numpy as np
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-ALLATTRS = [
-    "min",
-    "max",
-    "mean",
-    "var",
-    "rms",
-    "maxpos",
-    "maxneg",
-    "maxabs",
-    "meanabs",
-    "meanpos",
-    "meanneg",
-    "sumpos",
-    "sumneg",
-    "sumabs",
-]
-
-
-def slice_cube_window(
-    self,
-    cube,
-    zsurf=None,
-    other=None,
-    other_position="below",
-    sampling="nearest",
-    mask=True,
-    zrange=10,
-    ndiv=None,
-    attribute="max",
-    maskthreshold=0.1,
-    snapxy=False,
-    showprogress=False,
-    deadtraces=True,
-):
-    if not snapxy:
-        attrs = _slice_cube_window_resample(
-            self,
-            cube,
-            zsurf,
-            other,
-            other_position,
-            sampling,
-            mask,
-            zrange,
-            ndiv,
-            attribute,
-            maskthreshold,
-            showprogress,
-            deadtraces,
-        )
-
-    else:
-        attrs = _slice_cube_window(
-            self,
-            cube,
-            zsurf,
-            other,
-            other_position,
-            sampling,
-            mask,
-            zrange,
-            ndiv,
-            attribute,
-            maskthreshold,
-            showprogress,
-            deadtraces,
-        )
-
-    # if attribute is str, self shall be updated and None returned, otherwise a dict
-    # of attributes objects shall be returned
-    if isinstance(attrs, dict) and len(attrs) == 1 and isinstance(attribute, str):
-        self.values = attrs[attribute].values
-        return None
-
-    return attrs
-
-
-def _slice_cube_window(
-    self,
-    cube,
-    zsurf,
-    other,
-    other_position,
-    sampling,
-    mask,
-    zrange,
-    ndiv,
-    attribute,
-    maskthreshold,
-    showprogress,
-    deadtraces,
-):  # pylint: disable=too-many-branches, too-many-statements
-    """Slice Cube between surfaces to find attributes
-
-    New from May 2020, to provide a much faster algorithm and correct some issues
-    found in previous version
-    """
-
-    logger.info("Slice cube window method v2")
-
-    olddead = None
-    if deadtraces:
-        olddead = cube.values_dead_traces(xtgeo.UNDEF)
-
-    optprogress = 0
-    if showprogress:
-        optprogress = 1
-
-    if not isinstance(attribute, list):
-        if attribute == "all":
-            attrlist = ALLATTRS
-        else:
-            attrlist = [attribute]
-    else:
-        attrlist = attribute
-
-    if zsurf is not None:
-        this = zsurf
-    else:
-        this = self.copy()
-
-    if other is not None:
-        zdelta = np.absolute(this.values - other.values)
-        zdiameter = zdelta.max()
-        if other_position.lower() == "below":
-            surf1 = this
-            surf2 = other
-        else:
-            surf1 = other  # BEWARE self
-            surf2 = this
-    else:
-        surf1 = this.copy()
-        surf2 = this.copy()
-        surf1.values -= zrange
-        surf2.values += zrange
-        zdiameter = 2 * zrange
-
-    if ndiv is None:
-        ndiv = int(2 * zdiameter / cube.zinc)
-        if ndiv < 1:
-            ndiv = 1
-            logger.warning("NDIV < 1; reset to 1")
-
-    # force/overrule a coarse sampling for sampling option "cube"
-    ndivdisc = int((zdiameter) * 1.0001 / cube.zinc)
-    if sampling == "cube":
-        ndiv = ndivdisc
-
-    zrinc = zdiameter / ndiv
-    logger.debug("zdiameter and cube zinc: %s %s", zdiameter, cube.zinc)
-    logger.debug("zrinc and ndiv: %s %s", zrinc, ndiv)
-
-    optsum = 0
-    if any("sum" in word for word in attrlist):
-        optsum = 1
-
-    results = _attributes_betw_surfaces(
-        self,
-        cube,
-        surf1,
-        surf2,
-        sampling,
-        mask,
-        zrinc,
-        ndiv,
-        ndivdisc,
-        optprogress,
-        maskthreshold,
-        optsum,
-    )
-
-    if deadtraces:
-        cube.values_dead_traces(olddead)  # reset value for dead traces
-
-    # build the returning result
-    alist = {}
-    for att in attrlist:
-        num = ALLATTRS.index(att)
-        alist[att] = self.copy()
-        alist[att].values = results[num, :]
-
-    return alist
-
-
-def _attributes_betw_surfaces(
-    self,
-    cube,
-    surf1,
-    surf2,
-    sampling,
-    maskopt,
-    zrinc,
-    ndiv,
-    ndivdisc,
-    optprogress,
-    maskthreshold,
-    optsum,
-):
-    """This is the actual lowlevel engine communicating with C code"""
-
-    logger.info("Attributes between surfaces")
-
-    results = np.zeros((len(ALLATTRS) * self.ncol * self.nrow), dtype=np.float64)
-
-    optnearest = 0
-    if sampling in ["nearest", "cube"]:
-        optnearest = 1
-
-    _cxtgeo.surf_cube_attr_intv(
-        cube.ncol,
-        cube.nrow,
-        cube.nlay,
-        cube.zori,
-        cube.zinc,
-        cube.values,
-        surf1.values.data,
-        surf2.values.data,
-        surf1.values.mask,
-        surf2.values.mask,
-        zrinc,
-        ndiv,
-        ndivdisc,
-        results,
-        optnearest,
-        maskopt,
-        optprogress,
-        maskthreshold,
-        optsum,
-    )
-
-    logger.info("Results updated, with size %s", results.shape)
-
-    results = results.reshape((len(ALLATTRS), self.ncol * self.nrow), order="C")
-
-    return results
-
-
-def _slice_cube_window_resample(
-    self,
-    cube,
-    zsurf,
-    other,
-    other_position,
-    sampling,
-    mask,
-    zrange,
-    ndiv,
-    attribute,
-    maskthreshold,
-    showprogress,
-    deadtraces,
-):
-    """Makes a resample from original surfaces first to fit cube topology"""
-
-    logger.info("Attributes between surfaces, resampling version")
-
-    scube = xtgeo.surface_from_cube(cube, 0.0)
-
-    scube.resample(self)
-
-    szsurf = None
-    if zsurf:
-        szsurf = scube.copy()
-        szsurf.resample(zsurf)
-
-    sother = None
-    if other:
-        sother = scube.copy()
-        sother.resample(other)
-
-    attrs = _slice_cube_window(
-        scube,
-        cube,
-        szsurf,
-        sother,
-        other_position,
-        sampling,
-        mask,
-        zrange,
-        ndiv,
-        attribute,
-        maskthreshold,
-        showprogress,
-        deadtraces,
-    )
-
-    # now resample attrs back to a copy of self
-    zelf = self.copy()
-    for key, _val in attrs.items():
-        zelf.resample(attrs[key])
-        attrs[key] = zelf.copy()
-
-    return attrs
+# -*- coding: utf-8 -*-
+"""Regular surface vs Cube, slice a window interval v2"""
+
+
+import numpy as np
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+ALLATTRS = [
+    "min",
+    "max",
+    "mean",
+    "var",
+    "rms",
+    "maxpos",
+    "maxneg",
+    "maxabs",
+    "meanabs",
+    "meanpos",
+    "meanneg",
+    "sumpos",
+    "sumneg",
+    "sumabs",
+]
+
+
+def slice_cube_window(
+    self,
+    cube,
+    zsurf=None,
+    other=None,
+    other_position="below",
+    sampling="nearest",
+    mask=True,
+    zrange=10,
+    ndiv=None,
+    attribute="max",
+    maskthreshold=0.1,
+    snapxy=False,
+    showprogress=False,
+    deadtraces=True,
+):
+    if not snapxy:
+        attrs = _slice_cube_window_resample(
+            self,
+            cube,
+            zsurf,
+            other,
+            other_position,
+            sampling,
+            mask,
+            zrange,
+            ndiv,
+            attribute,
+            maskthreshold,
+            showprogress,
+            deadtraces,
+        )
+
+    else:
+        attrs = _slice_cube_window(
+            self,
+            cube,
+            zsurf,
+            other,
+            other_position,
+            sampling,
+            mask,
+            zrange,
+            ndiv,
+            attribute,
+            maskthreshold,
+            showprogress,
+            deadtraces,
+        )
+
+    # if attribute is str, self shall be updated and None returned, otherwise a dict
+    # of attributes objects shall be returned
+    if isinstance(attrs, dict) and len(attrs) == 1 and isinstance(attribute, str):
+        self.values = attrs[attribute].values
+        return None
+
+    return attrs
+
+
+def _slice_cube_window(
+    self,
+    cube,
+    zsurf,
+    other,
+    other_position,
+    sampling,
+    mask,
+    zrange,
+    ndiv,
+    attribute,
+    maskthreshold,
+    showprogress,
+    deadtraces,
+):  # pylint: disable=too-many-branches, too-many-statements
+    """Slice Cube between surfaces to find attributes
+
+    New from May 2020, to provide a much faster algorithm and correct some issues
+    found in previous version
+    """
+
+    logger.info("Slice cube window method v2")
+
+    olddead = None
+    if deadtraces:
+        olddead = cube.values_dead_traces(xtgeo.UNDEF)
+
+    optprogress = 0
+    if showprogress:
+        optprogress = 1
+
+    if not isinstance(attribute, list):
+        if attribute == "all":
+            attrlist = ALLATTRS
+        else:
+            attrlist = [attribute]
+    else:
+        attrlist = attribute
+
+    if zsurf is not None:
+        this = zsurf
+    else:
+        this = self.copy()
+
+    if other is not None:
+        zdelta = np.absolute(this.values - other.values)
+        zdiameter = zdelta.max()
+        if other_position.lower() == "below":
+            surf1 = this
+            surf2 = other
+        else:
+            surf1 = other  # BEWARE self
+            surf2 = this
+    else:
+        surf1 = this.copy()
+        surf2 = this.copy()
+        surf1.values -= zrange
+        surf2.values += zrange
+        zdiameter = 2 * zrange
+
+    if ndiv is None:
+        ndiv = int(2 * zdiameter / cube.zinc)
+        if ndiv < 1:
+            ndiv = 1
+            logger.warning("NDIV < 1; reset to 1")
+
+    # force/overrule a coarse sampling for sampling option "cube"
+    ndivdisc = int((zdiameter) * 1.0001 / cube.zinc)
+    if sampling == "cube":
+        ndiv = ndivdisc
+
+    zrinc = zdiameter / ndiv
+    logger.debug("zdiameter and cube zinc: %s %s", zdiameter, cube.zinc)
+    logger.debug("zrinc and ndiv: %s %s", zrinc, ndiv)
+
+    optsum = 0
+    if any("sum" in word for word in attrlist):
+        optsum = 1
+
+    results = _attributes_betw_surfaces(
+        self,
+        cube,
+        surf1,
+        surf2,
+        sampling,
+        mask,
+        zrinc,
+        ndiv,
+        ndivdisc,
+        optprogress,
+        maskthreshold,
+        optsum,
+    )
+
+    if deadtraces:
+        cube.values_dead_traces(olddead)  # reset value for dead traces
+
+    # build the returning result
+    alist = {}
+    for att in attrlist:
+        num = ALLATTRS.index(att)
+        alist[att] = self.copy()
+        alist[att].values = results[num, :]
+
+    return alist
+
+
+def _attributes_betw_surfaces(
+    self,
+    cube,
+    surf1,
+    surf2,
+    sampling,
+    maskopt,
+    zrinc,
+    ndiv,
+    ndivdisc,
+    optprogress,
+    maskthreshold,
+    optsum,
+):
+    """This is the actual lowlevel engine communicating with C code"""
+
+    logger.info("Attributes between surfaces")
+
+    results = np.zeros((len(ALLATTRS) * self.ncol * self.nrow), dtype=np.float64)
+
+    optnearest = 0
+    if sampling in ["nearest", "cube"]:
+        optnearest = 1
+
+    _cxtgeo.surf_cube_attr_intv(
+        cube.ncol,
+        cube.nrow,
+        cube.nlay,
+        cube.zori,
+        cube.zinc,
+        cube.values,
+        surf1.values.data,
+        surf2.values.data,
+        surf1.values.mask,
+        surf2.values.mask,
+        zrinc,
+        ndiv,
+        ndivdisc,
+        results,
+        optnearest,
+        maskopt,
+        optprogress,
+        maskthreshold,
+        optsum,
+    )
+
+    logger.info("Results updated, with size %s", results.shape)
+
+    results = results.reshape((len(ALLATTRS), self.ncol * self.nrow), order="C")
+
+    return results
+
+
+def _slice_cube_window_resample(
+    self,
+    cube,
+    zsurf,
+    other,
+    other_position,
+    sampling,
+    mask,
+    zrange,
+    ndiv,
+    attribute,
+    maskthreshold,
+    showprogress,
+    deadtraces,
+):
+    """Makes a resample from original surfaces first to fit cube topology"""
+
+    logger.info("Attributes between surfaces, resampling version")
+
+    scube = xtgeo.surface_from_cube(cube, 0.0)
+
+    scube.resample(self)
+
+    szsurf = None
+    if zsurf:
+        szsurf = scube.copy()
+        szsurf.resample(zsurf)
+
+    sother = None
+    if other:
+        sother = scube.copy()
+        sother.resample(other)
+
+    attrs = _slice_cube_window(
+        scube,
+        cube,
+        szsurf,
+        sother,
+        other_position,
+        sampling,
+        mask,
+        zrange,
+        ndiv,
+        attribute,
+        maskthreshold,
+        showprogress,
+        deadtraces,
+    )
+
+    # now resample attrs back to a copy of self
+    zelf = self.copy()
+    for key, _val in attrs.items():
+        zelf.resample(attrs[key])
+        attrs[key] = zelf.copy()
+
+    return attrs
```

## xtgeo/surface/_regsurf_export.py

 * *Ordering differences only*

```diff
@@ -1,485 +1,485 @@
-# -*- coding: utf-8 -*-
-"""Export RegularSurface data."""
-
-import json
-
-# pylint: disable=protected-access
-# import hashlib
-import struct
-
-import h5py
-import hdf5plugin
-import numpy as np
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo  # pylint: disable=import-error
-from xtgeo.common import XTGeoDialog
-from xtgeo.common.constants import UNDEF_MAP_IRAPA, UNDEF_MAP_IRAPB
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-DEBUG = 0
-
-if DEBUG < 0:
-    DEBUG = 0
-
-PMD_DATAUNITDISTANCE = {
-    15: "meter",
-    16: "km",
-    17: "feet",
-    18: "yard",
-    19: "mile",
-    221: "global",
-}
-
-PMD_DATAUNITZ = {
-    10: "10",
-    31: "31",
-    44: "44",
-    300: "300",
-}
-
-
-def export_irap_ascii(self, mfile, engine="cxtgeo"):
-    """Export to Irap RMS ascii format."""
-    if mfile.memstream is True or engine == "python":
-        _export_irap_ascii_purepy(self, mfile)
-    else:
-        _export_irap_ascii(self, mfile)
-
-
-def _export_irap_ascii_purepy(self, mfile):
-    """Export to Irap RMS ascii using pure python, slower? but safer for memstreams."""
-    vals = self.get_values1d(fill_value=UNDEF_MAP_IRAPA, order="F")
-
-    xmax = self.xori + (self.ncol - 1) * self.xinc
-    ymax = self.yori + (self.nrow - 1) * self.yinc
-
-    buf = f"-996 {self.nrow} {self.xinc} {self.yinc}\n"
-    buf += f"{self.xori} {xmax} {self.yori} {ymax}\n"
-    buf += f"{self.ncol} {self.rotation} {self.xori} {self.yori}\n"
-    buf += "0  0  0  0  0  0  0\n"
-    vals = vals.astype("str").tolist()
-    nrow = 0
-    for val in vals:
-        buf += val
-        nrow += 1
-        if nrow == 6:
-            buf += "\n"
-            nrow = 0
-        else:
-            buf += " "
-
-    if nrow != 0:
-        buf += "\n"
-
-    # convert buffer to ascii
-    buf = buf.encode("latin1")
-
-    if mfile.memstream:
-        mfile.file.write(buf)
-    else:
-        with open(mfile.name, "wb") as fout:
-            fout.write(buf)
-
-    del vals
-
-
-def _export_irap_ascii(self, mfile):
-    """Export to Irap RMS ascii format using cxtgeo."""
-    vals = self.get_values1d(fill_value=xtgeo.UNDEF)
-
-    ier = _cxtgeo.surf_export_irap_ascii(
-        mfile.get_cfhandle(),
-        self._ncol,
-        self._nrow,
-        self._xori,
-        self._yori,
-        self._xinc,
-        self._yflip * self._yinc,
-        self._rotation,
-        vals,
-        0,
-    )
-    if ier != 0:
-        raise RuntimeError(f"Export to Irap Ascii went wrong, code is {ier}")
-
-    del vals
-
-    mfile.cfclose()
-
-
-def export_irap_binary(self, mfile, engine="cxtgeo"):
-    """Export to Irap RMS binary format.
-
-    Note that mfile can also a be a BytesIO instance
-    """
-    if mfile.memstream or engine == "python":
-        _export_irap_binary_python(self, mfile)
-    else:
-        _export_irap_binary_cxtgeo(self, mfile)
-
-
-def _export_irap_binary_python(self, mfile):
-    """Export to Irap RMS binary format but use python only.
-
-    This is approx 2-5 times slower than the C method, but may a be more robust in cases
-    with BytesIO.
-    """
-    vals = self.get_values1d(fill_value=UNDEF_MAP_IRAPB, order="F")
-
-    ap = struct.pack(
-        ">3i6f3i3f10i",  # > means big endian storage
-        32,
-        -996,
-        self.nrow,
-        self.xori,
-        self.xori + self.xinc * (self.ncol - 1),
-        self.yori,
-        self.yori + self.yinc * (self.nrow - 1),
-        self.xinc,
-        self.yinc,
-        32,
-        16,
-        self.ncol,
-        self.rotation,
-        self.xori,
-        self.yori,
-        16,
-        28,
-        0,
-        0,
-        0,
-        0,
-        0,
-        0,
-        0,
-        28,
-    )
-    inum = self.nrow * self.ncol
-
-    # export to Irap binary in ncol chunks (the only chunk size accepted by RMS)
-    nchunk = self.ncol
-    chunks = [nchunk] * (inum // nchunk)
-    if (inum % nchunk) > 0:
-        chunks.append(inum % nchunk)
-    start = 0
-    for chunk in chunks:
-        ap += struct.pack(">i", chunk * 4)
-        ap += struct.pack(f">{chunk:d}f", *vals[start : start + chunk])
-        ap += struct.pack(">i", chunk * 4)
-        start += chunk
-
-    if mfile.memstream:
-        mfile.file.write(ap)
-    else:
-        with open(mfile.name, "wb") as fout:
-            fout.write(ap)
-
-
-def _export_irap_binary_cxtgeo(self, mfile):
-    """Export to Irap binary using C backend.
-
-    Args:
-        mfile (_XTGeoFile): xtgeo file instance
-
-    Raises:
-        RuntimeError: Export to Irap Binary went wrong...
-    """
-    vals = self.get_values1d(fill_value=UNDEF_MAP_IRAPB, order="F")
-    ier = _cxtgeo.surf_export_irap_bin(
-        mfile.get_cfhandle(),
-        self._ncol,
-        self._nrow,
-        self._xori,
-        self._yori,
-        self._xinc,
-        self._yflip * self._yinc,
-        self._rotation,
-        vals,
-        0,
-    )
-
-    if ier != 0:
-        mfile.cfclose(strict=False)  # strict False as C routine may have closed
-        raise RuntimeError(f"Export to Irap Binary went wrong, code is {ier}")
-
-    mfile.cfclose()
-
-
-def export_ijxyz_ascii(self, mfile):
-    """Export to DSG IJXYZ ascii format."""
-    vals = self.get_values1d(fill_value=xtgeo.UNDEF)
-    ier = _cxtgeo.surf_export_ijxyz(
-        mfile.get_cfhandle(),
-        self._ncol,
-        self._nrow,
-        self._xori,
-        self._yori,
-        self._xinc,
-        self._yinc,
-        self._rotation,
-        self._yflip,
-        self._ilines,
-        self._xlines,
-        vals,
-        0,
-    )
-
-    if ier != 0:
-        raise RuntimeError(f"Export to IJXYZ format went wrong, code is {ier}")
-
-    mfile.cfclose()
-
-
-def export_zmap_ascii(self, mfile, engine="cxtgeo"):
-    """Export to ZMAP ascii format (non-rotated)."""
-
-    # zmap can only deal with non-rotated formats; hence make a copy
-    # of the instance and derotate that prior to export, so that the
-    # original instance is unchanged
-
-    if mfile.memstream or engine == "python":
-        _export_zmap_ascii_purepy(self, mfile)
-    else:
-        _export_zmap_ascii(self, mfile)
-
-
-def _export_zmap_ascii_purepy(self, mfile):
-    """Export to ZMAP ascii format (non-rotated), pure python for memstreams"""
-
-    scopy = self.copy()
-
-    undef = -99999.0
-    if abs(scopy.rotation) > 1.0e-20:
-        scopy.unrotate()
-
-    yinc = scopy._yinc * scopy._yflip
-
-    vals = scopy.get_values1d(order="C", asmasked=False, fill_value=undef)
-
-    xmax = scopy.xori + (scopy.ncol - 1) * scopy.xinc
-    ymax = scopy.yori + (scopy.nrow - 1) * yinc
-
-    fcode = 8
-    if scopy.values.min() > -10 and scopy.values.max() < 10:
-        fcode = 4
-
-    nfrow = scopy.nrow if scopy.nrow < 5 else 5
-
-    buf = "! Export from XTGeo (python engine)\n"
-    buf += f"@ GRIDFILE, GRID, {nfrow}\n"
-    buf += f"20, {undef}, , {fcode}, 1\n"
-    buf += f"{scopy.nrow}, {scopy.ncol}, {scopy.xori}, {xmax}, {scopy.yori}, {ymax}\n"
-
-    buf += "0.0, 0.0, 0.0\n"
-    buf += "@\n"
-
-    vals = vals.tolist()
-    ncol = 0
-    for icol in range(scopy.ncol):
-        for jrow in range(scopy.nrow - 1, -1, -1):
-            ic = icol * scopy.nrow + jrow
-            buf += f" {vals[ic]:19.{fcode}f}"
-            ncol += 1
-            if ncol == 5 or jrow == 0:
-                buf += "\n"
-                ncol = 0
-
-    # convert buffer to ascii
-    buf = buf.encode("latin1")
-
-    if mfile.memstream:
-        mfile.file.write(buf)
-    else:
-        with open(mfile.name, "wb") as fout:
-            fout.write(buf)
-
-    del vals
-
-
-def _export_zmap_ascii(self, mfile):
-    """Export to ZMAP ascii format (non-rotated)."""
-
-    scopy = self.copy()
-
-    if abs(scopy.rotation) > 1.0e-20:
-        scopy.unrotate()
-
-    zmin = scopy.values.min()
-    zmax = scopy.values.max()
-
-    yinc = scopy._yinc * scopy._yflip
-
-    vals = scopy.get_values1d(order="C", asmasked=False, fill_value=xtgeo.UNDEF)
-
-    ier = _cxtgeo.surf_export_zmap_ascii(
-        mfile.get_cfhandle(),
-        scopy._ncol,
-        scopy._nrow,
-        scopy._xori,
-        scopy._yori,
-        scopy._xinc,
-        yinc,
-        vals,
-        zmin,
-        zmax,
-        0,
-    )
-    if ier != 0:
-        raise RuntimeError(f"Export to ZMAP Ascii went wrong, code is {ier}")
-    del scopy
-
-    mfile.cfclose()
-
-
-def export_storm_binary(self, mfile):
-    """Export to Storm binary format (non-rotated)."""
-
-    # storm can only deal with non-rotated formats; hence make a copy
-    # of the instance and derotate that prior to export, so that the
-    # original instance is unchanged
-
-    scopy = self.copy()
-
-    if abs(scopy.rotation) > 1.0e-20:
-        scopy.unrotate()
-
-    zmin = scopy.values.min()
-    zmax = scopy.values.max()
-
-    yinc = scopy._yinc * scopy._yflip
-
-    ier = _cxtgeo.surf_export_storm_bin(
-        mfile.get_cfhandle(),
-        scopy._ncol,
-        scopy._nrow,
-        scopy._xori,
-        scopy._yori,
-        scopy._xinc,
-        yinc,
-        scopy.get_values1d(order="F", asmasked=False, fill_value=self.undef),
-        zmin,
-        zmax,
-        0,
-    )
-    if ier != 0:
-        raise RuntimeError(f"Export to Storm binary went wrong, code is {ier}")
-    del scopy
-
-    mfile.cfclose()
-
-
-def export_petromod_binary(self, mfile, pmd_dataunits):
-    """Export to petromod binary format."""
-    validunits = False
-    unitd = 15
-    unitz = 10
-    if isinstance(pmd_dataunits, tuple) and len(pmd_dataunits) == 2:
-        unitd, unitz = pmd_dataunits
-        if isinstance(unitd, int) and isinstance(unitz, int):
-            if unitd in PMD_DATAUNITDISTANCE.keys() and unitz in PMD_DATAUNITZ.keys():
-                validunits = True
-
-            if unitd <= 0 or unitz <= 0:
-                raise ValueError("Values for pmd_dataunits cannot be negative!")
-
-    if not validunits:
-        UserWarning(
-            "Format or values for pmd_dataunits out of range: Pair should be in ranges "
-            f"{PMD_DATAUNITDISTANCE} and {PMD_DATAUNITZ}"
-        )
-
-    undef = 99999
-
-    dsc = "Content=Map,"
-    dsc += f"DataUnitDistance={unitd},"
-    dsc += f"DataUnitZ={unitz},"
-    dsc += f"GridNoX={self.ncol},"
-    dsc += f"GridNoY={self.nrow},"
-    dsc += f"GridStepX={self.xinc},"
-    dsc += f"GridStepY={self.yinc},"
-    dsc += "MapType=GridMap,"
-    dsc += f"OriginX={self.xori},"
-    dsc += f"OriginY={self.yori},"
-    dsc += f"RotationAngle={self.rotation},"
-    dsc += f"RotationOriginX={self.xori},"
-    dsc += f"RotationOriginY={self.yori},"
-    dsc += f"Undefined={undef},"
-    dsc += "Version=1.0"
-
-    values = np.ma.filled(self.values1d, fill_value=undef)
-
-    _cxtgeo.surf_export_petromod_bin(
-        mfile.get_cfhandle(),
-        dsc,
-        values,
-    )
-
-    mfile.cfclose()
-
-
-def export_xtgregsurf(self, mfile):
-    """Export to experimental xtgregsurf format, python version."""
-    logger.info("Export to xtgregsurf format...")
-
-    self.metadata.required = self
-
-    vals = self.get_values1d(fill_value=self._undef, order="C").astype(np.float32)
-
-    prevalues = (1, 1101, 4, self.ncol, self.nrow)
-    mystruct = struct.Struct("= i i i q q")
-    hdr = mystruct.pack(*prevalues)
-
-    meta = self.metadata.get_metadata()
-
-    jmeta = json.dumps(meta).encode()
-
-    with open(mfile.name, "wb") as fout:
-        fout.write(hdr)
-
-    with open(mfile.name, "ab") as fout:
-        vals.tofile(fout)
-
-    with open(mfile.name, "ab") as fout:
-        fout.write("\nXTGMETA.v01\n".encode())
-
-    with open(mfile.name, "ab") as fout:
-        fout.write(jmeta)
-
-    logger.info("Export to xtgregsurf format... done!")
-
-
-def export_hdf5_regsurf(self, mfile, compression="lzf", dtype="float32"):
-    """Export to experimental hdf5 format."""
-    logger.info("Export to hdf5 format...")
-
-    self.metadata.required = self
-
-    meta = self.metadata.get_metadata()
-    jmeta = json.dumps(meta).encode()
-
-    if compression and compression == "blosc":
-        compression = hdf5plugin.Blosc(
-            cname="blosclz", clevel=9, shuffle=hdf5plugin.Blosc.SHUFFLE
-        )
-
-    if dtype not in ("float32", "float64", np.float32, np.float64):
-        raise ValueError("Wrong dtype input, must be 'float32' or 'float64'")
-
-    with h5py.File(mfile.name, "w") as fh5:
-        grp = fh5.create_group("RegularSurface")
-        grp.create_dataset(
-            "values",
-            data=np.ma.filled(self.values, fill_value=self.undef).astype(dtype),
-            compression=compression,
-            chunks=True,
-        )
-        grp.attrs["metadata"] = jmeta
-        grp.attrs["provider"] = "xtgeo"
-        grp.attrs["format-idcode"] = 1101
-
-    logger.info("Export to hdf5 format... done!")
+# -*- coding: utf-8 -*-
+"""Export RegularSurface data."""
+
+import json
+
+# pylint: disable=protected-access
+# import hashlib
+import struct
+
+import h5py
+import hdf5plugin
+import numpy as np
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo  # pylint: disable=import-error
+from xtgeo.common import XTGeoDialog
+from xtgeo.common.constants import UNDEF_MAP_IRAPA, UNDEF_MAP_IRAPB
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+DEBUG = 0
+
+if DEBUG < 0:
+    DEBUG = 0
+
+PMD_DATAUNITDISTANCE = {
+    15: "meter",
+    16: "km",
+    17: "feet",
+    18: "yard",
+    19: "mile",
+    221: "global",
+}
+
+PMD_DATAUNITZ = {
+    10: "10",
+    31: "31",
+    44: "44",
+    300: "300",
+}
+
+
+def export_irap_ascii(self, mfile, engine="cxtgeo"):
+    """Export to Irap RMS ascii format."""
+    if mfile.memstream is True or engine == "python":
+        _export_irap_ascii_purepy(self, mfile)
+    else:
+        _export_irap_ascii(self, mfile)
+
+
+def _export_irap_ascii_purepy(self, mfile):
+    """Export to Irap RMS ascii using pure python, slower? but safer for memstreams."""
+    vals = self.get_values1d(fill_value=UNDEF_MAP_IRAPA, order="F")
+
+    xmax = self.xori + (self.ncol - 1) * self.xinc
+    ymax = self.yori + (self.nrow - 1) * self.yinc
+
+    buf = f"-996 {self.nrow} {self.xinc} {self.yinc}\n"
+    buf += f"{self.xori} {xmax} {self.yori} {ymax}\n"
+    buf += f"{self.ncol} {self.rotation} {self.xori} {self.yori}\n"
+    buf += "0  0  0  0  0  0  0\n"
+    vals = vals.astype("str").tolist()
+    nrow = 0
+    for val in vals:
+        buf += val
+        nrow += 1
+        if nrow == 6:
+            buf += "\n"
+            nrow = 0
+        else:
+            buf += " "
+
+    if nrow != 0:
+        buf += "\n"
+
+    # convert buffer to ascii
+    buf = buf.encode("latin1")
+
+    if mfile.memstream:
+        mfile.file.write(buf)
+    else:
+        with open(mfile.name, "wb") as fout:
+            fout.write(buf)
+
+    del vals
+
+
+def _export_irap_ascii(self, mfile):
+    """Export to Irap RMS ascii format using cxtgeo."""
+    vals = self.get_values1d(fill_value=xtgeo.UNDEF)
+
+    ier = _cxtgeo.surf_export_irap_ascii(
+        mfile.get_cfhandle(),
+        self._ncol,
+        self._nrow,
+        self._xori,
+        self._yori,
+        self._xinc,
+        self._yflip * self._yinc,
+        self._rotation,
+        vals,
+        0,
+    )
+    if ier != 0:
+        raise RuntimeError(f"Export to Irap Ascii went wrong, code is {ier}")
+
+    del vals
+
+    mfile.cfclose()
+
+
+def export_irap_binary(self, mfile, engine="cxtgeo"):
+    """Export to Irap RMS binary format.
+
+    Note that mfile can also a be a BytesIO instance
+    """
+    if mfile.memstream or engine == "python":
+        _export_irap_binary_python(self, mfile)
+    else:
+        _export_irap_binary_cxtgeo(self, mfile)
+
+
+def _export_irap_binary_python(self, mfile):
+    """Export to Irap RMS binary format but use python only.
+
+    This is approx 2-5 times slower than the C method, but may a be more robust in cases
+    with BytesIO.
+    """
+    vals = self.get_values1d(fill_value=UNDEF_MAP_IRAPB, order="F")
+
+    ap = struct.pack(
+        ">3i6f3i3f10i",  # > means big endian storage
+        32,
+        -996,
+        self.nrow,
+        self.xori,
+        self.xori + self.xinc * (self.ncol - 1),
+        self.yori,
+        self.yori + self.yinc * (self.nrow - 1),
+        self.xinc,
+        self.yinc,
+        32,
+        16,
+        self.ncol,
+        self.rotation,
+        self.xori,
+        self.yori,
+        16,
+        28,
+        0,
+        0,
+        0,
+        0,
+        0,
+        0,
+        0,
+        28,
+    )
+    inum = self.nrow * self.ncol
+
+    # export to Irap binary in ncol chunks (the only chunk size accepted by RMS)
+    nchunk = self.ncol
+    chunks = [nchunk] * (inum // nchunk)
+    if (inum % nchunk) > 0:
+        chunks.append(inum % nchunk)
+    start = 0
+    for chunk in chunks:
+        ap += struct.pack(">i", chunk * 4)
+        ap += struct.pack(f">{chunk:d}f", *vals[start : start + chunk])
+        ap += struct.pack(">i", chunk * 4)
+        start += chunk
+
+    if mfile.memstream:
+        mfile.file.write(ap)
+    else:
+        with open(mfile.name, "wb") as fout:
+            fout.write(ap)
+
+
+def _export_irap_binary_cxtgeo(self, mfile):
+    """Export to Irap binary using C backend.
+
+    Args:
+        mfile (_XTGeoFile): xtgeo file instance
+
+    Raises:
+        RuntimeError: Export to Irap Binary went wrong...
+    """
+    vals = self.get_values1d(fill_value=UNDEF_MAP_IRAPB, order="F")
+    ier = _cxtgeo.surf_export_irap_bin(
+        mfile.get_cfhandle(),
+        self._ncol,
+        self._nrow,
+        self._xori,
+        self._yori,
+        self._xinc,
+        self._yflip * self._yinc,
+        self._rotation,
+        vals,
+        0,
+    )
+
+    if ier != 0:
+        mfile.cfclose(strict=False)  # strict False as C routine may have closed
+        raise RuntimeError(f"Export to Irap Binary went wrong, code is {ier}")
+
+    mfile.cfclose()
+
+
+def export_ijxyz_ascii(self, mfile):
+    """Export to DSG IJXYZ ascii format."""
+    vals = self.get_values1d(fill_value=xtgeo.UNDEF)
+    ier = _cxtgeo.surf_export_ijxyz(
+        mfile.get_cfhandle(),
+        self._ncol,
+        self._nrow,
+        self._xori,
+        self._yori,
+        self._xinc,
+        self._yinc,
+        self._rotation,
+        self._yflip,
+        self._ilines,
+        self._xlines,
+        vals,
+        0,
+    )
+
+    if ier != 0:
+        raise RuntimeError(f"Export to IJXYZ format went wrong, code is {ier}")
+
+    mfile.cfclose()
+
+
+def export_zmap_ascii(self, mfile, engine="cxtgeo"):
+    """Export to ZMAP ascii format (non-rotated)."""
+
+    # zmap can only deal with non-rotated formats; hence make a copy
+    # of the instance and derotate that prior to export, so that the
+    # original instance is unchanged
+
+    if mfile.memstream or engine == "python":
+        _export_zmap_ascii_purepy(self, mfile)
+    else:
+        _export_zmap_ascii(self, mfile)
+
+
+def _export_zmap_ascii_purepy(self, mfile):
+    """Export to ZMAP ascii format (non-rotated), pure python for memstreams"""
+
+    scopy = self.copy()
+
+    undef = -99999.0
+    if abs(scopy.rotation) > 1.0e-20:
+        scopy.unrotate()
+
+    yinc = scopy._yinc * scopy._yflip
+
+    vals = scopy.get_values1d(order="C", asmasked=False, fill_value=undef)
+
+    xmax = scopy.xori + (scopy.ncol - 1) * scopy.xinc
+    ymax = scopy.yori + (scopy.nrow - 1) * yinc
+
+    fcode = 8
+    if scopy.values.min() > -10 and scopy.values.max() < 10:
+        fcode = 4
+
+    nfrow = scopy.nrow if scopy.nrow < 5 else 5
+
+    buf = "! Export from XTGeo (python engine)\n"
+    buf += f"@ GRIDFILE, GRID, {nfrow}\n"
+    buf += f"20, {undef}, , {fcode}, 1\n"
+    buf += f"{scopy.nrow}, {scopy.ncol}, {scopy.xori}, {xmax}, {scopy.yori}, {ymax}\n"
+
+    buf += "0.0, 0.0, 0.0\n"
+    buf += "@\n"
+
+    vals = vals.tolist()
+    ncol = 0
+    for icol in range(scopy.ncol):
+        for jrow in range(scopy.nrow - 1, -1, -1):
+            ic = icol * scopy.nrow + jrow
+            buf += f" {vals[ic]:19.{fcode}f}"
+            ncol += 1
+            if ncol == 5 or jrow == 0:
+                buf += "\n"
+                ncol = 0
+
+    # convert buffer to ascii
+    buf = buf.encode("latin1")
+
+    if mfile.memstream:
+        mfile.file.write(buf)
+    else:
+        with open(mfile.name, "wb") as fout:
+            fout.write(buf)
+
+    del vals
+
+
+def _export_zmap_ascii(self, mfile):
+    """Export to ZMAP ascii format (non-rotated)."""
+
+    scopy = self.copy()
+
+    if abs(scopy.rotation) > 1.0e-20:
+        scopy.unrotate()
+
+    zmin = scopy.values.min()
+    zmax = scopy.values.max()
+
+    yinc = scopy._yinc * scopy._yflip
+
+    vals = scopy.get_values1d(order="C", asmasked=False, fill_value=xtgeo.UNDEF)
+
+    ier = _cxtgeo.surf_export_zmap_ascii(
+        mfile.get_cfhandle(),
+        scopy._ncol,
+        scopy._nrow,
+        scopy._xori,
+        scopy._yori,
+        scopy._xinc,
+        yinc,
+        vals,
+        zmin,
+        zmax,
+        0,
+    )
+    if ier != 0:
+        raise RuntimeError(f"Export to ZMAP Ascii went wrong, code is {ier}")
+    del scopy
+
+    mfile.cfclose()
+
+
+def export_storm_binary(self, mfile):
+    """Export to Storm binary format (non-rotated)."""
+
+    # storm can only deal with non-rotated formats; hence make a copy
+    # of the instance and derotate that prior to export, so that the
+    # original instance is unchanged
+
+    scopy = self.copy()
+
+    if abs(scopy.rotation) > 1.0e-20:
+        scopy.unrotate()
+
+    zmin = scopy.values.min()
+    zmax = scopy.values.max()
+
+    yinc = scopy._yinc * scopy._yflip
+
+    ier = _cxtgeo.surf_export_storm_bin(
+        mfile.get_cfhandle(),
+        scopy._ncol,
+        scopy._nrow,
+        scopy._xori,
+        scopy._yori,
+        scopy._xinc,
+        yinc,
+        scopy.get_values1d(order="F", asmasked=False, fill_value=self.undef),
+        zmin,
+        zmax,
+        0,
+    )
+    if ier != 0:
+        raise RuntimeError(f"Export to Storm binary went wrong, code is {ier}")
+    del scopy
+
+    mfile.cfclose()
+
+
+def export_petromod_binary(self, mfile, pmd_dataunits):
+    """Export to petromod binary format."""
+    validunits = False
+    unitd = 15
+    unitz = 10
+    if isinstance(pmd_dataunits, tuple) and len(pmd_dataunits) == 2:
+        unitd, unitz = pmd_dataunits
+        if isinstance(unitd, int) and isinstance(unitz, int):
+            if unitd in PMD_DATAUNITDISTANCE.keys() and unitz in PMD_DATAUNITZ.keys():
+                validunits = True
+
+            if unitd <= 0 or unitz <= 0:
+                raise ValueError("Values for pmd_dataunits cannot be negative!")
+
+    if not validunits:
+        UserWarning(
+            "Format or values for pmd_dataunits out of range: Pair should be in ranges "
+            f"{PMD_DATAUNITDISTANCE} and {PMD_DATAUNITZ}"
+        )
+
+    undef = 99999
+
+    dsc = "Content=Map,"
+    dsc += f"DataUnitDistance={unitd},"
+    dsc += f"DataUnitZ={unitz},"
+    dsc += f"GridNoX={self.ncol},"
+    dsc += f"GridNoY={self.nrow},"
+    dsc += f"GridStepX={self.xinc},"
+    dsc += f"GridStepY={self.yinc},"
+    dsc += "MapType=GridMap,"
+    dsc += f"OriginX={self.xori},"
+    dsc += f"OriginY={self.yori},"
+    dsc += f"RotationAngle={self.rotation},"
+    dsc += f"RotationOriginX={self.xori},"
+    dsc += f"RotationOriginY={self.yori},"
+    dsc += f"Undefined={undef},"
+    dsc += "Version=1.0"
+
+    values = np.ma.filled(self.values1d, fill_value=undef)
+
+    _cxtgeo.surf_export_petromod_bin(
+        mfile.get_cfhandle(),
+        dsc,
+        values,
+    )
+
+    mfile.cfclose()
+
+
+def export_xtgregsurf(self, mfile):
+    """Export to experimental xtgregsurf format, python version."""
+    logger.info("Export to xtgregsurf format...")
+
+    self.metadata.required = self
+
+    vals = self.get_values1d(fill_value=self._undef, order="C").astype(np.float32)
+
+    prevalues = (1, 1101, 4, self.ncol, self.nrow)
+    mystruct = struct.Struct("= i i i q q")
+    hdr = mystruct.pack(*prevalues)
+
+    meta = self.metadata.get_metadata()
+
+    jmeta = json.dumps(meta).encode()
+
+    with open(mfile.name, "wb") as fout:
+        fout.write(hdr)
+
+    with open(mfile.name, "ab") as fout:
+        vals.tofile(fout)
+
+    with open(mfile.name, "ab") as fout:
+        fout.write("\nXTGMETA.v01\n".encode())
+
+    with open(mfile.name, "ab") as fout:
+        fout.write(jmeta)
+
+    logger.info("Export to xtgregsurf format... done!")
+
+
+def export_hdf5_regsurf(self, mfile, compression="lzf", dtype="float32"):
+    """Export to experimental hdf5 format."""
+    logger.info("Export to hdf5 format...")
+
+    self.metadata.required = self
+
+    meta = self.metadata.get_metadata()
+    jmeta = json.dumps(meta).encode()
+
+    if compression and compression == "blosc":
+        compression = hdf5plugin.Blosc(
+            cname="blosclz", clevel=9, shuffle=hdf5plugin.Blosc.SHUFFLE
+        )
+
+    if dtype not in ("float32", "float64", np.float32, np.float64):
+        raise ValueError("Wrong dtype input, must be 'float32' or 'float64'")
+
+    with h5py.File(mfile.name, "w") as fh5:
+        grp = fh5.create_group("RegularSurface")
+        grp.create_dataset(
+            "values",
+            data=np.ma.filled(self.values, fill_value=self.undef).astype(dtype),
+            compression=compression,
+            chunks=True,
+        )
+        grp.attrs["metadata"] = jmeta
+        grp.attrs["provider"] = "xtgeo"
+        grp.attrs["format-idcode"] = 1101
+
+    logger.info("Export to hdf5 format... done!")
```

## xtgeo/surface/_regsurf_grid3d.py

 * *Ordering differences only*

```diff
@@ -1,187 +1,187 @@
-# -*- coding: utf-8 -*-
-"""Regular surface vs Grid3D"""
-
-
-import numpy as np
-import numpy.ma as ma
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.grid3d import _gridprop_lowlevel
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-#
-
-
-# self = RegularSurface instance!
-# pylint: disable=protected-access
-
-
-def slice_grid3d(self, grid, prop, zsurf=None, sbuffer=1):
-    """Private function for the Grid3D slicing."""
-
-    grid._xtgformat1()
-    if zsurf is not None:
-        other = zsurf
-    else:
-        logger.info('The current surface is copied as "other"')
-        other = self.copy()
-    if not self.compare_topology(other, strict=False):
-        raise RuntimeError("Topology of maps differ. Stop!")
-
-    zslice = other.copy()
-
-    nsurf = self.ncol * self.nrow
-
-    p_prop = _gridprop_lowlevel.update_carray(prop, discrete=False)
-
-    istat, updatedval = _cxtgeo.surf_slice_grd3d(
-        self.ncol,
-        self.nrow,
-        self.xori,
-        self.xinc,
-        self.yori,
-        self.yinc,
-        self.rotation,
-        self.yflip,
-        zslice.get_values1d(),
-        nsurf,
-        grid.ncol,
-        grid.nrow,
-        grid.nlay,
-        grid._coordsv,
-        grid._zcornsv,
-        grid._actnumsv,
-        p_prop,
-        sbuffer,
-    )
-
-    if istat != 0:
-        logger.warning("Problem, ISTAT = %s", istat)
-
-    self.set_values1d(updatedval)
-
-    return istat
-
-
-def from_grid3d(grid, template=None, where="top", mode="depth", rfactor=1):
-    """Private function for deriving a surface from a 3D grid.
-
-    Note that rotated maps are currently not supported!
-
-    .. versionadded:: 2.1
-    """
-    if where == "top":
-        klayer = 1
-        option = 0
-    elif where == "base":
-        klayer = grid.nlay
-        option = 1
-    else:
-        klayer, what = where.split("_")
-        klayer = int(klayer)
-        if grid.nlay < klayer < 0:
-            raise ValueError(f"Klayer out of range in where={where}")
-        option = 0
-        if what == "base":
-            option = 1
-
-    if rfactor < 0.5:
-        raise KeyError("Refinefactor rfactor is too small, should be >= 0.5")
-
-    args = _update_regsurf(template, grid, rfactor=float(rfactor))
-    args["rotation"] = 0.0
-    # call C function to make a map
-    val = args["values"]
-    val = val.ravel()
-    val = ma.filled(val, fill_value=xtgeo.UNDEF)
-
-    svalues = val * 0.0 + xtgeo.UNDEF
-    ivalues = svalues.copy()
-    jvalues = svalues.copy()
-
-    grid._xtgformat1()
-    _cxtgeo.surf_sample_grd3d_lay(
-        grid.ncol,
-        grid.nrow,
-        grid.nlay,
-        grid._coordsv,
-        grid._zcornsv,
-        grid._actnumsv,
-        klayer,
-        args["ncol"],
-        args["nrow"],
-        args["xori"],
-        args["xinc"],
-        args["yori"],
-        args["yinc"],
-        args["rotation"],
-        svalues,
-        ivalues,
-        jvalues,
-        option,
-    )
-
-    logger.info("Extracted surfaces from 3D grid...")
-    svalues = np.ma.masked_greater(svalues, xtgeo.UNDEF_LIMIT)
-    ivalues = np.ma.masked_greater(ivalues, xtgeo.UNDEF_LIMIT)
-    jvalues = np.ma.masked_greater(jvalues, xtgeo.UNDEF_LIMIT)
-
-    if mode == "i":
-        ivalues = ivalues.reshape((args["ncol"], args["nrow"]))
-        ivalues = ma.masked_invalid(ivalues)
-        args["values"] = ivalues
-        return args, None, None
-
-    if mode == "j":
-        jvalues = jvalues.reshape((args["ncol"], args["nrow"]))
-        jvalues = ma.masked_invalid(jvalues)
-        args["values"] = jvalues
-        return args, None, None
-
-    svalues = svalues.reshape((args["ncol"], args["nrow"]))
-    svalues = ma.masked_invalid(svalues)
-    args["values"] = svalues
-
-    return args, ivalues, jvalues
-
-
-def _update_regsurf(template, grid, rfactor=1.0):
-    args = {}
-    if template is None:
-        # need to estimate map settings from the existing grid. this
-        # may a bit time consuming for large grids.
-        geom = grid.get_geometrics(
-            allcells=True, cellcenter=True, return_dict=True, _ver=2
-        )
-
-        xlen = 1.1 * (geom["xmax"] - geom["xmin"])
-        ylen = 1.1 * (geom["ymax"] - geom["ymin"])
-        xori = geom["xmin"] - 0.05 * xlen
-        yori = geom["ymin"] - 0.05 * ylen
-        # take same xinc and yinc
-
-        xinc = yinc = (1.0 / rfactor) * 0.5 * (geom["avg_dx"] + geom["avg_dy"])
-        ncol = int(xlen / xinc)
-        nrow = int(ylen / yinc)
-
-        args["xori"] = xori
-        args["yori"] = yori
-        args["xinc"] = xinc
-        args["yinc"] = yinc
-        args["ncol"] = ncol
-        args["nrow"] = nrow
-        args["values"] = np.ma.zeros((ncol, nrow), dtype=np.float64)
-    else:
-        args["xori"] = template.xori
-        args["yori"] = template.yori
-        args["xinc"] = template.xinc
-        args["yinc"] = template.yinc
-        args["ncol"] = template.ncol
-        args["nrow"] = template.nrow
-        args["values"] = template.values.copy()
-    return args
+# -*- coding: utf-8 -*-
+"""Regular surface vs Grid3D"""
+
+
+import numpy as np
+import numpy.ma as ma
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.grid3d import _gridprop_lowlevel
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+#
+
+
+# self = RegularSurface instance!
+# pylint: disable=protected-access
+
+
+def slice_grid3d(self, grid, prop, zsurf=None, sbuffer=1):
+    """Private function for the Grid3D slicing."""
+
+    grid._xtgformat1()
+    if zsurf is not None:
+        other = zsurf
+    else:
+        logger.info('The current surface is copied as "other"')
+        other = self.copy()
+    if not self.compare_topology(other, strict=False):
+        raise RuntimeError("Topology of maps differ. Stop!")
+
+    zslice = other.copy()
+
+    nsurf = self.ncol * self.nrow
+
+    p_prop = _gridprop_lowlevel.update_carray(prop, discrete=False)
+
+    istat, updatedval = _cxtgeo.surf_slice_grd3d(
+        self.ncol,
+        self.nrow,
+        self.xori,
+        self.xinc,
+        self.yori,
+        self.yinc,
+        self.rotation,
+        self.yflip,
+        zslice.get_values1d(),
+        nsurf,
+        grid.ncol,
+        grid.nrow,
+        grid.nlay,
+        grid._coordsv,
+        grid._zcornsv,
+        grid._actnumsv,
+        p_prop,
+        sbuffer,
+    )
+
+    if istat != 0:
+        logger.warning("Problem, ISTAT = %s", istat)
+
+    self.set_values1d(updatedval)
+
+    return istat
+
+
+def from_grid3d(grid, template=None, where="top", mode="depth", rfactor=1):
+    """Private function for deriving a surface from a 3D grid.
+
+    Note that rotated maps are currently not supported!
+
+    .. versionadded:: 2.1
+    """
+    if where == "top":
+        klayer = 1
+        option = 0
+    elif where == "base":
+        klayer = grid.nlay
+        option = 1
+    else:
+        klayer, what = where.split("_")
+        klayer = int(klayer)
+        if grid.nlay < klayer < 0:
+            raise ValueError(f"Klayer out of range in where={where}")
+        option = 0
+        if what == "base":
+            option = 1
+
+    if rfactor < 0.5:
+        raise KeyError("Refinefactor rfactor is too small, should be >= 0.5")
+
+    args = _update_regsurf(template, grid, rfactor=float(rfactor))
+    args["rotation"] = 0.0
+    # call C function to make a map
+    val = args["values"]
+    val = val.ravel()
+    val = ma.filled(val, fill_value=xtgeo.UNDEF)
+
+    svalues = val * 0.0 + xtgeo.UNDEF
+    ivalues = svalues.copy()
+    jvalues = svalues.copy()
+
+    grid._xtgformat1()
+    _cxtgeo.surf_sample_grd3d_lay(
+        grid.ncol,
+        grid.nrow,
+        grid.nlay,
+        grid._coordsv,
+        grid._zcornsv,
+        grid._actnumsv,
+        klayer,
+        args["ncol"],
+        args["nrow"],
+        args["xori"],
+        args["xinc"],
+        args["yori"],
+        args["yinc"],
+        args["rotation"],
+        svalues,
+        ivalues,
+        jvalues,
+        option,
+    )
+
+    logger.info("Extracted surfaces from 3D grid...")
+    svalues = np.ma.masked_greater(svalues, xtgeo.UNDEF_LIMIT)
+    ivalues = np.ma.masked_greater(ivalues, xtgeo.UNDEF_LIMIT)
+    jvalues = np.ma.masked_greater(jvalues, xtgeo.UNDEF_LIMIT)
+
+    if mode == "i":
+        ivalues = ivalues.reshape((args["ncol"], args["nrow"]))
+        ivalues = ma.masked_invalid(ivalues)
+        args["values"] = ivalues
+        return args, None, None
+
+    if mode == "j":
+        jvalues = jvalues.reshape((args["ncol"], args["nrow"]))
+        jvalues = ma.masked_invalid(jvalues)
+        args["values"] = jvalues
+        return args, None, None
+
+    svalues = svalues.reshape((args["ncol"], args["nrow"]))
+    svalues = ma.masked_invalid(svalues)
+    args["values"] = svalues
+
+    return args, ivalues, jvalues
+
+
+def _update_regsurf(template, grid, rfactor=1.0):
+    args = {}
+    if template is None:
+        # need to estimate map settings from the existing grid. this
+        # may a bit time consuming for large grids.
+        geom = grid.get_geometrics(
+            allcells=True, cellcenter=True, return_dict=True, _ver=2
+        )
+
+        xlen = 1.1 * (geom["xmax"] - geom["xmin"])
+        ylen = 1.1 * (geom["ymax"] - geom["ymin"])
+        xori = geom["xmin"] - 0.05 * xlen
+        yori = geom["ymin"] - 0.05 * ylen
+        # take same xinc and yinc
+
+        xinc = yinc = (1.0 / rfactor) * 0.5 * (geom["avg_dx"] + geom["avg_dy"])
+        ncol = int(xlen / xinc)
+        nrow = int(ylen / yinc)
+
+        args["xori"] = xori
+        args["yori"] = yori
+        args["xinc"] = xinc
+        args["yinc"] = yinc
+        args["ncol"] = ncol
+        args["nrow"] = nrow
+        args["values"] = np.ma.zeros((ncol, nrow), dtype=np.float64)
+    else:
+        args["xori"] = template.xori
+        args["yori"] = template.yori
+        args["xinc"] = template.xinc
+        args["yinc"] = template.yinc
+        args["ncol"] = template.ncol
+        args["nrow"] = template.nrow
+        args["values"] = template.values.copy()
+    return args
```

## xtgeo/surface/_regsurf_gridding.py

 * *Ordering differences only*

```diff
@@ -1,343 +1,343 @@
-# -*- coding: utf-8 -*-
-"""Do gridding from 3D parameters"""
-
-
-import warnings
-
-import numpy as np
-import numpy.ma as ma
-import scipy.interpolate
-import scipy.ndimage
-
-import xtgeo
-
-xtg = xtgeo.common.XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-# Note: 'self' is an instance of RegularSurface
-# pylint: disable=too-many-branches, too-many-statements, too-many-locals
-
-
-def points_gridding(self, points, method="linear", coarsen=1):
-    """Do gridding from a points data set."""
-
-    xiv, yiv = self.get_xy_values()
-
-    dfra = points.dataframe
-
-    xcv = dfra[points.xname].values
-    ycv = dfra[points.yname].values
-    zcv = dfra[points.zname].values
-
-    if coarsen > 1:
-        xcv = xcv[::coarsen]
-        ycv = ycv[::coarsen]
-        zcv = zcv[::coarsen]
-
-    validmethods = ["linear", "nearest", "cubic"]
-    if method not in set(validmethods):
-        raise ValueError(
-            f"Invalid method for gridding: {method}, valid options are {validmethods}"
-        )
-
-    try:
-        znew = scipy.interpolate.griddata(
-            (xcv, ycv), zcv, (xiv, yiv), method=method, fill_value=np.nan
-        )
-    except ValueError as verr:
-        raise RuntimeError(f"Could not do gridding: {verr}")
-
-    logger.info("Gridding point ... DONE")
-
-    self._ensure_correct_values(znew)
-
-
-def avgsum_from_3dprops_gridding(
-    self,
-    summing=False,
-    xprop=None,
-    yprop=None,
-    mprop=None,
-    dzprop=None,
-    truncate_le=None,
-    zoneprop=None,
-    zone_minmax=None,
-    coarsen=1,
-    zone_avg=False,
-    mask_outside=False,
-):
-    """Get surface average from a 3D grid prop."""
-    # NOTE:
-    # This do _either_ averaging _or_ sum gridding (if summing is True)
-    # - Inputs shall be pure 3D numpies, not masked!
-    # - Xprop and yprop must be made for all cells
-    # - Also dzprop for all cells, and dzprop = 0 for inactive cells!
-
-    logger.info("Avgsum calculation %s", __name__)
-
-    if zone_minmax is None:
-        raise ValueError("zone_minmax is required")
-
-    if dzprop is None:
-        raise ValueError("DZ property is required")
-
-    xprop, yprop, zoneprop, mprop, dzprop = _zone_averaging(
-        xprop,
-        yprop,
-        zoneprop,
-        zone_minmax,
-        coarsen,
-        zone_avg,
-        dzprop,
-        mprop,
-        summing=summing,
-    )
-
-    gnlay = xprop.shape[2]
-
-    # avoid artifacts from inactive cells that slips through somehow...(?)
-    if dzprop.max() > xtgeo.UNDEF_LIMIT:
-        raise RuntimeError("Bug: DZ with unphysical values present")
-
-    trimbydz = False
-    if not summing:
-        trimbydz = True
-
-    if summing and mask_outside:
-        trimbydz = True
-
-    xiv, yiv = self.get_xy_values()
-
-    # weight are needed if zoneprop is not follow layers, but rather regions
-    weights = dzprop.copy() * 0.0 + 1.0
-    weights[zoneprop < zone_minmax[0]] = 0.0
-    weights[zoneprop > zone_minmax[1]] = 0.0
-
-    # this operation is needed if zoneprop is aka a region ("irregular zone")
-    zoneprop = ma.masked_less(zoneprop, zone_minmax[0])
-    zoneprop = ma.masked_greater(zoneprop, zone_minmax[1])
-
-    for klay0 in range(gnlay):
-        k1lay = klay0 + 1
-
-        if k1lay == 1:
-            msum = np.zeros((self.ncol, self.nrow), order="C")
-            dzsum = np.zeros((self.ncol, self.nrow), order="C")
-
-        numz = zoneprop[::, ::, klay0].mean()
-        if isinstance(numz, float):
-            numz = int(round(zoneprop[::, ::, klay0].mean()))
-            if numz < zone_minmax[0] or numz > zone_minmax[1]:
-                continue
-        else:
-            continue
-
-        qmcompute = True
-        if summing:
-            propsum = mprop[:, :, klay0].sum()
-            if abs(propsum) < 1e-12:
-                logger.info("Too little HC, skip layer K = %s", k1lay)
-                qmcompute = False
-            else:
-                logger.debug("Z property sum is %s", propsum)
-
-        logger.info("Mapping for layer or zone %s ....", k1lay)
-
-        xcv = xprop[::, ::, klay0].ravel(order="C")
-        ycv = yprop[::, ::, klay0].ravel(order="C")
-        mvv = mprop[::, ::, klay0].ravel(order="C")
-        dzv = dzprop[::, ::, klay0].ravel(order="C")
-        wei = weights[::, ::, klay0].ravel(order="C")
-
-        # this is done to avoid problems if undef values still remains
-        # in the coordinates (assume Y undef where X undef):
-        xcc = xcv.copy()
-        xcv = xcv[xcc < 1e20]
-        ycv = ycv[xcc < 1e20]
-        mvv = mvv[xcc < 1e20]
-        dzv = dzv[xcc < 1e20]
-        wei = wei[xcc < 1e20]
-
-        if summing:
-            mvdz = mvv * wei
-        else:
-            mvdz = mvv * dzv * wei
-
-        if qmcompute:
-            try:
-                mvdzi = scipy.interpolate.griddata(
-                    (xcv, ycv), mvdz, (xiv, yiv), method="linear", fill_value=0.0
-                )
-            except ValueError:
-                warnings.warn("Some problems in gridding ... will contue", UserWarning)
-                continue
-
-            msum = msum + mvdzi
-
-        if trimbydz:
-            try:
-                dzi = scipy.interpolate.griddata(
-                    (xcv, ycv), dzv, (xiv, yiv), method="linear", fill_value=0.0
-                )
-            except ValueError:
-                continue
-
-            dzsum = dzsum + dzi
-
-    if not summing:
-        dzsum[dzsum == 0.0] = 1e-20
-        vvz = msum / dzsum
-        vvz = ma.masked_invalid(vvz)
-    else:
-        vvz = msum
-
-    if trimbydz:
-        vvz = ma.masked_where(dzsum < 1.1e-20, vvz)
-    else:
-        vvz = ma.array(vvz)  # so the result becomes a ma array
-
-    if truncate_le:
-        vvz = ma.masked_less(vvz, truncate_le)
-
-    self.values = vvz
-    logger.info("Avgsum calculation done! %s", __name__)
-
-    return True
-
-
-def _zone_averaging(
-    xprop, yprop, zoneprop, zone_minmax, coarsen, zone_avg, dzprop, mprop, summing=False
-):
-    # General preprocessing, and...
-    # Change the 3D numpy array so they get layers by
-    # averaging across zones. This may speed up a lot,
-    # but will reduce the resolution.
-    # The x y coordinates shall be averaged (ideally
-    # with thickness weighting...) while e.g. hcpfzprop
-    # must be summed.
-    # Somewhat different processing whether this is a hc thickness
-    # or an average.
-
-    xpr = xprop
-    ypr = yprop
-    zpr = zoneprop
-    dpr = dzprop
-
-    mpr = mprop
-
-    if coarsen > 1:
-        xpr = xprop[::coarsen, ::coarsen, ::].copy(order="C")
-        ypr = yprop[::coarsen, ::coarsen, ::].copy(order="C")
-        zpr = zoneprop[::coarsen, ::coarsen, ::].copy(order="C")
-        dpr = dzprop[::coarsen, ::coarsen, ::].copy(order="C")
-        mpr = mprop[::coarsen, ::coarsen, ::].copy(order="C")
-        zpr.astype(np.int32)
-
-    if zone_avg:
-        zmin = int(zone_minmax[0])
-        zmax = int(zone_minmax[1])
-        if zpr.min() > zmin:
-            zmin = zpr.min()
-        if zpr.max() < zmax:
-            zmax = zpr.max()
-
-        newx = []
-        newy = []
-        newz = []
-        newm = []
-        newd = []
-
-        for izv in range(zmin, zmax + 1):
-            logger.info("Averaging for zone %s ...", izv)
-            xpr2 = ma.masked_where(zpr != izv, xpr)
-            ypr2 = ma.masked_where(zpr != izv, ypr)
-            zpr2 = ma.masked_where(zpr != izv, zpr)
-            dpr2 = ma.masked_where(zpr != izv, dpr)
-            mpr2 = ma.masked_where(zpr != izv, mpr)
-
-            # get the thickness and normalize along axis 2 (vertical)
-            # to get normalized thickness weights
-            lay_sums = dpr2.sum(axis=2)
-            normed_dz = dpr2 / lay_sums[:, :, np.newaxis]
-
-            # assume that coordinates have equal weights within a zone
-            xpr2 = ma.average(xpr2, axis=2)
-            ypr2 = ma.average(ypr2, axis=2)
-            zpr2 = ma.average(zpr2, axis=2)  # avg zone
-
-            dpr2 = ma.sum(dpr2, axis=2)
-
-            if summing:
-                mpr2 = ma.sum(mpr2, axis=2)
-            else:
-                mpr2 = ma.average(mpr2, weights=normed_dz, axis=2)  # avg zone
-
-            newx.append(xpr2)
-            newy.append(ypr2)
-            newz.append(zpr2)
-            newd.append(dpr2)
-            newm.append(mpr2)
-
-        xpr = ma.dstack(newx)
-        ypr = ma.dstack(newy)
-        zpr = ma.dstack(newz)
-        dpr = ma.dstack(newd)
-        mpr = ma.dstack(newm)
-        zpr.astype(np.int32)
-
-    xpr = ma.filled(xpr, fill_value=xtgeo.UNDEF)
-    ypr = ma.filled(ypr, fill_value=xtgeo.UNDEF)
-    zpr = ma.filled(zpr, fill_value=0)
-    dpr = ma.filled(dpr, fill_value=0.0)
-
-    mpr = ma.filled(mpr, fill_value=0.0)
-
-    return xpr, ypr, zpr, mpr, dpr
-
-
-def surf_fill(self, fill_value=None):
-    """Replace the value of invalid 'data' cells (indicated by 'invalid')
-    by the value of the nearest valid data cell or a constant.
-
-    This is a quite fast method to fill undefined areas of the map.
-    The surface values are updated 'in-place'
-
-    .. versionadded:: 2.1
-    .. versionchanged:: 2.6 Added fill_value
-    """
-    logger.info("Do fill...")
-
-    if fill_value is not None:
-        if np.isscalar(fill_value) and not isinstance(fill_value, str):
-            self.values = ma.filled(self.values, fill_value=float(fill_value))
-        else:
-            raise ValueError("Keyword fill_value must be int or float")
-    else:
-        invalid = ma.getmaskarray(self.values)
-
-        ind = scipy.ndimage.distance_transform_edt(
-            invalid, return_distances=False, return_indices=True
-        )
-        self._values = self._values[tuple(ind)]
-        logger.info("Do fill... DONE")
-
-
-def smooth_median(self, iterations=1, width=1):
-    """Smooth a surface using a median filter.
-
-    .. versionadded:: 2.1
-    """
-
-    mask = ma.getmaskarray(self.values)
-    tmpv = ma.filled(self.values, fill_value=np.nan)
-
-    for _itr in range(iterations):
-        tmpv = scipy.ndimage.median_filter(tmpv, width)
-
-    tmpv = ma.masked_invalid(tmpv)
-
-    # seems that false areas of invalids (masked) may be made; combat that:
-    self.values = tmpv
-    self.fill()
-    self.values = ma.array(self.values, mask=mask)
+# -*- coding: utf-8 -*-
+"""Do gridding from 3D parameters"""
+
+
+import warnings
+
+import numpy as np
+import numpy.ma as ma
+import scipy.interpolate
+import scipy.ndimage
+
+import xtgeo
+
+xtg = xtgeo.common.XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+# Note: 'self' is an instance of RegularSurface
+# pylint: disable=too-many-branches, too-many-statements, too-many-locals
+
+
+def points_gridding(self, points, method="linear", coarsen=1):
+    """Do gridding from a points data set."""
+
+    xiv, yiv = self.get_xy_values()
+
+    dfra = points.dataframe
+
+    xcv = dfra[points.xname].values
+    ycv = dfra[points.yname].values
+    zcv = dfra[points.zname].values
+
+    if coarsen > 1:
+        xcv = xcv[::coarsen]
+        ycv = ycv[::coarsen]
+        zcv = zcv[::coarsen]
+
+    validmethods = ["linear", "nearest", "cubic"]
+    if method not in set(validmethods):
+        raise ValueError(
+            f"Invalid method for gridding: {method}, valid options are {validmethods}"
+        )
+
+    try:
+        znew = scipy.interpolate.griddata(
+            (xcv, ycv), zcv, (xiv, yiv), method=method, fill_value=np.nan
+        )
+    except ValueError as verr:
+        raise RuntimeError(f"Could not do gridding: {verr}")
+
+    logger.info("Gridding point ... DONE")
+
+    self._ensure_correct_values(znew)
+
+
+def avgsum_from_3dprops_gridding(
+    self,
+    summing=False,
+    xprop=None,
+    yprop=None,
+    mprop=None,
+    dzprop=None,
+    truncate_le=None,
+    zoneprop=None,
+    zone_minmax=None,
+    coarsen=1,
+    zone_avg=False,
+    mask_outside=False,
+):
+    """Get surface average from a 3D grid prop."""
+    # NOTE:
+    # This do _either_ averaging _or_ sum gridding (if summing is True)
+    # - Inputs shall be pure 3D numpies, not masked!
+    # - Xprop and yprop must be made for all cells
+    # - Also dzprop for all cells, and dzprop = 0 for inactive cells!
+
+    logger.info("Avgsum calculation %s", __name__)
+
+    if zone_minmax is None:
+        raise ValueError("zone_minmax is required")
+
+    if dzprop is None:
+        raise ValueError("DZ property is required")
+
+    xprop, yprop, zoneprop, mprop, dzprop = _zone_averaging(
+        xprop,
+        yprop,
+        zoneprop,
+        zone_minmax,
+        coarsen,
+        zone_avg,
+        dzprop,
+        mprop,
+        summing=summing,
+    )
+
+    gnlay = xprop.shape[2]
+
+    # avoid artifacts from inactive cells that slips through somehow...(?)
+    if dzprop.max() > xtgeo.UNDEF_LIMIT:
+        raise RuntimeError("Bug: DZ with unphysical values present")
+
+    trimbydz = False
+    if not summing:
+        trimbydz = True
+
+    if summing and mask_outside:
+        trimbydz = True
+
+    xiv, yiv = self.get_xy_values()
+
+    # weight are needed if zoneprop is not follow layers, but rather regions
+    weights = dzprop.copy() * 0.0 + 1.0
+    weights[zoneprop < zone_minmax[0]] = 0.0
+    weights[zoneprop > zone_minmax[1]] = 0.0
+
+    # this operation is needed if zoneprop is aka a region ("irregular zone")
+    zoneprop = ma.masked_less(zoneprop, zone_minmax[0])
+    zoneprop = ma.masked_greater(zoneprop, zone_minmax[1])
+
+    for klay0 in range(gnlay):
+        k1lay = klay0 + 1
+
+        if k1lay == 1:
+            msum = np.zeros((self.ncol, self.nrow), order="C")
+            dzsum = np.zeros((self.ncol, self.nrow), order="C")
+
+        numz = zoneprop[::, ::, klay0].mean()
+        if isinstance(numz, float):
+            numz = int(round(zoneprop[::, ::, klay0].mean()))
+            if numz < zone_minmax[0] or numz > zone_minmax[1]:
+                continue
+        else:
+            continue
+
+        qmcompute = True
+        if summing:
+            propsum = mprop[:, :, klay0].sum()
+            if abs(propsum) < 1e-12:
+                logger.info("Too little HC, skip layer K = %s", k1lay)
+                qmcompute = False
+            else:
+                logger.debug("Z property sum is %s", propsum)
+
+        logger.info("Mapping for layer or zone %s ....", k1lay)
+
+        xcv = xprop[::, ::, klay0].ravel(order="C")
+        ycv = yprop[::, ::, klay0].ravel(order="C")
+        mvv = mprop[::, ::, klay0].ravel(order="C")
+        dzv = dzprop[::, ::, klay0].ravel(order="C")
+        wei = weights[::, ::, klay0].ravel(order="C")
+
+        # this is done to avoid problems if undef values still remains
+        # in the coordinates (assume Y undef where X undef):
+        xcc = xcv.copy()
+        xcv = xcv[xcc < 1e20]
+        ycv = ycv[xcc < 1e20]
+        mvv = mvv[xcc < 1e20]
+        dzv = dzv[xcc < 1e20]
+        wei = wei[xcc < 1e20]
+
+        if summing:
+            mvdz = mvv * wei
+        else:
+            mvdz = mvv * dzv * wei
+
+        if qmcompute:
+            try:
+                mvdzi = scipy.interpolate.griddata(
+                    (xcv, ycv), mvdz, (xiv, yiv), method="linear", fill_value=0.0
+                )
+            except ValueError:
+                warnings.warn("Some problems in gridding ... will contue", UserWarning)
+                continue
+
+            msum = msum + mvdzi
+
+        if trimbydz:
+            try:
+                dzi = scipy.interpolate.griddata(
+                    (xcv, ycv), dzv, (xiv, yiv), method="linear", fill_value=0.0
+                )
+            except ValueError:
+                continue
+
+            dzsum = dzsum + dzi
+
+    if not summing:
+        dzsum[dzsum == 0.0] = 1e-20
+        vvz = msum / dzsum
+        vvz = ma.masked_invalid(vvz)
+    else:
+        vvz = msum
+
+    if trimbydz:
+        vvz = ma.masked_where(dzsum < 1.1e-20, vvz)
+    else:
+        vvz = ma.array(vvz)  # so the result becomes a ma array
+
+    if truncate_le:
+        vvz = ma.masked_less(vvz, truncate_le)
+
+    self.values = vvz
+    logger.info("Avgsum calculation done! %s", __name__)
+
+    return True
+
+
+def _zone_averaging(
+    xprop, yprop, zoneprop, zone_minmax, coarsen, zone_avg, dzprop, mprop, summing=False
+):
+    # General preprocessing, and...
+    # Change the 3D numpy array so they get layers by
+    # averaging across zones. This may speed up a lot,
+    # but will reduce the resolution.
+    # The x y coordinates shall be averaged (ideally
+    # with thickness weighting...) while e.g. hcpfzprop
+    # must be summed.
+    # Somewhat different processing whether this is a hc thickness
+    # or an average.
+
+    xpr = xprop
+    ypr = yprop
+    zpr = zoneprop
+    dpr = dzprop
+
+    mpr = mprop
+
+    if coarsen > 1:
+        xpr = xprop[::coarsen, ::coarsen, ::].copy(order="C")
+        ypr = yprop[::coarsen, ::coarsen, ::].copy(order="C")
+        zpr = zoneprop[::coarsen, ::coarsen, ::].copy(order="C")
+        dpr = dzprop[::coarsen, ::coarsen, ::].copy(order="C")
+        mpr = mprop[::coarsen, ::coarsen, ::].copy(order="C")
+        zpr.astype(np.int32)
+
+    if zone_avg:
+        zmin = int(zone_minmax[0])
+        zmax = int(zone_minmax[1])
+        if zpr.min() > zmin:
+            zmin = zpr.min()
+        if zpr.max() < zmax:
+            zmax = zpr.max()
+
+        newx = []
+        newy = []
+        newz = []
+        newm = []
+        newd = []
+
+        for izv in range(zmin, zmax + 1):
+            logger.info("Averaging for zone %s ...", izv)
+            xpr2 = ma.masked_where(zpr != izv, xpr)
+            ypr2 = ma.masked_where(zpr != izv, ypr)
+            zpr2 = ma.masked_where(zpr != izv, zpr)
+            dpr2 = ma.masked_where(zpr != izv, dpr)
+            mpr2 = ma.masked_where(zpr != izv, mpr)
+
+            # get the thickness and normalize along axis 2 (vertical)
+            # to get normalized thickness weights
+            lay_sums = dpr2.sum(axis=2)
+            normed_dz = dpr2 / lay_sums[:, :, np.newaxis]
+
+            # assume that coordinates have equal weights within a zone
+            xpr2 = ma.average(xpr2, axis=2)
+            ypr2 = ma.average(ypr2, axis=2)
+            zpr2 = ma.average(zpr2, axis=2)  # avg zone
+
+            dpr2 = ma.sum(dpr2, axis=2)
+
+            if summing:
+                mpr2 = ma.sum(mpr2, axis=2)
+            else:
+                mpr2 = ma.average(mpr2, weights=normed_dz, axis=2)  # avg zone
+
+            newx.append(xpr2)
+            newy.append(ypr2)
+            newz.append(zpr2)
+            newd.append(dpr2)
+            newm.append(mpr2)
+
+        xpr = ma.dstack(newx)
+        ypr = ma.dstack(newy)
+        zpr = ma.dstack(newz)
+        dpr = ma.dstack(newd)
+        mpr = ma.dstack(newm)
+        zpr.astype(np.int32)
+
+    xpr = ma.filled(xpr, fill_value=xtgeo.UNDEF)
+    ypr = ma.filled(ypr, fill_value=xtgeo.UNDEF)
+    zpr = ma.filled(zpr, fill_value=0)
+    dpr = ma.filled(dpr, fill_value=0.0)
+
+    mpr = ma.filled(mpr, fill_value=0.0)
+
+    return xpr, ypr, zpr, mpr, dpr
+
+
+def surf_fill(self, fill_value=None):
+    """Replace the value of invalid 'data' cells (indicated by 'invalid')
+    by the value of the nearest valid data cell or a constant.
+
+    This is a quite fast method to fill undefined areas of the map.
+    The surface values are updated 'in-place'
+
+    .. versionadded:: 2.1
+    .. versionchanged:: 2.6 Added fill_value
+    """
+    logger.info("Do fill...")
+
+    if fill_value is not None:
+        if np.isscalar(fill_value) and not isinstance(fill_value, str):
+            self.values = ma.filled(self.values, fill_value=float(fill_value))
+        else:
+            raise ValueError("Keyword fill_value must be int or float")
+    else:
+        invalid = ma.getmaskarray(self.values)
+
+        ind = scipy.ndimage.distance_transform_edt(
+            invalid, return_distances=False, return_indices=True
+        )
+        self._values = self._values[tuple(ind)]
+        logger.info("Do fill... DONE")
+
+
+def smooth_median(self, iterations=1, width=1):
+    """Smooth a surface using a median filter.
+
+    .. versionadded:: 2.1
+    """
+
+    mask = ma.getmaskarray(self.values)
+    tmpv = ma.filled(self.values, fill_value=np.nan)
+
+    for _itr in range(iterations):
+        tmpv = scipy.ndimage.median_filter(tmpv, width)
+
+    tmpv = ma.masked_invalid(tmpv)
+
+    # seems that false areas of invalids (masked) may be made; combat that:
+    self.values = tmpv
+    self.fill()
+    self.values = ma.array(self.values, mask=mask)
```

## xtgeo/surface/_regsurf_import.py

 * *Ordering differences only*

```diff
@@ -1,533 +1,533 @@
-"""Import RegularSurface data."""
-# pylint: disable=protected-access
-
-import json
-from collections import OrderedDict
-from struct import unpack
-
-import h5py
-import numpy as np
-import numpy.ma as ma
-import xtgeo
-import xtgeo.common.sys as xsys
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo  # pylint: disable=no-name-in-module
-from xtgeo.common import XTGeoDialog
-from xtgeo.common.constants import UNDEF_MAP_IRAPA, UNDEF_MAP_IRAPB
-from xtgeo.surface._zmap_parser import parse_zmap
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_irap_binary(mfile, values=True, engine="cxtgeo", **_):
-    """Import Irap binary format.
-
-    Args:
-        mfile (_XTGeoFile): Instance of xtgeo file class
-        values (bool, optional): Getting values or just scan. Defaults to True.
-
-    Raises:
-        RuntimeError: Error in reading Irap binary file
-        RuntimeError: Problem....
-    """
-    if mfile.memstream is True or engine == "python":
-        return _import_irap_binary_purepy(mfile)
-    else:
-        return _import_irap_binary(mfile, values=values)
-
-
-def _import_irap_binary_purepy(mfile, values=True):
-    """Using pure python, better for memorymapping/threading."""
-    # Borrowed some code from https://github.com/equinor/grequi/../fmt_irapbin.py
-
-    logger.info("Enter function %s", __name__)
-
-    if mfile.memstream:
-        mfile.file.seek(0)
-        buf = mfile.file.read()
-    else:
-        with open(mfile.file, "rb") as fhandle:
-            buf = fhandle.read()
-
-    # unpack header with big-endian format string
-    hed = unpack(">3i6f3i3f10i", buf[:100])
-
-    args = {}
-    args["nrow"] = hed[2]
-    args["xori"] = hed[3]
-    args["yori"] = hed[5]
-    args["xinc"] = hed[7]
-    args["yinc"] = hed[8]
-    args["ncol"] = hed[11]
-    args["rotation"] = hed[12]
-
-    args["yflip"] = 1
-    if args["yinc"] < 0.0:
-        args["yinc"] *= -1
-        args["yflip"] = -1
-
-    if not values:
-        return args
-
-    # Values: traverse through data blocks
-    stv = 100  # Starting byte
-    datav = []
-
-    while True:
-        # start block integer - number of bytes of floats in following block
-        blockv = unpack(">i", buf[stv : stv + 4])[0]
-        stv += 4
-        # floats
-        datav.append(
-            np.array(unpack(">" + str(int(blockv / 4)) + "f", buf[stv : blockv + stv]))
-        )
-        stv += blockv
-        # end block integer not needed really
-        _ = unpack(">i", buf[stv : stv + 4])[0]
-        stv += 4
-        if stv == len(buf):
-            break
-
-    values = np.hstack(datav)
-    values = np.reshape(values, (args["ncol"], args["nrow"]), order="F")
-    values = np.array(values, order="C")
-    values = np.ma.masked_greater_equal(values, UNDEF_MAP_IRAPB)
-    args["values"] = np.ma.masked_invalid(values)
-
-    del buf
-    return args
-
-
-def _import_irap_binary(mfile, values=True):
-    logger.info("Enter function %s", __name__)
-
-    cfhandle = mfile.get_cfhandle()
-    args = {}
-    # read with mode 0, to get mx my and other metadata
-    (
-        ier,
-        args["ncol"],
-        args["nrow"],
-        _,
-        args["xori"],
-        args["yori"],
-        args["xinc"],
-        args["yinc"],
-        args["rotation"],
-        val,
-    ) = _cxtgeo.surf_import_irap_bin(cfhandle, 0, 1, 0)
-
-    if ier != 0:
-        mfile.cfclose()
-        raise RuntimeError("Error in reading Irap binary file")
-
-    args["yflip"] = 1
-    if args["yinc"] < 0.0:
-        args["yinc"] *= -1
-        args["yflip"] = -1
-
-    # lazy loading, not reading the arrays
-    if values:
-        nval = args["ncol"] * args["nrow"]
-        xlist = _cxtgeo.surf_import_irap_bin(cfhandle, 1, nval, 0)
-        if xlist[0] != 0:
-            mfile.cfclose()
-            raise RuntimeError(f"Problem in {__name__}, code {ier}")
-
-        val = xlist[-1]
-
-        val = np.reshape(val, (args["ncol"], args["nrow"]), order="C")
-
-        val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
-
-        if np.isnan(val).any():
-            logger.info("NaN values are found, will mask...")
-            val = ma.masked_invalid(val)
-
-        args["values"] = val
-
-    mfile.cfclose()
-    return args
-
-
-def import_irap_ascii(mfile, engine="cxtgeo", **_):
-    """Import Irap ascii format, where mfile is a _XTGeoFile instance."""
-    #   -996  2010      5.000000      5.000000
-    #    461587.553724   467902.553724  5927061.430176  5937106.430176
-    #   1264       30.000011   461587.553724  5927061.430176
-    #      0     0     0     0     0     0     0
-    #     1677.3239    1677.3978    1677.4855    1677.5872    1677.7034    1677.8345
-    #     1677.9807    1678.1420    1678.3157    1678.5000    1678.6942    1678.8973
-    #     1679.1086    1679.3274    1679.5524    1679.7831    1680.0186    1680.2583
-    #     1680.5016    1680.7480    1680.9969    1681.2479    1681.5004    1681.7538
-    #
-
-    if mfile.memstream is True or engine == "python":
-        return _import_irap_ascii_purepy(mfile)
-    else:
-        return _import_irap_ascii(mfile)
-
-
-def _import_irap_ascii_purepy(mfile):
-    """Import Irap in pure python code, suitable for memstreams, but less efficient."""
-    # timer tests suggest approx double load time compared with cxtgeo method
-
-    if mfile.memstream:
-        mfile.file.seek(0)
-        buf = mfile.file.read()
-        buf = buf.decode().split()
-    else:
-        with open(mfile.file) as fhandle:
-            buf = fhandle.read().split()
-    args = {}
-    args["nrow"] = int(buf[1])
-    args["xinc"] = float(buf[2])
-    args["yinc"] = float(buf[3])
-    args["xori"] = float(buf[4])
-    args["yori"] = float(buf[6])
-    args["ncol"] = int(buf[8])
-    args["rotation"] = float(buf[9])
-
-    values = np.array(buf[19:]).astype(np.float64)
-    values = np.reshape(values, (args["ncol"], args["nrow"]), order="F")
-    values = np.array(values, order="C")
-    args["values"] = np.ma.masked_greater_equal(values, UNDEF_MAP_IRAPA)
-
-    args["yflip"] = 1
-    if args["yinc"] < 0.0:
-        args["yinc"] *= -1
-        args["yflip"] = -1
-
-    del buf
-    return args
-
-
-def _import_irap_ascii(mfile):
-    """Import Irap ascii format via C routines (fast, but less suited for bytesio)."""
-    logger.debug("Enter function...")
-
-    cfhandle = mfile.get_cfhandle()
-
-    # read with mode 0, scan to get mx my
-    xlist = _cxtgeo.surf_import_irap_ascii(cfhandle, 0, 1, 0)
-
-    nvn = xlist[1] * xlist[2]  # mx * my
-    xlist = _cxtgeo.surf_import_irap_ascii(cfhandle, 1, nvn, 0)
-
-    ier, ncol, nrow, _, xori, yori, xinc, yinc, rot, val = xlist
-
-    if ier != 0:
-        mfile.cfclose()
-        raise RuntimeError(f"Problem in {__name__}, code {ier}")
-
-    val = np.reshape(val, (ncol, nrow), order="C")
-
-    val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
-
-    if np.isnan(val).any():
-        logger.info("NaN values are found, will mask...")
-        val = ma.masked_invalid(val)
-
-    yflip = 1
-    if yinc < 0.0:
-        yinc = yinc * -1
-        yflip = -1
-    args = {}
-    args["ncol"] = ncol
-    args["nrow"] = nrow
-    args["xori"] = xori
-    args["yori"] = yori
-    args["xinc"] = xinc
-    args["yinc"] = yinc
-    args["yflip"] = yflip
-    args["rotation"] = rot
-
-    args["values"] = val
-
-    mfile.cfclose()
-    return args
-
-
-def import_ijxyz(mfile, template=None, **_):
-    """Import OW/DSG IJXYZ ascii format."""
-
-    if not template:
-        return _import_ijxyz(mfile)
-    else:
-        return _import_ijxyz_tmpl(mfile, template)
-
-
-def _import_ijxyz(mfile):  # pylint: disable=too-many-locals
-    """Import OW/DSG IJXYZ ascii format."""
-    # import of seismic column system on the form:
-    # 2588	1179	476782.2897888889	6564025.6954	1000.0
-    # 2588	1180	476776.7181777778	6564014.5058	1000.0
-    logger.debug("Read data from file... (scan for dimensions)")
-
-    cfhandle = mfile.get_cfhandle()
-
-    xlist = _cxtgeo.surf_import_ijxyz(cfhandle, 0, 1, 1, 1, 0)
-
-    (
-        ier,
-        ncol,
-        nrow,
-        _,
-        xori,
-        yori,
-        xinc,
-        yinc,
-        rot,
-        iln,
-        xln,
-        val,
-        yflip,
-    ) = xlist
-
-    if ier != 0:
-        mfile.cfclose()
-        raise RuntimeError("Import from C is wrong...")
-
-    # now real read mode
-    xlist = _cxtgeo.surf_import_ijxyz(cfhandle, 1, ncol, nrow, ncol * nrow, 0)
-
-    ier, ncol, nrow, _, xori, yori, xinc, yinc, rot, iln, xln, val, yflip = xlist
-
-    if ier != 0:
-        raise RuntimeError("Import from C is wrong...")
-
-    logger.info(xlist)
-
-    val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
-    args = {}
-    args["xori"] = xori
-    args["xinc"] = xinc
-    args["yori"] = yori
-    args["yinc"] = yinc
-    args["ncol"] = ncol
-    args["nrow"] = nrow
-    args["rotation"] = rot
-    args["yflip"] = yflip
-
-    args["values"] = val.reshape((args["ncol"], args["nrow"]))
-
-    args["ilines"] = iln
-    args["xlines"] = xln
-
-    mfile.cfclose()
-    return args
-
-
-def _import_ijxyz_tmpl(mfile, template):
-    """Import OW/DSG IJXYZ ascii format, with a Cube or RegularSurface as template."""
-    cfhandle = mfile.get_cfhandle()
-
-    if isinstance(template, (xtgeo.cube.Cube, xtgeo.surface.RegularSurface)):
-        logger.info("OK template")
-    else:
-        raise ValueError(f"Template is of wrong type: {type(template)}")
-
-    nxy = template.ncol * template.nrow
-    ier, val = _cxtgeo.surf_import_ijxyz_tmpl(
-        cfhandle, template.ilines, template.xlines, nxy, 0
-    )
-
-    if ier == -1:
-        raise ValueError(
-            f"The file {mfile.name} and template map or cube has inconsistent "
-            "inline and/or xlines numbering. Try importing without template "
-            "and use e.g. resampling instead."
-        )
-
-    elif ier != 0:
-        raise RuntimeError("Unknown error when trying to import the IJXYZ based file!")
-
-    val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
-
-    args = {}
-    args["xori"] = template.xori
-    args["xinc"] = template.xinc
-    args["yori"] = template.yori
-    args["yinc"] = template.yinc
-    args["ncol"] = template.ncol
-    args["nrow"] = template.nrow
-    args["rotation"] = template.rotation
-    args["yflip"] = template.yflip
-    args["values"] = val.reshape((args["ncol"], args["nrow"]))
-
-    args["ilines"] = template._ilines.copy()
-    args["xlines"] = template._xlines.copy()
-
-    mfile.cfclose()
-    return args
-
-
-def import_petromod(mfile, **_):
-    """Import Petromod binary format."""
-
-    cfhandle = mfile.get_cfhandle()
-
-    logger.info("Enter function %s", __name__)
-
-    # read with mode 0, to get mx my and other metadata
-    dsc, _ = _cxtgeo.surf_import_petromod_bin(cfhandle, 0, 0.0, 0, 0, 0)
-
-    fields = dsc.split(",")
-
-    rota_xori = 0
-    rota_yori = 0
-    undef = 999999.0
-    args = {}
-    for field in fields:
-        key, value = field.split("=")
-        if key == "GridNoX":
-            args["ncol"] = int(value)
-        if key == "GridNoY":
-            args["nrow"] = int(value)
-        if key == "OriginX":
-            args["xori"] = float(value)
-        if key == "OriginY":
-            args["yori"] = float(value)
-        if key == "RotationOriginX":
-            rota_xori = float(value)
-        if key == "RotationOriginY":
-            rota_yori = float(value)
-        if key == "GridStepX":
-            args["xinc"] = float(value)
-        if key == "GridStepY":
-            args["yinc"] = float(value)
-        if key == "RotationAngle":
-            args["rotation"] = float(value)
-        if key == "Undefined":
-            undef = float(value)
-
-    if args["rotation"] != 0.0 and (
-        rota_xori != args["xori"] or rota_yori != args["yori"]
-    ):
-        xtg.warnuser("Rotation origin and data origin do match")
-
-    # reread file for map values
-
-    dsc, values = _cxtgeo.surf_import_petromod_bin(
-        cfhandle, 1, undef, args["ncol"], args["nrow"], args["ncol"] * args["nrow"]
-    )
-
-    values = np.ma.masked_greater(values, xtgeo.UNDEF_LIMIT)
-
-    args["values"] = values.reshape(args["ncol"], args["nrow"])
-
-    mfile.cfclose()
-    return args
-
-
-def import_zmap_ascii(mfile, values=True, **_):
-    """Importing ZMAP + ascii files, in pure python only.
-
-    Some sources
-
-    https://mycarta.wordpress.com/2019/03/23/working-with-zmap-grid-files-in-python/
-    https://blog.nitorinfotech.com/what-is-zmap-plus-file-format/
-
-    """
-    zmap_data = parse_zmap(mfile.file, load_values=values)
-    try:
-        args = {
-            "ncol": zmap_data.ncol,
-            "nrow": zmap_data.nrow,
-            "xori": zmap_data.xmin,
-            "yori": zmap_data.ymin,
-            "xinc": (zmap_data.xmax - zmap_data.xmin) / (zmap_data.ncol - 1),
-            "yinc": (zmap_data.ymax - zmap_data.ymin) / (zmap_data.nrow - 1),
-        }
-    except ZeroDivisionError as err:
-        raise ValueError(
-            f"A zmap surface must have ncol ({zmap_data.ncol}) "
-            f"and nrow ({zmap_data.ncol}) > 1"
-        ) from err
-    if values:
-        loaded_values = np.reshape(
-            zmap_data.values, (zmap_data.ncol, zmap_data.nrow), order="C"
-        )
-        loaded_values = np.flip(loaded_values, axis=1)
-        args["values"] = loaded_values
-
-    return args
-
-
-def import_xtg(mfile, values=True, **kwargs):
-    """Using pure python for experimental XTGEO import."""
-    logger.debug("Additional, probably unused kwargs: %s", **kwargs)
-
-    offset = 28
-    with open(mfile.file, "rb") as fhandle:
-        buf = fhandle.read(offset)
-
-    # unpack header
-    swap, magic, nfloat, ncol, nrow = unpack("= i i i q q", buf)
-
-    if swap != 1 or magic != 1101:
-        raise ValueError("Invalid file format (wrong swap id or magic number).")
-
-    dtype = np.float32 if nfloat == 4 else np.float64
-
-    vals = None
-    narr = ncol * nrow
-    if values:
-        vals = xsys.npfromfile(mfile.file, dtype=dtype, count=narr, offset=offset)
-
-    # read metadata which will be at position offet + nfloat*narr +13
-    pos = offset + nfloat * narr + 13
-
-    with open(mfile.file, "rb") as fhandle:
-        fhandle.seek(pos)
-        jmeta = fhandle.read().decode()
-
-    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-    req = meta["_required_"]
-
-    reqattrs = xtgeo.MetaDataRegularSurface.REQUIRED
-
-    args = {}
-    for myattr in reqattrs:
-        args[myattr] = req[myattr]
-
-    if values:
-        args["values"] = np.ma.masked_equal(
-            vals.reshape(args["ncol"], args["nrow"]), xtgeo.UNDEF
-        )
-
-    return args
-
-
-def import_hdf5_regsurf(mfile, values=True, **_):
-    """Importing h5/hdf5 storage."""
-    reqattrs = xtgeo.MetaDataRegularSurface.REQUIRED
-
-    invalues = None
-    with h5py.File(mfile.name, "r") as h5h:
-        grp = h5h["RegularSurface"]
-        idcode = grp.attrs["format-idcode"]
-        provider = grp.attrs["provider"]
-        if idcode != 1101:
-            raise ValueError(f"Wrong id code: {idcode}")
-        logger.info("Provider is %s", provider)
-
-        if values:
-            invalues = grp["values"][:]
-
-        jmeta = grp.attrs["metadata"]
-        meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-
-        req = meta["_required_"]
-
-    args = {}
-    for myattr in reqattrs:
-        args[myattr] = req[myattr]
-
-    if values:
-        args["values"] = np.ma.masked_equal(
-            invalues.reshape(args["ncol"], args["nrow"]), xtgeo.UNDEF
-        )
-
-    return args
+"""Import RegularSurface data."""
+# pylint: disable=protected-access
+
+import json
+from collections import OrderedDict
+from struct import unpack
+
+import h5py
+import numpy as np
+import numpy.ma as ma
+import xtgeo
+import xtgeo.common.sys as xsys
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo  # pylint: disable=no-name-in-module
+from xtgeo.common import XTGeoDialog
+from xtgeo.common.constants import UNDEF_MAP_IRAPA, UNDEF_MAP_IRAPB
+from xtgeo.surface._zmap_parser import parse_zmap
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_irap_binary(mfile, values=True, engine="cxtgeo", **_):
+    """Import Irap binary format.
+
+    Args:
+        mfile (_XTGeoFile): Instance of xtgeo file class
+        values (bool, optional): Getting values or just scan. Defaults to True.
+
+    Raises:
+        RuntimeError: Error in reading Irap binary file
+        RuntimeError: Problem....
+    """
+    if mfile.memstream is True or engine == "python":
+        return _import_irap_binary_purepy(mfile)
+    else:
+        return _import_irap_binary(mfile, values=values)
+
+
+def _import_irap_binary_purepy(mfile, values=True):
+    """Using pure python, better for memorymapping/threading."""
+    # Borrowed some code from https://github.com/equinor/grequi/../fmt_irapbin.py
+
+    logger.info("Enter function %s", __name__)
+
+    if mfile.memstream:
+        mfile.file.seek(0)
+        buf = mfile.file.read()
+    else:
+        with open(mfile.file, "rb") as fhandle:
+            buf = fhandle.read()
+
+    # unpack header with big-endian format string
+    hed = unpack(">3i6f3i3f10i", buf[:100])
+
+    args = {}
+    args["nrow"] = hed[2]
+    args["xori"] = hed[3]
+    args["yori"] = hed[5]
+    args["xinc"] = hed[7]
+    args["yinc"] = hed[8]
+    args["ncol"] = hed[11]
+    args["rotation"] = hed[12]
+
+    args["yflip"] = 1
+    if args["yinc"] < 0.0:
+        args["yinc"] *= -1
+        args["yflip"] = -1
+
+    if not values:
+        return args
+
+    # Values: traverse through data blocks
+    stv = 100  # Starting byte
+    datav = []
+
+    while True:
+        # start block integer - number of bytes of floats in following block
+        blockv = unpack(">i", buf[stv : stv + 4])[0]
+        stv += 4
+        # floats
+        datav.append(
+            np.array(unpack(">" + str(int(blockv / 4)) + "f", buf[stv : blockv + stv]))
+        )
+        stv += blockv
+        # end block integer not needed really
+        _ = unpack(">i", buf[stv : stv + 4])[0]
+        stv += 4
+        if stv == len(buf):
+            break
+
+    values = np.hstack(datav)
+    values = np.reshape(values, (args["ncol"], args["nrow"]), order="F")
+    values = np.array(values, order="C")
+    values = np.ma.masked_greater_equal(values, UNDEF_MAP_IRAPB)
+    args["values"] = np.ma.masked_invalid(values)
+
+    del buf
+    return args
+
+
+def _import_irap_binary(mfile, values=True):
+    logger.info("Enter function %s", __name__)
+
+    cfhandle = mfile.get_cfhandle()
+    args = {}
+    # read with mode 0, to get mx my and other metadata
+    (
+        ier,
+        args["ncol"],
+        args["nrow"],
+        _,
+        args["xori"],
+        args["yori"],
+        args["xinc"],
+        args["yinc"],
+        args["rotation"],
+        val,
+    ) = _cxtgeo.surf_import_irap_bin(cfhandle, 0, 1, 0)
+
+    if ier != 0:
+        mfile.cfclose()
+        raise RuntimeError("Error in reading Irap binary file")
+
+    args["yflip"] = 1
+    if args["yinc"] < 0.0:
+        args["yinc"] *= -1
+        args["yflip"] = -1
+
+    # lazy loading, not reading the arrays
+    if values:
+        nval = args["ncol"] * args["nrow"]
+        xlist = _cxtgeo.surf_import_irap_bin(cfhandle, 1, nval, 0)
+        if xlist[0] != 0:
+            mfile.cfclose()
+            raise RuntimeError(f"Problem in {__name__}, code {ier}")
+
+        val = xlist[-1]
+
+        val = np.reshape(val, (args["ncol"], args["nrow"]), order="C")
+
+        val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
+
+        if np.isnan(val).any():
+            logger.info("NaN values are found, will mask...")
+            val = ma.masked_invalid(val)
+
+        args["values"] = val
+
+    mfile.cfclose()
+    return args
+
+
+def import_irap_ascii(mfile, engine="cxtgeo", **_):
+    """Import Irap ascii format, where mfile is a _XTGeoFile instance."""
+    #   -996  2010      5.000000      5.000000
+    #    461587.553724   467902.553724  5927061.430176  5937106.430176
+    #   1264       30.000011   461587.553724  5927061.430176
+    #      0     0     0     0     0     0     0
+    #     1677.3239    1677.3978    1677.4855    1677.5872    1677.7034    1677.8345
+    #     1677.9807    1678.1420    1678.3157    1678.5000    1678.6942    1678.8973
+    #     1679.1086    1679.3274    1679.5524    1679.7831    1680.0186    1680.2583
+    #     1680.5016    1680.7480    1680.9969    1681.2479    1681.5004    1681.7538
+    #
+
+    if mfile.memstream is True or engine == "python":
+        return _import_irap_ascii_purepy(mfile)
+    else:
+        return _import_irap_ascii(mfile)
+
+
+def _import_irap_ascii_purepy(mfile):
+    """Import Irap in pure python code, suitable for memstreams, but less efficient."""
+    # timer tests suggest approx double load time compared with cxtgeo method
+
+    if mfile.memstream:
+        mfile.file.seek(0)
+        buf = mfile.file.read()
+        buf = buf.decode().split()
+    else:
+        with open(mfile.file) as fhandle:
+            buf = fhandle.read().split()
+    args = {}
+    args["nrow"] = int(buf[1])
+    args["xinc"] = float(buf[2])
+    args["yinc"] = float(buf[3])
+    args["xori"] = float(buf[4])
+    args["yori"] = float(buf[6])
+    args["ncol"] = int(buf[8])
+    args["rotation"] = float(buf[9])
+
+    values = np.array(buf[19:]).astype(np.float64)
+    values = np.reshape(values, (args["ncol"], args["nrow"]), order="F")
+    values = np.array(values, order="C")
+    args["values"] = np.ma.masked_greater_equal(values, UNDEF_MAP_IRAPA)
+
+    args["yflip"] = 1
+    if args["yinc"] < 0.0:
+        args["yinc"] *= -1
+        args["yflip"] = -1
+
+    del buf
+    return args
+
+
+def _import_irap_ascii(mfile):
+    """Import Irap ascii format via C routines (fast, but less suited for bytesio)."""
+    logger.debug("Enter function...")
+
+    cfhandle = mfile.get_cfhandle()
+
+    # read with mode 0, scan to get mx my
+    xlist = _cxtgeo.surf_import_irap_ascii(cfhandle, 0, 1, 0)
+
+    nvn = xlist[1] * xlist[2]  # mx * my
+    xlist = _cxtgeo.surf_import_irap_ascii(cfhandle, 1, nvn, 0)
+
+    ier, ncol, nrow, _, xori, yori, xinc, yinc, rot, val = xlist
+
+    if ier != 0:
+        mfile.cfclose()
+        raise RuntimeError(f"Problem in {__name__}, code {ier}")
+
+    val = np.reshape(val, (ncol, nrow), order="C")
+
+    val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
+
+    if np.isnan(val).any():
+        logger.info("NaN values are found, will mask...")
+        val = ma.masked_invalid(val)
+
+    yflip = 1
+    if yinc < 0.0:
+        yinc = yinc * -1
+        yflip = -1
+    args = {}
+    args["ncol"] = ncol
+    args["nrow"] = nrow
+    args["xori"] = xori
+    args["yori"] = yori
+    args["xinc"] = xinc
+    args["yinc"] = yinc
+    args["yflip"] = yflip
+    args["rotation"] = rot
+
+    args["values"] = val
+
+    mfile.cfclose()
+    return args
+
+
+def import_ijxyz(mfile, template=None, **_):
+    """Import OW/DSG IJXYZ ascii format."""
+
+    if not template:
+        return _import_ijxyz(mfile)
+    else:
+        return _import_ijxyz_tmpl(mfile, template)
+
+
+def _import_ijxyz(mfile):  # pylint: disable=too-many-locals
+    """Import OW/DSG IJXYZ ascii format."""
+    # import of seismic column system on the form:
+    # 2588	1179	476782.2897888889	6564025.6954	1000.0
+    # 2588	1180	476776.7181777778	6564014.5058	1000.0
+    logger.debug("Read data from file... (scan for dimensions)")
+
+    cfhandle = mfile.get_cfhandle()
+
+    xlist = _cxtgeo.surf_import_ijxyz(cfhandle, 0, 1, 1, 1, 0)
+
+    (
+        ier,
+        ncol,
+        nrow,
+        _,
+        xori,
+        yori,
+        xinc,
+        yinc,
+        rot,
+        iln,
+        xln,
+        val,
+        yflip,
+    ) = xlist
+
+    if ier != 0:
+        mfile.cfclose()
+        raise RuntimeError("Import from C is wrong...")
+
+    # now real read mode
+    xlist = _cxtgeo.surf_import_ijxyz(cfhandle, 1, ncol, nrow, ncol * nrow, 0)
+
+    ier, ncol, nrow, _, xori, yori, xinc, yinc, rot, iln, xln, val, yflip = xlist
+
+    if ier != 0:
+        raise RuntimeError("Import from C is wrong...")
+
+    logger.info(xlist)
+
+    val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
+    args = {}
+    args["xori"] = xori
+    args["xinc"] = xinc
+    args["yori"] = yori
+    args["yinc"] = yinc
+    args["ncol"] = ncol
+    args["nrow"] = nrow
+    args["rotation"] = rot
+    args["yflip"] = yflip
+
+    args["values"] = val.reshape((args["ncol"], args["nrow"]))
+
+    args["ilines"] = iln
+    args["xlines"] = xln
+
+    mfile.cfclose()
+    return args
+
+
+def _import_ijxyz_tmpl(mfile, template):
+    """Import OW/DSG IJXYZ ascii format, with a Cube or RegularSurface as template."""
+    cfhandle = mfile.get_cfhandle()
+
+    if isinstance(template, (xtgeo.cube.Cube, xtgeo.surface.RegularSurface)):
+        logger.info("OK template")
+    else:
+        raise ValueError(f"Template is of wrong type: {type(template)}")
+
+    nxy = template.ncol * template.nrow
+    ier, val = _cxtgeo.surf_import_ijxyz_tmpl(
+        cfhandle, template.ilines, template.xlines, nxy, 0
+    )
+
+    if ier == -1:
+        raise ValueError(
+            f"The file {mfile.name} and template map or cube has inconsistent "
+            "inline and/or xlines numbering. Try importing without template "
+            "and use e.g. resampling instead."
+        )
+
+    elif ier != 0:
+        raise RuntimeError("Unknown error when trying to import the IJXYZ based file!")
+
+    val = ma.masked_greater(val, xtgeo.UNDEF_LIMIT)
+
+    args = {}
+    args["xori"] = template.xori
+    args["xinc"] = template.xinc
+    args["yori"] = template.yori
+    args["yinc"] = template.yinc
+    args["ncol"] = template.ncol
+    args["nrow"] = template.nrow
+    args["rotation"] = template.rotation
+    args["yflip"] = template.yflip
+    args["values"] = val.reshape((args["ncol"], args["nrow"]))
+
+    args["ilines"] = template._ilines.copy()
+    args["xlines"] = template._xlines.copy()
+
+    mfile.cfclose()
+    return args
+
+
+def import_petromod(mfile, **_):
+    """Import Petromod binary format."""
+
+    cfhandle = mfile.get_cfhandle()
+
+    logger.info("Enter function %s", __name__)
+
+    # read with mode 0, to get mx my and other metadata
+    dsc, _ = _cxtgeo.surf_import_petromod_bin(cfhandle, 0, 0.0, 0, 0, 0)
+
+    fields = dsc.split(",")
+
+    rota_xori = 0
+    rota_yori = 0
+    undef = 999999.0
+    args = {}
+    for field in fields:
+        key, value = field.split("=")
+        if key == "GridNoX":
+            args["ncol"] = int(value)
+        if key == "GridNoY":
+            args["nrow"] = int(value)
+        if key == "OriginX":
+            args["xori"] = float(value)
+        if key == "OriginY":
+            args["yori"] = float(value)
+        if key == "RotationOriginX":
+            rota_xori = float(value)
+        if key == "RotationOriginY":
+            rota_yori = float(value)
+        if key == "GridStepX":
+            args["xinc"] = float(value)
+        if key == "GridStepY":
+            args["yinc"] = float(value)
+        if key == "RotationAngle":
+            args["rotation"] = float(value)
+        if key == "Undefined":
+            undef = float(value)
+
+    if args["rotation"] != 0.0 and (
+        rota_xori != args["xori"] or rota_yori != args["yori"]
+    ):
+        xtg.warnuser("Rotation origin and data origin do match")
+
+    # reread file for map values
+
+    dsc, values = _cxtgeo.surf_import_petromod_bin(
+        cfhandle, 1, undef, args["ncol"], args["nrow"], args["ncol"] * args["nrow"]
+    )
+
+    values = np.ma.masked_greater(values, xtgeo.UNDEF_LIMIT)
+
+    args["values"] = values.reshape(args["ncol"], args["nrow"])
+
+    mfile.cfclose()
+    return args
+
+
+def import_zmap_ascii(mfile, values=True, **_):
+    """Importing ZMAP + ascii files, in pure python only.
+
+    Some sources
+
+    https://mycarta.wordpress.com/2019/03/23/working-with-zmap-grid-files-in-python/
+    https://blog.nitorinfotech.com/what-is-zmap-plus-file-format/
+
+    """
+    zmap_data = parse_zmap(mfile.file, load_values=values)
+    try:
+        args = {
+            "ncol": zmap_data.ncol,
+            "nrow": zmap_data.nrow,
+            "xori": zmap_data.xmin,
+            "yori": zmap_data.ymin,
+            "xinc": (zmap_data.xmax - zmap_data.xmin) / (zmap_data.ncol - 1),
+            "yinc": (zmap_data.ymax - zmap_data.ymin) / (zmap_data.nrow - 1),
+        }
+    except ZeroDivisionError as err:
+        raise ValueError(
+            f"A zmap surface must have ncol ({zmap_data.ncol}) "
+            f"and nrow ({zmap_data.ncol}) > 1"
+        ) from err
+    if values:
+        loaded_values = np.reshape(
+            zmap_data.values, (zmap_data.ncol, zmap_data.nrow), order="C"
+        )
+        loaded_values = np.flip(loaded_values, axis=1)
+        args["values"] = loaded_values
+
+    return args
+
+
+def import_xtg(mfile, values=True, **kwargs):
+    """Using pure python for experimental XTGEO import."""
+    logger.debug("Additional, probably unused kwargs: %s", **kwargs)
+
+    offset = 28
+    with open(mfile.file, "rb") as fhandle:
+        buf = fhandle.read(offset)
+
+    # unpack header
+    swap, magic, nfloat, ncol, nrow = unpack("= i i i q q", buf)
+
+    if swap != 1 or magic != 1101:
+        raise ValueError("Invalid file format (wrong swap id or magic number).")
+
+    dtype = np.float32 if nfloat == 4 else np.float64
+
+    vals = None
+    narr = ncol * nrow
+    if values:
+        vals = xsys.npfromfile(mfile.file, dtype=dtype, count=narr, offset=offset)
+
+    # read metadata which will be at position offet + nfloat*narr +13
+    pos = offset + nfloat * narr + 13
+
+    with open(mfile.file, "rb") as fhandle:
+        fhandle.seek(pos)
+        jmeta = fhandle.read().decode()
+
+    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+    req = meta["_required_"]
+
+    reqattrs = xtgeo.MetaDataRegularSurface.REQUIRED
+
+    args = {}
+    for myattr in reqattrs:
+        args[myattr] = req[myattr]
+
+    if values:
+        args["values"] = np.ma.masked_equal(
+            vals.reshape(args["ncol"], args["nrow"]), xtgeo.UNDEF
+        )
+
+    return args
+
+
+def import_hdf5_regsurf(mfile, values=True, **_):
+    """Importing h5/hdf5 storage."""
+    reqattrs = xtgeo.MetaDataRegularSurface.REQUIRED
+
+    invalues = None
+    with h5py.File(mfile.name, "r") as h5h:
+        grp = h5h["RegularSurface"]
+        idcode = grp.attrs["format-idcode"]
+        provider = grp.attrs["provider"]
+        if idcode != 1101:
+            raise ValueError(f"Wrong id code: {idcode}")
+        logger.info("Provider is %s", provider)
+
+        if values:
+            invalues = grp["values"][:]
+
+        jmeta = grp.attrs["metadata"]
+        meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+
+        req = meta["_required_"]
+
+    args = {}
+    for myattr in reqattrs:
+        args[myattr] = req[myattr]
+
+    if values:
+        args["values"] = np.ma.masked_equal(
+            invalues.reshape(args["ncol"], args["nrow"]), xtgeo.UNDEF
+        )
+
+    return args
```

## xtgeo/surface/_regsurf_lowlevel.py

 * *Ordering differences only*

```diff
@@ -1,122 +1,122 @@
-"""RegularSurface utilities (low level)"""
-
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-#
-
-
-# ======================================================================================
-# Helper methods, for internal usage
-
-
-def get_carr_double(self):
-    """Return the SWIG Carray object"""
-
-    carr = _cxtgeo.new_doublearray(self.ncol * self.nrow)
-
-    _cxtgeo.swig_numpy_to_carr_1d(self.get_values1d(), carr)
-
-    return carr
-
-
-# ======================================================================================
-# METHODS BELOW SHALL BE DEPRECATED!!
-# Helper methods, for internal usage
-# --------------------------------------------------------------------------------------
-# copy self (update) values from SWIG carray to numpy, 1D array
-
-
-# def _update_values(self):
-#     nnum = self._ncol * self._nrow
-
-#     if self._cvalues is None and self._values is not None:
-#         return
-
-#     elif self._cvalues is None and self._values is None:
-#         logger.critical('_cvalues and _values is None in '
-#                         '_update_values. STOP')
-#         sys.exit(9)
-
-#     xvv = _cxtgeo.swig_carr_to_numpy_1d(nnum, self._cvalues)
-
-#     xvv = np.reshape(xvv, (self._ncol, self._nrow), order='F')
-
-#     # make it masked
-#     xvv = ma.masked_greater(xvv, xtgeo.UNDEF_LIMIT)
-
-#     self._values = xvv
-
-#     self._delete_cvalues()
-
-# # copy (update) values from numpy to SWIG, 1D array
-
-
-# def _update_cvalues(self):
-#     logger.debug('Enter update cvalues method...')
-#     nnum = self._ncol * self._nrow
-
-#     if self._values is None and self._cvalues is not None:
-#         logger.debug('CVALUES unchanged')
-#         return
-
-#     elif self._cvalues is None and self._values is None:
-#         logger.critical('_cvalues and _values is None in '
-#                         '_update_cvalues. STOP')
-#         sys.exit(9)
-
-#     elif self._cvalues is not None and self._values is None:
-#         logger.critical('_cvalues and _values are both present in '
-#                         '_update_cvalues. STOP')
-#         sys.exit(9)
-
-#     # make a 1D F order numpy array, and update C array
-#     xvv = ma.filled(self._values, xtgeo.UNDEF)
-#     xvv = np.reshape(xvv, -1, order='F')
-
-#     self._cvalues = _cxtgeo.new_doublearray(nnum)
-
-#     _cxtgeo.swig_numpy_to_carr_1d(xvv, self._cvalues)
-#     logger.debug('Enter method... DONE')
-
-#     self._values = None
-
-
-# def _delete_cvalues(self):
-#     logger.debug('Enter delete cvalues values method...')
-
-#     if self._cvalues is not None:
-#         _cxtgeo.delete_doublearray(self._cvalues)
-
-#     self._cvalues = None
-#     logger.debug('Enter method... DONE')
-
-# # check i
-# f values shape is OK (return True or False)
-
-
-# def _check_shape_ok(self, values):
-
-#     if not values.flags['F_CONTIGUOUS']:
-#         logger.error('Wrong order; shall be Fortran (Flags: {}'
-#                      .format(values.flags))
-#         return False
-
-#     (ncol, nrow) = values.shape
-#     if ncol != self._ncol or nrow != self._nrow:
-#         logger.error('Wrong shape: Dimens of values {} {} vs {} {}'
-#                      .format(ncol, nrow, self._ncol, self._nrow))
-#         return False
-#     return True
-
-
-# def _convert_carr_double_np(self, carray, nlen=None):
-#     """Convert a C array to numpy, assuming double type."""
-#     if nlen is None:
-#         nlen = len(self._df.index)
-
-#     nparray = _cxtgeo.swig_carr_to_numpy_1d(nlen, carray)
-
-#     return nparray
+"""RegularSurface utilities (low level)"""
+
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+#
+
+
+# ======================================================================================
+# Helper methods, for internal usage
+
+
+def get_carr_double(self):
+    """Return the SWIG Carray object"""
+
+    carr = _cxtgeo.new_doublearray(self.ncol * self.nrow)
+
+    _cxtgeo.swig_numpy_to_carr_1d(self.get_values1d(), carr)
+
+    return carr
+
+
+# ======================================================================================
+# METHODS BELOW SHALL BE DEPRECATED!!
+# Helper methods, for internal usage
+# --------------------------------------------------------------------------------------
+# copy self (update) values from SWIG carray to numpy, 1D array
+
+
+# def _update_values(self):
+#     nnum = self._ncol * self._nrow
+
+#     if self._cvalues is None and self._values is not None:
+#         return
+
+#     elif self._cvalues is None and self._values is None:
+#         logger.critical('_cvalues and _values is None in '
+#                         '_update_values. STOP')
+#         sys.exit(9)
+
+#     xvv = _cxtgeo.swig_carr_to_numpy_1d(nnum, self._cvalues)
+
+#     xvv = np.reshape(xvv, (self._ncol, self._nrow), order='F')
+
+#     # make it masked
+#     xvv = ma.masked_greater(xvv, xtgeo.UNDEF_LIMIT)
+
+#     self._values = xvv
+
+#     self._delete_cvalues()
+
+# # copy (update) values from numpy to SWIG, 1D array
+
+
+# def _update_cvalues(self):
+#     logger.debug('Enter update cvalues method...')
+#     nnum = self._ncol * self._nrow
+
+#     if self._values is None and self._cvalues is not None:
+#         logger.debug('CVALUES unchanged')
+#         return
+
+#     elif self._cvalues is None and self._values is None:
+#         logger.critical('_cvalues and _values is None in '
+#                         '_update_cvalues. STOP')
+#         sys.exit(9)
+
+#     elif self._cvalues is not None and self._values is None:
+#         logger.critical('_cvalues and _values are both present in '
+#                         '_update_cvalues. STOP')
+#         sys.exit(9)
+
+#     # make a 1D F order numpy array, and update C array
+#     xvv = ma.filled(self._values, xtgeo.UNDEF)
+#     xvv = np.reshape(xvv, -1, order='F')
+
+#     self._cvalues = _cxtgeo.new_doublearray(nnum)
+
+#     _cxtgeo.swig_numpy_to_carr_1d(xvv, self._cvalues)
+#     logger.debug('Enter method... DONE')
+
+#     self._values = None
+
+
+# def _delete_cvalues(self):
+#     logger.debug('Enter delete cvalues values method...')
+
+#     if self._cvalues is not None:
+#         _cxtgeo.delete_doublearray(self._cvalues)
+
+#     self._cvalues = None
+#     logger.debug('Enter method... DONE')
+
+# # check i
+# f values shape is OK (return True or False)
+
+
+# def _check_shape_ok(self, values):
+
+#     if not values.flags['F_CONTIGUOUS']:
+#         logger.error('Wrong order; shall be Fortran (Flags: {}'
+#                      .format(values.flags))
+#         return False
+
+#     (ncol, nrow) = values.shape
+#     if ncol != self._ncol or nrow != self._nrow:
+#         logger.error('Wrong shape: Dimens of values {} {} vs {} {}'
+#                      .format(ncol, nrow, self._ncol, self._nrow))
+#         return False
+#     return True
+
+
+# def _convert_carr_double_np(self, carray, nlen=None):
+#     """Convert a C array to numpy, assuming double type."""
+#     if nlen is None:
+#         nlen = len(self._df.index)
+
+#     nparray = _cxtgeo.swig_carr_to_numpy_1d(nlen, carray)
+
+#     return nparray
```

## xtgeo/surface/_regsurf_oper.py

 * *Ordering differences only*

```diff
@@ -1,541 +1,541 @@
-# coding: utf-8
-"""Various operations"""
-
-import numbers
-
-import numpy as np
-import numpy.ma as ma
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo  # type: ignore
-from xtgeo import XTGeoCLibError
-from xtgeo.common import XTGeoDialog
-from xtgeo.xyz import Polygons
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-VALID_OPER = (
-    "add",
-    "iadd",
-    "sub",
-    "isub",
-    "mul",
-    "imul",
-    "div",
-    "idiv",
-    "lt",
-    "gt",
-    "le",
-    "eq",
-    "ne",
-)
-
-VALID_OPER_POLYS = (
-    "add",
-    "sub",
-    "mul",
-    "div",
-    "set",
-    "eli",
-)
-
-
-def operations_two(self, other, oper="add"):  # pylint: disable=too-many-branches
-    """General operations between two maps"""
-
-    other = _check_other(self, other)
-
-    okstatus = self.compare_topology(other)
-
-    useother = other
-    if not okstatus:
-        # to avoid that the "other" instance is changed
-        useother = self.copy()
-        useother.resample(other)
-
-    if oper not in VALID_OPER:
-        raise ValueError(f"Operation key oper has invalid value: {oper}")
-
-    retvalue = None
-
-    if oper == "add":
-        self.values = self.values + useother.values
-    elif oper == "iadd":
-        self.values += useother.values
-    elif oper == "sub":
-        self.values = self.values - useother.values
-    elif oper == "isub":
-        self._values -= useother._values
-    elif oper == "mul":
-        self.values = self.values * useother.values
-    elif oper == "imul":
-        self._values *= useother._values
-    elif oper == "div":
-        self.values = self.values / useother.values
-    elif oper == "idiv":
-        self._values /= useother._values
-
-    # comparisons:
-    elif oper == "lt":
-        retvalue = self.values < other.values
-    elif oper == "gt":
-        retvalue = self.values > other.values
-    elif oper == "le":
-        retvalue = self.values <= other.values
-    elif oper == "ge":
-        retvalue = self.values >= other.values
-    elif oper == "eq":
-        retvalue = self.values == other.values
-    elif oper == "ne":
-        retvalue = self.values != other.values
-
-    if useother is not other:
-        del useother
-
-    self._filesrc = "Calculated"
-
-    # return None or a boolean array
-    return retvalue
-
-
-def _check_other(self, other):
-    """Will convert an other scalar to a valid numpy array"""
-
-    if isinstance(other, numbers.Number):
-        vals = other
-        other = self.copy()
-        other.values *= 0
-        other.values += vals
-        other._filesrc = None
-
-    return other
-
-
-def resample(self, other, mask=True, sampling="bilinear"):
-    """Resample from other surface object to this surf."""
-
-    logger.info("Resampling...")
-
-    # a special case occur of the maps have same topology, but
-    # different masks
-    if self.compare_topology(other, strict=False):
-        self.values = other.values.copy()
-        return
-
-    svalues = np.ma.filled(self.values, fill_value=xtgeo.UNDEF)
-    ovalues = np.ma.filled(other.values, fill_value=xtgeo.UNDEF)
-
-    _cxtgeo.surf_resample(
-        other._ncol,
-        other._nrow,
-        other._xori,
-        other._xinc,
-        other._yori,
-        other._yinc,
-        other._yflip,
-        other._rotation,
-        ovalues,
-        self._ncol,
-        self._nrow,
-        self._xori,
-        self._xinc,
-        self._yori,
-        self._yinc,
-        self._yflip,
-        self._rotation,
-        svalues,
-        0 if not mask else 1,
-        2 if sampling == "nearest" else 0,
-    )
-
-    self.values = np.ma.masked_greater(svalues, xtgeo.UNDEF_LIMIT)
-
-    self.set_values1d(svalues)
-    self._filesrc = "Resampled"
-
-
-def distance_from_point(self, point=(0, 0), azimuth=0.0):
-    """Find distance bwteen point and surface."""
-    xpv, ypv = point
-
-    svalues = self.get_values1d()
-
-    # call C routine
-    ier = _cxtgeo.surf_get_dist_values(
-        self._xori,
-        self._xinc,
-        self._yori,
-        self._yinc,
-        self._ncol,
-        self._nrow,
-        self._rotation,
-        xpv,
-        ypv,
-        azimuth,
-        svalues,
-        0,
-    )
-
-    if ier != 0:
-        logger.error("Something went wrong...")
-        raise RuntimeError(f"Something went wrong in {__name__}")
-
-    self.set_values1d(svalues)
-
-
-def get_value_from_xy(self, point=(0.0, 0.0), sampling="bilinear"):
-    """Find surface value for point X Y."""
-
-    xcoord, ycoord = point
-
-    option = 0 if sampling == "bilinear" else 2
-
-    zcoord = _cxtgeo.surf_get_z_from_xy(
-        float(xcoord),
-        float(ycoord),
-        self.ncol,
-        self.nrow,
-        self.xori,
-        self.yori,
-        self.xinc,
-        self.yinc,
-        self.yflip,
-        self.rotation,
-        self.get_values1d(),
-        option,
-    )
-    if zcoord > xtgeo.UNDEF_LIMIT:
-        return None
-
-    return zcoord
-
-
-def get_xy_value_from_ij(self, iloc, jloc, zvalues=None):
-    """Find X Y value from I J index"""
-
-    if zvalues is None:
-        zvalues = self.get_values1d()
-
-    try:
-        ier, xval, yval, value = _cxtgeo.surf_xyz_from_ij(
-            iloc,
-            jloc,
-            self.xori,
-            self.xinc,
-            self.yori,
-            self.yinc,
-            self.ncol,
-            self.nrow,
-            self._yflip,
-            self.rotation,
-            zvalues,
-            0,
-        )
-    except XTGeoCLibError:
-        raise ValueError(f"Index i {iloc} and/or j {jloc} out of bounds")
-
-    if value > xtgeo.UNDEF_LIMIT:
-        value = None
-
-    return xval, yval, value
-
-
-def get_ij_values(self, zero_based=False, order="C", asmasked=False):
-    """Get I J values as numpy 2D arrays.
-
-    Args:
-        zero_based (bool): If True, first index is 0. False (1) is default.
-        order (str): 'C' or 'F' order (row vs column major)
-
-    """
-
-    ixn, jyn = np.indices((self._ncol, self._nrow))
-
-    if order == "F":
-        ixn = np.asfortranarray(ixn)
-        jyn = np.asfortranarray(jyn)
-
-    if not zero_based:
-        ixn += 1
-        jyn += 1
-
-    if asmasked:
-        ixn = ixn[~self.values.mask]
-        jyn = jyn[~self.values.mask]
-
-    return ixn, jyn
-
-
-def get_ij_values1d(self, zero_based=False, activeonly=True, order="C"):
-    """Get I J values as numpy 1D arrays.
-
-    Args:
-        zero_based (bool): If True, first index is 0. False (1) is default.
-        activeonly (bool): If True, only for active nodes
-        order (str): 'C' or 'F' order (row vs column major)
-
-    """
-
-    ixn, jyn = self.get_ij_values(zero_based=zero_based, order=order)
-
-    ixn = ixn.ravel(order=order)
-    jyn = jyn.ravel(order=order)
-
-    if activeonly:
-        tmask = ma.getmaskarray(self.get_values1d(order=order, asmasked=True))
-        ixn = ma.array(ixn, mask=tmask)
-        ixn = ixn[~ixn.mask]
-        jyn = ma.array(jyn, mask=tmask)
-        jyn = jyn[~jyn.mask]
-
-    return ixn, jyn
-
-
-def get_xy_values(self, order="C", asmasked=False):
-    """Get X Y coordinate values as numpy 2D arrays."""
-    nno = self.ncol * self.nrow
-
-    ier, xvals, yvals = _cxtgeo.surf_xy_as_values(
-        self.xori,
-        self.xinc,
-        self.yori,
-        self.yinc * self.yflip,
-        self.ncol,
-        self.nrow,
-        self.rotation,
-        nno,
-        nno,
-        0,
-    )
-    if ier != 0:
-        raise XTGeoCLibError(f"Error in surf_xy_as_values, error code: {ier}")
-
-    # reshape
-    xvals = xvals.reshape((self.ncol, self.nrow))
-    yvals = yvals.reshape((self.ncol, self.nrow))
-
-    if order == "F":
-        xvals = np.array(xvals, order="F")
-        yvals = np.array(yvals, order="F")
-
-    if asmasked:
-        tmpv = ma.filled(self.values, fill_value=np.nan)
-        tmpv = np.array(tmpv, order=order)
-        tmpv = ma.masked_invalid(tmpv)
-        mymask = ma.getmaskarray(tmpv)
-        xvals = ma.array(xvals, mask=mymask, order=order)
-        yvals = ma.array(yvals, mask=mymask, order=order)
-
-    return xvals, yvals
-
-
-def get_xy_values1d(self, order="C", activeonly=True):
-    """Get X Y coordinate values as numpy 1D arrays."""
-
-    asmasked = False
-    if activeonly:
-        asmasked = True
-
-    xvals, yvals = self.get_xy_values(order=order, asmasked=asmasked)
-
-    xvals = xvals.ravel(order=order)
-    yvals = yvals.ravel(order=order)
-
-    if activeonly:
-        xvals = xvals[~xvals.mask]
-        yvals = yvals[~yvals.mask]
-
-    return xvals, yvals
-
-
-def get_fence(self, xyfence, sampling="bilinear"):
-    """Get surface values along fence."""
-
-    cxarr = xyfence[:, 0]
-    cyarr = xyfence[:, 1]
-    czarr = xyfence[:, 2].copy()
-
-    sampleoptions = {"bilinear": 0, "nearest": 2}
-
-    # czarr will be updated "inplace":
-    istat = _cxtgeo.surf_get_zv_from_xyv(
-        cxarr,
-        cyarr,
-        czarr,
-        self.ncol,
-        self.nrow,
-        self.xori,
-        self.yori,
-        self.xinc,
-        self.yinc,
-        self.yflip,
-        self.rotation,
-        self.get_values1d(),
-        sampleoptions.get(sampling, 0),
-    )
-
-    if istat != 0:
-        logger.warning("Seem to be rotten")
-
-    xyfence[:, 2] = czarr
-    xyfence = ma.masked_greater(xyfence, xtgeo.UNDEF_LIMIT)
-    xyfence = ma.mask_rows(xyfence)
-
-    return xyfence
-
-
-def get_randomline(
-    self, fencespec, hincrement=None, atleast=5, nextend=2, sampling="bilinear"
-):
-    """Get surface values along fence."""
-
-    if hincrement is None and isinstance(fencespec, xtgeo.Polygons):
-        logger.info("Estimate hincrement from instance...")
-        fencespec = _get_randomline_fence(self, fencespec, hincrement, atleast, nextend)
-        logger.info("Estimate hincrement from instance... DONE")
-
-    if fencespec is None or fencespec is False:
-        return None
-
-    sampleoptions = {"bilinear": 0, "nearest": 2}
-
-    xcoords = fencespec[:, 0]
-    ycoords = fencespec[:, 1]
-    zcoords = fencespec[:, 2].copy()
-    hcoords = fencespec[:, 3]
-
-    # zcoords will be updated "inplace":
-    istat = _cxtgeo.surf_get_zv_from_xyv(
-        xcoords,
-        ycoords,
-        zcoords,
-        self.ncol,
-        self.nrow,
-        self.xori,
-        self.yori,
-        self.xinc,
-        self.yinc,
-        self.yflip,
-        self.rotation,
-        self.get_values1d(),
-        sampleoptions.get(sampling, 0),
-    )
-
-    if istat != 0:
-        logger.warning("Seem to be rotten")
-
-    zcoords[zcoords > xtgeo.UNDEF_LIMIT] = np.nan
-    arr = np.vstack([hcoords, zcoords]).T
-
-    return arr
-
-
-def _get_randomline_fence(self, fencespec, hincrement, atleast, nextend):
-    """Compute a resampled fence from a Polygons instance"""
-
-    if hincrement is None:
-        avgdxdy = 0.5 * (self.xinc + self.yinc)
-        distance = 0.5 * avgdxdy
-    else:
-        distance = hincrement
-
-    logger.info("Getting fence from a Polygons instance...")
-    fspec = fencespec.get_fence(
-        distance=distance, atleast=atleast, nextend=nextend, asnumpy=True
-    )
-    logger.info("Getting fence from a Polygons instance... DONE")
-    return fspec
-
-
-def operation_polygons(self, poly, value, opname="add", inside=True):
-    """Operations restricted to polygons"""
-
-    if not isinstance(poly, Polygons):
-        raise ValueError("The poly input is not a Polygons instance")
-    if opname not in VALID_OPER_POLYS:
-        raise ValueError(f"Operation key opname has invalid value: {opname}")
-
-    # make a copy of the RegularSurface which is used a "filter" or "proxy"
-    # value will be 1 inside polygons, 0 outside. Undef cells are kept as is
-
-    proxy = self.copy()
-    proxy.values *= 0.0
-    vals = proxy.get_values1d(fill_value=xtgeo.UNDEF)
-
-    # value could be a scalar or another surface; if another surface,
-    # must ensure same topology
-
-    if isinstance(value, type(self)):
-        if not self.compare_topology(value):
-            raise ValueError("Input is RegularSurface, but not same map " "topology")
-        value = value.values.copy()
-    else:
-        # turn scalar value into numpy array
-        value = self.values.copy() * 0 + value
-
-    idgroups = poly.dataframe.groupby(poly.pname)
-
-    for _, grp in idgroups:
-        xcor = grp[poly.xname].values
-        ycor = grp[poly.yname].values
-
-        ier = _cxtgeo.surf_setval_poly(
-            proxy.xori,
-            proxy.xinc,
-            proxy.yori,
-            proxy.yinc,
-            proxy.ncol,
-            proxy.nrow,
-            proxy.yflip,
-            proxy.rotation,
-            vals,
-            xcor,
-            ycor,
-            1.0,
-            0,
-        )
-        if ier == -9:
-            xtg.warn("Polygon is not closed")
-
-    proxy.set_values1d(vals)
-    proxyv = proxy.values.astype(np.int8)
-
-    proxytarget = 1
-    if not inside:
-        proxytarget = 0
-
-    tmp = None
-    if opname == "add":
-        tmp = self.values.copy() + value
-    elif opname == "sub":
-        tmp = self.values.copy() - value
-    elif opname == "mul":
-        tmp = self.values.copy() * value
-    elif opname == "div":
-        # Dividing a map of zero is always a hazzle; try to obtain 0.0
-        # as result in these cases
-        if 0.0 in value:
-            xtg.warn(
-                "Dividing a surface with value or surface with zero "
-                "elements; may get unexpected results, try to "
-                "achieve zero values as result!"
-            )
-        with np.errstate(divide="ignore", invalid="ignore"):
-            this = ma.filled(self.values, fill_value=1.0)
-            that = ma.filled(value, fill_value=1.0)
-            mask = ma.getmaskarray(self.values)
-            tmp = np.true_divide(this, that)
-            tmp = np.where(np.isinf(tmp), 0, tmp)
-            tmp = np.nan_to_num(tmp)
-            tmp = ma.array(tmp, mask=mask)
-
-    elif opname == "set":
-        tmp = value
-    elif opname == "eli":
-        tmp = value * 0 + xtgeo.UNDEF
-        tmp = ma.masked_greater(tmp, xtgeo.UNDEF_LIMIT)
-
-    self.values[proxyv == proxytarget] = tmp[proxyv == proxytarget]
-    del tmp
+# coding: utf-8
+"""Various operations"""
+
+import numbers
+
+import numpy as np
+import numpy.ma as ma
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo  # type: ignore
+from xtgeo import XTGeoCLibError
+from xtgeo.common import XTGeoDialog
+from xtgeo.xyz import Polygons
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+VALID_OPER = (
+    "add",
+    "iadd",
+    "sub",
+    "isub",
+    "mul",
+    "imul",
+    "div",
+    "idiv",
+    "lt",
+    "gt",
+    "le",
+    "eq",
+    "ne",
+)
+
+VALID_OPER_POLYS = (
+    "add",
+    "sub",
+    "mul",
+    "div",
+    "set",
+    "eli",
+)
+
+
+def operations_two(self, other, oper="add"):  # pylint: disable=too-many-branches
+    """General operations between two maps"""
+
+    other = _check_other(self, other)
+
+    okstatus = self.compare_topology(other)
+
+    useother = other
+    if not okstatus:
+        # to avoid that the "other" instance is changed
+        useother = self.copy()
+        useother.resample(other)
+
+    if oper not in VALID_OPER:
+        raise ValueError(f"Operation key oper has invalid value: {oper}")
+
+    retvalue = None
+
+    if oper == "add":
+        self.values = self.values + useother.values
+    elif oper == "iadd":
+        self.values += useother.values
+    elif oper == "sub":
+        self.values = self.values - useother.values
+    elif oper == "isub":
+        self._values -= useother._values
+    elif oper == "mul":
+        self.values = self.values * useother.values
+    elif oper == "imul":
+        self._values *= useother._values
+    elif oper == "div":
+        self.values = self.values / useother.values
+    elif oper == "idiv":
+        self._values /= useother._values
+
+    # comparisons:
+    elif oper == "lt":
+        retvalue = self.values < other.values
+    elif oper == "gt":
+        retvalue = self.values > other.values
+    elif oper == "le":
+        retvalue = self.values <= other.values
+    elif oper == "ge":
+        retvalue = self.values >= other.values
+    elif oper == "eq":
+        retvalue = self.values == other.values
+    elif oper == "ne":
+        retvalue = self.values != other.values
+
+    if useother is not other:
+        del useother
+
+    self._filesrc = "Calculated"
+
+    # return None or a boolean array
+    return retvalue
+
+
+def _check_other(self, other):
+    """Will convert an other scalar to a valid numpy array"""
+
+    if isinstance(other, numbers.Number):
+        vals = other
+        other = self.copy()
+        other.values *= 0
+        other.values += vals
+        other._filesrc = None
+
+    return other
+
+
+def resample(self, other, mask=True, sampling="bilinear"):
+    """Resample from other surface object to this surf."""
+
+    logger.info("Resampling...")
+
+    # a special case occur of the maps have same topology, but
+    # different masks
+    if self.compare_topology(other, strict=False):
+        self.values = other.values.copy()
+        return
+
+    svalues = np.ma.filled(self.values, fill_value=xtgeo.UNDEF)
+    ovalues = np.ma.filled(other.values, fill_value=xtgeo.UNDEF)
+
+    _cxtgeo.surf_resample(
+        other._ncol,
+        other._nrow,
+        other._xori,
+        other._xinc,
+        other._yori,
+        other._yinc,
+        other._yflip,
+        other._rotation,
+        ovalues,
+        self._ncol,
+        self._nrow,
+        self._xori,
+        self._xinc,
+        self._yori,
+        self._yinc,
+        self._yflip,
+        self._rotation,
+        svalues,
+        0 if not mask else 1,
+        2 if sampling == "nearest" else 0,
+    )
+
+    self.values = np.ma.masked_greater(svalues, xtgeo.UNDEF_LIMIT)
+
+    self.set_values1d(svalues)
+    self._filesrc = "Resampled"
+
+
+def distance_from_point(self, point=(0, 0), azimuth=0.0):
+    """Find distance bwteen point and surface."""
+    xpv, ypv = point
+
+    svalues = self.get_values1d()
+
+    # call C routine
+    ier = _cxtgeo.surf_get_dist_values(
+        self._xori,
+        self._xinc,
+        self._yori,
+        self._yinc,
+        self._ncol,
+        self._nrow,
+        self._rotation,
+        xpv,
+        ypv,
+        azimuth,
+        svalues,
+        0,
+    )
+
+    if ier != 0:
+        logger.error("Something went wrong...")
+        raise RuntimeError(f"Something went wrong in {__name__}")
+
+    self.set_values1d(svalues)
+
+
+def get_value_from_xy(self, point=(0.0, 0.0), sampling="bilinear"):
+    """Find surface value for point X Y."""
+
+    xcoord, ycoord = point
+
+    option = 0 if sampling == "bilinear" else 2
+
+    zcoord = _cxtgeo.surf_get_z_from_xy(
+        float(xcoord),
+        float(ycoord),
+        self.ncol,
+        self.nrow,
+        self.xori,
+        self.yori,
+        self.xinc,
+        self.yinc,
+        self.yflip,
+        self.rotation,
+        self.get_values1d(),
+        option,
+    )
+    if zcoord > xtgeo.UNDEF_LIMIT:
+        return None
+
+    return zcoord
+
+
+def get_xy_value_from_ij(self, iloc, jloc, zvalues=None):
+    """Find X Y value from I J index"""
+
+    if zvalues is None:
+        zvalues = self.get_values1d()
+
+    try:
+        ier, xval, yval, value = _cxtgeo.surf_xyz_from_ij(
+            iloc,
+            jloc,
+            self.xori,
+            self.xinc,
+            self.yori,
+            self.yinc,
+            self.ncol,
+            self.nrow,
+            self._yflip,
+            self.rotation,
+            zvalues,
+            0,
+        )
+    except XTGeoCLibError:
+        raise ValueError(f"Index i {iloc} and/or j {jloc} out of bounds")
+
+    if value > xtgeo.UNDEF_LIMIT:
+        value = None
+
+    return xval, yval, value
+
+
+def get_ij_values(self, zero_based=False, order="C", asmasked=False):
+    """Get I J values as numpy 2D arrays.
+
+    Args:
+        zero_based (bool): If True, first index is 0. False (1) is default.
+        order (str): 'C' or 'F' order (row vs column major)
+
+    """
+
+    ixn, jyn = np.indices((self._ncol, self._nrow))
+
+    if order == "F":
+        ixn = np.asfortranarray(ixn)
+        jyn = np.asfortranarray(jyn)
+
+    if not zero_based:
+        ixn += 1
+        jyn += 1
+
+    if asmasked:
+        ixn = ixn[~self.values.mask]
+        jyn = jyn[~self.values.mask]
+
+    return ixn, jyn
+
+
+def get_ij_values1d(self, zero_based=False, activeonly=True, order="C"):
+    """Get I J values as numpy 1D arrays.
+
+    Args:
+        zero_based (bool): If True, first index is 0. False (1) is default.
+        activeonly (bool): If True, only for active nodes
+        order (str): 'C' or 'F' order (row vs column major)
+
+    """
+
+    ixn, jyn = self.get_ij_values(zero_based=zero_based, order=order)
+
+    ixn = ixn.ravel(order=order)
+    jyn = jyn.ravel(order=order)
+
+    if activeonly:
+        tmask = ma.getmaskarray(self.get_values1d(order=order, asmasked=True))
+        ixn = ma.array(ixn, mask=tmask)
+        ixn = ixn[~ixn.mask]
+        jyn = ma.array(jyn, mask=tmask)
+        jyn = jyn[~jyn.mask]
+
+    return ixn, jyn
+
+
+def get_xy_values(self, order="C", asmasked=False):
+    """Get X Y coordinate values as numpy 2D arrays."""
+    nno = self.ncol * self.nrow
+
+    ier, xvals, yvals = _cxtgeo.surf_xy_as_values(
+        self.xori,
+        self.xinc,
+        self.yori,
+        self.yinc * self.yflip,
+        self.ncol,
+        self.nrow,
+        self.rotation,
+        nno,
+        nno,
+        0,
+    )
+    if ier != 0:
+        raise XTGeoCLibError(f"Error in surf_xy_as_values, error code: {ier}")
+
+    # reshape
+    xvals = xvals.reshape((self.ncol, self.nrow))
+    yvals = yvals.reshape((self.ncol, self.nrow))
+
+    if order == "F":
+        xvals = np.array(xvals, order="F")
+        yvals = np.array(yvals, order="F")
+
+    if asmasked:
+        tmpv = ma.filled(self.values, fill_value=np.nan)
+        tmpv = np.array(tmpv, order=order)
+        tmpv = ma.masked_invalid(tmpv)
+        mymask = ma.getmaskarray(tmpv)
+        xvals = ma.array(xvals, mask=mymask, order=order)
+        yvals = ma.array(yvals, mask=mymask, order=order)
+
+    return xvals, yvals
+
+
+def get_xy_values1d(self, order="C", activeonly=True):
+    """Get X Y coordinate values as numpy 1D arrays."""
+
+    asmasked = False
+    if activeonly:
+        asmasked = True
+
+    xvals, yvals = self.get_xy_values(order=order, asmasked=asmasked)
+
+    xvals = xvals.ravel(order=order)
+    yvals = yvals.ravel(order=order)
+
+    if activeonly:
+        xvals = xvals[~xvals.mask]
+        yvals = yvals[~yvals.mask]
+
+    return xvals, yvals
+
+
+def get_fence(self, xyfence, sampling="bilinear"):
+    """Get surface values along fence."""
+
+    cxarr = xyfence[:, 0]
+    cyarr = xyfence[:, 1]
+    czarr = xyfence[:, 2].copy()
+
+    sampleoptions = {"bilinear": 0, "nearest": 2}
+
+    # czarr will be updated "inplace":
+    istat = _cxtgeo.surf_get_zv_from_xyv(
+        cxarr,
+        cyarr,
+        czarr,
+        self.ncol,
+        self.nrow,
+        self.xori,
+        self.yori,
+        self.xinc,
+        self.yinc,
+        self.yflip,
+        self.rotation,
+        self.get_values1d(),
+        sampleoptions.get(sampling, 0),
+    )
+
+    if istat != 0:
+        logger.warning("Seem to be rotten")
+
+    xyfence[:, 2] = czarr
+    xyfence = ma.masked_greater(xyfence, xtgeo.UNDEF_LIMIT)
+    xyfence = ma.mask_rows(xyfence)
+
+    return xyfence
+
+
+def get_randomline(
+    self, fencespec, hincrement=None, atleast=5, nextend=2, sampling="bilinear"
+):
+    """Get surface values along fence."""
+
+    if hincrement is None and isinstance(fencespec, xtgeo.Polygons):
+        logger.info("Estimate hincrement from instance...")
+        fencespec = _get_randomline_fence(self, fencespec, hincrement, atleast, nextend)
+        logger.info("Estimate hincrement from instance... DONE")
+
+    if fencespec is None or fencespec is False:
+        return None
+
+    sampleoptions = {"bilinear": 0, "nearest": 2}
+
+    xcoords = fencespec[:, 0]
+    ycoords = fencespec[:, 1]
+    zcoords = fencespec[:, 2].copy()
+    hcoords = fencespec[:, 3]
+
+    # zcoords will be updated "inplace":
+    istat = _cxtgeo.surf_get_zv_from_xyv(
+        xcoords,
+        ycoords,
+        zcoords,
+        self.ncol,
+        self.nrow,
+        self.xori,
+        self.yori,
+        self.xinc,
+        self.yinc,
+        self.yflip,
+        self.rotation,
+        self.get_values1d(),
+        sampleoptions.get(sampling, 0),
+    )
+
+    if istat != 0:
+        logger.warning("Seem to be rotten")
+
+    zcoords[zcoords > xtgeo.UNDEF_LIMIT] = np.nan
+    arr = np.vstack([hcoords, zcoords]).T
+
+    return arr
+
+
+def _get_randomline_fence(self, fencespec, hincrement, atleast, nextend):
+    """Compute a resampled fence from a Polygons instance"""
+
+    if hincrement is None:
+        avgdxdy = 0.5 * (self.xinc + self.yinc)
+        distance = 0.5 * avgdxdy
+    else:
+        distance = hincrement
+
+    logger.info("Getting fence from a Polygons instance...")
+    fspec = fencespec.get_fence(
+        distance=distance, atleast=atleast, nextend=nextend, asnumpy=True
+    )
+    logger.info("Getting fence from a Polygons instance... DONE")
+    return fspec
+
+
+def operation_polygons(self, poly, value, opname="add", inside=True):
+    """Operations restricted to polygons"""
+
+    if not isinstance(poly, Polygons):
+        raise ValueError("The poly input is not a Polygons instance")
+    if opname not in VALID_OPER_POLYS:
+        raise ValueError(f"Operation key opname has invalid value: {opname}")
+
+    # make a copy of the RegularSurface which is used a "filter" or "proxy"
+    # value will be 1 inside polygons, 0 outside. Undef cells are kept as is
+
+    proxy = self.copy()
+    proxy.values *= 0.0
+    vals = proxy.get_values1d(fill_value=xtgeo.UNDEF)
+
+    # value could be a scalar or another surface; if another surface,
+    # must ensure same topology
+
+    if isinstance(value, type(self)):
+        if not self.compare_topology(value):
+            raise ValueError("Input is RegularSurface, but not same map " "topology")
+        value = value.values.copy()
+    else:
+        # turn scalar value into numpy array
+        value = self.values.copy() * 0 + value
+
+    idgroups = poly.dataframe.groupby(poly.pname)
+
+    for _, grp in idgroups:
+        xcor = grp[poly.xname].values
+        ycor = grp[poly.yname].values
+
+        ier = _cxtgeo.surf_setval_poly(
+            proxy.xori,
+            proxy.xinc,
+            proxy.yori,
+            proxy.yinc,
+            proxy.ncol,
+            proxy.nrow,
+            proxy.yflip,
+            proxy.rotation,
+            vals,
+            xcor,
+            ycor,
+            1.0,
+            0,
+        )
+        if ier == -9:
+            xtg.warn("Polygon is not closed")
+
+    proxy.set_values1d(vals)
+    proxyv = proxy.values.astype(np.int8)
+
+    proxytarget = 1
+    if not inside:
+        proxytarget = 0
+
+    tmp = None
+    if opname == "add":
+        tmp = self.values.copy() + value
+    elif opname == "sub":
+        tmp = self.values.copy() - value
+    elif opname == "mul":
+        tmp = self.values.copy() * value
+    elif opname == "div":
+        # Dividing a map of zero is always a hazzle; try to obtain 0.0
+        # as result in these cases
+        if 0.0 in value:
+            xtg.warn(
+                "Dividing a surface with value or surface with zero "
+                "elements; may get unexpected results, try to "
+                "achieve zero values as result!"
+            )
+        with np.errstate(divide="ignore", invalid="ignore"):
+            this = ma.filled(self.values, fill_value=1.0)
+            that = ma.filled(value, fill_value=1.0)
+            mask = ma.getmaskarray(self.values)
+            tmp = np.true_divide(this, that)
+            tmp = np.where(np.isinf(tmp), 0, tmp)
+            tmp = np.nan_to_num(tmp)
+            tmp = ma.array(tmp, mask=mask)
+
+    elif opname == "set":
+        tmp = value
+    elif opname == "eli":
+        tmp = value * 0 + xtgeo.UNDEF
+        tmp = ma.masked_greater(tmp, xtgeo.UNDEF_LIMIT)
+
+    self.values[proxyv == proxytarget] = tmp[proxyv == proxytarget]
+    del tmp
```

## xtgeo/surface/_regsurf_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,234 +1,234 @@
-# coding: utf-8
-"""Roxar API functions for XTGeo RegularSurface."""
-import os
-import tempfile
-
-from xtgeo import RoxUtils
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-VALID_STYPES = ["horizons", "zones", "clipboard", "general2d_data", "trends"]
-
-
-def _check_stypes_names_category(roxutils, stype, name, category):
-    """General check of some input values."""
-    stype = stype.lower()
-
-    if stype not in VALID_STYPES:
-        raise ValueError(
-            f"Given stype {stype} is not supported, legal stypes are: {VALID_STYPES}"
-        )
-
-    if not name:
-        raise ValueError("The name is missing or empty.")
-
-    if stype in ("horizons", "zones") and (name is None or not category):
-        raise ValueError(
-            "Need to spesify both name and category for horizons and zones"
-        )
-
-    if stype == "general2d_data" and not roxutils.version_required("1.6"):
-        raise NotImplementedError(
-            "API Support for general2d_data is missing in this RMS version"
-            f"(current API version is {roxutils.roxversion} - required is 1.6"
-        )
-
-
-def import_horizon_roxapi(
-    project, name, category, stype, realisation
-):  # pragma: no cover
-    """Import a Horizon surface via ROXAR API spec. to xtgeo."""
-    roxutils = RoxUtils(project, readonly=True)
-
-    _check_stypes_names_category(roxutils, stype, name, category)
-
-    proj = roxutils.project
-    args = _roxapi_import_surface(proj, name, category, stype, realisation)
-
-    roxutils.safe_close()
-    return args
-
-
-def _roxapi_import_surface(
-    proj, name, category, stype, realisation
-):  # pragma: no cover
-    args = {}
-    args["name"] = name
-
-    if stype == "horizons":
-        if name not in proj.horizons:
-            raise ValueError(f"Name {name} is not within Horizons")
-        if category not in proj.horizons.representations:
-            raise ValueError(f"Category {category} is not within Horizons categories")
-        try:
-            rox = proj.horizons[name][category].get_grid(realisation)
-            args.update(_roxapi_horizon_to_xtgeo(rox))
-        except KeyError as kwe:
-            logger.error(kwe)
-
-    elif stype == "zones":
-        if name not in proj.zones:
-            raise ValueError(f"Name {name} is not within Zones")
-        if category not in proj.zones.representations:
-            raise ValueError(f"Category {category} is not within Zones categories")
-        try:
-            rox = proj.zones[name][category].get_grid(realisation)
-            args.update(_roxapi_horizon_to_xtgeo(rox))
-        except KeyError as kwe:
-            logger.error(kwe)
-
-    elif stype in ("clipboard", "general2d_data"):
-        styperef = getattr(proj, stype)
-        if category:
-            if "|" in category:
-                folders = category.split("|")
-            else:
-                folders = category.split("/")
-            rox = styperef.folders[folders]
-        else:
-            rox = styperef
-
-        roxsurf = rox[name].get_grid(realisation)
-        args.update(_roxapi_horizon_to_xtgeo(roxsurf))
-
-    elif stype == "trends":
-        if name not in proj.trends.surfaces:
-            logger.info("Name %s is not present in trends", name)
-            raise ValueError(f"Name {name} is not within Trends")
-        rox = proj.trends.surfaces[name]
-
-        roxsurf = rox.get_grid(realisation)
-        args.update(_roxapi_horizon_to_xtgeo(roxsurf))
-
-    else:
-        raise ValueError(f"Invalid stype given: {stype}")  # should never reach here
-    return args
-
-
-def _roxapi_horizon_to_xtgeo(rox):  # pragma: no cover
-    """Tranforming surfaces from ROXAPI to XTGeo object."""
-    # local function
-    args = {}
-    logger.info("Surface from roxapi to xtgeo...")
-    args["xori"], args["yori"] = rox.origin
-    args["ncol"], args["nrow"] = rox.dimensions
-    args["xinc"], args["yinc"] = rox.increment
-    args["rotation"] = rox.rotation
-
-    args["values"] = rox.get_values()
-    logger.info("Surface from roxapi to xtgeo... DONE")
-    return args
-
-
-def export_horizon_roxapi(
-    self, project, name, category, stype, realisation
-):  # pragma: no cover
-    """Export (store) a Horizon surface to RMS via ROXAR API spec."""
-    roxutils = RoxUtils(project, readonly=False)
-
-    _check_stypes_names_category(roxutils, stype, name, category)
-
-    logger.info("Surface from xtgeo to roxapi...")
-    _roxapi_export_surface(self, roxutils.project, name, category, stype, realisation)
-
-    if roxutils._roxexternal:
-        roxutils.project.save()
-
-    logger.info("Surface from xtgeo to roxapi... DONE")
-    roxutils.safe_close()
-
-
-def _roxapi_export_surface(
-    self, proj, name, category, stype, realisation
-):  # pragma: no cover
-    if stype == "horizons":
-        if name not in proj.horizons:
-            raise ValueError(f"Name {name} is not within Horizons")
-        if category not in proj.horizons.representations:
-            raise ValueError(f"Category {category} is not within Horizons categories")
-        try:
-            roxroot = proj.horizons[name][category]
-            roxg = _xtgeo_to_roxapi_grid(self)
-            roxg.set_values(self.values)
-            roxroot.set_grid(roxg, realisation=realisation)
-        except KeyError as kwe:
-            logger.error(kwe)
-
-    elif stype == "zones":
-        if name not in proj.zones:
-            raise ValueError(f"Name {name} is not within Zones")
-        if category not in proj.zones.representations:
-            raise ValueError(f"Category {category} is not within Zones categories")
-        try:
-            roxroot = proj.zones[name][category]
-            roxg = _xtgeo_to_roxapi_grid(self)
-            roxg.set_values(self.values)
-            roxroot.set_grid(roxg)
-        except KeyError as kwe:
-            logger.error(kwe)
-
-    elif stype in ("clipboard", "general2d_data"):
-        folders = []
-        if category:
-            if "|" in category:
-                folders = category.split("|")
-            else:
-                folders = category.split("/")
-        styperef = getattr(proj, stype)
-        if folders:
-            styperef.folders.create(folders)
-
-        roxroot = styperef.create_surface(name, folders)
-        roxg = _xtgeo_to_roxapi_grid(self)
-        roxg.set_values(self.values)
-        roxroot.set_grid(roxg)
-
-    elif stype == "trends":
-        if name not in proj.trends.surfaces:
-            logger.info("Name %s is not present in trends", name)
-            raise ValueError(
-                f"Name {name} is not within Trends (it must exist in advance!)"
-            )
-        # here a workound; trends.surfaces are read-only in Roxar API, but is seems
-        # that load() in RMS is an (undocumented?) workaround...
-        try:
-            import roxar  # pylint: disable=import-outside-toplevel
-        except ImportError as err:
-            raise ImportError(
-                "roxar not available, this functionality is not available"
-            ) from err
-
-        roxsurf = proj.trends.surfaces[name]
-        with tempfile.TemporaryDirectory() as tmpdir:
-            logger.info("Made a tmp folder: %s", tmpdir)
-            self.to_file(os.path.join(tmpdir, "gxx.gri"), fformat="irap_binary")
-
-            roxsurf.load(os.path.join(tmpdir, "gxx.gri"), roxar.FileFormat.ROXAR_BINARY)
-
-    else:
-        raise ValueError(f"Invalid stype given: {stype}")  # should never reach here
-
-
-def _xtgeo_to_roxapi_grid(self):  # pragma: no cover
-    # Create a 2D grid
-    try:
-        import roxar  # pylint: disable=import-error, import-outside-toplevel
-    except ImportError as err:
-        raise ImportError(
-            "roxar not available, this functionality is not available"
-        ) from err
-
-    grid2d = roxar.RegularGrid2D.create(
-        x_origin=self.xori,
-        y_origin=self.yori,
-        i_inc=self.xinc,
-        j_inc=self.yinc,
-        ni=self.ncol,
-        nj=self.nrow,
-        rotation=self.rotation,
-    )
-
-    return grid2d
+# coding: utf-8
+"""Roxar API functions for XTGeo RegularSurface."""
+import os
+import tempfile
+
+from xtgeo import RoxUtils
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+VALID_STYPES = ["horizons", "zones", "clipboard", "general2d_data", "trends"]
+
+
+def _check_stypes_names_category(roxutils, stype, name, category):
+    """General check of some input values."""
+    stype = stype.lower()
+
+    if stype not in VALID_STYPES:
+        raise ValueError(
+            f"Given stype {stype} is not supported, legal stypes are: {VALID_STYPES}"
+        )
+
+    if not name:
+        raise ValueError("The name is missing or empty.")
+
+    if stype in ("horizons", "zones") and (name is None or not category):
+        raise ValueError(
+            "Need to spesify both name and category for horizons and zones"
+        )
+
+    if stype == "general2d_data" and not roxutils.version_required("1.6"):
+        raise NotImplementedError(
+            "API Support for general2d_data is missing in this RMS version"
+            f"(current API version is {roxutils.roxversion} - required is 1.6"
+        )
+
+
+def import_horizon_roxapi(
+    project, name, category, stype, realisation
+):  # pragma: no cover
+    """Import a Horizon surface via ROXAR API spec. to xtgeo."""
+    roxutils = RoxUtils(project, readonly=True)
+
+    _check_stypes_names_category(roxutils, stype, name, category)
+
+    proj = roxutils.project
+    args = _roxapi_import_surface(proj, name, category, stype, realisation)
+
+    roxutils.safe_close()
+    return args
+
+
+def _roxapi_import_surface(
+    proj, name, category, stype, realisation
+):  # pragma: no cover
+    args = {}
+    args["name"] = name
+
+    if stype == "horizons":
+        if name not in proj.horizons:
+            raise ValueError(f"Name {name} is not within Horizons")
+        if category not in proj.horizons.representations:
+            raise ValueError(f"Category {category} is not within Horizons categories")
+        try:
+            rox = proj.horizons[name][category].get_grid(realisation)
+            args.update(_roxapi_horizon_to_xtgeo(rox))
+        except KeyError as kwe:
+            logger.error(kwe)
+
+    elif stype == "zones":
+        if name not in proj.zones:
+            raise ValueError(f"Name {name} is not within Zones")
+        if category not in proj.zones.representations:
+            raise ValueError(f"Category {category} is not within Zones categories")
+        try:
+            rox = proj.zones[name][category].get_grid(realisation)
+            args.update(_roxapi_horizon_to_xtgeo(rox))
+        except KeyError as kwe:
+            logger.error(kwe)
+
+    elif stype in ("clipboard", "general2d_data"):
+        styperef = getattr(proj, stype)
+        if category:
+            if "|" in category:
+                folders = category.split("|")
+            else:
+                folders = category.split("/")
+            rox = styperef.folders[folders]
+        else:
+            rox = styperef
+
+        roxsurf = rox[name].get_grid(realisation)
+        args.update(_roxapi_horizon_to_xtgeo(roxsurf))
+
+    elif stype == "trends":
+        if name not in proj.trends.surfaces:
+            logger.info("Name %s is not present in trends", name)
+            raise ValueError(f"Name {name} is not within Trends")
+        rox = proj.trends.surfaces[name]
+
+        roxsurf = rox.get_grid(realisation)
+        args.update(_roxapi_horizon_to_xtgeo(roxsurf))
+
+    else:
+        raise ValueError(f"Invalid stype given: {stype}")  # should never reach here
+    return args
+
+
+def _roxapi_horizon_to_xtgeo(rox):  # pragma: no cover
+    """Tranforming surfaces from ROXAPI to XTGeo object."""
+    # local function
+    args = {}
+    logger.info("Surface from roxapi to xtgeo...")
+    args["xori"], args["yori"] = rox.origin
+    args["ncol"], args["nrow"] = rox.dimensions
+    args["xinc"], args["yinc"] = rox.increment
+    args["rotation"] = rox.rotation
+
+    args["values"] = rox.get_values()
+    logger.info("Surface from roxapi to xtgeo... DONE")
+    return args
+
+
+def export_horizon_roxapi(
+    self, project, name, category, stype, realisation
+):  # pragma: no cover
+    """Export (store) a Horizon surface to RMS via ROXAR API spec."""
+    roxutils = RoxUtils(project, readonly=False)
+
+    _check_stypes_names_category(roxutils, stype, name, category)
+
+    logger.info("Surface from xtgeo to roxapi...")
+    _roxapi_export_surface(self, roxutils.project, name, category, stype, realisation)
+
+    if roxutils._roxexternal:
+        roxutils.project.save()
+
+    logger.info("Surface from xtgeo to roxapi... DONE")
+    roxutils.safe_close()
+
+
+def _roxapi_export_surface(
+    self, proj, name, category, stype, realisation
+):  # pragma: no cover
+    if stype == "horizons":
+        if name not in proj.horizons:
+            raise ValueError(f"Name {name} is not within Horizons")
+        if category not in proj.horizons.representations:
+            raise ValueError(f"Category {category} is not within Horizons categories")
+        try:
+            roxroot = proj.horizons[name][category]
+            roxg = _xtgeo_to_roxapi_grid(self)
+            roxg.set_values(self.values)
+            roxroot.set_grid(roxg, realisation=realisation)
+        except KeyError as kwe:
+            logger.error(kwe)
+
+    elif stype == "zones":
+        if name not in proj.zones:
+            raise ValueError(f"Name {name} is not within Zones")
+        if category not in proj.zones.representations:
+            raise ValueError(f"Category {category} is not within Zones categories")
+        try:
+            roxroot = proj.zones[name][category]
+            roxg = _xtgeo_to_roxapi_grid(self)
+            roxg.set_values(self.values)
+            roxroot.set_grid(roxg)
+        except KeyError as kwe:
+            logger.error(kwe)
+
+    elif stype in ("clipboard", "general2d_data"):
+        folders = []
+        if category:
+            if "|" in category:
+                folders = category.split("|")
+            else:
+                folders = category.split("/")
+        styperef = getattr(proj, stype)
+        if folders:
+            styperef.folders.create(folders)
+
+        roxroot = styperef.create_surface(name, folders)
+        roxg = _xtgeo_to_roxapi_grid(self)
+        roxg.set_values(self.values)
+        roxroot.set_grid(roxg)
+
+    elif stype == "trends":
+        if name not in proj.trends.surfaces:
+            logger.info("Name %s is not present in trends", name)
+            raise ValueError(
+                f"Name {name} is not within Trends (it must exist in advance!)"
+            )
+        # here a workound; trends.surfaces are read-only in Roxar API, but is seems
+        # that load() in RMS is an (undocumented?) workaround...
+        try:
+            import roxar  # pylint: disable=import-outside-toplevel
+        except ImportError as err:
+            raise ImportError(
+                "roxar not available, this functionality is not available"
+            ) from err
+
+        roxsurf = proj.trends.surfaces[name]
+        with tempfile.TemporaryDirectory() as tmpdir:
+            logger.info("Made a tmp folder: %s", tmpdir)
+            self.to_file(os.path.join(tmpdir, "gxx.gri"), fformat="irap_binary")
+
+            roxsurf.load(os.path.join(tmpdir, "gxx.gri"), roxar.FileFormat.ROXAR_BINARY)
+
+    else:
+        raise ValueError(f"Invalid stype given: {stype}")  # should never reach here
+
+
+def _xtgeo_to_roxapi_grid(self):  # pragma: no cover
+    # Create a 2D grid
+    try:
+        import roxar  # pylint: disable=import-error, import-outside-toplevel
+    except ImportError as err:
+        raise ImportError(
+            "roxar not available, this functionality is not available"
+        ) from err
+
+    grid2d = roxar.RegularGrid2D.create(
+        x_origin=self.xori,
+        y_origin=self.yori,
+        i_inc=self.xinc,
+        j_inc=self.yinc,
+        ni=self.ncol,
+        nj=self.nrow,
+        rotation=self.rotation,
+    )
+
+    return grid2d
```

## xtgeo/surface/_regsurf_utils.py

 * *Ordering differences only*

```diff
@@ -1,55 +1,55 @@
-"""RegularSurface utilities"""
-
-
-import numpy as np
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.common.calc import _swap_axes
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-# pylint: disable=protected-access
-
-
-def swapaxes(self):
-    """Swap the axes columns vs rows, keep origin. Will change yflip."""
-    self._rotation, self._yflip, swapped_values = _swap_axes(
-        self._rotation,
-        self._yflip,
-        values=self.values.filled(xtgeo.UNDEF),
-    )
-    self._ncol, self._nrow = self._nrow, self._ncol
-    self._xinc, self._yinc = self._yinc, self._xinc
-    self.values = swapped_values["values"]
-
-
-def autocrop(self):
-    """Crop surface by looking at undefined areas, update instance"""
-
-    minvalue = self.values.min()
-
-    if np.isnan(minvalue):
-        return
-
-    arrx, arry = np.ma.where(self.values >= minvalue)
-
-    imin = int(arrx.min())
-    imax = int(arrx.max())
-
-    jmin = int(arry.min())
-    jmax = int(arry.max())
-
-    xori, yori, _dummy = self.get_xy_value_from_ij(imin + 1, jmin + 1)
-
-    ncol = imax - imin + 1
-    nrow = jmax - jmin + 1
-
-    self._values = self.values[imin : imax + 1, jmin : jmax + 1]
-    self._ilines = self.ilines[imin : imax + 1]
-    self._xlines = self.xlines[jmin : jmax + 1]
-    self._ncol = ncol
-    self._nrow = nrow
-    self._xori = xori
-    self._yori = yori
+"""RegularSurface utilities"""
+
+
+import numpy as np
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.common.calc import _swap_axes
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+# pylint: disable=protected-access
+
+
+def swapaxes(self):
+    """Swap the axes columns vs rows, keep origin. Will change yflip."""
+    self._rotation, self._yflip, swapped_values = _swap_axes(
+        self._rotation,
+        self._yflip,
+        values=self.values.filled(xtgeo.UNDEF),
+    )
+    self._ncol, self._nrow = self._nrow, self._ncol
+    self._xinc, self._yinc = self._yinc, self._xinc
+    self.values = swapped_values["values"]
+
+
+def autocrop(self):
+    """Crop surface by looking at undefined areas, update instance"""
+
+    minvalue = self.values.min()
+
+    if np.isnan(minvalue):
+        return
+
+    arrx, arry = np.ma.where(self.values >= minvalue)
+
+    imin = int(arrx.min())
+    imax = int(arrx.max())
+
+    jmin = int(arry.min())
+    jmax = int(arry.max())
+
+    xori, yori, _dummy = self.get_xy_value_from_ij(imin + 1, jmin + 1)
+
+    ncol = imax - imin + 1
+    nrow = jmax - jmin + 1
+
+    self._values = self.values[imin : imax + 1, jmin : jmax + 1]
+    self._ilines = self.ilines[imin : imax + 1]
+    self._xlines = self.xlines[jmin : jmax + 1]
+    self._ncol = ncol
+    self._nrow = nrow
+    self._xori = xori
+    self._yori = yori
```

## xtgeo/surface/_surfs_import.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-"""Import multiple surfaces"""
-# pylint: disable=protected-access
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def from_grid3d(grid, subgrids, rfactor):
-    """Get surfaces, subtype and order from 3D grid, including subgrids"""
-
-    logger.info("Extracting surface from 3D grid...")
-
-    # determine layers
-    layers = []
-    names = []
-    if subgrids and grid.subgrids is not None:
-        last = ""
-        for sgrd, srange in grid.subgrids.items():
-            layers.append(str(srange[0]) + "_top")
-            names.append(sgrd + "_top")
-            last = str(srange[-1])
-            lastname = sgrd
-        # base of last layer
-        layers.append(last + "_base")
-        names.append(lastname + "_base")
-    else:
-        layers.append("top")
-        names.append("top")
-        layers.append("base")
-        names.append("base")
-
-    # next extract these layers
-    layerstack = []
-    for inum, lay in enumerate(layers):
-        layer = xtgeo.surface_from_grid3d(
-            grid, template=None, where=lay, rfactor=rfactor
-        )
-        layer.name = names[inum]
-        layerstack.append(layer)
-
-    logger.info("Extracting surface from 3D grid... DONE")
-
-    return layerstack, "tops", "stratigraphic"
+"""Import multiple surfaces"""
+# pylint: disable=protected-access
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def from_grid3d(grid, subgrids, rfactor):
+    """Get surfaces, subtype and order from 3D grid, including subgrids"""
+
+    logger.info("Extracting surface from 3D grid...")
+
+    # determine layers
+    layers = []
+    names = []
+    if subgrids and grid.subgrids is not None:
+        last = ""
+        for sgrd, srange in grid.subgrids.items():
+            layers.append(str(srange[0]) + "_top")
+            names.append(sgrd + "_top")
+            last = str(srange[-1])
+            lastname = sgrd
+        # base of last layer
+        layers.append(last + "_base")
+        names.append(lastname + "_base")
+    else:
+        layers.append("top")
+        names.append("top")
+        layers.append("base")
+        names.append("base")
+
+    # next extract these layers
+    layerstack = []
+    for inum, lay in enumerate(layers):
+        layer = xtgeo.surface_from_grid3d(
+            grid, template=None, where=lay, rfactor=rfactor
+        )
+        layer.name = names[inum]
+        layerstack.append(layer)
+
+    logger.info("Extracting surface from 3D grid... DONE")
+
+    return layerstack, "tops", "stratigraphic"
```

## xtgeo/surface/_zmap_parser.py

 * *Ordering differences only*

```diff
@@ -1,142 +1,142 @@
-"""ZMAP plus parsing.
-
-cf https://saurabhkukade.com/posts/2020/07/understanding-zmap-file-format/
-
-Note also from example here:
-https://raw.githubusercontent.com/abduhbm/zmapio/main/examples/NSLCU.dat
-that header lines may end with trailing comma!
-"""
-
-import dataclasses
-import inspect
-from functools import wraps
-from pathlib import Path
-from typing import Optional
-
-import numpy as np
-
-
-@dataclasses.dataclass
-class ZMAPSurface:
-    nrow: int
-    ncol: int
-    xmin: float
-    xmax: float
-    ymin: float
-    ymax: float
-    node_width: int
-    precision: int
-    start_column: int
-    nan_value: float
-    nr_nodes_per_line: int
-    values: Optional[np.array] = None
-
-    def __post_init__(self):
-        for field in dataclasses.fields(self):
-            value = getattr(self, field.name)
-            if field.type in (int, float) and not isinstance(value, field.type):
-                setattr(self, field.name, field.type(value))
-
-
-def takes_stream(name, mode):
-    def decorator(func):
-        @wraps(func)
-        def wrapper(*args, **kwargs):
-            kwargs = inspect.getcallargs(func, *args, **kwargs)
-            if name in kwargs and isinstance(kwargs[name], (str, Path)):
-                with open(kwargs[name], mode) as f:
-                    kwargs[name] = f
-                    return func(**kwargs)
-            else:
-                return func(**kwargs)
-
-        return wrapper
-
-    return decorator
-
-
-def parse_header(zmap_data):
-    keys = {}
-    line_nr = 0
-    for line in zmap_data:
-        if is_comment(line):
-            continue
-        try:
-            line = [entry.strip() for entry in line.split(",")]
-            if not line[-1]:
-                line.pop()  # deal with input lines ending with comma ','
-            if line_nr == 0:
-                _, identifier, keys["nr_nodes_per_line"] = line
-                if identifier != "GRID":
-                    raise ZMAPFormatException(
-                        f"Expected GRID as second entry in line, "
-                        f"got: {identifier} in line: {line}"
-                    )
-            elif line_nr == 1:
-                (
-                    keys["node_width"],
-                    dft_nan_value,
-                    user_nan_value,
-                    keys["precision"],
-                    keys["start_column"],
-                ) = line
-                keys["nan_value"] = (
-                    user_nan_value if not dft_nan_value else dft_nan_value
-                )
-            elif line_nr == 2:
-                (
-                    keys["nrow"],
-                    keys["ncol"],
-                    keys["xmin"],
-                    keys["xmax"],
-                    keys["ymin"],
-                    keys["ymax"],
-                ) = line
-            elif line_nr == 3:
-                _, _, _ = line
-            elif line_nr >= 4 and line[0] != "@":
-                raise ZMAPFormatException(
-                    f"Did not reach the values section, expected @, found: {line}"
-                )
-            else:
-                return keys
-        except ValueError as err:
-            raise ZMAPFormatException(f"Failed to unpack line: {line}") from err
-        line_nr += 1
-    raise ZMAPFormatException("End reached without complete header")
-
-
-def is_comment(line):
-    if line.startswith("!") or line.startswith("+"):
-        return True
-    return False
-
-
-def parse_values(zmap_data, nan_value):
-    """Parse actual values in zmap plus ascii files.
-
-    Note that header's node_width and nr_nodes_per_line in ZMAP header are not applied,
-    meaning that values import here is more tolerant than original zmap spec.
-    """
-    values = []
-    for line in zmap_data:
-        if is_comment(line):
-            continue
-        else:
-            values += line.split()
-    values = np.array(values, dtype=np.float32)
-    values = np.ma.masked_equal(values, nan_value)
-    return values
-
-
-@takes_stream("zmap_file", "r")
-def parse_zmap(zmap_file, load_values=True):
-    header = parse_header(zmap_file)
-    zmap_data = ZMAPSurface(**header)
-    if load_values:
-        zmap_data.values = parse_values(zmap_file, zmap_data.nan_value)
-    return zmap_data
-
-
-class ZMAPFormatException(Exception):
-    pass
+"""ZMAP plus parsing.
+
+cf https://saurabhkukade.com/posts/2020/07/understanding-zmap-file-format/
+
+Note also from example here:
+https://raw.githubusercontent.com/abduhbm/zmapio/main/examples/NSLCU.dat
+that header lines may end with trailing comma!
+"""
+
+import dataclasses
+import inspect
+from functools import wraps
+from pathlib import Path
+from typing import Optional
+
+import numpy as np
+
+
+@dataclasses.dataclass
+class ZMAPSurface:
+    nrow: int
+    ncol: int
+    xmin: float
+    xmax: float
+    ymin: float
+    ymax: float
+    node_width: int
+    precision: int
+    start_column: int
+    nan_value: float
+    nr_nodes_per_line: int
+    values: Optional[np.array] = None
+
+    def __post_init__(self):
+        for field in dataclasses.fields(self):
+            value = getattr(self, field.name)
+            if field.type in (int, float) and not isinstance(value, field.type):
+                setattr(self, field.name, field.type(value))
+
+
+def takes_stream(name, mode):
+    def decorator(func):
+        @wraps(func)
+        def wrapper(*args, **kwargs):
+            kwargs = inspect.getcallargs(func, *args, **kwargs)
+            if name in kwargs and isinstance(kwargs[name], (str, Path)):
+                with open(kwargs[name], mode) as f:
+                    kwargs[name] = f
+                    return func(**kwargs)
+            else:
+                return func(**kwargs)
+
+        return wrapper
+
+    return decorator
+
+
+def parse_header(zmap_data):
+    keys = {}
+    line_nr = 0
+    for line in zmap_data:
+        if is_comment(line):
+            continue
+        try:
+            line = [entry.strip() for entry in line.split(",")]
+            if not line[-1]:
+                line.pop()  # deal with input lines ending with comma ','
+            if line_nr == 0:
+                _, identifier, keys["nr_nodes_per_line"] = line
+                if identifier != "GRID":
+                    raise ZMAPFormatException(
+                        f"Expected GRID as second entry in line, "
+                        f"got: {identifier} in line: {line}"
+                    )
+            elif line_nr == 1:
+                (
+                    keys["node_width"],
+                    dft_nan_value,
+                    user_nan_value,
+                    keys["precision"],
+                    keys["start_column"],
+                ) = line
+                keys["nan_value"] = (
+                    user_nan_value if not dft_nan_value else dft_nan_value
+                )
+            elif line_nr == 2:
+                (
+                    keys["nrow"],
+                    keys["ncol"],
+                    keys["xmin"],
+                    keys["xmax"],
+                    keys["ymin"],
+                    keys["ymax"],
+                ) = line
+            elif line_nr == 3:
+                _, _, _ = line
+            elif line_nr >= 4 and line[0] != "@":
+                raise ZMAPFormatException(
+                    f"Did not reach the values section, expected @, found: {line}"
+                )
+            else:
+                return keys
+        except ValueError as err:
+            raise ZMAPFormatException(f"Failed to unpack line: {line}") from err
+        line_nr += 1
+    raise ZMAPFormatException("End reached without complete header")
+
+
+def is_comment(line):
+    if line.startswith("!") or line.startswith("+"):
+        return True
+    return False
+
+
+def parse_values(zmap_data, nan_value):
+    """Parse actual values in zmap plus ascii files.
+
+    Note that header's node_width and nr_nodes_per_line in ZMAP header are not applied,
+    meaning that values import here is more tolerant than original zmap spec.
+    """
+    values = []
+    for line in zmap_data:
+        if is_comment(line):
+            continue
+        else:
+            values += line.split()
+    values = np.array(values, dtype=np.float32)
+    values = np.ma.masked_equal(values, nan_value)
+    return values
+
+
+@takes_stream("zmap_file", "r")
+def parse_zmap(zmap_file, load_values=True):
+    header = parse_header(zmap_file)
+    zmap_data = ZMAPSurface(**header)
+    if load_values:
+        zmap_data.values = parse_values(zmap_file, zmap_data.nan_value)
+    return zmap_data
+
+
+class ZMAPFormatException(Exception):
+    pass
```

## xtgeo/surface/regular_surface.py

 * *Ordering differences only*

```diff
@@ -1,3098 +1,3098 @@
-"""Module/class for regular surfaces with XTGeo.
-
-Regular surfaces have a constant distance between nodes (xinc, yinc),
-and this simplifies computations a lot. A regular surface is
-defined by an origin (xori, yori)
-in UTM, a number of columns (along X axis, if no rotation), a number of
-rows (along Y axis if no rotation), and increment (distance between nodes).
-
-The map itself is an array of values.
-
-Rotation is allowed and is measured in degrees, anticlock from X axis.
-
-Note that an instance of a regular surface can be made directly with::
-
- >>> import xtgeo
- >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-
-or::
-
- mysurf = xtgeo.surface_from_roxar('some_rms_project', 'TopX', 'DepthSurface')
-
-"""
-# --------------------------------------------------------------------------------------
-# Comment on 'asmasked' vs 'activeonly:
-# 'asmasked'=True will return a np.ma array, with some fill_value if
-# if asmasked = False
-#
-# while 'activeonly' will filter
-# out maked entries, or use np.nan if 'activeonly' is False
-#
-# For functions with mask=... ,the should be replaced with asmasked=...
-# --------------------------------------------------------------------------------------
-
-# pylint: disable=too-many-public-methods
-
-import functools
-import io
-import math
-import numbers
-import pathlib
-import warnings
-from collections import OrderedDict
-from copy import deepcopy
-from types import FunctionType
-from typing import List, Optional, Tuple, Union
-
-import deprecation
-import numpy as np
-import numpy.ma as ma
-import pandas as pd
-
-import xtgeo
-import xtgeo.common.sys as xtgeosys
-from xtgeo.common.constants import VERYLARGENEGATIVE, VERYLARGEPOSITIVE
-
-from . import (
-    _regsurf_boundary,
-    _regsurf_cube,
-    _regsurf_cube_window,
-    _regsurf_export,
-    _regsurf_grid3d,
-    _regsurf_gridding,
-    _regsurf_import,
-    _regsurf_oper,
-    _regsurf_roxapi,
-    _regsurf_utils,
-)
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-# ======================================================================================
-# METHODS as wrappers to class init + import
-
-
-def surface_from_file(mfile, fformat=None, template=None, values=True, engine="cxtgeo"):
-    """Make an instance of a RegularSurface directly from file import.
-
-    Args:
-        mfile (str): Name of file
-        fformat: File format, None/guess/irap_binary/irap_ascii/ijxyz/petromod/
-            zmap_ascii/xtg/hdf is currently supported. If None or guess, the file
-            'signature' is used to guess format first, then file extension.
-        template: Only valid if ``ijxyz`` format, where an existing Cube or
-            RegularSurface instance is applied to get correct topology.
-        values (bool): If True (default), surface values will be read (Irap binary only)
-        engine (str): Some import methods are implemnted in both C and Python.
-            The C method ``cxtgeo`` is default. Alternative use ``python``
-
-    Example::
-
-        >>> import xtgeo
-        >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-
-    .. versionchanged:: 2.1
-      Key "values" for Irap binary maps added
-
-    .. versionchanged:: 2.13 Key "engine" added
-    """
-
-    return RegularSurface._read_file(
-        mfile, fformat=fformat, load_values=values, engine=engine, template=template
-    )
-
-
-def surface_from_roxar(project, name, category, stype="horizons", realisation=0):
-    """This makes an instance of a RegularSurface directly from roxar input.
-
-    Args:
-        project (str or special): Name of project (as folder) if
-            outside RMS, og just use the magic project word if within RMS.
-        name (str): Name of surface/map
-        category (str): For horizons/zones or clipboard/general2d_data:
-            for example 'DS_extracted'. For clipboard/general2d_data this can
-            be empty or None, or use '/' for multiple folder levels (e.g. 'fld/subfld').
-            For 'trends', the category is not applied.
-        stype (str): RMS folder type, 'horizons' (default), 'zones', 'clipboard',
-            'general2d_data' or 'trends'
-        realisation (int): Realisation number, default is 0
-
-    Example::
-
-        # inside RMS:
-        import xtgeo
-        mysurf = xtgeo.surface_from_roxar(project, 'TopEtive', 'DepthSurface')
-
-    Note::
-
-        When dealing with surfaces to and from ``stype="trends"``, the surface must
-        exist in advance, i.e. the Roxar API do not allow creating new surfaces.
-        Actually trends are read only, but a workaround using ``load()`` in Roxar
-        API makes it possible to overwrite existing surface trends. In addition,
-        ``realisation`` is not applied in trends.
-
-    """
-
-    return RegularSurface._read_roxar(
-        project, name, category, stype=stype, realisation=realisation
-    )
-
-
-def surface_from_cube(cube, value):
-    """Make RegularSurface directly from a cube instance with a constant value.
-
-    The surface geometry will be exactly the same as for the Cube.
-
-    Args:
-        cube(xtgeo.cube.Cube): A Cube instance
-        value (float): A constant value for the surface
-
-    Example::
-
-       >>> import xtgeo
-       >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-       >>> mymap = xtgeo.surface_from_cube(mycube, 2700)
-
-    """
-    return RegularSurface._read_cube(cube, value)
-
-
-def surface_from_grid3d(grid, template=None, where="top", mode="depth", rfactor=1):
-    """This makes 3 instances of a RegularSurface directly from a Grid() instance.
-
-    Args:
-        grid (Grid): XTGeo Grid instance
-        template(RegularSurface): Optional to use an existing surface as
-            template for geometry
-        where (str): "top", "base" or use the syntax "2_top" where 2
-            is layer no. 2 and _top indicates top of cell, while "_base"
-            indicates base of cell
-        mode (str): "depth", "i" or "j"
-        rfactor (float): Determines how fine the extracted map is; higher values
-            for finer map (but computing time will increase). Will only apply if
-            template is None.
-
-    .. versionadded:: 2.1
-    """
-    return RegularSurface._read_grid3d(
-        grid, template=template, where=where, mode=mode, rfactor=rfactor
-    )
-
-
-def _data_reader_factory(file_format):
-    if file_format == "irap_binary":
-        return _regsurf_import.import_irap_binary
-    if file_format == "irap_ascii":
-        return _regsurf_import.import_irap_ascii
-    if file_format == "ijxyz":
-        return _regsurf_import.import_ijxyz
-    if file_format == "petromod":
-        return _regsurf_import.import_petromod
-    if file_format == "zmap_ascii":
-        return _regsurf_import.import_zmap_ascii
-    if file_format == "xtg":
-        return _regsurf_import.import_xtg
-    if file_format == "hdf":
-        return _regsurf_import.import_hdf5_regsurf
-    raise ValueError(f"Unknown file format {file_format}")
-
-
-def allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of RegularSurface and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        # Checking if we are doing an initialization
-        # from file and raise a deprecation warning if
-        # we are.
-        if "sfile" in kwargs or len(args) == 1:
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "mysurf = xtgeo.surface_from_file('some_name.gri') instead",
-                DeprecationWarning,
-            )
-            sfile = kwargs.get("sfile", args[0])
-            fformat = kwargs.get("fformat", None)
-            values = kwargs.get("values", None)
-            if isinstance(values, bool) and values is False:
-                load_values = False
-            else:
-                load_values = True
-            mfile = xtgeosys._XTGeoFile(sfile)
-            if fformat is None or fformat == "guess":
-                fformat = mfile.detect_fformat()
-            else:
-                fformat = mfile.generic_format_by_proposal(fformat)  # default
-            kwargs = _data_reader_factory(fformat)(mfile, values=load_values)
-            kwargs["filesrc"] = mfile.file
-            kwargs["fformat"] = fformat
-            return func(cls, **kwargs)
-
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-def allow_deprecated_default_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of RegularSurface and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        # This is (mostly) for cases where we are doing an empty
-        # initialization, so we need to inject default values
-        # for the required args. The excessive checking is in
-        # corner cases where we provide some positional arguments
-        # as keyword arguments.
-        _deprecation_msg = (
-            "X is a required argument, will no "
-            "longer be defaulted in xtgeo version 4.0"
-        )
-        if len(args) != 4:
-            if "ncol" not in kwargs and len(args) != 1:
-                warnings.warn(_deprecation_msg.replace("X", "ncol"), DeprecationWarning)
-                kwargs["ncol"] = 5
-            if "nrow" not in kwargs and len(args) != 2:
-                warnings.warn(_deprecation_msg.replace("X", "nrow"), DeprecationWarning)
-                kwargs["nrow"] = 3
-            if "xinc" not in kwargs and len(args) != 3:
-                warnings.warn(_deprecation_msg.replace("X", "xinc"), DeprecationWarning)
-                kwargs["xinc"] = 25.0
-            if "yinc" not in kwargs:
-                warnings.warn(_deprecation_msg.replace("X", "yinc"), DeprecationWarning)
-                kwargs["yinc"] = 25.0
-            default = (
-                kwargs.get("ncol", 5) == 5
-                and kwargs.get("nrow", 3) == 3
-                and kwargs.get("xori", 0.0) == kwargs.get("yori", 0.0) == 0.0
-                and kwargs.get("xinc", 25.0) == kwargs.get("yinc", 25.0) == 25.0
-            )
-            values = kwargs.get("values", None)
-            if values is None and default:
-                default_values = [
-                    [1, 6, 11],
-                    [2, 7, 12],
-                    [3, 8, 1e33],
-                    [4, 9, 14],
-                    [5, 10, 15],
-                ]
-                warnings.warn(
-                    f"Default values {default_values} for RegularSurface is "
-                    f"deprecated and will be set to an array of zero if not explicitly "
-                    f"given in version 3",
-                    DeprecationWarning,
-                )
-                # make default surface (mostly for unit testing)
-                kwargs["values"] = np.array(
-                    default_values,
-                    dtype=np.float64,
-                    order="C",
-                )
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-class RegularSurface:
-    """Class for a regular surface in the XTGeo framework.
-
-    The values can as default be accessed by the user as a 2D masked numpy
-    (ncol, nrow) float64 array, but also other representations or views are
-    possible (e.g. as 1D ordinary numpy).
-
-    """
-
-    @allow_deprecated_init
-    @allow_deprecated_default_init
-    def __init__(
-        self,
-        ncol: int,
-        nrow: int,
-        xinc: float,
-        yinc: float,
-        xori: Optional[float] = 0.0,
-        yori: Optional[float] = 0.0,
-        yflip: Optional[int] = 1,
-        rotation: Optional[float] = 0.0,
-        values: Optional[Union[List[float], float]] = None,
-        ilines: Optional[List[float]] = None,
-        xlines: Optional[List[float]] = None,
-        masked: Optional[bool] = True,
-        name: Optional[str] = "unknown",
-        filesrc: Optional[str] = None,
-        fformat: Optional[str] = None,
-        undef: Optional[float] = xtgeo.UNDEF,
-    ):
-        """Instantiating a RegularSurface::
-
-            vals = np.zeros(30 * 50)
-            surf = xtgeo.RegularSurface(
-                ncol=30, nrow=50, xori=1234.5, yori=4321.0, xinc=30.0, yinc=50.0,
-                rotation=30.0, values=vals, yflip=1,
-            )
-
-        Args:
-            ncol: Integer for number of X direction columns.
-            nrow: Integer for number of Y direction rows.
-            xori: X (East) origon coordinate.
-            yori: Y (North) origin coordinate.
-            xinc: X increment.
-            yinc: Y increment.
-            yflip: If 1, the map grid is left-handed (assuming depth downwards),
-                otherwise -1 means that Y axis is flipped (right-handed).
-            rotation: rotation in degrees, anticlock from X axis between 0, 360.
-            values: A scalar (for constant values) or a "array-like" input that has
-                ncol * nrow elements. As result, a 2D (masked) numpy array of shape
-                (ncol, nrow), C order will be made.
-            masked: Indicating if numpy array shall be masked or not. Default is True.
-            name: A given name for the surface, default is file name root or
-                'unknown' if constructed from scratch.
-
-        Examples:
-            The instance can be made by specification::
-
-                >>> surface = RegularSurface(
-                ... ncol=20,
-                ... nrow=10,
-                ... xori=2000.0,
-                ... yori=2000.0,
-                ... rotation=0.0,
-                ... xinc=25.0,
-                ... yinc=25.0,
-                ... values=np.zeros((20,10))
-                ... )
-
-
-        """
-        logger.info("Start __init__ method for RegularSurface object %s", id(self))
-        self._ncol = ncol
-        self._nrow = nrow
-        self._xori = xori
-        self._yori = yori
-        self._xinc = xinc
-        self._yinc = yinc
-        self._rotation = rotation
-        self._yflip = yflip
-        self._name = name
-
-        self._undef = undef
-        self._undef_limit = xtgeo.UNDEF_LIMIT
-
-        self._filesrc = filesrc  # Name of original input file or stream, if any
-
-        self._fformat = fformat  # current fileformat, useful for load()
-        self._metadata = xtgeo.MetaDataRegularSurface()
-
-        self._values = None
-        if values is None:
-            values = np.ma.zeros((self._ncol, self._nrow))
-            self._isloaded = False
-        else:
-            self._isloaded = True
-        self.values = values
-
-        if ilines is None:
-            self._ilines = np.array(range(1, self._ncol + 1), dtype=np.int32)
-            self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
-        else:
-            self._ilines = ilines
-            self._xlines = xlines
-
-        self._masked = masked  # TODO: check usecase
-        self._metadata.required = self
-
-    @classmethod
-    def _read_zmap_ascii(cls, mfile, values):
-        mfile = xtgeosys._XTGeoFile(mfile)
-        args = _data_reader_factory("zmap_ascii")(mfile, values=values)
-        return cls(**args)
-
-    def __repr__(self):
-        """Magic method __repr__."""
-        myrp = (
-            f"{self.__class__.__name__} (xori={self._xori!r}, yori={self._yori!r}, "
-            f"xinc={self._xinc!r}, yinc={self._yinc!r}, ncol={self._ncol!r}, "
-            f"nrow={self._nrow!r}, rotation={self._rotation!r}, yflip={self._yflip!r}, "
-            f"masked={self._masked!r}, filesrc={self._filesrc!r}, "
-            f"name={self._name!r}, ilines={self.ilines.shape!r}, "
-            f"xlines={self.xlines.shape!r}, values={self.values.shape!r}) "
-            f"ID={id(self)}."
-        )
-        return myrp
-
-    def __str__(self):
-        """Magic method __str__ for user friendly print."""
-        return self.describe(flush=False)
-
-    def __getitem__(self, index):
-        """Magic method."""
-        col, row = index
-        return self._values[col, row]
-
-    def __add__(self, other):
-        """Magic method."""
-        news = self.copy()
-        _regsurf_oper.operations_two(news, other, oper="add")
-
-        return news
-
-    def __iadd__(self, other):
-        """Magic method."""
-        _regsurf_oper.operations_two(self, other, oper="iadd")
-        return self
-
-    def __sub__(self, other):
-        """Magic method."""
-        news = self.copy()
-        _regsurf_oper.operations_two(news, other, oper="sub")
-
-        return news
-
-    def __isub__(self, other):
-        """Magic method."""
-        _regsurf_oper.operations_two(self, other, oper="isub")
-        return self
-
-    def __mul__(self, other):
-        """Magic method."""
-        news = self.copy()
-        _regsurf_oper.operations_two(news, other, oper="mul")
-
-        return news
-
-    def __imul__(self, other):
-        """Magic method."""
-        _regsurf_oper.operations_two(self, other, oper="imul")
-        return self
-
-    def __truediv__(self, other):
-        """Magic method."""
-        news = self.copy()
-        _regsurf_oper.operations_two(news, other, oper="div")
-
-        return news
-
-    def __idiv__(self, other):
-        """Magic method."""
-        _regsurf_oper.operations_two(self, other, oper="idiv")
-        return self
-
-    # comparison operators, return boolean arrays
-
-    def __lt__(self, other):
-        """Magic method."""
-        return _regsurf_oper.operations_two(self, other, oper="lt")
-
-    def __gt__(self, other):
-        """Magic method."""
-        return _regsurf_oper.operations_two(self, other, oper="gt")
-
-    def __le__(self, other):
-        """Magic method."""
-        return _regsurf_oper.operations_two(self, other, oper="le")
-
-    def __ge__(self, other):
-        """Magic method."""
-        return _regsurf_oper.operations_two(self, other, oper="ge")
-
-    def __eq__(self, other):
-        """Magic method."""
-        return _regsurf_oper.operations_two(self, other, oper="eq")
-
-    def __ne__(self, other):
-        """Magic method."""
-        return _regsurf_oper.operations_two(self, other, oper="ne")
-
-    # ==================================================================================
-    # Class and special methods
-    # ==================================================================================
-
-    @classmethod
-    def methods(cls):
-        """Returns the names of the methods in the class.
-
-        >>> print(RegularSurface.methods())
-        METHODS for RegularSurface():
-        ======================
-        __init__
-        __repr__
-        ...
-
-        """
-        mets = [x for x, y in cls.__dict__.items() if isinstance(y, FunctionType)]
-
-        txt = "METHODS for RegularSurface():\n======================\n"
-        for met in mets:
-            txt += str(met) + "\n"
-
-        return txt
-
-    # ==================================================================================
-    # Properties
-    # ==================================================================================
-
-    @property
-    def metadata(self):
-        """Return metadata object instance of type MetaDataRegularSurface."""
-        return self._metadata
-
-    @metadata.setter
-    def metadata(self, obj):
-        # The current metadata object can be replaced. A bit dangerous so further
-        # check must be done to validate. TODO.
-        if not isinstance(obj, xtgeo.MetaDataRegularSurface):
-            raise ValueError("Input obj not an instance of MetaDataRegularSurface")
-
-        self._metadata = obj  # checking is currently missing! TODO
-
-    @property
-    def ncol(self):
-        """The NCOL (NX or N-Idir) number, as property (read only)."""
-        return self._ncol
-
-    @property
-    def nrow(self):
-        """The NROW (NY or N-Jdir) number, as property (read only)."""
-        return self._nrow
-
-    @property
-    def dimensions(self):
-        """2-tuple: The surface dimensions as a tuple of 2 integers (read only)."""
-        return (self._ncol, self._nrow)
-
-    @property
-    def nactive(self):
-        """Number of active map nodes (read only)."""
-        if self._isloaded:
-            return self._values.count()
-        return None
-
-    @property
-    def rotation(self):
-        """The rotation, anticlock from X axis, in degrees [0..360>."""
-        return self._rotation
-
-    @rotation.setter
-    def rotation(self, rota):
-        if 0 <= rota < 360:
-            self._rotation = rota
-        else:
-            raise ValueError("Rotation must be in interval [0, 360>")
-
-    @property
-    def xinc(self):
-        """The X increment (or I dir increment)."""
-        return self._xinc
-
-    @property
-    def yinc(self):
-        """The Y increment (or I dir increment)."""
-        return self._yinc
-
-    @property
-    def yflip(self):
-        """The Y flip (handedness) indicator 1, or -1 (read only).
-
-        The value 1 (default) means a left-handed system if depth values are
-        positive downwards. Assume -1 is rare, but may happen when
-        surface is derived from seismic cube.
-        """
-        return self._yflip
-
-    @property
-    def xori(self):
-        """The X coordinate origin of the map."""
-        return self._xori
-
-    @xori.setter
-    def xori(self, xnew):
-        self._xori = xnew
-
-    @property
-    def yori(self):
-        """The Y coordinate origin of the map."""
-        return self._yori
-
-    @yori.setter
-    def yori(self, ynew):
-        self._yori = ynew
-
-    @property
-    def ilines(self):
-        """The inlines numbering vector (read only)."""
-        return self._ilines
-
-    @ilines.setter
-    def ilines(self, values):
-        if isinstance(values, np.ndarray) and values.shape[0] == self._ncol:
-            self._ilines = values
-
-    @property
-    def xlines(self):
-        """The xlines numbering vector (read only)."""
-        return self._xlines
-
-    @xlines.setter
-    def xlines(self, values):
-        if isinstance(values, np.ndarray) and values.shape[0] == self._nrow:
-            self._xlines = values
-
-    @property
-    def xmin(self):
-        """The minimim X coordinate (read only)."""
-        corners = self.get_map_xycorners()
-
-        xmin = VERYLARGEPOSITIVE
-        for corner in corners:
-            if corner[0] < xmin:
-                xmin = corner[0]
-        return xmin
-
-    @property
-    def xmax(self):
-        """The maximum X coordinate (read only)."""
-        corners = self.get_map_xycorners()
-
-        xmax = VERYLARGENEGATIVE
-        for corner in corners:
-            if corner[0] > xmax:
-                xmax = corner[0]
-        return xmax
-
-    @property
-    def ymin(self):
-        """The minimim Y coordinate (read only)."""
-        corners = self.get_map_xycorners()
-
-        ymin = VERYLARGEPOSITIVE
-        for corner in corners:
-            if corner[1] < ymin:
-                ymin = corner[1]
-        return ymin
-
-    @property
-    def ymax(self):
-        """The maximum Y xoordinate (read only)."""
-        corners = self.get_map_xycorners()
-
-        ymax = VERYLARGENEGATIVE
-        for corner in corners:
-            if corner[1] > ymax:
-                ymax = corner[1]
-        return ymax
-
-    @property
-    def values(self):
-        """The map values, as 2D masked numpy (float64), shape (ncol, nrow).
-
-        When setting values as a scalar, the current mask will be preserved.
-
-        When setting values, list-like input (lists, tuples) is also accepted, as
-        long as the length is correct and the entries are number-like.
-
-        In order to specify undefined values, you can specify the ``undef`` attribute
-        in the list, or use ``float("nan")``.
-
-        Example::
-
-            # list like input where nrow=3 and ncol=5 (15 entries)
-            newvalues = list(range(15))
-            newvalues[2] = srf.undef
-            srf.values = newvalues  # here, entry 2 will be undefined
-        """
-        return self._values
-
-    @values.setter
-    def values(self, values):
-        self._ensure_correct_values(values)
-
-    @property
-    def values1d(self):
-        """(Read only) Map values, as 1D numpy masked, normally a numpy view(?).
-
-        Example::
-
-            map = xtgeo.surface_from_file('myfile.gri')
-            map.values1d
-        """
-        return self.get_values1d(asmasked=True)
-
-    @property
-    def npvalues1d(self):
-        """(Read only) Map values, as 1D numpy (not masked), undef as np.nan.
-
-        In most cases this will be a copy of the values.
-
-        Example::
-
-            >>> import xtgeo
-            >>> map = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> values = map.npvalues1d
-            >>> mean = np.nanmean(values)
-            >>> values[values <= 0] = np.nan
-            >>> print(values)
-            [nan nan ... nan]
-        """
-        return self.get_values1d(asmasked=False, fill_value=np.nan)
-
-    @property
-    def name(self):
-        """A free form name for the surface, to be used in display etc."""
-        return self._name
-
-    @name.setter
-    def name(self, newname):
-        if isinstance(newname, str):
-            self._name = newname
-
-    @property
-    def undef(self):
-        """Returns the undef map value (read only)."""
-        return self._undef
-
-    @property
-    def undef_limit(self):
-        """Returns the undef_limit map value (read only)."""
-        return self._undef_limit
-
-    @property
-    def filesrc(self):
-        """Gives the name of the file source (if any)."""
-        return self._filesrc
-
-    @filesrc.setter
-    def filesrc(self, name):
-        self._filesrc = name  # checking is currently missing
-
-    # ==================================================================================
-    # Describe, import and export
-    # ==================================================================================
-
-    def generate_hash(self, hashmethod="md5"):
-        """Return a unique hash ID for current instance.
-
-        See :meth:`~xtgeo.common.sys.generic_hash()` for documentation.
-
-        .. versionadded:: 2.14
-        """
-        required = (
-            "ncol",
-            "nrow",
-            "xori",
-            "yori",
-            "xinc",
-            "yinc",
-            "yflip",
-            "rotation",
-        )
-        gid = ""
-        for req in required:
-            gid += f"{getattr(self, '_' + req)}"
-        # Ignore the mask
-        gid += self._values.data.tobytes().hex()
-
-        hash_ = xtgeosys.generic_hash(gid, hashmethod=hashmethod)
-        return hash_
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout."""
-        #
-        dsc = xtgeo.common.XTGDescription()
-        dsc.title("Description of RegularSurface instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("File source", self._filesrc)
-        dsc.txt("Shape: NCOL, NROW", self.ncol, self.nrow)
-        dsc.txt("Active cells vs total", self.nactive, self.nrow * self.ncol)
-        dsc.txt("Origins XORI, YORI", self.xori, self.yori)
-        dsc.txt("Increments XINC YINC", self.xinc, self.yinc)
-        dsc.txt("Rotation (anti-clock from X)", self.rotation)
-        dsc.txt("YFLIP flag", self._yflip)
-        np.set_printoptions(threshold=16)
-        dsc.txt("Inlines vector", self._ilines)
-        dsc.txt("Xlines vector", self._xlines)
-        np.set_printoptions(threshold=1000)
-        if self._isloaded:
-            dsc.txt("Values", self._values.reshape(-1), self._values.dtype)
-            dsc.txt(
-                "Values: mean, stdev, minimum, maximum",
-                self.values.mean(),
-                self.values.std(),
-                self.values.min(),
-                self.values.max(),
-            )
-            msize = float(self.values.size * 8) / (1024 * 1024 * 1024)
-            dsc.txt("Minimum memory usage of array (GB)", msize)
-        else:
-            dsc.txt("Values:", "Not loaded")
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    @deprecation.deprecated(
-        deprecated_in="2.15",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.surface_from_file() instead",
-    )
-    def from_file(
-        self,
-        mfile: Union[str, pathlib.Path, io.BytesIO],
-        fformat: Optional[str] = None,
-        values: Optional[bool] = True,
-        **kwargs,
-    ):
-        """Import surface (regular map) from file.
-
-        Note that the ``fformat=None`` or ``guess`` option will guess format by
-        looking at the file or stream signature or file extension.
-        For the signature, the first bytes are scanned for 'patterns'. If that
-        does not work (and input is not a memory stream), it will try to use
-        file extension where e.g. "gri" will assume irap_binary and "fgr"
-        assume Irap Ascii. If file extension is missing, Irap binary is assumed.
-
-        The ``ijxyz`` format is the typical seismic format, on the form
-        (ILINE, XLINE, X, Y, VALUE) as a table of points. Map values are
-        estimated from the given values, or by using an existing map or
-        cube as template, and match by ILINE/XLINE numbering.
-
-        BytesIO input is supported for Irap binary, Irap Ascii, ZMAP ascii.
-
-        Args:
-            mfile: File-like or memory stream instance.
-            fformat: File format, None/guess/irap_binary/irap_ascii/ijxyz
-                is currently supported. If None or guess, the file 'signature' is
-                used to guess format first, then file extension.
-            values: If True (default), then full array is read, if False
-                only metadata will be read. Valid for Irap binary only. This allows
-                lazy loading in e.g. ensembles.
-            kwargs: some readers allow additonal options:
-            template: Only valid if ``ijxyz`` format, where an
-                existing Cube or RegularSurface instance is applied to
-                get correct topology.
-            engine: Default is "cxtgeo" which use a C backend. Optionally a pure
-                python "python" reader will be used, which in general is slower
-                but may be safer when reading memory streams and/or threading.
-                Engine is relevant for Irap binary, Irap ascii and zmap.
-
-        Returns:
-            Object instance.
-
-        Example:
-            Here the from_file method is used to initiate the object
-            directly::
-
-            >>> surf = RegularSurface().from_file(surface_dir + "/topreek_rota.gri")
-
-        .. versionchanged:: 2.1
-          Key "values" for Irap binary maps added
-
-        .. versionchanged:: 2.2
-          Input io.BytesIO instance instead of file is now possible
-
-        .. versionchanged:: 2.13
-            ZMAP + import is added, and io.BytesIO input is extended to more formats
-        """
-        logger.info("Import RegularSurface from file or memstream...")
-
-        mfile = xtgeosys._XTGeoFile(mfile)
-
-        if fformat is None or fformat == "guess":
-            fformat = mfile.detect_fformat()
-        else:
-            fformat = mfile.generic_format_by_proposal(fformat)  # default
-
-        kwargs = _data_reader_factory(fformat)(mfile, values=values, **kwargs)
-        if values:
-            self._isloaded = True
-        self._reset(**kwargs)
-
-    def _reset(self, **kwargs):
-        self._ncol = kwargs["ncol"]
-        self._nrow = kwargs["nrow"]
-        self._xinc = kwargs["xinc"]
-        self._yinc = kwargs["yinc"]
-        self._xori = kwargs.get("xori", self._xori)
-        self._yori = kwargs.get("yori", self._yori)
-        self._yflip = kwargs.get("yflip", self._yflip)
-        self._rotation = kwargs.get("rotation", self._rotation)
-        self._ilines = kwargs.get("ilines", self._ilines)
-        self._xlines = kwargs.get("xlines", self._xlines)
-        self.values = kwargs.get("values", self._values)
-
-    @classmethod
-    def _read_file(
-        cls,
-        mfile: Union[str, pathlib.Path, io.BytesIO],
-        fformat: Optional[str] = None,
-        load_values: bool = True,
-        **kwargs,
-    ):
-        """Import surface (regular map) from file.
-
-        Note that the ``fformat=None`` or ``guess`` option will guess format by
-        looking at the file or stream signature or file extension.
-        For the signature, the first bytes are scanned for 'patterns'. If that
-        does not work (and input is not a memory stream), it will try to use
-        file extension where e.g. "gri" will assume irap_binary and "fgr"
-        assume Irap Ascii. If file extension is missing, Irap binary is assumed.
-
-        The ``ijxyz`` format is the typical seismic format, on the form
-        (ILINE, XLINE, X, Y, VALUE) as a table of points. Map values are
-        estimated from the given values, or by using an existing map or
-        cube as template, and match by ILINE/XLINE numbering.
-
-        BytesIO input is supported for Irap binary, Irap Ascii, ZMAP ascii.
-
-        Args:
-            mfile: File-like or memory stream instance.
-            fformat: File format, None/guess/irap_binary/irap_ascii/ijxyz
-                is currently supported. If None or guess, the file 'signature' is
-                used to guess format first, then file extension.
-            load_values: If True (default), then full array is read, if False
-                only metadata will be read. Valid for Irap binary only. This allows
-                lazy loading in e.g. ensembles.
-            kwargs: some readers allow additonal options
-
-        Keyword Args:
-            template: Only valid if ``ijxyz`` format, where an
-                existing Cube or RegularSurface instance is applied to
-                get correct topology.
-            engine: Default is "cxtgeo" which use a C backend. Optionally a pure
-                python "python" reader will be used, which in general is slower
-                but may be safer when reading memory streams and/or threading.
-                Engine is relevant for Irap binary, Irap ascii and zmap.
-
-        Returns:
-            Object instance.
-
-        Example::
-
-           >>> surf = RegularSurface._read_file(surface_dir + "/topreek_rota.gri")
-
-        .. versionadded:: 2.14
-
-        """
-        mfile = xtgeosys._XTGeoFile(mfile)
-        mfile.check_file(raiseerror=ValueError)
-        if fformat is None or fformat == "guess":
-            fformat = mfile.detect_fformat()
-        else:
-            fformat = mfile.generic_format_by_proposal(fformat)  # default
-        kwargs = _data_reader_factory(fformat)(mfile, values=load_values, **kwargs)
-        kwargs["filesrc"] = mfile.file
-        kwargs["fformat"] = fformat
-        return cls(**kwargs)
-
-    def load_values(self):
-        """Import surface values in cases where metadata only is loaded.
-
-        Currently, only Irap binary format is supported.
-
-        Example::
-
-            surfs = []
-            for i in range(1000):
-                surfs.append(xtgeo.surface_from_file(f"myfile{i}.gri", values=False))
-
-            # load values in number 88:
-            surfs[88].load_values()
-
-        .. versionadded:: 2.1
-        """
-
-        if not self._isloaded:
-            if self.filesrc is None:
-                raise ValueError(
-                    "Can only load values into object initialised from file"
-                )
-
-            with warnings.catch_warnings():
-                warnings.filterwarnings("ignore", category=DeprecationWarning)
-
-                mfile = xtgeosys._XTGeoFile(self.filesrc)
-                kwargs = _data_reader_factory(self._fformat)(mfile, values=True)
-                self.values = kwargs.get("values", self._values)
-
-            self._isloaded = True
-
-    def to_file(
-        self,
-        mfile: Union[str, pathlib.Path, io.BytesIO],
-        fformat: Optional[str] = "irap_binary",
-        pmd_dataunits: Optional[Tuple[int, int]] = (15, 10),
-        engine: Optional[str] = "cxtgeo",
-    ):
-        """Export a surface (map) to file.
-
-        Note, for zmap_ascii and storm_binary an unrotation will be done
-        automatically. The sampling will be somewhat finer than the
-        original map in order to prevent aliasing. See :func:`unrotate`.
-
-        Args:
-            mfile: Name of file,
-                Path instance or IOBytestream instance. An alias can be e.g.
-                "%md5sum%" or "%fmu-v1%" with string or Path() input.
-            fformat: File format, irap_binary/irap_ascii/zmap_ascii/
-                storm_binary/ijxyz/petromod/xtg*. Default is irap_binary.
-            pmd_dataunits: A tuple of length 2 for petromod format,
-                spesifying metadata for units (DataUnitDistance, DataUnitZ).
-            engine: Default is "cxtgeo" which use a C backend. Optionally a pure
-                python "python" reader will be used, which in general is slower
-                but may be safer when reading memory streams and/or threading. Engine
-                is relevant for Irap binary, Irap ascii and zmap. This is mainly a
-                developer setting.
-
-        Returns:
-            ofile (pathlib.Path): The actual file instance, or None if io.Bytestream
-
-        Examples::
-
-            >>> # read and write to ordinary file
-            >>> surf = xtgeo.surface_from_file(
-            ...     surface_dir + '/topreek_rota.fgr',
-            ...     fformat = 'irap_ascii'
-            ... )
-            >>> surf.values = surf.values + 300
-            >>> filename = surf.to_file(
-            ...     outdir + '/topreek_rota300.fgr',
-            ...     fformat = 'irap_ascii'
-            ... )
-
-            >>> # writing to io.BytesIO:
-            >>> stream = io.BytesIO()
-            >>> surf.to_file(stream, fformat="irap_binary")
-
-            >>> # read from memory stream:
-            >>> _ = stream.seek(0)
-            >>> newsurf = xtgeo.surface_from_file(stream, fformat = 'irap_binary')
-
-        .. versionchanged:: 2.5 Added support for BytesIO
-        .. versionchanged:: 2.13 Improved support for BytesIO
-        .. versionchanged:: 2.14 Support for alias file name and return value
-        """
-        logger.info("Export RegularSurface to file or memstream...")
-        mfile = xtgeosys._XTGeoFile(mfile, mode="wb", obj=self)
-
-        if not mfile.memstream:
-            mfile.check_folder(raiseerror=OSError)
-        else:
-            engine = "python"
-
-        if fformat in ("irap_ascii", "irapascii", "irap_txt", "irapasc"):
-            _regsurf_export.export_irap_ascii(self, mfile, engine=engine)
-
-        elif fformat in ("irap_binary", "irapbinary", "irapbin", "irap", "gri"):
-            _regsurf_export.export_irap_binary(self, mfile, engine=engine)
-
-        elif "zmap" in fformat:
-            _regsurf_export.export_zmap_ascii(self, mfile, engine=engine)
-
-        elif fformat == "storm_binary":
-            _regsurf_export.export_storm_binary(self, mfile)
-
-        elif fformat == "petromod":
-            _regsurf_export.export_petromod_binary(self, mfile, pmd_dataunits)
-
-        elif fformat == "ijxyz":
-            _regsurf_export.export_ijxyz_ascii(self, mfile)
-
-        # developing, in prep and experimental!
-        elif fformat == "xtgregsurf":
-            _regsurf_export.export_xtgregsurf(self, mfile)
-
-        else:
-            raise ValueError(f"Invalid file format: {fformat}")
-
-        logger.info("Export RegularSurface to file or memstream... done")
-
-        if mfile.memstream:
-            return None
-        return mfile.file
-
-    @deprecation.deprecated(
-        deprecated_in="2.15",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.surface_from_hdf() instead",
-    )
-    def from_hdf(
-        self,
-        mfile: Union[str, pathlib.Path, io.BytesIO],
-        values: Optional[bool] = True,
-    ):
-        """Import/load a surface (map) with metadata from a HDF5 file.
-
-        Warning:
-            This implementation is currently experimental and only recommended
-            for testing.
-
-        The file extension shall be '.hdf'.
-
-        Args:
-            mfile: File name or Path object or memory stream
-            values: If False, only metadatadata are read
-
-        Returns:
-            RegularSurface() instance
-
-        Example:
-            >>> import xtgeo
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> filepath = surf.to_hdf(outdir + "/topreek_rota.hdf")
-            >>> mysurf = xtgeo.RegularSurface().from_hdf(filepath)
-
-        .. versionadded:: 2.14
-        """
-        # developing, in prep and experimental!
-        mfile = xtgeosys._XTGeoFile(mfile, mode="rb", obj=self)
-
-        kwargs = _regsurf_import.import_hdf5_regsurf(mfile, values=values)
-
-        self._reset(**kwargs)
-
-        _self: self.__class__ = self
-        return _self  # to make obj = xtgeo.RegularSurface().from_hdf(stream) work
-
-    def to_hdf(
-        self,
-        mfile: Union[str, pathlib.Path, io.BytesIO],
-        compression: Optional[str] = "lzf",
-    ) -> pathlib.Path:
-        """Export a surface (map) with metadata to a HDF5 file.
-
-        Warning:
-            This implementation is currently experimental and only recommended
-            for testing.
-
-        The file extension shall be '.hdf'
-
-        Args:
-            mfile: Name of file, Path instance or BytesIO instance. An alias can
-                be e.g. ``$md5sum.hdf``,  ``$fmu-v1.hdf`` with string or Path() input.
-            compression: Compression method, None, lzf (default), blosc
-
-        Returns:
-            pathlib.Path: The actual file instance, or None if io.Bytestream
-
-        Example:
-            >>> import xtgeo
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> filepath = surf.to_hdf(outdir + "/topreek_rota.hdf")
-
-        .. versionadded:: 2.14
-
-        """
-        # developing, in prep and experimental!
-        mfile = xtgeosys._XTGeoFile(mfile, mode="wb", obj=self)
-
-        if not mfile.memstream:
-            mfile.check_folder(raiseerror=OSError)
-
-        _regsurf_export.export_hdf5_regsurf(self, mfile, compression=compression)
-        return mfile.file
-
-    @classmethod
-    def _read_roxar(
-        cls, project, name, category, stype="horizons", realisation=0
-    ):  # pragma: no cover
-        """Load a surface from a Roxar RMS project.
-
-        The import from the RMS project can be done either within the project
-        or outside the project.
-
-        Note that a shortform to::
-
-          import xtgeo
-          mysurf = xtgeo.surface_from_roxar(project, 'name', 'category')
-
-        Note also that horizon/zone name and category must exists in advance,
-        otherwise an Exception will be raised.
-
-        Args:
-            project (str or special): Name of project (as folder) if
-                outside RMS, og just use the magic project word if within RMS.
-            name (str): Name of surface/map
-            category (str): For horizons/zones or clipboard/general2d_data: for
-                example 'DS_extracted'
-            stype (str): RMS folder type, 'horizons' (default), 'zones' or 'clipboard'
-            realisation (int): Realisation number, default is 0
-
-        """
-        kwargs = _regsurf_roxapi.import_horizon_roxapi(
-            project, name, category, stype, realisation
-        )
-
-        return cls(**kwargs)
-
-    @deprecation.deprecated(
-        deprecated_in="2.15",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.surface_from_roxar() instead",
-    )
-    def from_roxar(
-        self, project, name, category, stype="horizons", realisation=0
-    ):  # pragma: no cover
-        """Load a surface from a Roxar RMS project.
-
-        The import from the RMS project can be done either within the project
-        or outside the project.
-
-        Note that a shortform to::
-
-          import xtgeo
-          mysurf = xtgeo.surface.RegularSurface()
-          mysurf.from_roxar(project, 'TopAare', 'DepthSurface')
-
-        is::
-
-          import xtgeo
-          mysurf = xtgeo.surface_from_roxar(project, 'TopAare', 'DepthSurface')
-
-        Note also that horizon/zone name and category must exists in advance,
-        otherwise an Exception will be raised.
-
-        Args:
-            project (str or special): Name of project (as folder) if
-                outside RMS, og just use the magic project word if within RMS.
-            name (str): Name of surface/map
-            category (str): For horizons/zones or clipboard/general2d_data: for
-                example 'DS_extracted'
-            stype (str): RMS folder type, 'horizons' (default), 'zones', 'clipboard'
-                or 'general2d_data'
-            realisation (int): Realisation number, default is 0
-
-        Returns:
-            Object instance updated
-
-        Raises:
-            ValueError: Various types of invalid inputs.
-
-        Example:
-            Here the from_roxar method is used to initiate the object
-            directly::
-
-              mymap = RegularSurface()
-              mymap.from_roxar(project, 'TopAare', 'DepthSurface')
-
-
-        """
-        kwargs = _regsurf_roxapi.import_horizon_roxapi(
-            project, name, category, stype, realisation
-        )
-
-        self.metadata.required = self
-        self._reset(**kwargs)
-
-    def to_roxar(
-        self, project, name, category, stype="horizons", realisation=0
-    ):  # pragma: no cover
-        """Store (export) a regular surface to a Roxar RMS project.
-
-        The export to the RMS project can be done either within the project
-        or outside the project. The storing is done to the Horizons or the
-        Zones folder in RMS.
-
-        Note:
-            The horizon or zone name and category must exists in advance,
-            otherwise an Exception will be raised.
-
-            When project is file path (direct access, outside RMS) then
-            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-            will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str or special): Name of project (as folder) if
-                outside RMS, og just use the magic project word if within RMS.
-            name (str): Name of surface/map
-            category (str): Required for horizons/zones: e.g. 'DS_extracted'. For
-                clipboard/general2d_data is reperesent the folder(s), where "" or None
-                means no folder, while e.g. "myfolder/subfolder" means that folders
-                myfolder/subfolder will be created if not already present. For
-                stype = 'trends', the category will not be applied
-            stype (str): RMS folder type, 'horizons' (default), 'zones', 'clipboard'
-                'general2d_data', 'trends'
-            realisation (int): Realisation number, default is 0
-
-        Raises:
-            ValueError: If name or category does not exist in the project
-
-        Example:
-            Here the from_roxar method is used to initiate the object
-            directly::
-
-              import xtgeo
-              topupperreek = xtgeo.surface_from_roxar(project, 'TopUpperReek',
-                                                    'DS_extracted')
-              topupperreek.values += 200
-
-              # export to file:
-              topupperreek.to_file('topupperreek.gri')
-
-              # store in project
-              topupperreek.to_roxar(project, 'TopUpperReek', 'DS_something')
-
-        Note::
-
-            When dealing with surfaces to and from ``stype="trends"``, the surface must
-            exist in advance, i.e. the Roxar API do not allow creating new surfaces.
-            Actually trends are read only, but a workaround using ``load()`` in Roxar
-            API makes it possible to overwrite existing surface trends. In addition,
-            ``realisation`` is not applied in trends.
-
-
-        .. versionadded:: 2.1 clipboard support
-        .. versionadded:: 2.19 general2d_data and trends support
-
-        """
-        _regsurf_roxapi.export_horizon_roxapi(
-            self, project, name, category, stype, realisation
-        )
-
-    @deprecation.deprecated(
-        deprecated_in="2.15",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.surface.surface_from_cube() instead",
-    )
-    def from_cube(self, cube, zlevel):
-        """Make a constant surface from a Cube, at a given time/depth level.
-
-        The surface instance will have exactly the same origins and increments
-        as the cube.
-
-        Args:
-            cube (Cube): XTGeo Cube instance
-            zlevel (float): Depth or Time (or whatever) value of the surface
-
-        Returns:
-            Object instance updated
-
-        Example:
-            Here the from_roxar method is used to initiate the object
-            directly::
-
-            >>> import xtgeo
-            >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-            >>> mymap = xtgeo.RegularSurface()
-            >>> mymap.from_cube(mycube, 2700)
-
-        """
-        props = [
-            "ncol",
-            "nrow",
-            "xori",
-            "yori",
-            "xinc",
-            "yinc",
-            "rotation",
-            "ilines",
-            "xlines",
-            "yflip",
-        ]
-
-        input_dict = {key: deepcopy(getattr(cube, key)) for key in props}
-
-        input_dict["values"] = ma.array(
-            np.full((input_dict["ncol"], input_dict["nrow"]), zlevel, dtype=np.float64)
-        )
-        self._reset(**input_dict)
-
-    @classmethod
-    def _read_cube(cls, cube, zlevel):
-        """Make a constant surface from a Cube, at a given time/depth level.
-
-        The surface instance will have exactly the same origins and increments
-        as the cube.
-
-        Args:
-            cube (Cube): XTGeo Cube instance
-            zlevel (float): Depth or Time (or whatever) value of the surface
-
-        Returns:
-            Object instance updated
-
-        Example:
-            Here the from_roxar method is used to initiate the object
-            directly::
-
-            >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-            >>> mymap = RegularSurface._read_cube(mycube, 2700)
-
-        """
-        props = [
-            "ncol",
-            "nrow",
-            "xori",
-            "yori",
-            "xinc",
-            "yinc",
-            "rotation",
-            "ilines",
-            "xlines",
-            "yflip",
-        ]
-
-        input_dict = {key: deepcopy(getattr(cube, key)) for key in props}
-
-        input_dict["values"] = ma.array(
-            np.full((input_dict["ncol"], input_dict["nrow"]), zlevel, dtype=np.float64)
-        )
-        return cls(**input_dict)
-
-    @classmethod
-    def _read_grid3d(cls, grid, template=None, where="top", mode="depth", rfactor=1):
-        """Extract a surface from a 3D grid.
-
-        Args:
-            grid (Grid): XTGeo Grid instance
-            template (RegularSurface): Using an existing surface as template
-            where (str): "top", "base" or use the syntax "2_top" where 2
-                is layer no. 2 and _top indicates top of cell, while "_base"
-                indicates base of cell
-            mode (str): "depth", "i" or "j"
-            rfactor (float): Determines how fine the extracted map is; higher values
-                for finer map (but computing time will increase). Will only apply if
-                template is None.
-
-        Returns:
-            Object instance
-
-        Example::
-
-
-            >>> import xtgeo
-            >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> # make surface from top (default)
-            >>> mymap = RegularSurface._read_grid3d(mygrid)
-
-        .. versionadded:: 2.14
-
-        """
-        args, _, _ = _regsurf_grid3d.from_grid3d(
-            grid, template=template, where=where, mode=mode, rfactor=rfactor
-        )
-        return cls(**args)
-
-    @deprecation.deprecated(
-        deprecated_in="2.15",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.surface_from_grid3d() instead",
-    )
-    def from_grid3d(self, grid, template=None, where="top", mode="depth", rfactor=1):
-        # It would perhaps to be natural to have this as a Grid() method also?
-        """Extract a surface from a 3D grid.
-
-        Args:
-            grid (Grid): XTGeo Grid instance
-            template(RegularSurface): Optional to use an existing surface as
-                template for geometry
-            where (str): "top", "base" or use the syntax "2_top" where 2
-                is layer no. 2 and _top indicates top of cell, while "_base"
-                indicates base of cell
-            mode (str): "depth", "i" or "j"
-            rfactor (float): Determines how fine the extracted map is; higher values
-                for finer map (but computing time will increase). Will only apply if
-                template is None.
-
-        Returns:
-            Object instance is updated in-place
-            When mode="depth", two RegularSurface: icols and jrows are also returned.
-
-        Example::
-
-            >>> import xtgeo
-            >>> mymap = RegularSurface()
-            >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
-            >>> # return two additonal maps
-            >>> ic, jr = mymap.from_grid3d(mygrid)
-
-        .. versionadded:: 2.1
-
-        """
-        args, ivalues, jvalues = _regsurf_grid3d.from_grid3d(
-            grid, template=template, where=where, mode=mode, rfactor=rfactor
-        )
-        self._reset(**args)
-        if ivalues is not None and jvalues is not None:
-            ivals = self.copy()
-            args["values"] = ivalues
-            ivals._reset(**args)
-            jvals = self.copy()
-            args["values"] = jvalues
-            jvals._reset(**args)
-            return ivals, jvals
-        else:
-            return None
-
-    def copy(self):
-        """Deep copy of a RegularSurface object to another instance.
-
-        Example::
-
-            >>> mymap = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> mymapcopy = mymap.copy()
-
-        """
-        # pylint: disable=protected-access
-
-        xsurf = RegularSurface(
-            ncol=self.ncol,
-            nrow=self.nrow,
-            xinc=self.xinc,
-            yinc=self.yinc,
-            xori=self.xori,
-            yori=self.yori,
-            rotation=self.rotation,
-            yflip=self.yflip,
-        )
-
-        xsurf._values = self._values.copy()
-
-        xsurf.ilines = self._ilines.copy()
-        xsurf.xlines = self._xlines.copy()
-        xsurf.filesrc = self._filesrc
-        xsurf.metadata.required = xsurf
-
-        return xsurf
-
-    def get_values1d(
-        self, order="C", asmasked=False, fill_value=xtgeo.UNDEF, activeonly=False
-    ):
-        """Get a 1D numpy or masked array of the map values.
-
-        Args:
-            order (str): Flatteting is in C (default) or F order
-            asmasked (bool): If true, return as MaskedArray, other as standard
-                numpy ndarray with undef as np.nan or fill_value
-            fill_value (str): Relevent only if asmasked is False, this
-                will be the value of undef entries
-            activeonly (bool): If True, only active cells. Keys 'asmasked' and
-                'fill_value' are not revelant.
-
-        Returns:
-            A numpy 1D array or MaskedArray
-
-        """
-        val = self.values.copy()
-
-        if order == "F":
-            val = ma.array(val.data, mask=val.mask, order="F")
-
-        val = val.ravel(order=order)
-
-        if activeonly:
-            val = val[~val.mask]
-
-        if not asmasked and not activeonly:
-            val = ma.filled(val, fill_value=fill_value)
-
-        return val
-
-    def set_values1d(self, val, order="C"):
-        """Update the values attribute based on a 1D input, multiple options.
-
-        If values are np.nan or values are > UNDEF_LIMIT, they will be
-        masked.
-
-        Args:
-            val (list-like): Set values as a 1D array
-            order (str): Input is C (default) or F order
-        """
-        if order == "F":
-            val = np.copy(val, order="C")
-
-        val = val.reshape((self.ncol, self.nrow))
-
-        if not isinstance(val, ma.MaskedArray):
-            val = ma.array(val)
-
-        val = ma.masked_greater(val, self.undef_limit)
-        val = ma.masked_invalid(val)
-
-        self.values = val
-
-    def get_rotation(self):
-        """Returns the surface roation, in degrees, from X, anti-clock."""
-        return self._rotation
-
-    def get_nx(self):
-        """Same as ncol (nx) (for backward compatibility)."""
-        return self._ncol
-
-    def get_ny(self):
-        """Same as nrow (ny) (for backward compatibility)."""
-        return self._nrow
-
-    def get_xori(self):
-        """Same as property xori (for backward compatibility)."""
-        return self._xori
-
-    def get_yori(self):
-        """Same as property yori (for backward compatibility)."""
-        return self._yori
-
-    def get_xinc(self):
-        """Same as property xinc (for backward compatibility)."""
-        return self._xinc
-
-    def get_yinc(self):
-        """Same as property yinc (for backward compatibility)."""
-        return self._yinc
-
-    def similarity_index(self, other):
-        """Report the degree of similarity between two maps, by comparing mean.
-
-        The method computes the average per surface, and the similarity
-        is difference in mean divided on mean of self. I.e. values close
-        to 0.0 mean small difference.
-
-        Args:
-            other (surface object): The other surface to compare with
-
-        """
-        ovalues = other.values
-        svalues = self.values
-
-        diff = math.pow(svalues.mean() - ovalues.mean(), 2)
-        diff = math.sqrt(diff)
-
-        try:
-            diff = diff / svalues.mean()
-        except ZeroDivisionError:
-            diff = -999
-
-        return diff
-
-    def compare_topology(self, other, strict=True):
-        """Check that two object has the same topology, i.e. map definitions.
-
-        Map definitions such as origin, dimensions, number of defined cells...
-
-        Args:
-            other (surface object): The other surface to compare with
-            strict (bool): If false, the masks are not compared
-
-        Returns:
-            True of same topology, False if not
-        """
-        tstatus = True
-
-        # consider refactor to getattr() instead!
-        chklist = set(
-            ["_ncol", "_nrow", "_xori", "_yori", "_xinc", "_yinc", "_rotation"]
-        )
-        for skey, sval in self.__dict__.items():
-            if skey in chklist:
-                for okey, oval in other.__dict__.items():
-                    if skey == okey and sval != oval:
-                        logger.info("CMP %s: %s vs %s", skey, sval, oval)
-                        tstatus = False
-
-        if not tstatus:
-            return False
-
-        # check that masks are equal
-        mas1 = ma.getmaskarray(self.values)
-        mas2 = ma.getmaskarray(other.values)
-        if (
-            strict
-            and isinstance(mas1, np.ndarray)
-            and isinstance(mas2, np.ndarray)
-            and not np.array_equal(mas1, mas2)
-        ):
-            logger.warning("Masks differ, not consistent with 'strict'")
-            return False
-        return True
-
-    def swapaxes(self):
-        """Swap (flip) the axes columns vs rows, keep origin but reverse yflip."""
-        _regsurf_utils.swapaxes(self)
-
-    def get_map_xycorners(self):
-        """Get the X and Y coordinates of the map corners.
-
-        Returns a tuple on the form
-        ((x0, y0), (x1, y1), (x2, y2), (x3, y3)) where
-        (if unrotated and normal flip) 0 is the lower left
-        corner, 1 is the right, 2 is the upper left, 3 is the upper right.
-        """
-        rot1 = self._rotation * math.pi / 180
-        rot2 = rot1 + (math.pi / 2.0)
-
-        xc0 = self._xori
-        yc0 = self._yori
-
-        xc1 = self._xori + (self.ncol - 1) * math.cos(rot1) * self._xinc
-        yc1 = self._yori + (self.ncol - 1) * math.sin(rot1) * self._xinc
-
-        xc2 = self._xori + (self.nrow - 1) * math.cos(rot2) * self._yinc * self._yflip
-        yc2 = self._yori + (self.nrow - 1) * math.sin(rot2) * self._yinc * self._yflip
-
-        xc3 = xc2 + (self.ncol - 1) * math.cos(rot1) * self._xinc
-        yc3 = yc2 + (self.ncol - 1) * math.sin(rot1) * self._xinc
-
-        return ((xc0, yc0), (xc1, yc1), (xc2, yc2), (xc3, yc3))
-
-    def get_value_from_xy(self, point=(0.0, 0.0), sampling="bilinear"):
-        """Return the map value given a X Y point.
-
-        Args:
-            point (float tuple): Position of X and Y coordinate
-            sampling (str): Sampling method, either "bilinear" for bilinear
-                interpolation, or "nearest" for nearest node sampling (e.g. facies maps)
-
-        Returns:
-            The map value (interpolated). None if XY is outside defined map
-
-        Example::
-            mvalue = map.get_value_from_xy(point=(539291.12, 6788228.2))
-
-
-        .. versionchanged:: 2.14 Added keyword option `sampling`
-        """
-        zcoord = _regsurf_oper.get_value_from_xy(self, point=point, sampling=sampling)
-
-        return zcoord
-
-    def get_xy_value_from_ij(self, iloc, jloc, zvalues=None):
-        """Returns x, y, z(value) from a single i j location.
-
-        Args:
-            iloc (int): I (col) location (base is 1)
-            jloc (int): J (row) location (base is 1)
-            zvalues (ndarray). If this is used in a loop it is wise
-                to precompute the numpy surface once in the caller,
-                and submit the numpy array (use surf.get_values1d()).
-
-        Returns:
-            The x, y, z values at location iloc, jloc
-        """
-        xval, yval, value = _regsurf_oper.get_xy_value_from_ij(
-            self, iloc, jloc, zvalues=zvalues
-        )
-
-        return xval, yval, value
-
-    def get_ij_values(self, zero_based=False, asmasked=False, order="C"):
-        """Return I J numpy 2D arrays, optionally as masked arrays.
-
-        Args:
-            zero_based (bool): If False, first number is 1, not 0
-            asmasked (bool): If True, UNDEF map nodes are skipped
-            order (str): 'C' (default) or 'F' order (row vs column major)
-        """
-        return _regsurf_oper.get_ij_values(
-            self, zero_based=zero_based, asmasked=asmasked, order=order
-        )
-
-    def get_ij_values1d(self, zero_based=False, activeonly=True, order="C"):
-        """Return I J numpy as 1D arrays.
-
-        Args:
-            zero_based (bool): If False, first number is 1, not 0
-            activeonly (bool): If True, UNDEF map nodes are skipped
-            order (str): 'C' (default) or 'F' order (row vs column major)
-        """
-        return _regsurf_oper.get_ij_values1d(
-            self, zero_based=zero_based, activeonly=activeonly, order=order
-        )
-
-    def get_xy_values(self, order="C", asmasked=True):
-        """Return coordinates for X and Y as numpy (masked) 2D arrays.
-
-        Args:
-            order (str): 'C' (default) or 'F' order (row major vs column major)
-            asmasked (bool): If True , inactive nodes are masked.
-        """
-        xvals, yvals = _regsurf_oper.get_xy_values(self, order=order, asmasked=asmasked)
-
-        return xvals, yvals
-
-    def get_xy_values1d(self, order="C", activeonly=True):
-        """Return coordinates for X and Y as numpy 1D arrays.
-
-        Args:
-            order (str): 'C' (default) or 'F' order (row major vs column major)
-            activeonly (bool): Only active cells are returned.
-        """
-        xvals, yvals = _regsurf_oper.get_xy_values1d(
-            self, order=order, activeonly=activeonly
-        )
-
-        return xvals, yvals
-
-    def get_xyz_values(self):
-        """Return coordinates for X Y and Z (values) as numpy (masked) 2D arrays."""
-        xcoord, ycoord = self.get_xy_values(asmasked=True)
-
-        values = self.values.copy()
-
-        return xcoord, ycoord, values
-
-    def get_xyz_values1d(self, order="C", activeonly=True, fill_value=np.nan):
-        """Return coordinates for X Y and Z (values) as numpy 1D arrays.
-
-        Args:
-            order (str): 'C' (default) or 'F' order (row major vs column major)
-            activeonly (bool): Only active cells are returned.
-            fill_value (float): If activeonly is False, value of inactive nodes
-        """
-        xcoord, ycoord = self.get_xy_values1d(order=order, activeonly=activeonly)
-
-        values = self.get_values1d(
-            order=order, asmasked=False, fill_value=fill_value, activeonly=activeonly
-        )
-
-        return xcoord, ycoord, values
-
-    def get_dataframe(
-        self, ijcolumns=False, ij=False, order="C", activeonly=True, fill_value=np.nan
-    ):
-        """Return a Pandas dataframe object, with columns X_UTME, Y_UTMN, VALUES.
-
-        Args:
-            ijcolumns (bool): If True, and IX and JY indices will be
-               added as dataframe columns. Redundant, use "ij" instead.
-            ij (bool): Same as ijcolumns. If True, and IX and JY indices will be
-               added as dataframe columns. Preferred syntax
-            order (str): 'C' (default) for C order (row fastest), or 'F'
-               for Fortran order (column fastest)
-            activeonly (bool): If True, only active nodes are listed. If
-                False, the values will have fill_value default None = NaN
-                as values
-            fill_value (float): Value of inactive nodes if activeonly is False
-
-        Example::
-
-            >>> import xtgeo
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> dfr = surf.get_dataframe()
-            >>> dfr.to_csv('somecsv.csv')
-
-        Returns:
-            A Pandas dataframe object.
-        """
-        xcoord, ycoord, values = self.get_xyz_values1d(
-            order=order, activeonly=activeonly, fill_value=fill_value
-        )
-
-        entry = OrderedDict()
-
-        if ijcolumns or ij:
-            ixn, jyn = self.get_ij_values1d(order=order, activeonly=activeonly)
-            entry["IX"] = ixn
-            entry["JY"] = jyn
-
-        entry.update([("X_UTME", xcoord), ("Y_UTMN", ycoord), ("VALUES", values)])
-
-        dataframe = pd.DataFrame(entry)
-        return dataframe
-
-    dataframe = get_dataframe  # for compatibility backwards
-
-    def get_xy_value_lists(self, lformat="webportal", xyfmt=None, valuefmt=None):
-        """Returns two lists for coordinates (x, y) and values.
-
-        For lformat = 'webportal' (default):
-
-        The lists are returned as xylist and valuelist, where xylist
-        is on the format:
-
-            [(x1, y1), (x2, y2) ...] (a list of x, y tuples)
-
-        and valuelist is one the format
-
-            [v1, v2, ...]
-
-        Inactive cells will be ignored.
-
-        Args:
-            lformat (string): List return format ('webportal' is default,
-                other options later)
-            xyfmt (string): The formatter for xy numbers, e.g. '12.2f'
-                (default None). Note no checks on valid input.
-            valuefmt (string): The formatter for values e.g. '8.4f'
-                (default None). Note no checks on valid input.
-
-        Returns:
-            xylist, valuelist
-
-        Example:
-
-            >>> import xtgeo
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> xylist, valuelist = surf.get_xy_value_lists(valuefmt='6.2f')
-        """
-        xylist = []
-        valuelist = []
-
-        zvalues = self.get_values1d()
-
-        if lformat != "webportal":
-            raise ValueError("Unsupported lformat")
-
-        for jnum in range(self.nrow):
-            for inum in range(self.ncol):
-                xcv, ycv, vcv = self.get_xy_value_from_ij(
-                    inum + 1, jnum + 1, zvalues=zvalues
-                )
-
-                if vcv is not None:
-                    if xyfmt is not None:
-                        xcv = float(f"{xcv:{xyfmt}}")
-                        ycv = float(f"{ycv:{xyfmt}}")
-                    if valuefmt is not None:
-                        vcv = float(f"{vcv:{valuefmt}}")
-                    valuelist.append(vcv)
-                    xylist.append((xcv, ycv))
-
-        return xylist, valuelist
-
-    # ==================================================================================
-    # Crop, interpolation, smooth or fill of values (possibly many methods here)
-    # ==================================================================================
-
-    def autocrop(self):
-        """Automatic cropping of the surface to minimize undefined areas.
-
-        This method is simply removing undefined "white areas". The
-        instance will be updated with new values for xori, yori, ncol, etc. Rotation
-        will never change
-
-        Returns:
-            RegularSurface instance is updated in-place
-
-        .. versionadded:: 2.12
-        """
-        _regsurf_utils.autocrop(self)
-
-    def fill(self, fill_value=None):
-        """Fast infilling of undefined values.
-
-        Note that minimum and maximum values will not change.
-
-        Algorithm if `fill_value` is not set is based on a nearest node extrapolation.
-        Technically, ``scipy.ndimage.distance_transform_edt`` is applied. If fill_value
-        is set by a scalar, that (constant) value be be applied
-
-        Args:
-            fill_value (float): If defined, fills all undefined cells with that value.
-
-        Returns:
-            RegularSurface instance is updated in-place
-
-        .. versionadded:: 2.1
-        .. versionchanged:: 2.6 Added option key `fill_value`
-        """
-        _regsurf_gridding.surf_fill(self, fill_value=fill_value)
-
-    def smooth(self, method="median", iterations=1, width=1):
-        """Various smoothing methods for surfaces.
-
-        Args:
-            method: Smoothing method (median)
-            iterations: Number of iterations
-            width: Range of influence (in nodes)
-
-        .. versionadded:: 2.1
-        """
-        if method == "median":
-            _regsurf_gridding.smooth_median(self, iterations=iterations, width=width)
-        else:
-            raise ValueError("Unsupported method for smoothing")
-
-    # ==================================================================================
-    # Operation on map values (list to be extended)
-    # ==================================================================================
-
-    def operation(self, opname, value):
-        """Do operation on map values.
-
-        Do operations on the current map values. Valid operations are:
-
-        * 'elilt' or 'eliminatelessthan': Eliminate less than <value>
-
-        * 'elile' or 'eliminatelessequal': Eliminate less or equal than <value>
-
-        Args:
-            opname (str): Name of operation. See list above.
-            value (*): A scalar number (float) or a tuple of two floats,
-                dependent on operation opname.
-
-        Examples::
-
-            surf.operation('elilt', 200)  # set all values < 200 as undef
-        """
-        if opname in ("elilt", "eliminatelessthan"):
-            self._values = ma.masked_less(self._values, value)
-        elif opname in ("elile", "eliminatelessequal"):
-            self._values = ma.masked_less_equal(self._values, value)
-        else:
-            raise ValueError("Invalid operation name")
-
-    # ==================================================================================
-    # Operations restricted to inside/outside polygons
-    # ==================================================================================
-
-    def operation_polygons(self, poly, value, opname="add", inside=True):
-        """A generic function for map operations inside or outside polygon(s).
-
-        Args:
-            poly (Polygons): A XTGeo Polygons instance
-            value(float or RegularSurface): Value to add, subtract etc
-            opname (str): Name of operation... 'add', 'sub', etc
-            inside (bool): If True do operation inside polygons; else outside.
-        """
-        _regsurf_oper.operation_polygons(
-            self, poly, value, opname=opname, inside=inside
-        )
-
-    # shortforms
-    def add_inside(self, poly, value):
-        """Add a value (scalar or other map) inside polygons."""
-        self.operation_polygons(poly, value, opname="add", inside=True)
-
-    def add_outside(self, poly, value):
-        """Add a value (scalar or other map) outside polygons."""
-        self.operation_polygons(poly, value, opname="add", inside=False)
-
-    def sub_inside(self, poly, value):
-        """Subtract a value (scalar or other map) inside polygons."""
-        self.operation_polygons(poly, value, opname="sub", inside=True)
-
-    def sub_outside(self, poly, value):
-        """Subtract a value (scalar or other map) outside polygons."""
-        self.operation_polygons(poly, value, opname="sub", inside=False)
-
-    def mul_inside(self, poly, value):
-        """Multiply a value (scalar or other map) inside polygons."""
-        self.operation_polygons(poly, value, opname="mul", inside=True)
-
-    def mul_outside(self, poly, value):
-        """Multiply a value (scalar or other map) outside polygons."""
-        self.operation_polygons(poly, value, opname="mul", inside=False)
-
-    def div_inside(self, poly, value):
-        """Divide a value (scalar or other map) inside polygons."""
-        self.operation_polygons(poly, value, opname="div", inside=True)
-
-    def div_outside(self, poly, value):
-        """Divide a value (scalar or other map) outside polygons."""
-        self.operation_polygons(poly, value, opname="div", inside=False)
-
-    def set_inside(self, poly, value):
-        """Set a value (scalar or other map) inside polygons."""
-        self.operation_polygons(poly, value, opname="set", inside=True)
-
-    def set_outside(self, poly, value):
-        """Set a value (scalar or other map) outside polygons."""
-        self.operation_polygons(poly, value, opname="set", inside=False)
-
-    def eli_inside(self, poly):
-        """Eliminate current map values inside polygons."""
-        self.operation_polygons(poly, 0, opname="eli", inside=True)
-
-    def eli_outside(self, poly):
-        """Eliminate current map values outside polygons."""
-        self.operation_polygons(poly, 0, opname="eli", inside=False)
-
-    # ==================================================================================
-    # Operation with secondary map
-    # ==================================================================================
-
-    def add(self, other):
-        """Add another map to current map."""
-        _regsurf_oper.operations_two(self, other, oper="add")
-
-    def subtract(self, other):
-        """Subtract another map from current map."""
-        _regsurf_oper.operations_two(self, other, oper="sub")
-
-    def multiply(self, other):
-        """Multiply another map and current map."""
-        _regsurf_oper.operations_two(self, other, oper="mul")
-
-    def divide(self, other):
-        """Divide current map with another map."""
-        _regsurf_oper.operations_two(self, other, oper="div")
-
-    # ==================================================================================
-    # Interacion with points
-    # ==================================================================================
-
-    def gridding(self, points, method="linear", coarsen=1):
-        """Grid a surface from points.
-
-        Args:
-            points(Points): XTGeo Points instance.
-            method (str): Gridding method option: linear / cubic / nearest
-            coarsen (int): Coarsen factor, to speed up gridding, but will
-                give poorer result.
-
-        Example::
-
-            >>> import xtgeo
-            >>> mypoints = xtgeo.Points(points_dir + '/pointset2.poi')
-            >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-
-            >>> # update the surface by gridding the points
-            >>> mysurf.gridding(mypoints)
-
-        Raises:
-            RuntimeError: If not possible to grid for some reason
-            ValueError: If invalid input
-
-        """
-        if not isinstance(points, xtgeo.xyz.Points):
-            raise ValueError("Argument not a Points instance")
-
-        logger.info("Do gridding...")
-
-        _regsurf_gridding.points_gridding(self, points, coarsen=coarsen, method=method)
-
-    # ==================================================================================
-    # Interacion with other surface
-    # ==================================================================================
-
-    def resample(self, other, mask=True, sampling="bilinear"):
-        """Resample an instance surface values from another surface instance.
-
-        Note that there may be some 'loss' of nodes at the edges of the
-        updated map, as only the 'inside' nodes in the updated map
-        versus the input map are applied.
-
-        The interpolation algorithm in resample is bilinear interpolation. The
-        topolopogy of the surface (map definitions, rotation, ...) will not change,
-        only the map values. Areas with undefined nodes in ``other`` will become
-        undefined in the instance if mask is True; othewise they will be kept as is.
-
-        Args:
-            other (RegularSurface): Surface to resample from.
-            mask (bool): If True (default) nodes outside will be made undefined,
-                if False then values will be kept as original
-            sampling (str): Either 'bilinear' interpolation (default) or, 'nearest' for
-                nearest node. The latter can be useful for resampling discrete maps.
-
-        Example::
-
-            # map with 230x210 columns, rotation 20
-            surf1 = xtgeo.surface_from_file("some1.gri")
-            # map with 270x190 columns, rotation 0
-            surf2 = xtgeo.surface_from_file("some2.gri")
-            # will sample (interpolate) surf2's values to surf1
-            surf1.resample(surf2)
-
-        Returns:
-            Instance's surface values will be updated in-place.
-
-
-        .. versionchanged:: 2.9
-           Added ``mask`` keyword, default is True for backward compatibility.
-
-        .. versionchanged:: 2.21
-           Added ``sampling`` keyword option.
-
-        """
-        if not isinstance(other, RegularSurface):
-            raise ValueError("Argument not a RegularSurface instance")
-
-        logger.info("Do resampling...")
-
-        _regsurf_oper.resample(self, other, mask=mask, sampling=sampling)
-
-    # ==================================================================================
-    # Change a surface more fundamentally
-    # ==================================================================================
-
-    def unrotate(self, factor=2):
-        r"""Unrotete a map instance, and this will also change nrow, ncol, xinc, etc.
-
-        The default sampling (factor=2) makes a finer grid in order to
-        avoid artifacts, and this default can be used in most cases.
-
-        If an even finer grid is wanted, increase the factor. Theoretically the
-        new increment for factor=N is between :math:`\\frac{1}{N}` and
-        :math:`\\frac{1}{N}\\sqrt{2}` of the original increment,
-        dependent on the rotation of the original surface.
-
-        If the current instance already is unrotated, nothing is done.
-
-        Args:
-            factor (int): Refinement factor (>= 1)
-
-        """
-        if abs(self.rotation) < 0.00001:
-            logger.info("Surface has no rotation, nothing is done")
-            return
-
-        if factor < 1:
-            raise ValueError("Unrotate refinement factor cannot be be less than 1")
-
-        if not isinstance(factor, int):
-            raise ValueError("Refinementfactor must an integer")
-
-        scopy = self
-        if scopy._yflip < 0:
-            scopy = self.copy()
-            scopy.swapaxes()
-
-        xlen = scopy.xmax - scopy.xmin
-        ylen = scopy.ymax - scopy.ymin
-        ncol = scopy.ncol * factor
-        nrow = scopy.nrow * factor
-        xinc = xlen / (ncol - 1)  # node based, not cell center based
-        yinc = ylen / (nrow - 1)
-        vals = ma.zeros((ncol, nrow), order="C")
-
-        nonrot = RegularSurface(
-            xori=scopy.xmin,
-            yori=scopy.ymin,
-            xinc=xinc,
-            yinc=yinc,
-            ncol=ncol,
-            nrow=nrow,
-            values=vals,
-            yflip=1,
-        )
-        nonrot.resample(scopy)
-
-        self._values = nonrot.values
-        self._nrow = nonrot.nrow
-        self._ncol = nonrot.ncol
-        self._rotation = nonrot.rotation
-        self._xori = nonrot.xori
-        self._yori = nonrot.yori
-        self._xinc = nonrot.xinc
-        self._yinc = nonrot.yinc
-        self._yflip = nonrot.yflip
-        self._ilines = nonrot.ilines
-        self._xlines = nonrot.xlines
-
-    def refine(self, factor):
-        """Refine a surface with a factor.
-
-        Range for factor is 2 to 10.
-
-        Note that there may be some 'loss' of nodes at the edges of the
-        updated map, as only the 'inside' nodes in the updated map
-        versus the input map are applied.
-
-        Args:
-            factor (int): Refinement factor
-        """
-        logger.info("Do refining...")
-
-        if not isinstance(factor, int):
-            raise ValueError("Argument not a, Integer")
-
-        if factor < 2 or factor >= 10:
-            raise ValueError("Argument exceeds range 2 .. 10")
-
-        xlen = self._xinc * (self._ncol - 1)
-        ylen = self._yinc * (self._nrow - 1)
-
-        proxy = self.copy()
-        self._ncol = proxy.ncol * factor
-        self._nrow = proxy.nrow * factor
-        self._xinc = xlen / (self._ncol - 1)
-        self._yinc = ylen / (self._nrow - 1)
-
-        self._values = ma.zeros((self._ncol, self._nrow))
-
-        self._ilines = np.array(range(1, self._ncol + 1), dtype=np.int32)
-        self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
-
-        self.resample(proxy)
-
-        del proxy
-        logger.info("Do refining... done")
-
-    def coarsen(self, factor):
-        """Coarsen a surface with a factor.
-
-        Range for coarsening is 2 to 10, where e.g. 2 meaning half the number of
-        columns and rows.
-
-        Note that there may be some 'loss' of nodes at the edges of the
-        updated map, as only the 'inside' nodes in the updated map
-        versus the input map are applied.
-
-        Args:
-            factor (int): Coarsen factor (2 .. 10)
-
-        Raises:
-            ValueError: Coarsen is too large, giving too few nodes in result
-        """
-        logger.info("Do coarsening...")
-        if not isinstance(factor, int):
-            raise ValueError("Argument not a, Integer")
-
-        if factor < 2 or factor >= 10:
-            raise ValueError("Argument exceeds range 2 .. 10")
-
-        proxy = self.copy()
-        xlen = self._xinc * (self._ncol - 1)
-        ylen = self._yinc * (self._nrow - 1)
-
-        ncol = int(round(proxy._ncol / factor))
-        nrow = int(round(proxy._nrow / factor))
-
-        if ncol < 4 or nrow < 4:
-            raise ValueError(
-                "Coarsen is too large, giving ncol or nrow less than 4 nodes"
-            )
-
-        self._ncol = ncol
-        self._nrow = nrow
-
-        self._xinc = xlen / (self._ncol - 1)
-        self._yinc = ylen / (self._nrow - 1)
-
-        self._values = ma.zeros((self._ncol, self._nrow))
-
-        self._ilines = np.array(range(1, self._ncol + 1), dtype=np.int32)
-        self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
-
-        self.resample(proxy)
-
-        del proxy
-        logger.info("Do coarsening... done")
-
-    # ==================================================================================
-    # Interacion with a grid3d
-    # ==================================================================================
-
-    def slice_grid3d(self, grid, prop, zsurf=None, sbuffer=1):
-        """Slice the grid property and update the instance surface to sampled values.
-
-        Args:
-            grid (Grid): Instance of a Grid.
-            prop (GridProperty): Instance of a GridProperty, belongs to grid
-            zsurf (surface object): Instance of map, which is used a slicer.
-                If None, then the surface instance itself is used a slice
-                criteria. Note that zsurf must have same map defs as the
-                surface instance.
-            sbuffer (int): Default is 1; if "holes" after sampling
-                extend this to e.g. 3
-        Example::
-
-            >>> import xtgeo
-            >>> grd = xtgeo.grid_from_file(reek_dir + '/REEK.EGRID')
-            >>> prop = xtgeo.gridproperty_from_file(
-            ...     reek_dir + '/REEK.UNRST',
-            ...     name='PRESSURE',
-            ...     date="first",
-            ...     grid=grd,
-            ... )
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> # update surf to sample the 3D grid property:
-            >>> surf.slice_grid3d(grd, prop)
-
-        Raises:
-            Exception if maps have different definitions (topology)
-        """
-        if not isinstance(grid, xtgeo.grid3d.Grid):
-            raise ValueError("First argument must be a grid instance")
-
-        ier = _regsurf_grid3d.slice_grid3d(
-            self, grid, prop, zsurf=zsurf, sbuffer=sbuffer
-        )
-
-        if ier != 0:
-            raise RuntimeError(
-                "Wrong status from routine; something went wrong. Contact the author"
-            )
-
-    # ==================================================================================
-    # Interacion with a cube
-    # ==================================================================================
-
-    def slice_cube(
-        self,
-        cube,
-        zsurf=None,
-        sampling="nearest",
-        mask=True,
-        snapxy=False,
-        deadtraces=True,
-        algorithm=2,
-    ):
-        """Slice the cube and update the instance surface to sampled cube values.
-
-        Args:
-            cube (object): Instance of a Cube()
-            zsurf (surface object): Instance of a depth (or time) map, which
-                is the depth or time map (or...) that is used a slicer.
-                If None, then the surface instance itself is used a slice
-                criteria. Note that zsurf must have same map defs as the
-                surface instance.
-            sampling (str): 'nearest' for nearest node (default), or
-                'trilinear' for trilinear interpolation.
-            mask (bool): If True (default), then the map values outside
-                the cube will be undef. Otherwise, map will be kept as is.
-            snapxy (bool): If True (optional), then the map values will get
-                values at nearest Cube XY location. Only relevant to use if
-                surface is derived from seismic coordinates (e.g. Auto4D).
-            deadtraces (bool): If True (default) then dead cube traces
-                (given as value 2 in SEGY trace headers), are treated as
-                undefined, and map will become undefined at dead trace location.
-            algorithm (int): 1 for legacy method, 2 (default from 2.9) for
-                new method available in xtgeo from version 2.9
-
-        Example::
-
-            >>> import xtgeo
-            >>> cube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> # update surf to sample cube values:
-            >>> surf.slice_cube(cube)
-
-        Raises:
-            Exception if maps have different definitions (topology)
-            RuntimeWarning if number of sampled nodes is less than 10%
-
-        .. versionchanged:: 2.9 Added ``algorithm`` keyword, default is 2
-        """
-        ier = _regsurf_cube.slice_cube(
-            self,
-            cube,
-            zsurf=zsurf,
-            sampling=sampling,
-            mask=mask,
-            snapxy=snapxy,
-            deadtraces=deadtraces,
-            algorithm=algorithm,
-        )
-
-        if ier == -4:
-            xtg.warnuser("Number of sampled surface nodes < 10 percent of Cube nodes")
-            print("Number of sampled surface nodes < 10 percent of Cube nodes")
-        elif ier == -5:
-            xtg.warn("No nodes sampled: map is 100 percent outside of cube?")
-
-    def slice_cube_window(
-        self,
-        cube,
-        zsurf=None,
-        other=None,
-        other_position="below",
-        sampling="nearest",
-        mask=True,
-        zrange=None,
-        ndiv=None,
-        attribute="max",
-        maskthreshold=0.1,
-        snapxy=False,
-        showprogress=False,
-        deadtraces=True,
-        algorithm=2,
-    ):
-        """Slice the cube within a vertical window and get the statistical attrubutes.
-
-        The statistical attributes can be min, max etc. Attributes are:
-
-        * 'max' for maximum
-
-        * 'min' for minimum
-
-        * 'rms' for root mean square
-
-        * 'mean' for expected value
-
-        * 'var' for variance (population var; https://en.wikipedia.org/wiki/Variance)
-
-        * 'maxpos' for maximum of positive values
-
-        * 'maxneg' for negative maximum of negative values
-
-        * 'maxabs' for maximum of absolute values
-
-        * 'sumpos' for sum of positive values using cube sampling resolution
-
-        * 'sumneg' for sum of negative values using cube sampling resolution
-
-        * 'meanabs' for mean of absolute values
-
-        * 'meanpos' for mean of positive values
-
-        * 'meanneg' for mean of negative values
-
-        Note that 'all' can be used to select all attributes that are currently
-        available.
-
-        Args:
-            cube (Cube): Instance of a Cube()
-            zsurf (RegularSurface): Instance of a depth (or time) map, which
-                is the depth or time map (or...) that is used a slicer.
-                If None, then the surface instance itself is used a slice
-                criteria. Note that zsurf must have same map defs as the
-                surface instance.
-            other (RegularSurface): Instance of other surface if window is
-                between surfaces instead of a static window. The zrange
-                input is then not applied.
-            sampling (str): 'nearest'/'trilinear'/'cube' for nearest node (default),
-                 or 'trilinear' for trilinear interpolation. The 'cube' option is
-                 only available with algorithm = 2 and will overrule ndiv and sample
-                 at the cube's Z increment resolution.
-            mask (bool): If True (default), then the map values outside
-                the cube will be undef, otherwise map will be kept as-is
-            zrange (float): The one-sided "radius" range of the window, e.g. 10
-                (10 is default) units (e.g. meters if in depth mode).
-                The full window is +- zrange (i.e. diameter).
-                If other surface is present, zrange is computed based on that.
-            ndiv (int): Number of intervals for sampling within zrange. None
-                means 'auto' sampling, using 0.5 of cube Z increment as basis. If
-                algorithm = 2 and sampling is 'cube', the cube Z increment
-                will be used.
-            attribute (str or list): The requested attribute(s), e.g.
-                'max' value. May also be a list of attributes, e.g.
-                ['min', 'rms', 'max']. By such, a dict of surface objects is
-                returned. Note 'all' will make a list of possible attributes
-            maskthreshold (float): Only if two surface; if isochore is less
-                than given value, the result will be masked.
-            snapxy (bool): If True (optional), then the map values will get
-                values at nearest Cube XY location. Only relevant to use if
-                surface is derived from seismic coordinates (e.g. Auto4D).
-            showprogress (bool): If True, then a progress is printed to stdout.
-            deadtraces (bool): If True (default) then dead cube traces
-                (given as value 2 in SEGY trace headers), are treated as
-                undefined, nad map will be undefined at dead trace location.
-            algorithm (int): 1 for legacy method, 2 (default) for new faster
-                and more precise method available from xtgeo version 2.9.
-
-        Example::
-
-            >>> import xtgeo
-            >>> cube = xtgeo.Cube(cube_dir + "/ib_test_cube2.segy")
-            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> # update surf to sample cube values in a total range of 30 m:
-            >>> surf.slice_cube_window(cube, attribute='min', zrange=15.0)
-
-            >>> # Here a list is given instead:
-            >>> alst = ['min', 'max', 'rms']
-
-            >>> myattrs = surf.slice_cube_window(cube, attribute=alst, zrange=15.0)
-            >>> for attr in myattrs.keys():
-            ...     _ = myattrs[attr].to_file(
-            ...         outdir + '/myfile_' + attr + '.gri'
-            ...     )
-
-        Raises:
-            Exception if maps have different definitions (topology)
-            ValueError if attribute is invalid.
-
-        Returns:
-            If `attribute` is a string, then the instance is updated and
-            None is returned. If `attribute` is a list, then a dictionary
-            of surface objects is returned.
-
-        .. versionchanged:: 2.9 Added ``algorithm`` keyword, default is now 2,
-                            while 1 is the legacy version
-        """
-        if other is None and zrange is None:
-            zrange = 10
-
-        asurfs = _regsurf_cube_window.slice_cube_window(
-            self,
-            cube,
-            zsurf=zsurf,
-            other=other,
-            other_position=other_position,
-            sampling=sampling,
-            mask=mask,
-            zrange=zrange,
-            ndiv=ndiv,
-            attribute=attribute,
-            maskthreshold=maskthreshold,
-            snapxy=snapxy,
-            showprogress=showprogress,
-            deadtraces=deadtraces,
-            algorithm=algorithm,
-        )
-
-        return asurfs
-
-    # ==================================================================================
-    # Special methods
-    # ==================================================================================
-
-    def get_boundary_polygons(
-        self,
-        alpha_factor: Optional[float] = 1.0,
-        convex: Optional[bool] = False,
-        simplify: Optional[bool] = True,
-    ):
-        """Extract boundary polygons from the surface.
-
-        A regular surface may often contain areas of undefined (masked) entries which
-        makes the surface appear 'ragged' and/or 'patchy'.
-
-        This method extracts boundaries around the surface patches, and the
-        precision depends on the keyword settings. As default, the ``alpha_factor``
-        of 1 makes a precise boundary, while a larger alpha_factor makes more rough
-        polygons.
-
-        .. image:: images/regsurf_boundary_polygons.png
-           :width: 600
-           :align: center
-
-        |
-
-        Args:
-            alpha_factor: An alpha multiplier, where lowest allowed value is 1.0.
-                A higher number will produce smoother and less accurate polygons. Not
-                applied if convex is set to True.
-            convex: The default is False, which means that a "concave hull" algorithm
-                is used. If convex is True, the alpha factor is overridden to a large
-                number, producing a 'convex' shape boundary instead.
-            simplify: If True, a simplification is done in order to reduce the number
-                of points in the polygons, where tolerance is 0.1. Another
-                alternative to True is to input a Dict on the form
-                ``{"tolerance": 2.0, "preserve_topology": True}``, cf. the
-                :func:`Polygons.simplify()` method. For details on e.g. tolerance, see
-                Shapely's simplify() method.
-
-        Returns:
-            A XTGeo Polygons instance
-
-        Example::
-
-            surf = xtgeo.surface_from_file("mytop.gri")
-            # eliminate all values below a depth, e.g. a fluid contact
-            surf.values = np.ma.masked_greater(surf.values, 2100.0)
-            boundary = surf.get_boundary_polygons()
-            # the boundary may contain several smaller polygons; keep only the
-            # largest (first) polygon which is number 0:
-            boundary.filter_byid([0])  # polygon is updated in-place
-
-        See also:
-            The :func:`Polygons.boundary_from_points()` class method.
-
-        .. versionadded:: 3.1
-        """
-        return _regsurf_boundary.create_boundary(self, alpha_factor, convex, simplify)
-
-    def get_fence(
-        self, xyfence: np.ndarray, sampling: Optional[str] = "bilinear"
-    ) -> np.ma.MaskedArray:
-        """Sample the surface along X and Y positions (numpy arrays) and get Z.
-
-        .. versionchanged:: 2.14 Added keyword option `sampling`
-
-        Returns a masked numpy 2D array similar as input, but with updated
-        Z values, which are masked if undefined.
-
-        Args:
-            xyfence: A 2D numpy array with shape (N, 3) where columns
-                are (X, Y, Z). The Z will be updated to the map.
-            sampling: Use "bilinear" (default) for interpolation or "nearest" for
-                snapping to nearest node.
-
-        """
-        xyfence = _regsurf_oper.get_fence(self, xyfence, sampling=sampling)
-
-        return xyfence
-
-    def get_randomline(
-        self,
-        fencespec: Union[np.ndarray, object],
-        hincrement: Optional[Union[bool, float]] = None,
-        atleast: Optional[int] = 5,
-        nextend: Optional[int] = 2,
-        sampling: Optional[str] = "bilinear",
-    ) -> np.ndarray:
-        """Extract a line along a fencespec.
-
-        .. versionadded:: 2.1
-        .. versionchanged:: 2.14 Added keyword option `sampling`
-
-        Here, horizontal axis is "length" and vertical axis is sampled depth, and
-        this is used for fence plots.
-
-        The input fencespec is either a 2D numpy where each row is X, Y, Z, HLEN,
-        where X, Y are UTM coordinates, Z is depth/time, and HLEN is a
-        length along the fence, or a Polygons instance.
-
-        If input fencspec is a numpy 2D, it is important that the HLEN array
-        has a constant increment and ideally a sampling that is less than the
-        map resolution. If a Polygons() instance, this is automated if hincrement is
-        None, and ignored if hincrement is False.
-
-        Returns a ndarray with shape (:, 2).
-
-        Args:
-            fencespec:
-                2D numpy with X, Y, Z, HLEN as rows or a xtgeo Polygons() object.
-            hincrement: Resampling horizontally. This applies only
-                if the fencespec is a Polygons() instance. If None (default),
-                the distance will be deduced automatically. If False, then it assumes
-                the Polygons can be used as-is.
-            atleast: Minimum number of horizontal samples (only if
-                fencespec is a Polygons instance and hincrement != False)
-            nextend: Extend with nextend * hincrement in both ends (only if
-                fencespec is a Polygons instance and hincrement != False)
-            sampling: Use "bilinear" (default) for interpolation or "nearest" for
-                snapping to nearest node.
-
-
-        Example::
-
-            fence = xtgeo.Polygons("somefile.pol")
-            fspec = fence.get_fence(distance=20, nextend=5, asnumpy=True)
-            surf = xtgeo.RegularSurface("somefile.gri")
-
-            arr = surf.get_randomline(fspec)
-
-            distance = arr[:, 0]
-            zval = arr[:, 1]
-            # matplotlib...
-            plt.plot(distance, zval)
-
-        .. seealso::
-           Class :class:`~xtgeo.xyz.polygons.Polygons`
-              The method :meth:`~xtgeo.xyz.polygons.Polygons.get_fence()` which can be
-              used to pregenerate `fencespec`
-        """
-        xyfence = _regsurf_oper.get_randomline(
-            self,
-            fencespec,
-            hincrement=hincrement,
-            atleast=atleast,
-            nextend=nextend,
-            sampling=sampling,
-        )
-
-        return xyfence
-
-    def hc_thickness_from_3dprops(
-        self,
-        xprop=None,
-        yprop=None,
-        hcpfzprop=None,
-        zoneprop=None,
-        zone_minmax=None,
-        dzprop=None,
-        zone_avg=False,
-        coarsen=1,
-        mask_outside=False,
-    ):
-        """Make a thickness weighted HC thickness map.
-
-        Make a HC thickness map based on numpy arrays of properties
-        from a 3D grid. The numpy arrays here shall be ndarray,
-        not masked numpies (MaskedArray).
-
-        Note that the input hcpfzprop is hydrocarbon fraction multiplied
-        with thickness, which can be achieved by e.g.:
-        cpfz = dz*poro*ntg*shc or by hcpfz = dz*hcpv/vbulk
-
-        Args:
-            xprop (ndarray): 3D numpy array of X coordinates
-            yprop (ndarray): 3D numpy array of Y coordinates
-            hcpfzprop (ndarray): 3D numpy array of HC fraction multiplied
-                with DZ per cell.
-            zoneprop (ndarray): 3D numpy array indicating zonation
-                property, where 1 is the lowest (0 values can be used to
-                exclude parts of the grid)
-            dzprop (ndarray): 3D numpy array holding DZ thickness. Will
-                be applied in weighting if zone_avg is active.
-            zone_minmax (tuple): (optional) 2 element list indicating start
-                and stop zonation (both start and end spec are included)
-            zone_avg (bool): A zone averaging is done prior to map gridding.
-                This may speed up the process a lot, but result will be less
-                precise. Default is False.
-            coarsen (int): Select every N'th X Y point in the gridding. Will
-                speed up process, but less precise result. Default=1
-            mask_outside (bool): Will mask the result map undef where sum of DZ
-                is zero. Default is False as it costs some extra CPU.
-
-        Returns:
-            True if operation went OK (but check result!), False if not
-        """
-        for inum, myprop in enumerate([xprop, yprop, hcpfzprop, zoneprop]):
-            if isinstance(myprop, ma.MaskedArray):
-                raise ValueError(
-                    f"Property input {inum} with avg {myprop.mean()} to {__name__} "
-                    "is a masked array, not a plain numpy ndarray"
-                )
-
-        status = _regsurf_gridding.avgsum_from_3dprops_gridding(
-            self,
-            summing=True,
-            xprop=xprop,
-            yprop=yprop,
-            mprop=hcpfzprop,
-            dzprop=dzprop,
-            zoneprop=zoneprop,
-            zone_minmax=zone_minmax,
-            zone_avg=zone_avg,
-            coarsen=coarsen,
-            mask_outside=mask_outside,
-        )
-
-        if status is False:
-            raise RuntimeError("Failure from hc thickness calculation")
-
-    def avg_from_3dprop(
-        self,
-        xprop=None,
-        yprop=None,
-        mprop=None,
-        dzprop=None,
-        truncate_le=None,
-        zoneprop=None,
-        zone_minmax=None,
-        coarsen=1,
-        zone_avg=False,
-    ):
-        """Average map (DZ weighted) based on numpy arrays of properties from a 3D grid.
-
-        The 3D arrays mush be ordinary numpies of size (nx,ny,nz). Undef
-        entries must be given weights 0 by using DZ=0
-
-        Args:
-            xprop: 3D numpy of all X coordinates (also inactive cells)
-            yprop: 3D numpy of all Y coordinates (also inactive cells)
-            mprop: 3D numpy of requested property (e.g. porosity) all
-            dzprop: 3D numpy of dz values (for weighting)
-                NB zero for undef cells
-            truncate_le (float): Optional. Truncate value (mask) if
-                value is less
-            zoneprop: 3D numpy to a zone property
-            zone_minmax: a tuple with from-to zones to combine
-                (e.g. (1,3))
-
-        Returns:
-            Nothing explicit, but updates the surface object.
-        """
-        for inum, myprop in enumerate([xprop, yprop, mprop, dzprop, zoneprop]):
-            if isinstance(myprop, ma.MaskedArray):
-                raise ValueError(
-                    f"Property input {inum} with avg {myprop.mean()} to {__name__} "
-                    "is a masked array, not a plain numpy ndarray"
-                )
-
-        _regsurf_gridding.avgsum_from_3dprops_gridding(
-            self,
-            summing=False,
-            xprop=xprop,
-            yprop=yprop,
-            mprop=mprop,
-            dzprop=dzprop,
-            truncate_le=truncate_le,
-            zoneprop=zoneprop,
-            zone_minmax=zone_minmax,
-            coarsen=coarsen,
-            zone_avg=zone_avg,
-        )
-
-    def quickplot(
-        self,
-        filename=None,
-        title="QuickPlot for Surfaces",
-        subtitle=None,
-        infotext=None,
-        minmax=(None, None),
-        xlabelrotation=None,
-        colormap="rainbow",
-        colortable=None,
-        faults=None,
-        logarithmic=False,
-    ):
-        """Fast surface plot of maps using matplotlib.
-
-        Args:
-            filename (str): Name of plot file; None will plot to screen.
-            title (str): Title of plot
-            subtitle (str): Subtitle of plot
-            infotext (str): Additonal info on plot.
-            minmax (tuple): Tuple of min and max values to be plotted. Note
-                that values outside range will be set equal to range limits
-            xlabelrotation (float): Rotation in degrees of X labels.
-            colormap (str): Name of matplotlib or RMS file or XTGeo
-                colormap. Default is matplotlib's 'rainbow'
-            colortable (str): Deprecated, for backward compatibility! used
-                colormap instead.
-            faults (dict): If fault plot is wanted, a dictionary on the
-                form => {'faults': XTGeo Polygons object, 'color': 'k'}
-            logarithmic (bool): If True, a logarithmic contouring color scale
-                will be used.
-
-        """
-        # This is using the more versatile Map class in XTGeo. Most kwargs
-        # is just passed as is. Prefer using Map() directly in apps?
-
-        ncount = self.values.count()
-        if ncount < 5:
-            xtg.warn(f"None or too few map nodes for plotting. Skip output {filename}!")
-            return
-
-        mymap = xtgeo.plot.Map()
-
-        logger.info("Infotext is <%s>", infotext)
-        mymap.canvas(title=title, subtitle=subtitle, infotext=infotext)
-
-        minvalue = minmax[0]
-        maxvalue = minmax[1]
-
-        if colortable is not None:
-            xtg.warndeprecated(
-                "The colortable parameter is deprecated,"
-                "and will be removed in version 4.0. Use colormap instead."
-            )
-            colormap = colortable
-
-        mymap.colormap = colormap
-
-        mymap.plot_surface(
-            self,
-            minvalue=minvalue,
-            maxvalue=maxvalue,
-            xlabelrotation=xlabelrotation,
-            logarithmic=logarithmic,
-        )
-        if faults:
-            poly = faults.pop("faults")
-            mymap.plot_faults(poly, **faults)
-
-        if filename is None:
-            mymap.show()
-        else:
-            mymap.savefig(filename)
-
-    def distance_from_point(self, point=(0, 0), azimuth=0.0):
-        """Make map values as horizontal distance from a point with azimuth direction.
-
-        Args:
-            point (tuple): Point to measure from
-            azimuth (float): Angle from North (clockwise) in degrees
-
-        """
-        _regsurf_oper.distance_from_point(self, point=point, azimuth=azimuth)
-
-    def translate_coordinates(self, translate=(0, 0, 0)):
-        """Translate a map in X Y VALUE space.
-
-        Args:
-            translate (tuple): Translate (shift) distance in X Y Z
-
-        Example::
-
-            >>> import xtgeo
-            >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
-            >>> print(mysurf.xori, mysurf.yori)
-            468895.125 5932889.5
-            >>> mysurf.translate_coordinates((300,500,0))
-            >>> print(mysurf.xori, mysurf.yori)
-            469195.125 5933389.5
-
-        """
-        xshift, yshift, zshift = translate
-
-        # just shift the xori and yori
-        self.xori = self.xori + xshift
-        self.yori = self.yori + yshift
-
-        # note the Z coordinates are perhaps not depth
-        # numpy operation:
-        self.values = self.values + zshift
-
-    # ==================================================================================
-    # Private
-    # ==================================================================================
-
-    def _ensure_correct_values(
-        self, values
-    ):  # pylint: disable=too-many-branches, too-many-statements
-        """Ensures that values is a 2D masked numpy (ncol, nrow), C order.
-
-        This is an improved but private version over ensure_correct_values
-
-        Args:
-            values (array-like or scalar): Values to process.
-
-        Return:
-            Nothing, self._values will be updated inplace
-
-        """
-        currentmask = None
-        if isinstance(self.values, ma.MaskedArray):
-            currentmask = ma.getmaskarray(self.values)
-
-        if isinstance(values, ma.MaskedArray):
-            newmask = ma.getmaskarray(values)
-            vals = values.astype(np.float64)
-            vals = ma.masked_greater(vals, self.undef_limit)
-            vals = ma.masked_invalid(vals)
-            if (
-                currentmask is not None
-                and np.array_equal(currentmask, newmask)
-                and self.values.shape == values.shape
-                and values.flags.c_contiguous is True
-            ):
-                self._values *= 0
-                self._values += vals
-            else:
-                vals = vals.reshape((self._ncol, self._nrow))
-                if not vals.flags.c_contiguous:
-                    mask = ma.getmaskarray(values)
-                    mask = np.asanyarray(mask, order="C")
-                    vals = np.asanyarray(vals, order="C")
-                    vals = ma.array(vals, mask=mask, order="C")
-                self._values = vals
-
-        elif isinstance(values, numbers.Number):
-            if currentmask is not None:
-                vals = np.ones(self.dimensions, dtype=np.float64) * values
-                vals = np.ma.array(vals, mask=currentmask)
-
-                # there maybe cases where values scalar input is some kind of UNDEF
-                # which will change the mask
-                vals = ma.masked_greater(vals, self.undef_limit, copy=False)
-                vals = ma.masked_invalid(vals, copy=False)
-                self._values *= 0
-                self._values += vals
-            else:
-                vals = ma.zeros((self.ncol, self.nrow), order="C", dtype=np.float64)
-                self._values = vals + float(values)
-
-        elif isinstance(values, (list, tuple, np.ndarray)):  # ie values ~ list-like
-            vals = ma.array(values, order="C", dtype=np.float64)
-            vals = ma.masked_greater(vals, self.undef_limit, copy=True)
-            vals = ma.masked_invalid(vals, copy=True)
-
-            if vals.shape != (self.ncol, self.nrow):
-                try:
-                    vals = ma.reshape(vals, (self.ncol, self.nrow), order="C")
-                except ValueError as emsg:
-                    raise ValueError(f"Cannot reshape array: {values}") from emsg
-
-            self._values = vals
-
-        elif isinstance(values, (list, tuple)):  # ie values ~ list-like
-            vals = ma.array(values, order="C", dtype=np.float64)
-            vals = ma.masked_greater(vals, self.undef_limit, copy=True)
-            vals = ma.masked_invalid(vals, copy=True)
-
-            if vals.shape != (self.ncol, self.nrow):
-                try:
-                    vals = ma.reshape(vals, (self.ncol, self.nrow), order="C")
-                except ValueError as emsg:
-                    raise ValueError(f"Cannot reshape array: {values}") from emsg
-
-            self._values = vals
-
-        else:
-            raise ValueError(f"Input values are in invalid format: {values}")
-
-        if self._values.mask is ma.nomask:
-            self._values = ma.array(self._values, mask=ma.getmaskarray(self._values))
+"""Module/class for regular surfaces with XTGeo.
+
+Regular surfaces have a constant distance between nodes (xinc, yinc),
+and this simplifies computations a lot. A regular surface is
+defined by an origin (xori, yori)
+in UTM, a number of columns (along X axis, if no rotation), a number of
+rows (along Y axis if no rotation), and increment (distance between nodes).
+
+The map itself is an array of values.
+
+Rotation is allowed and is measured in degrees, anticlock from X axis.
+
+Note that an instance of a regular surface can be made directly with::
+
+ >>> import xtgeo
+ >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+
+or::
+
+ mysurf = xtgeo.surface_from_roxar('some_rms_project', 'TopX', 'DepthSurface')
+
+"""
+# --------------------------------------------------------------------------------------
+# Comment on 'asmasked' vs 'activeonly:
+# 'asmasked'=True will return a np.ma array, with some fill_value if
+# if asmasked = False
+#
+# while 'activeonly' will filter
+# out maked entries, or use np.nan if 'activeonly' is False
+#
+# For functions with mask=... ,the should be replaced with asmasked=...
+# --------------------------------------------------------------------------------------
+
+# pylint: disable=too-many-public-methods
+
+import functools
+import io
+import math
+import numbers
+import pathlib
+import warnings
+from collections import OrderedDict
+from copy import deepcopy
+from types import FunctionType
+from typing import List, Optional, Tuple, Union
+
+import deprecation
+import numpy as np
+import numpy.ma as ma
+import pandas as pd
+
+import xtgeo
+import xtgeo.common.sys as xtgeosys
+from xtgeo.common.constants import VERYLARGENEGATIVE, VERYLARGEPOSITIVE
+
+from . import (
+    _regsurf_boundary,
+    _regsurf_cube,
+    _regsurf_cube_window,
+    _regsurf_export,
+    _regsurf_grid3d,
+    _regsurf_gridding,
+    _regsurf_import,
+    _regsurf_oper,
+    _regsurf_roxapi,
+    _regsurf_utils,
+)
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+# ======================================================================================
+# METHODS as wrappers to class init + import
+
+
+def surface_from_file(mfile, fformat=None, template=None, values=True, engine="cxtgeo"):
+    """Make an instance of a RegularSurface directly from file import.
+
+    Args:
+        mfile (str): Name of file
+        fformat: File format, None/guess/irap_binary/irap_ascii/ijxyz/petromod/
+            zmap_ascii/xtg/hdf is currently supported. If None or guess, the file
+            'signature' is used to guess format first, then file extension.
+        template: Only valid if ``ijxyz`` format, where an existing Cube or
+            RegularSurface instance is applied to get correct topology.
+        values (bool): If True (default), surface values will be read (Irap binary only)
+        engine (str): Some import methods are implemnted in both C and Python.
+            The C method ``cxtgeo`` is default. Alternative use ``python``
+
+    Example::
+
+        >>> import xtgeo
+        >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+
+    .. versionchanged:: 2.1
+      Key "values" for Irap binary maps added
+
+    .. versionchanged:: 2.13 Key "engine" added
+    """
+
+    return RegularSurface._read_file(
+        mfile, fformat=fformat, load_values=values, engine=engine, template=template
+    )
+
+
+def surface_from_roxar(project, name, category, stype="horizons", realisation=0):
+    """This makes an instance of a RegularSurface directly from roxar input.
+
+    Args:
+        project (str or special): Name of project (as folder) if
+            outside RMS, og just use the magic project word if within RMS.
+        name (str): Name of surface/map
+        category (str): For horizons/zones or clipboard/general2d_data:
+            for example 'DS_extracted'. For clipboard/general2d_data this can
+            be empty or None, or use '/' for multiple folder levels (e.g. 'fld/subfld').
+            For 'trends', the category is not applied.
+        stype (str): RMS folder type, 'horizons' (default), 'zones', 'clipboard',
+            'general2d_data' or 'trends'
+        realisation (int): Realisation number, default is 0
+
+    Example::
+
+        # inside RMS:
+        import xtgeo
+        mysurf = xtgeo.surface_from_roxar(project, 'TopEtive', 'DepthSurface')
+
+    Note::
+
+        When dealing with surfaces to and from ``stype="trends"``, the surface must
+        exist in advance, i.e. the Roxar API do not allow creating new surfaces.
+        Actually trends are read only, but a workaround using ``load()`` in Roxar
+        API makes it possible to overwrite existing surface trends. In addition,
+        ``realisation`` is not applied in trends.
+
+    """
+
+    return RegularSurface._read_roxar(
+        project, name, category, stype=stype, realisation=realisation
+    )
+
+
+def surface_from_cube(cube, value):
+    """Make RegularSurface directly from a cube instance with a constant value.
+
+    The surface geometry will be exactly the same as for the Cube.
+
+    Args:
+        cube(xtgeo.cube.Cube): A Cube instance
+        value (float): A constant value for the surface
+
+    Example::
+
+       >>> import xtgeo
+       >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+       >>> mymap = xtgeo.surface_from_cube(mycube, 2700)
+
+    """
+    return RegularSurface._read_cube(cube, value)
+
+
+def surface_from_grid3d(grid, template=None, where="top", mode="depth", rfactor=1):
+    """This makes 3 instances of a RegularSurface directly from a Grid() instance.
+
+    Args:
+        grid (Grid): XTGeo Grid instance
+        template(RegularSurface): Optional to use an existing surface as
+            template for geometry
+        where (str): "top", "base" or use the syntax "2_top" where 2
+            is layer no. 2 and _top indicates top of cell, while "_base"
+            indicates base of cell
+        mode (str): "depth", "i" or "j"
+        rfactor (float): Determines how fine the extracted map is; higher values
+            for finer map (but computing time will increase). Will only apply if
+            template is None.
+
+    .. versionadded:: 2.1
+    """
+    return RegularSurface._read_grid3d(
+        grid, template=template, where=where, mode=mode, rfactor=rfactor
+    )
+
+
+def _data_reader_factory(file_format):
+    if file_format == "irap_binary":
+        return _regsurf_import.import_irap_binary
+    if file_format == "irap_ascii":
+        return _regsurf_import.import_irap_ascii
+    if file_format == "ijxyz":
+        return _regsurf_import.import_ijxyz
+    if file_format == "petromod":
+        return _regsurf_import.import_petromod
+    if file_format == "zmap_ascii":
+        return _regsurf_import.import_zmap_ascii
+    if file_format == "xtg":
+        return _regsurf_import.import_xtg
+    if file_format == "hdf":
+        return _regsurf_import.import_hdf5_regsurf
+    raise ValueError(f"Unknown file format {file_format}")
+
+
+def allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of RegularSurface and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        # Checking if we are doing an initialization
+        # from file and raise a deprecation warning if
+        # we are.
+        if "sfile" in kwargs or len(args) == 1:
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "mysurf = xtgeo.surface_from_file('some_name.gri') instead",
+                DeprecationWarning,
+            )
+            sfile = kwargs.get("sfile", args[0])
+            fformat = kwargs.get("fformat", None)
+            values = kwargs.get("values", None)
+            if isinstance(values, bool) and values is False:
+                load_values = False
+            else:
+                load_values = True
+            mfile = xtgeosys._XTGeoFile(sfile)
+            if fformat is None or fformat == "guess":
+                fformat = mfile.detect_fformat()
+            else:
+                fformat = mfile.generic_format_by_proposal(fformat)  # default
+            kwargs = _data_reader_factory(fformat)(mfile, values=load_values)
+            kwargs["filesrc"] = mfile.file
+            kwargs["fformat"] = fformat
+            return func(cls, **kwargs)
+
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+def allow_deprecated_default_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of RegularSurface and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        # This is (mostly) for cases where we are doing an empty
+        # initialization, so we need to inject default values
+        # for the required args. The excessive checking is in
+        # corner cases where we provide some positional arguments
+        # as keyword arguments.
+        _deprecation_msg = (
+            "X is a required argument, will no "
+            "longer be defaulted in xtgeo version 4.0"
+        )
+        if len(args) != 4:
+            if "ncol" not in kwargs and len(args) != 1:
+                warnings.warn(_deprecation_msg.replace("X", "ncol"), DeprecationWarning)
+                kwargs["ncol"] = 5
+            if "nrow" not in kwargs and len(args) != 2:
+                warnings.warn(_deprecation_msg.replace("X", "nrow"), DeprecationWarning)
+                kwargs["nrow"] = 3
+            if "xinc" not in kwargs and len(args) != 3:
+                warnings.warn(_deprecation_msg.replace("X", "xinc"), DeprecationWarning)
+                kwargs["xinc"] = 25.0
+            if "yinc" not in kwargs:
+                warnings.warn(_deprecation_msg.replace("X", "yinc"), DeprecationWarning)
+                kwargs["yinc"] = 25.0
+            default = (
+                kwargs.get("ncol", 5) == 5
+                and kwargs.get("nrow", 3) == 3
+                and kwargs.get("xori", 0.0) == kwargs.get("yori", 0.0) == 0.0
+                and kwargs.get("xinc", 25.0) == kwargs.get("yinc", 25.0) == 25.0
+            )
+            values = kwargs.get("values", None)
+            if values is None and default:
+                default_values = [
+                    [1, 6, 11],
+                    [2, 7, 12],
+                    [3, 8, 1e33],
+                    [4, 9, 14],
+                    [5, 10, 15],
+                ]
+                warnings.warn(
+                    f"Default values {default_values} for RegularSurface is "
+                    f"deprecated and will be set to an array of zero if not explicitly "
+                    f"given in version 3",
+                    DeprecationWarning,
+                )
+                # make default surface (mostly for unit testing)
+                kwargs["values"] = np.array(
+                    default_values,
+                    dtype=np.float64,
+                    order="C",
+                )
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+class RegularSurface:
+    """Class for a regular surface in the XTGeo framework.
+
+    The values can as default be accessed by the user as a 2D masked numpy
+    (ncol, nrow) float64 array, but also other representations or views are
+    possible (e.g. as 1D ordinary numpy).
+
+    """
+
+    @allow_deprecated_init
+    @allow_deprecated_default_init
+    def __init__(
+        self,
+        ncol: int,
+        nrow: int,
+        xinc: float,
+        yinc: float,
+        xori: Optional[float] = 0.0,
+        yori: Optional[float] = 0.0,
+        yflip: Optional[int] = 1,
+        rotation: Optional[float] = 0.0,
+        values: Optional[Union[List[float], float]] = None,
+        ilines: Optional[List[float]] = None,
+        xlines: Optional[List[float]] = None,
+        masked: Optional[bool] = True,
+        name: Optional[str] = "unknown",
+        filesrc: Optional[str] = None,
+        fformat: Optional[str] = None,
+        undef: Optional[float] = xtgeo.UNDEF,
+    ):
+        """Instantiating a RegularSurface::
+
+            vals = np.zeros(30 * 50)
+            surf = xtgeo.RegularSurface(
+                ncol=30, nrow=50, xori=1234.5, yori=4321.0, xinc=30.0, yinc=50.0,
+                rotation=30.0, values=vals, yflip=1,
+            )
+
+        Args:
+            ncol: Integer for number of X direction columns.
+            nrow: Integer for number of Y direction rows.
+            xori: X (East) origon coordinate.
+            yori: Y (North) origin coordinate.
+            xinc: X increment.
+            yinc: Y increment.
+            yflip: If 1, the map grid is left-handed (assuming depth downwards),
+                otherwise -1 means that Y axis is flipped (right-handed).
+            rotation: rotation in degrees, anticlock from X axis between 0, 360.
+            values: A scalar (for constant values) or a "array-like" input that has
+                ncol * nrow elements. As result, a 2D (masked) numpy array of shape
+                (ncol, nrow), C order will be made.
+            masked: Indicating if numpy array shall be masked or not. Default is True.
+            name: A given name for the surface, default is file name root or
+                'unknown' if constructed from scratch.
+
+        Examples:
+            The instance can be made by specification::
+
+                >>> surface = RegularSurface(
+                ... ncol=20,
+                ... nrow=10,
+                ... xori=2000.0,
+                ... yori=2000.0,
+                ... rotation=0.0,
+                ... xinc=25.0,
+                ... yinc=25.0,
+                ... values=np.zeros((20,10))
+                ... )
+
+
+        """
+        logger.info("Start __init__ method for RegularSurface object %s", id(self))
+        self._ncol = ncol
+        self._nrow = nrow
+        self._xori = xori
+        self._yori = yori
+        self._xinc = xinc
+        self._yinc = yinc
+        self._rotation = rotation
+        self._yflip = yflip
+        self._name = name
+
+        self._undef = undef
+        self._undef_limit = xtgeo.UNDEF_LIMIT
+
+        self._filesrc = filesrc  # Name of original input file or stream, if any
+
+        self._fformat = fformat  # current fileformat, useful for load()
+        self._metadata = xtgeo.MetaDataRegularSurface()
+
+        self._values = None
+        if values is None:
+            values = np.ma.zeros((self._ncol, self._nrow))
+            self._isloaded = False
+        else:
+            self._isloaded = True
+        self.values = values
+
+        if ilines is None:
+            self._ilines = np.array(range(1, self._ncol + 1), dtype=np.int32)
+            self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
+        else:
+            self._ilines = ilines
+            self._xlines = xlines
+
+        self._masked = masked  # TODO: check usecase
+        self._metadata.required = self
+
+    @classmethod
+    def _read_zmap_ascii(cls, mfile, values):
+        mfile = xtgeosys._XTGeoFile(mfile)
+        args = _data_reader_factory("zmap_ascii")(mfile, values=values)
+        return cls(**args)
+
+    def __repr__(self):
+        """Magic method __repr__."""
+        myrp = (
+            f"{self.__class__.__name__} (xori={self._xori!r}, yori={self._yori!r}, "
+            f"xinc={self._xinc!r}, yinc={self._yinc!r}, ncol={self._ncol!r}, "
+            f"nrow={self._nrow!r}, rotation={self._rotation!r}, yflip={self._yflip!r}, "
+            f"masked={self._masked!r}, filesrc={self._filesrc!r}, "
+            f"name={self._name!r}, ilines={self.ilines.shape!r}, "
+            f"xlines={self.xlines.shape!r}, values={self.values.shape!r}) "
+            f"ID={id(self)}."
+        )
+        return myrp
+
+    def __str__(self):
+        """Magic method __str__ for user friendly print."""
+        return self.describe(flush=False)
+
+    def __getitem__(self, index):
+        """Magic method."""
+        col, row = index
+        return self._values[col, row]
+
+    def __add__(self, other):
+        """Magic method."""
+        news = self.copy()
+        _regsurf_oper.operations_two(news, other, oper="add")
+
+        return news
+
+    def __iadd__(self, other):
+        """Magic method."""
+        _regsurf_oper.operations_two(self, other, oper="iadd")
+        return self
+
+    def __sub__(self, other):
+        """Magic method."""
+        news = self.copy()
+        _regsurf_oper.operations_two(news, other, oper="sub")
+
+        return news
+
+    def __isub__(self, other):
+        """Magic method."""
+        _regsurf_oper.operations_two(self, other, oper="isub")
+        return self
+
+    def __mul__(self, other):
+        """Magic method."""
+        news = self.copy()
+        _regsurf_oper.operations_two(news, other, oper="mul")
+
+        return news
+
+    def __imul__(self, other):
+        """Magic method."""
+        _regsurf_oper.operations_two(self, other, oper="imul")
+        return self
+
+    def __truediv__(self, other):
+        """Magic method."""
+        news = self.copy()
+        _regsurf_oper.operations_two(news, other, oper="div")
+
+        return news
+
+    def __idiv__(self, other):
+        """Magic method."""
+        _regsurf_oper.operations_two(self, other, oper="idiv")
+        return self
+
+    # comparison operators, return boolean arrays
+
+    def __lt__(self, other):
+        """Magic method."""
+        return _regsurf_oper.operations_two(self, other, oper="lt")
+
+    def __gt__(self, other):
+        """Magic method."""
+        return _regsurf_oper.operations_two(self, other, oper="gt")
+
+    def __le__(self, other):
+        """Magic method."""
+        return _regsurf_oper.operations_two(self, other, oper="le")
+
+    def __ge__(self, other):
+        """Magic method."""
+        return _regsurf_oper.operations_two(self, other, oper="ge")
+
+    def __eq__(self, other):
+        """Magic method."""
+        return _regsurf_oper.operations_two(self, other, oper="eq")
+
+    def __ne__(self, other):
+        """Magic method."""
+        return _regsurf_oper.operations_two(self, other, oper="ne")
+
+    # ==================================================================================
+    # Class and special methods
+    # ==================================================================================
+
+    @classmethod
+    def methods(cls):
+        """Returns the names of the methods in the class.
+
+        >>> print(RegularSurface.methods())
+        METHODS for RegularSurface():
+        ======================
+        __init__
+        __repr__
+        ...
+
+        """
+        mets = [x for x, y in cls.__dict__.items() if isinstance(y, FunctionType)]
+
+        txt = "METHODS for RegularSurface():\n======================\n"
+        for met in mets:
+            txt += str(met) + "\n"
+
+        return txt
+
+    # ==================================================================================
+    # Properties
+    # ==================================================================================
+
+    @property
+    def metadata(self):
+        """Return metadata object instance of type MetaDataRegularSurface."""
+        return self._metadata
+
+    @metadata.setter
+    def metadata(self, obj):
+        # The current metadata object can be replaced. A bit dangerous so further
+        # check must be done to validate. TODO.
+        if not isinstance(obj, xtgeo.MetaDataRegularSurface):
+            raise ValueError("Input obj not an instance of MetaDataRegularSurface")
+
+        self._metadata = obj  # checking is currently missing! TODO
+
+    @property
+    def ncol(self):
+        """The NCOL (NX or N-Idir) number, as property (read only)."""
+        return self._ncol
+
+    @property
+    def nrow(self):
+        """The NROW (NY or N-Jdir) number, as property (read only)."""
+        return self._nrow
+
+    @property
+    def dimensions(self):
+        """2-tuple: The surface dimensions as a tuple of 2 integers (read only)."""
+        return (self._ncol, self._nrow)
+
+    @property
+    def nactive(self):
+        """Number of active map nodes (read only)."""
+        if self._isloaded:
+            return self._values.count()
+        return None
+
+    @property
+    def rotation(self):
+        """The rotation, anticlock from X axis, in degrees [0..360>."""
+        return self._rotation
+
+    @rotation.setter
+    def rotation(self, rota):
+        if 0 <= rota < 360:
+            self._rotation = rota
+        else:
+            raise ValueError("Rotation must be in interval [0, 360>")
+
+    @property
+    def xinc(self):
+        """The X increment (or I dir increment)."""
+        return self._xinc
+
+    @property
+    def yinc(self):
+        """The Y increment (or I dir increment)."""
+        return self._yinc
+
+    @property
+    def yflip(self):
+        """The Y flip (handedness) indicator 1, or -1 (read only).
+
+        The value 1 (default) means a left-handed system if depth values are
+        positive downwards. Assume -1 is rare, but may happen when
+        surface is derived from seismic cube.
+        """
+        return self._yflip
+
+    @property
+    def xori(self):
+        """The X coordinate origin of the map."""
+        return self._xori
+
+    @xori.setter
+    def xori(self, xnew):
+        self._xori = xnew
+
+    @property
+    def yori(self):
+        """The Y coordinate origin of the map."""
+        return self._yori
+
+    @yori.setter
+    def yori(self, ynew):
+        self._yori = ynew
+
+    @property
+    def ilines(self):
+        """The inlines numbering vector (read only)."""
+        return self._ilines
+
+    @ilines.setter
+    def ilines(self, values):
+        if isinstance(values, np.ndarray) and values.shape[0] == self._ncol:
+            self._ilines = values
+
+    @property
+    def xlines(self):
+        """The xlines numbering vector (read only)."""
+        return self._xlines
+
+    @xlines.setter
+    def xlines(self, values):
+        if isinstance(values, np.ndarray) and values.shape[0] == self._nrow:
+            self._xlines = values
+
+    @property
+    def xmin(self):
+        """The minimim X coordinate (read only)."""
+        corners = self.get_map_xycorners()
+
+        xmin = VERYLARGEPOSITIVE
+        for corner in corners:
+            if corner[0] < xmin:
+                xmin = corner[0]
+        return xmin
+
+    @property
+    def xmax(self):
+        """The maximum X coordinate (read only)."""
+        corners = self.get_map_xycorners()
+
+        xmax = VERYLARGENEGATIVE
+        for corner in corners:
+            if corner[0] > xmax:
+                xmax = corner[0]
+        return xmax
+
+    @property
+    def ymin(self):
+        """The minimim Y coordinate (read only)."""
+        corners = self.get_map_xycorners()
+
+        ymin = VERYLARGEPOSITIVE
+        for corner in corners:
+            if corner[1] < ymin:
+                ymin = corner[1]
+        return ymin
+
+    @property
+    def ymax(self):
+        """The maximum Y xoordinate (read only)."""
+        corners = self.get_map_xycorners()
+
+        ymax = VERYLARGENEGATIVE
+        for corner in corners:
+            if corner[1] > ymax:
+                ymax = corner[1]
+        return ymax
+
+    @property
+    def values(self):
+        """The map values, as 2D masked numpy (float64), shape (ncol, nrow).
+
+        When setting values as a scalar, the current mask will be preserved.
+
+        When setting values, list-like input (lists, tuples) is also accepted, as
+        long as the length is correct and the entries are number-like.
+
+        In order to specify undefined values, you can specify the ``undef`` attribute
+        in the list, or use ``float("nan")``.
+
+        Example::
+
+            # list like input where nrow=3 and ncol=5 (15 entries)
+            newvalues = list(range(15))
+            newvalues[2] = srf.undef
+            srf.values = newvalues  # here, entry 2 will be undefined
+        """
+        return self._values
+
+    @values.setter
+    def values(self, values):
+        self._ensure_correct_values(values)
+
+    @property
+    def values1d(self):
+        """(Read only) Map values, as 1D numpy masked, normally a numpy view(?).
+
+        Example::
+
+            map = xtgeo.surface_from_file('myfile.gri')
+            map.values1d
+        """
+        return self.get_values1d(asmasked=True)
+
+    @property
+    def npvalues1d(self):
+        """(Read only) Map values, as 1D numpy (not masked), undef as np.nan.
+
+        In most cases this will be a copy of the values.
+
+        Example::
+
+            >>> import xtgeo
+            >>> map = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> values = map.npvalues1d
+            >>> mean = np.nanmean(values)
+            >>> values[values <= 0] = np.nan
+            >>> print(values)
+            [nan nan ... nan]
+        """
+        return self.get_values1d(asmasked=False, fill_value=np.nan)
+
+    @property
+    def name(self):
+        """A free form name for the surface, to be used in display etc."""
+        return self._name
+
+    @name.setter
+    def name(self, newname):
+        if isinstance(newname, str):
+            self._name = newname
+
+    @property
+    def undef(self):
+        """Returns the undef map value (read only)."""
+        return self._undef
+
+    @property
+    def undef_limit(self):
+        """Returns the undef_limit map value (read only)."""
+        return self._undef_limit
+
+    @property
+    def filesrc(self):
+        """Gives the name of the file source (if any)."""
+        return self._filesrc
+
+    @filesrc.setter
+    def filesrc(self, name):
+        self._filesrc = name  # checking is currently missing
+
+    # ==================================================================================
+    # Describe, import and export
+    # ==================================================================================
+
+    def generate_hash(self, hashmethod="md5"):
+        """Return a unique hash ID for current instance.
+
+        See :meth:`~xtgeo.common.sys.generic_hash()` for documentation.
+
+        .. versionadded:: 2.14
+        """
+        required = (
+            "ncol",
+            "nrow",
+            "xori",
+            "yori",
+            "xinc",
+            "yinc",
+            "yflip",
+            "rotation",
+        )
+        gid = ""
+        for req in required:
+            gid += f"{getattr(self, '_' + req)}"
+        # Ignore the mask
+        gid += self._values.data.tobytes().hex()
+
+        hash_ = xtgeosys.generic_hash(gid, hashmethod=hashmethod)
+        return hash_
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout."""
+        #
+        dsc = xtgeo.common.XTGDescription()
+        dsc.title("Description of RegularSurface instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("File source", self._filesrc)
+        dsc.txt("Shape: NCOL, NROW", self.ncol, self.nrow)
+        dsc.txt("Active cells vs total", self.nactive, self.nrow * self.ncol)
+        dsc.txt("Origins XORI, YORI", self.xori, self.yori)
+        dsc.txt("Increments XINC YINC", self.xinc, self.yinc)
+        dsc.txt("Rotation (anti-clock from X)", self.rotation)
+        dsc.txt("YFLIP flag", self._yflip)
+        np.set_printoptions(threshold=16)
+        dsc.txt("Inlines vector", self._ilines)
+        dsc.txt("Xlines vector", self._xlines)
+        np.set_printoptions(threshold=1000)
+        if self._isloaded:
+            dsc.txt("Values", self._values.reshape(-1), self._values.dtype)
+            dsc.txt(
+                "Values: mean, stdev, minimum, maximum",
+                self.values.mean(),
+                self.values.std(),
+                self.values.min(),
+                self.values.max(),
+            )
+            msize = float(self.values.size * 8) / (1024 * 1024 * 1024)
+            dsc.txt("Minimum memory usage of array (GB)", msize)
+        else:
+            dsc.txt("Values:", "Not loaded")
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    @deprecation.deprecated(
+        deprecated_in="2.15",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.surface_from_file() instead",
+    )
+    def from_file(
+        self,
+        mfile: Union[str, pathlib.Path, io.BytesIO],
+        fformat: Optional[str] = None,
+        values: Optional[bool] = True,
+        **kwargs,
+    ):
+        """Import surface (regular map) from file.
+
+        Note that the ``fformat=None`` or ``guess`` option will guess format by
+        looking at the file or stream signature or file extension.
+        For the signature, the first bytes are scanned for 'patterns'. If that
+        does not work (and input is not a memory stream), it will try to use
+        file extension where e.g. "gri" will assume irap_binary and "fgr"
+        assume Irap Ascii. If file extension is missing, Irap binary is assumed.
+
+        The ``ijxyz`` format is the typical seismic format, on the form
+        (ILINE, XLINE, X, Y, VALUE) as a table of points. Map values are
+        estimated from the given values, or by using an existing map or
+        cube as template, and match by ILINE/XLINE numbering.
+
+        BytesIO input is supported for Irap binary, Irap Ascii, ZMAP ascii.
+
+        Args:
+            mfile: File-like or memory stream instance.
+            fformat: File format, None/guess/irap_binary/irap_ascii/ijxyz
+                is currently supported. If None or guess, the file 'signature' is
+                used to guess format first, then file extension.
+            values: If True (default), then full array is read, if False
+                only metadata will be read. Valid for Irap binary only. This allows
+                lazy loading in e.g. ensembles.
+            kwargs: some readers allow additonal options:
+            template: Only valid if ``ijxyz`` format, where an
+                existing Cube or RegularSurface instance is applied to
+                get correct topology.
+            engine: Default is "cxtgeo" which use a C backend. Optionally a pure
+                python "python" reader will be used, which in general is slower
+                but may be safer when reading memory streams and/or threading.
+                Engine is relevant for Irap binary, Irap ascii and zmap.
+
+        Returns:
+            Object instance.
+
+        Example:
+            Here the from_file method is used to initiate the object
+            directly::
+
+            >>> surf = RegularSurface().from_file(surface_dir + "/topreek_rota.gri")
+
+        .. versionchanged:: 2.1
+          Key "values" for Irap binary maps added
+
+        .. versionchanged:: 2.2
+          Input io.BytesIO instance instead of file is now possible
+
+        .. versionchanged:: 2.13
+            ZMAP + import is added, and io.BytesIO input is extended to more formats
+        """
+        logger.info("Import RegularSurface from file or memstream...")
+
+        mfile = xtgeosys._XTGeoFile(mfile)
+
+        if fformat is None or fformat == "guess":
+            fformat = mfile.detect_fformat()
+        else:
+            fformat = mfile.generic_format_by_proposal(fformat)  # default
+
+        kwargs = _data_reader_factory(fformat)(mfile, values=values, **kwargs)
+        if values:
+            self._isloaded = True
+        self._reset(**kwargs)
+
+    def _reset(self, **kwargs):
+        self._ncol = kwargs["ncol"]
+        self._nrow = kwargs["nrow"]
+        self._xinc = kwargs["xinc"]
+        self._yinc = kwargs["yinc"]
+        self._xori = kwargs.get("xori", self._xori)
+        self._yori = kwargs.get("yori", self._yori)
+        self._yflip = kwargs.get("yflip", self._yflip)
+        self._rotation = kwargs.get("rotation", self._rotation)
+        self._ilines = kwargs.get("ilines", self._ilines)
+        self._xlines = kwargs.get("xlines", self._xlines)
+        self.values = kwargs.get("values", self._values)
+
+    @classmethod
+    def _read_file(
+        cls,
+        mfile: Union[str, pathlib.Path, io.BytesIO],
+        fformat: Optional[str] = None,
+        load_values: bool = True,
+        **kwargs,
+    ):
+        """Import surface (regular map) from file.
+
+        Note that the ``fformat=None`` or ``guess`` option will guess format by
+        looking at the file or stream signature or file extension.
+        For the signature, the first bytes are scanned for 'patterns'. If that
+        does not work (and input is not a memory stream), it will try to use
+        file extension where e.g. "gri" will assume irap_binary and "fgr"
+        assume Irap Ascii. If file extension is missing, Irap binary is assumed.
+
+        The ``ijxyz`` format is the typical seismic format, on the form
+        (ILINE, XLINE, X, Y, VALUE) as a table of points. Map values are
+        estimated from the given values, or by using an existing map or
+        cube as template, and match by ILINE/XLINE numbering.
+
+        BytesIO input is supported for Irap binary, Irap Ascii, ZMAP ascii.
+
+        Args:
+            mfile: File-like or memory stream instance.
+            fformat: File format, None/guess/irap_binary/irap_ascii/ijxyz
+                is currently supported. If None or guess, the file 'signature' is
+                used to guess format first, then file extension.
+            load_values: If True (default), then full array is read, if False
+                only metadata will be read. Valid for Irap binary only. This allows
+                lazy loading in e.g. ensembles.
+            kwargs: some readers allow additonal options
+
+        Keyword Args:
+            template: Only valid if ``ijxyz`` format, where an
+                existing Cube or RegularSurface instance is applied to
+                get correct topology.
+            engine: Default is "cxtgeo" which use a C backend. Optionally a pure
+                python "python" reader will be used, which in general is slower
+                but may be safer when reading memory streams and/or threading.
+                Engine is relevant for Irap binary, Irap ascii and zmap.
+
+        Returns:
+            Object instance.
+
+        Example::
+
+           >>> surf = RegularSurface._read_file(surface_dir + "/topreek_rota.gri")
+
+        .. versionadded:: 2.14
+
+        """
+        mfile = xtgeosys._XTGeoFile(mfile)
+        mfile.check_file(raiseerror=ValueError)
+        if fformat is None or fformat == "guess":
+            fformat = mfile.detect_fformat()
+        else:
+            fformat = mfile.generic_format_by_proposal(fformat)  # default
+        kwargs = _data_reader_factory(fformat)(mfile, values=load_values, **kwargs)
+        kwargs["filesrc"] = mfile.file
+        kwargs["fformat"] = fformat
+        return cls(**kwargs)
+
+    def load_values(self):
+        """Import surface values in cases where metadata only is loaded.
+
+        Currently, only Irap binary format is supported.
+
+        Example::
+
+            surfs = []
+            for i in range(1000):
+                surfs.append(xtgeo.surface_from_file(f"myfile{i}.gri", values=False))
+
+            # load values in number 88:
+            surfs[88].load_values()
+
+        .. versionadded:: 2.1
+        """
+
+        if not self._isloaded:
+            if self.filesrc is None:
+                raise ValueError(
+                    "Can only load values into object initialised from file"
+                )
+
+            with warnings.catch_warnings():
+                warnings.filterwarnings("ignore", category=DeprecationWarning)
+
+                mfile = xtgeosys._XTGeoFile(self.filesrc)
+                kwargs = _data_reader_factory(self._fformat)(mfile, values=True)
+                self.values = kwargs.get("values", self._values)
+
+            self._isloaded = True
+
+    def to_file(
+        self,
+        mfile: Union[str, pathlib.Path, io.BytesIO],
+        fformat: Optional[str] = "irap_binary",
+        pmd_dataunits: Optional[Tuple[int, int]] = (15, 10),
+        engine: Optional[str] = "cxtgeo",
+    ):
+        """Export a surface (map) to file.
+
+        Note, for zmap_ascii and storm_binary an unrotation will be done
+        automatically. The sampling will be somewhat finer than the
+        original map in order to prevent aliasing. See :func:`unrotate`.
+
+        Args:
+            mfile: Name of file,
+                Path instance or IOBytestream instance. An alias can be e.g.
+                "%md5sum%" or "%fmu-v1%" with string or Path() input.
+            fformat: File format, irap_binary/irap_ascii/zmap_ascii/
+                storm_binary/ijxyz/petromod/xtg*. Default is irap_binary.
+            pmd_dataunits: A tuple of length 2 for petromod format,
+                spesifying metadata for units (DataUnitDistance, DataUnitZ).
+            engine: Default is "cxtgeo" which use a C backend. Optionally a pure
+                python "python" reader will be used, which in general is slower
+                but may be safer when reading memory streams and/or threading. Engine
+                is relevant for Irap binary, Irap ascii and zmap. This is mainly a
+                developer setting.
+
+        Returns:
+            ofile (pathlib.Path): The actual file instance, or None if io.Bytestream
+
+        Examples::
+
+            >>> # read and write to ordinary file
+            >>> surf = xtgeo.surface_from_file(
+            ...     surface_dir + '/topreek_rota.fgr',
+            ...     fformat = 'irap_ascii'
+            ... )
+            >>> surf.values = surf.values + 300
+            >>> filename = surf.to_file(
+            ...     outdir + '/topreek_rota300.fgr',
+            ...     fformat = 'irap_ascii'
+            ... )
+
+            >>> # writing to io.BytesIO:
+            >>> stream = io.BytesIO()
+            >>> surf.to_file(stream, fformat="irap_binary")
+
+            >>> # read from memory stream:
+            >>> _ = stream.seek(0)
+            >>> newsurf = xtgeo.surface_from_file(stream, fformat = 'irap_binary')
+
+        .. versionchanged:: 2.5 Added support for BytesIO
+        .. versionchanged:: 2.13 Improved support for BytesIO
+        .. versionchanged:: 2.14 Support for alias file name and return value
+        """
+        logger.info("Export RegularSurface to file or memstream...")
+        mfile = xtgeosys._XTGeoFile(mfile, mode="wb", obj=self)
+
+        if not mfile.memstream:
+            mfile.check_folder(raiseerror=OSError)
+        else:
+            engine = "python"
+
+        if fformat in ("irap_ascii", "irapascii", "irap_txt", "irapasc"):
+            _regsurf_export.export_irap_ascii(self, mfile, engine=engine)
+
+        elif fformat in ("irap_binary", "irapbinary", "irapbin", "irap", "gri"):
+            _regsurf_export.export_irap_binary(self, mfile, engine=engine)
+
+        elif "zmap" in fformat:
+            _regsurf_export.export_zmap_ascii(self, mfile, engine=engine)
+
+        elif fformat == "storm_binary":
+            _regsurf_export.export_storm_binary(self, mfile)
+
+        elif fformat == "petromod":
+            _regsurf_export.export_petromod_binary(self, mfile, pmd_dataunits)
+
+        elif fformat == "ijxyz":
+            _regsurf_export.export_ijxyz_ascii(self, mfile)
+
+        # developing, in prep and experimental!
+        elif fformat == "xtgregsurf":
+            _regsurf_export.export_xtgregsurf(self, mfile)
+
+        else:
+            raise ValueError(f"Invalid file format: {fformat}")
+
+        logger.info("Export RegularSurface to file or memstream... done")
+
+        if mfile.memstream:
+            return None
+        return mfile.file
+
+    @deprecation.deprecated(
+        deprecated_in="2.15",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.surface_from_hdf() instead",
+    )
+    def from_hdf(
+        self,
+        mfile: Union[str, pathlib.Path, io.BytesIO],
+        values: Optional[bool] = True,
+    ):
+        """Import/load a surface (map) with metadata from a HDF5 file.
+
+        Warning:
+            This implementation is currently experimental and only recommended
+            for testing.
+
+        The file extension shall be '.hdf'.
+
+        Args:
+            mfile: File name or Path object or memory stream
+            values: If False, only metadatadata are read
+
+        Returns:
+            RegularSurface() instance
+
+        Example:
+            >>> import xtgeo
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> filepath = surf.to_hdf(outdir + "/topreek_rota.hdf")
+            >>> mysurf = xtgeo.RegularSurface().from_hdf(filepath)
+
+        .. versionadded:: 2.14
+        """
+        # developing, in prep and experimental!
+        mfile = xtgeosys._XTGeoFile(mfile, mode="rb", obj=self)
+
+        kwargs = _regsurf_import.import_hdf5_regsurf(mfile, values=values)
+
+        self._reset(**kwargs)
+
+        _self: self.__class__ = self
+        return _self  # to make obj = xtgeo.RegularSurface().from_hdf(stream) work
+
+    def to_hdf(
+        self,
+        mfile: Union[str, pathlib.Path, io.BytesIO],
+        compression: Optional[str] = "lzf",
+    ) -> pathlib.Path:
+        """Export a surface (map) with metadata to a HDF5 file.
+
+        Warning:
+            This implementation is currently experimental and only recommended
+            for testing.
+
+        The file extension shall be '.hdf'
+
+        Args:
+            mfile: Name of file, Path instance or BytesIO instance. An alias can
+                be e.g. ``$md5sum.hdf``,  ``$fmu-v1.hdf`` with string or Path() input.
+            compression: Compression method, None, lzf (default), blosc
+
+        Returns:
+            pathlib.Path: The actual file instance, or None if io.Bytestream
+
+        Example:
+            >>> import xtgeo
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> filepath = surf.to_hdf(outdir + "/topreek_rota.hdf")
+
+        .. versionadded:: 2.14
+
+        """
+        # developing, in prep and experimental!
+        mfile = xtgeosys._XTGeoFile(mfile, mode="wb", obj=self)
+
+        if not mfile.memstream:
+            mfile.check_folder(raiseerror=OSError)
+
+        _regsurf_export.export_hdf5_regsurf(self, mfile, compression=compression)
+        return mfile.file
+
+    @classmethod
+    def _read_roxar(
+        cls, project, name, category, stype="horizons", realisation=0
+    ):  # pragma: no cover
+        """Load a surface from a Roxar RMS project.
+
+        The import from the RMS project can be done either within the project
+        or outside the project.
+
+        Note that a shortform to::
+
+          import xtgeo
+          mysurf = xtgeo.surface_from_roxar(project, 'name', 'category')
+
+        Note also that horizon/zone name and category must exists in advance,
+        otherwise an Exception will be raised.
+
+        Args:
+            project (str or special): Name of project (as folder) if
+                outside RMS, og just use the magic project word if within RMS.
+            name (str): Name of surface/map
+            category (str): For horizons/zones or clipboard/general2d_data: for
+                example 'DS_extracted'
+            stype (str): RMS folder type, 'horizons' (default), 'zones' or 'clipboard'
+            realisation (int): Realisation number, default is 0
+
+        """
+        kwargs = _regsurf_roxapi.import_horizon_roxapi(
+            project, name, category, stype, realisation
+        )
+
+        return cls(**kwargs)
+
+    @deprecation.deprecated(
+        deprecated_in="2.15",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.surface_from_roxar() instead",
+    )
+    def from_roxar(
+        self, project, name, category, stype="horizons", realisation=0
+    ):  # pragma: no cover
+        """Load a surface from a Roxar RMS project.
+
+        The import from the RMS project can be done either within the project
+        or outside the project.
+
+        Note that a shortform to::
+
+          import xtgeo
+          mysurf = xtgeo.surface.RegularSurface()
+          mysurf.from_roxar(project, 'TopAare', 'DepthSurface')
+
+        is::
+
+          import xtgeo
+          mysurf = xtgeo.surface_from_roxar(project, 'TopAare', 'DepthSurface')
+
+        Note also that horizon/zone name and category must exists in advance,
+        otherwise an Exception will be raised.
+
+        Args:
+            project (str or special): Name of project (as folder) if
+                outside RMS, og just use the magic project word if within RMS.
+            name (str): Name of surface/map
+            category (str): For horizons/zones or clipboard/general2d_data: for
+                example 'DS_extracted'
+            stype (str): RMS folder type, 'horizons' (default), 'zones', 'clipboard'
+                or 'general2d_data'
+            realisation (int): Realisation number, default is 0
+
+        Returns:
+            Object instance updated
+
+        Raises:
+            ValueError: Various types of invalid inputs.
+
+        Example:
+            Here the from_roxar method is used to initiate the object
+            directly::
+
+              mymap = RegularSurface()
+              mymap.from_roxar(project, 'TopAare', 'DepthSurface')
+
+
+        """
+        kwargs = _regsurf_roxapi.import_horizon_roxapi(
+            project, name, category, stype, realisation
+        )
+
+        self.metadata.required = self
+        self._reset(**kwargs)
+
+    def to_roxar(
+        self, project, name, category, stype="horizons", realisation=0
+    ):  # pragma: no cover
+        """Store (export) a regular surface to a Roxar RMS project.
+
+        The export to the RMS project can be done either within the project
+        or outside the project. The storing is done to the Horizons or the
+        Zones folder in RMS.
+
+        Note:
+            The horizon or zone name and category must exists in advance,
+            otherwise an Exception will be raised.
+
+            When project is file path (direct access, outside RMS) then
+            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+            will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str or special): Name of project (as folder) if
+                outside RMS, og just use the magic project word if within RMS.
+            name (str): Name of surface/map
+            category (str): Required for horizons/zones: e.g. 'DS_extracted'. For
+                clipboard/general2d_data is reperesent the folder(s), where "" or None
+                means no folder, while e.g. "myfolder/subfolder" means that folders
+                myfolder/subfolder will be created if not already present. For
+                stype = 'trends', the category will not be applied
+            stype (str): RMS folder type, 'horizons' (default), 'zones', 'clipboard'
+                'general2d_data', 'trends'
+            realisation (int): Realisation number, default is 0
+
+        Raises:
+            ValueError: If name or category does not exist in the project
+
+        Example:
+            Here the from_roxar method is used to initiate the object
+            directly::
+
+              import xtgeo
+              topupperreek = xtgeo.surface_from_roxar(project, 'TopUpperReek',
+                                                    'DS_extracted')
+              topupperreek.values += 200
+
+              # export to file:
+              topupperreek.to_file('topupperreek.gri')
+
+              # store in project
+              topupperreek.to_roxar(project, 'TopUpperReek', 'DS_something')
+
+        Note::
+
+            When dealing with surfaces to and from ``stype="trends"``, the surface must
+            exist in advance, i.e. the Roxar API do not allow creating new surfaces.
+            Actually trends are read only, but a workaround using ``load()`` in Roxar
+            API makes it possible to overwrite existing surface trends. In addition,
+            ``realisation`` is not applied in trends.
+
+
+        .. versionadded:: 2.1 clipboard support
+        .. versionadded:: 2.19 general2d_data and trends support
+
+        """
+        _regsurf_roxapi.export_horizon_roxapi(
+            self, project, name, category, stype, realisation
+        )
+
+    @deprecation.deprecated(
+        deprecated_in="2.15",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.surface.surface_from_cube() instead",
+    )
+    def from_cube(self, cube, zlevel):
+        """Make a constant surface from a Cube, at a given time/depth level.
+
+        The surface instance will have exactly the same origins and increments
+        as the cube.
+
+        Args:
+            cube (Cube): XTGeo Cube instance
+            zlevel (float): Depth or Time (or whatever) value of the surface
+
+        Returns:
+            Object instance updated
+
+        Example:
+            Here the from_roxar method is used to initiate the object
+            directly::
+
+            >>> import xtgeo
+            >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+            >>> mymap = xtgeo.RegularSurface()
+            >>> mymap.from_cube(mycube, 2700)
+
+        """
+        props = [
+            "ncol",
+            "nrow",
+            "xori",
+            "yori",
+            "xinc",
+            "yinc",
+            "rotation",
+            "ilines",
+            "xlines",
+            "yflip",
+        ]
+
+        input_dict = {key: deepcopy(getattr(cube, key)) for key in props}
+
+        input_dict["values"] = ma.array(
+            np.full((input_dict["ncol"], input_dict["nrow"]), zlevel, dtype=np.float64)
+        )
+        self._reset(**input_dict)
+
+    @classmethod
+    def _read_cube(cls, cube, zlevel):
+        """Make a constant surface from a Cube, at a given time/depth level.
+
+        The surface instance will have exactly the same origins and increments
+        as the cube.
+
+        Args:
+            cube (Cube): XTGeo Cube instance
+            zlevel (float): Depth or Time (or whatever) value of the surface
+
+        Returns:
+            Object instance updated
+
+        Example:
+            Here the from_roxar method is used to initiate the object
+            directly::
+
+            >>> mycube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+            >>> mymap = RegularSurface._read_cube(mycube, 2700)
+
+        """
+        props = [
+            "ncol",
+            "nrow",
+            "xori",
+            "yori",
+            "xinc",
+            "yinc",
+            "rotation",
+            "ilines",
+            "xlines",
+            "yflip",
+        ]
+
+        input_dict = {key: deepcopy(getattr(cube, key)) for key in props}
+
+        input_dict["values"] = ma.array(
+            np.full((input_dict["ncol"], input_dict["nrow"]), zlevel, dtype=np.float64)
+        )
+        return cls(**input_dict)
+
+    @classmethod
+    def _read_grid3d(cls, grid, template=None, where="top", mode="depth", rfactor=1):
+        """Extract a surface from a 3D grid.
+
+        Args:
+            grid (Grid): XTGeo Grid instance
+            template (RegularSurface): Using an existing surface as template
+            where (str): "top", "base" or use the syntax "2_top" where 2
+                is layer no. 2 and _top indicates top of cell, while "_base"
+                indicates base of cell
+            mode (str): "depth", "i" or "j"
+            rfactor (float): Determines how fine the extracted map is; higher values
+                for finer map (but computing time will increase). Will only apply if
+                template is None.
+
+        Returns:
+            Object instance
+
+        Example::
+
+
+            >>> import xtgeo
+            >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> # make surface from top (default)
+            >>> mymap = RegularSurface._read_grid3d(mygrid)
+
+        .. versionadded:: 2.14
+
+        """
+        args, _, _ = _regsurf_grid3d.from_grid3d(
+            grid, template=template, where=where, mode=mode, rfactor=rfactor
+        )
+        return cls(**args)
+
+    @deprecation.deprecated(
+        deprecated_in="2.15",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.surface_from_grid3d() instead",
+    )
+    def from_grid3d(self, grid, template=None, where="top", mode="depth", rfactor=1):
+        # It would perhaps to be natural to have this as a Grid() method also?
+        """Extract a surface from a 3D grid.
+
+        Args:
+            grid (Grid): XTGeo Grid instance
+            template(RegularSurface): Optional to use an existing surface as
+                template for geometry
+            where (str): "top", "base" or use the syntax "2_top" where 2
+                is layer no. 2 and _top indicates top of cell, while "_base"
+                indicates base of cell
+            mode (str): "depth", "i" or "j"
+            rfactor (float): Determines how fine the extracted map is; higher values
+                for finer map (but computing time will increase). Will only apply if
+                template is None.
+
+        Returns:
+            Object instance is updated in-place
+            When mode="depth", two RegularSurface: icols and jrows are also returned.
+
+        Example::
+
+            >>> import xtgeo
+            >>> mymap = RegularSurface()
+            >>> mygrid = xtgeo.grid_from_file(reek_dir + "/REEK.EGRID")
+            >>> # return two additonal maps
+            >>> ic, jr = mymap.from_grid3d(mygrid)
+
+        .. versionadded:: 2.1
+
+        """
+        args, ivalues, jvalues = _regsurf_grid3d.from_grid3d(
+            grid, template=template, where=where, mode=mode, rfactor=rfactor
+        )
+        self._reset(**args)
+        if ivalues is not None and jvalues is not None:
+            ivals = self.copy()
+            args["values"] = ivalues
+            ivals._reset(**args)
+            jvals = self.copy()
+            args["values"] = jvalues
+            jvals._reset(**args)
+            return ivals, jvals
+        else:
+            return None
+
+    def copy(self):
+        """Deep copy of a RegularSurface object to another instance.
+
+        Example::
+
+            >>> mymap = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> mymapcopy = mymap.copy()
+
+        """
+        # pylint: disable=protected-access
+
+        xsurf = RegularSurface(
+            ncol=self.ncol,
+            nrow=self.nrow,
+            xinc=self.xinc,
+            yinc=self.yinc,
+            xori=self.xori,
+            yori=self.yori,
+            rotation=self.rotation,
+            yflip=self.yflip,
+        )
+
+        xsurf._values = self._values.copy()
+
+        xsurf.ilines = self._ilines.copy()
+        xsurf.xlines = self._xlines.copy()
+        xsurf.filesrc = self._filesrc
+        xsurf.metadata.required = xsurf
+
+        return xsurf
+
+    def get_values1d(
+        self, order="C", asmasked=False, fill_value=xtgeo.UNDEF, activeonly=False
+    ):
+        """Get a 1D numpy or masked array of the map values.
+
+        Args:
+            order (str): Flatteting is in C (default) or F order
+            asmasked (bool): If true, return as MaskedArray, other as standard
+                numpy ndarray with undef as np.nan or fill_value
+            fill_value (str): Relevent only if asmasked is False, this
+                will be the value of undef entries
+            activeonly (bool): If True, only active cells. Keys 'asmasked' and
+                'fill_value' are not revelant.
+
+        Returns:
+            A numpy 1D array or MaskedArray
+
+        """
+        val = self.values.copy()
+
+        if order == "F":
+            val = ma.array(val.data, mask=val.mask, order="F")
+
+        val = val.ravel(order=order)
+
+        if activeonly:
+            val = val[~val.mask]
+
+        if not asmasked and not activeonly:
+            val = ma.filled(val, fill_value=fill_value)
+
+        return val
+
+    def set_values1d(self, val, order="C"):
+        """Update the values attribute based on a 1D input, multiple options.
+
+        If values are np.nan or values are > UNDEF_LIMIT, they will be
+        masked.
+
+        Args:
+            val (list-like): Set values as a 1D array
+            order (str): Input is C (default) or F order
+        """
+        if order == "F":
+            val = np.copy(val, order="C")
+
+        val = val.reshape((self.ncol, self.nrow))
+
+        if not isinstance(val, ma.MaskedArray):
+            val = ma.array(val)
+
+        val = ma.masked_greater(val, self.undef_limit)
+        val = ma.masked_invalid(val)
+
+        self.values = val
+
+    def get_rotation(self):
+        """Returns the surface roation, in degrees, from X, anti-clock."""
+        return self._rotation
+
+    def get_nx(self):
+        """Same as ncol (nx) (for backward compatibility)."""
+        return self._ncol
+
+    def get_ny(self):
+        """Same as nrow (ny) (for backward compatibility)."""
+        return self._nrow
+
+    def get_xori(self):
+        """Same as property xori (for backward compatibility)."""
+        return self._xori
+
+    def get_yori(self):
+        """Same as property yori (for backward compatibility)."""
+        return self._yori
+
+    def get_xinc(self):
+        """Same as property xinc (for backward compatibility)."""
+        return self._xinc
+
+    def get_yinc(self):
+        """Same as property yinc (for backward compatibility)."""
+        return self._yinc
+
+    def similarity_index(self, other):
+        """Report the degree of similarity between two maps, by comparing mean.
+
+        The method computes the average per surface, and the similarity
+        is difference in mean divided on mean of self. I.e. values close
+        to 0.0 mean small difference.
+
+        Args:
+            other (surface object): The other surface to compare with
+
+        """
+        ovalues = other.values
+        svalues = self.values
+
+        diff = math.pow(svalues.mean() - ovalues.mean(), 2)
+        diff = math.sqrt(diff)
+
+        try:
+            diff = diff / svalues.mean()
+        except ZeroDivisionError:
+            diff = -999
+
+        return diff
+
+    def compare_topology(self, other, strict=True):
+        """Check that two object has the same topology, i.e. map definitions.
+
+        Map definitions such as origin, dimensions, number of defined cells...
+
+        Args:
+            other (surface object): The other surface to compare with
+            strict (bool): If false, the masks are not compared
+
+        Returns:
+            True of same topology, False if not
+        """
+        tstatus = True
+
+        # consider refactor to getattr() instead!
+        chklist = set(
+            ["_ncol", "_nrow", "_xori", "_yori", "_xinc", "_yinc", "_rotation"]
+        )
+        for skey, sval in self.__dict__.items():
+            if skey in chklist:
+                for okey, oval in other.__dict__.items():
+                    if skey == okey and sval != oval:
+                        logger.info("CMP %s: %s vs %s", skey, sval, oval)
+                        tstatus = False
+
+        if not tstatus:
+            return False
+
+        # check that masks are equal
+        mas1 = ma.getmaskarray(self.values)
+        mas2 = ma.getmaskarray(other.values)
+        if (
+            strict
+            and isinstance(mas1, np.ndarray)
+            and isinstance(mas2, np.ndarray)
+            and not np.array_equal(mas1, mas2)
+        ):
+            logger.warning("Masks differ, not consistent with 'strict'")
+            return False
+        return True
+
+    def swapaxes(self):
+        """Swap (flip) the axes columns vs rows, keep origin but reverse yflip."""
+        _regsurf_utils.swapaxes(self)
+
+    def get_map_xycorners(self):
+        """Get the X and Y coordinates of the map corners.
+
+        Returns a tuple on the form
+        ((x0, y0), (x1, y1), (x2, y2), (x3, y3)) where
+        (if unrotated and normal flip) 0 is the lower left
+        corner, 1 is the right, 2 is the upper left, 3 is the upper right.
+        """
+        rot1 = self._rotation * math.pi / 180
+        rot2 = rot1 + (math.pi / 2.0)
+
+        xc0 = self._xori
+        yc0 = self._yori
+
+        xc1 = self._xori + (self.ncol - 1) * math.cos(rot1) * self._xinc
+        yc1 = self._yori + (self.ncol - 1) * math.sin(rot1) * self._xinc
+
+        xc2 = self._xori + (self.nrow - 1) * math.cos(rot2) * self._yinc * self._yflip
+        yc2 = self._yori + (self.nrow - 1) * math.sin(rot2) * self._yinc * self._yflip
+
+        xc3 = xc2 + (self.ncol - 1) * math.cos(rot1) * self._xinc
+        yc3 = yc2 + (self.ncol - 1) * math.sin(rot1) * self._xinc
+
+        return ((xc0, yc0), (xc1, yc1), (xc2, yc2), (xc3, yc3))
+
+    def get_value_from_xy(self, point=(0.0, 0.0), sampling="bilinear"):
+        """Return the map value given a X Y point.
+
+        Args:
+            point (float tuple): Position of X and Y coordinate
+            sampling (str): Sampling method, either "bilinear" for bilinear
+                interpolation, or "nearest" for nearest node sampling (e.g. facies maps)
+
+        Returns:
+            The map value (interpolated). None if XY is outside defined map
+
+        Example::
+            mvalue = map.get_value_from_xy(point=(539291.12, 6788228.2))
+
+
+        .. versionchanged:: 2.14 Added keyword option `sampling`
+        """
+        zcoord = _regsurf_oper.get_value_from_xy(self, point=point, sampling=sampling)
+
+        return zcoord
+
+    def get_xy_value_from_ij(self, iloc, jloc, zvalues=None):
+        """Returns x, y, z(value) from a single i j location.
+
+        Args:
+            iloc (int): I (col) location (base is 1)
+            jloc (int): J (row) location (base is 1)
+            zvalues (ndarray). If this is used in a loop it is wise
+                to precompute the numpy surface once in the caller,
+                and submit the numpy array (use surf.get_values1d()).
+
+        Returns:
+            The x, y, z values at location iloc, jloc
+        """
+        xval, yval, value = _regsurf_oper.get_xy_value_from_ij(
+            self, iloc, jloc, zvalues=zvalues
+        )
+
+        return xval, yval, value
+
+    def get_ij_values(self, zero_based=False, asmasked=False, order="C"):
+        """Return I J numpy 2D arrays, optionally as masked arrays.
+
+        Args:
+            zero_based (bool): If False, first number is 1, not 0
+            asmasked (bool): If True, UNDEF map nodes are skipped
+            order (str): 'C' (default) or 'F' order (row vs column major)
+        """
+        return _regsurf_oper.get_ij_values(
+            self, zero_based=zero_based, asmasked=asmasked, order=order
+        )
+
+    def get_ij_values1d(self, zero_based=False, activeonly=True, order="C"):
+        """Return I J numpy as 1D arrays.
+
+        Args:
+            zero_based (bool): If False, first number is 1, not 0
+            activeonly (bool): If True, UNDEF map nodes are skipped
+            order (str): 'C' (default) or 'F' order (row vs column major)
+        """
+        return _regsurf_oper.get_ij_values1d(
+            self, zero_based=zero_based, activeonly=activeonly, order=order
+        )
+
+    def get_xy_values(self, order="C", asmasked=True):
+        """Return coordinates for X and Y as numpy (masked) 2D arrays.
+
+        Args:
+            order (str): 'C' (default) or 'F' order (row major vs column major)
+            asmasked (bool): If True , inactive nodes are masked.
+        """
+        xvals, yvals = _regsurf_oper.get_xy_values(self, order=order, asmasked=asmasked)
+
+        return xvals, yvals
+
+    def get_xy_values1d(self, order="C", activeonly=True):
+        """Return coordinates for X and Y as numpy 1D arrays.
+
+        Args:
+            order (str): 'C' (default) or 'F' order (row major vs column major)
+            activeonly (bool): Only active cells are returned.
+        """
+        xvals, yvals = _regsurf_oper.get_xy_values1d(
+            self, order=order, activeonly=activeonly
+        )
+
+        return xvals, yvals
+
+    def get_xyz_values(self):
+        """Return coordinates for X Y and Z (values) as numpy (masked) 2D arrays."""
+        xcoord, ycoord = self.get_xy_values(asmasked=True)
+
+        values = self.values.copy()
+
+        return xcoord, ycoord, values
+
+    def get_xyz_values1d(self, order="C", activeonly=True, fill_value=np.nan):
+        """Return coordinates for X Y and Z (values) as numpy 1D arrays.
+
+        Args:
+            order (str): 'C' (default) or 'F' order (row major vs column major)
+            activeonly (bool): Only active cells are returned.
+            fill_value (float): If activeonly is False, value of inactive nodes
+        """
+        xcoord, ycoord = self.get_xy_values1d(order=order, activeonly=activeonly)
+
+        values = self.get_values1d(
+            order=order, asmasked=False, fill_value=fill_value, activeonly=activeonly
+        )
+
+        return xcoord, ycoord, values
+
+    def get_dataframe(
+        self, ijcolumns=False, ij=False, order="C", activeonly=True, fill_value=np.nan
+    ):
+        """Return a Pandas dataframe object, with columns X_UTME, Y_UTMN, VALUES.
+
+        Args:
+            ijcolumns (bool): If True, and IX and JY indices will be
+               added as dataframe columns. Redundant, use "ij" instead.
+            ij (bool): Same as ijcolumns. If True, and IX and JY indices will be
+               added as dataframe columns. Preferred syntax
+            order (str): 'C' (default) for C order (row fastest), or 'F'
+               for Fortran order (column fastest)
+            activeonly (bool): If True, only active nodes are listed. If
+                False, the values will have fill_value default None = NaN
+                as values
+            fill_value (float): Value of inactive nodes if activeonly is False
+
+        Example::
+
+            >>> import xtgeo
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> dfr = surf.get_dataframe()
+            >>> dfr.to_csv('somecsv.csv')
+
+        Returns:
+            A Pandas dataframe object.
+        """
+        xcoord, ycoord, values = self.get_xyz_values1d(
+            order=order, activeonly=activeonly, fill_value=fill_value
+        )
+
+        entry = OrderedDict()
+
+        if ijcolumns or ij:
+            ixn, jyn = self.get_ij_values1d(order=order, activeonly=activeonly)
+            entry["IX"] = ixn
+            entry["JY"] = jyn
+
+        entry.update([("X_UTME", xcoord), ("Y_UTMN", ycoord), ("VALUES", values)])
+
+        dataframe = pd.DataFrame(entry)
+        return dataframe
+
+    dataframe = get_dataframe  # for compatibility backwards
+
+    def get_xy_value_lists(self, lformat="webportal", xyfmt=None, valuefmt=None):
+        """Returns two lists for coordinates (x, y) and values.
+
+        For lformat = 'webportal' (default):
+
+        The lists are returned as xylist and valuelist, where xylist
+        is on the format:
+
+            [(x1, y1), (x2, y2) ...] (a list of x, y tuples)
+
+        and valuelist is one the format
+
+            [v1, v2, ...]
+
+        Inactive cells will be ignored.
+
+        Args:
+            lformat (string): List return format ('webportal' is default,
+                other options later)
+            xyfmt (string): The formatter for xy numbers, e.g. '12.2f'
+                (default None). Note no checks on valid input.
+            valuefmt (string): The formatter for values e.g. '8.4f'
+                (default None). Note no checks on valid input.
+
+        Returns:
+            xylist, valuelist
+
+        Example:
+
+            >>> import xtgeo
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> xylist, valuelist = surf.get_xy_value_lists(valuefmt='6.2f')
+        """
+        xylist = []
+        valuelist = []
+
+        zvalues = self.get_values1d()
+
+        if lformat != "webportal":
+            raise ValueError("Unsupported lformat")
+
+        for jnum in range(self.nrow):
+            for inum in range(self.ncol):
+                xcv, ycv, vcv = self.get_xy_value_from_ij(
+                    inum + 1, jnum + 1, zvalues=zvalues
+                )
+
+                if vcv is not None:
+                    if xyfmt is not None:
+                        xcv = float(f"{xcv:{xyfmt}}")
+                        ycv = float(f"{ycv:{xyfmt}}")
+                    if valuefmt is not None:
+                        vcv = float(f"{vcv:{valuefmt}}")
+                    valuelist.append(vcv)
+                    xylist.append((xcv, ycv))
+
+        return xylist, valuelist
+
+    # ==================================================================================
+    # Crop, interpolation, smooth or fill of values (possibly many methods here)
+    # ==================================================================================
+
+    def autocrop(self):
+        """Automatic cropping of the surface to minimize undefined areas.
+
+        This method is simply removing undefined "white areas". The
+        instance will be updated with new values for xori, yori, ncol, etc. Rotation
+        will never change
+
+        Returns:
+            RegularSurface instance is updated in-place
+
+        .. versionadded:: 2.12
+        """
+        _regsurf_utils.autocrop(self)
+
+    def fill(self, fill_value=None):
+        """Fast infilling of undefined values.
+
+        Note that minimum and maximum values will not change.
+
+        Algorithm if `fill_value` is not set is based on a nearest node extrapolation.
+        Technically, ``scipy.ndimage.distance_transform_edt`` is applied. If fill_value
+        is set by a scalar, that (constant) value be be applied
+
+        Args:
+            fill_value (float): If defined, fills all undefined cells with that value.
+
+        Returns:
+            RegularSurface instance is updated in-place
+
+        .. versionadded:: 2.1
+        .. versionchanged:: 2.6 Added option key `fill_value`
+        """
+        _regsurf_gridding.surf_fill(self, fill_value=fill_value)
+
+    def smooth(self, method="median", iterations=1, width=1):
+        """Various smoothing methods for surfaces.
+
+        Args:
+            method: Smoothing method (median)
+            iterations: Number of iterations
+            width: Range of influence (in nodes)
+
+        .. versionadded:: 2.1
+        """
+        if method == "median":
+            _regsurf_gridding.smooth_median(self, iterations=iterations, width=width)
+        else:
+            raise ValueError("Unsupported method for smoothing")
+
+    # ==================================================================================
+    # Operation on map values (list to be extended)
+    # ==================================================================================
+
+    def operation(self, opname, value):
+        """Do operation on map values.
+
+        Do operations on the current map values. Valid operations are:
+
+        * 'elilt' or 'eliminatelessthan': Eliminate less than <value>
+
+        * 'elile' or 'eliminatelessequal': Eliminate less or equal than <value>
+
+        Args:
+            opname (str): Name of operation. See list above.
+            value (*): A scalar number (float) or a tuple of two floats,
+                dependent on operation opname.
+
+        Examples::
+
+            surf.operation('elilt', 200)  # set all values < 200 as undef
+        """
+        if opname in ("elilt", "eliminatelessthan"):
+            self._values = ma.masked_less(self._values, value)
+        elif opname in ("elile", "eliminatelessequal"):
+            self._values = ma.masked_less_equal(self._values, value)
+        else:
+            raise ValueError("Invalid operation name")
+
+    # ==================================================================================
+    # Operations restricted to inside/outside polygons
+    # ==================================================================================
+
+    def operation_polygons(self, poly, value, opname="add", inside=True):
+        """A generic function for map operations inside or outside polygon(s).
+
+        Args:
+            poly (Polygons): A XTGeo Polygons instance
+            value(float or RegularSurface): Value to add, subtract etc
+            opname (str): Name of operation... 'add', 'sub', etc
+            inside (bool): If True do operation inside polygons; else outside.
+        """
+        _regsurf_oper.operation_polygons(
+            self, poly, value, opname=opname, inside=inside
+        )
+
+    # shortforms
+    def add_inside(self, poly, value):
+        """Add a value (scalar or other map) inside polygons."""
+        self.operation_polygons(poly, value, opname="add", inside=True)
+
+    def add_outside(self, poly, value):
+        """Add a value (scalar or other map) outside polygons."""
+        self.operation_polygons(poly, value, opname="add", inside=False)
+
+    def sub_inside(self, poly, value):
+        """Subtract a value (scalar or other map) inside polygons."""
+        self.operation_polygons(poly, value, opname="sub", inside=True)
+
+    def sub_outside(self, poly, value):
+        """Subtract a value (scalar or other map) outside polygons."""
+        self.operation_polygons(poly, value, opname="sub", inside=False)
+
+    def mul_inside(self, poly, value):
+        """Multiply a value (scalar or other map) inside polygons."""
+        self.operation_polygons(poly, value, opname="mul", inside=True)
+
+    def mul_outside(self, poly, value):
+        """Multiply a value (scalar or other map) outside polygons."""
+        self.operation_polygons(poly, value, opname="mul", inside=False)
+
+    def div_inside(self, poly, value):
+        """Divide a value (scalar or other map) inside polygons."""
+        self.operation_polygons(poly, value, opname="div", inside=True)
+
+    def div_outside(self, poly, value):
+        """Divide a value (scalar or other map) outside polygons."""
+        self.operation_polygons(poly, value, opname="div", inside=False)
+
+    def set_inside(self, poly, value):
+        """Set a value (scalar or other map) inside polygons."""
+        self.operation_polygons(poly, value, opname="set", inside=True)
+
+    def set_outside(self, poly, value):
+        """Set a value (scalar or other map) outside polygons."""
+        self.operation_polygons(poly, value, opname="set", inside=False)
+
+    def eli_inside(self, poly):
+        """Eliminate current map values inside polygons."""
+        self.operation_polygons(poly, 0, opname="eli", inside=True)
+
+    def eli_outside(self, poly):
+        """Eliminate current map values outside polygons."""
+        self.operation_polygons(poly, 0, opname="eli", inside=False)
+
+    # ==================================================================================
+    # Operation with secondary map
+    # ==================================================================================
+
+    def add(self, other):
+        """Add another map to current map."""
+        _regsurf_oper.operations_two(self, other, oper="add")
+
+    def subtract(self, other):
+        """Subtract another map from current map."""
+        _regsurf_oper.operations_two(self, other, oper="sub")
+
+    def multiply(self, other):
+        """Multiply another map and current map."""
+        _regsurf_oper.operations_two(self, other, oper="mul")
+
+    def divide(self, other):
+        """Divide current map with another map."""
+        _regsurf_oper.operations_two(self, other, oper="div")
+
+    # ==================================================================================
+    # Interacion with points
+    # ==================================================================================
+
+    def gridding(self, points, method="linear", coarsen=1):
+        """Grid a surface from points.
+
+        Args:
+            points(Points): XTGeo Points instance.
+            method (str): Gridding method option: linear / cubic / nearest
+            coarsen (int): Coarsen factor, to speed up gridding, but will
+                give poorer result.
+
+        Example::
+
+            >>> import xtgeo
+            >>> mypoints = xtgeo.Points(points_dir + '/pointset2.poi')
+            >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+
+            >>> # update the surface by gridding the points
+            >>> mysurf.gridding(mypoints)
+
+        Raises:
+            RuntimeError: If not possible to grid for some reason
+            ValueError: If invalid input
+
+        """
+        if not isinstance(points, xtgeo.xyz.Points):
+            raise ValueError("Argument not a Points instance")
+
+        logger.info("Do gridding...")
+
+        _regsurf_gridding.points_gridding(self, points, coarsen=coarsen, method=method)
+
+    # ==================================================================================
+    # Interacion with other surface
+    # ==================================================================================
+
+    def resample(self, other, mask=True, sampling="bilinear"):
+        """Resample an instance surface values from another surface instance.
+
+        Note that there may be some 'loss' of nodes at the edges of the
+        updated map, as only the 'inside' nodes in the updated map
+        versus the input map are applied.
+
+        The interpolation algorithm in resample is bilinear interpolation. The
+        topolopogy of the surface (map definitions, rotation, ...) will not change,
+        only the map values. Areas with undefined nodes in ``other`` will become
+        undefined in the instance if mask is True; othewise they will be kept as is.
+
+        Args:
+            other (RegularSurface): Surface to resample from.
+            mask (bool): If True (default) nodes outside will be made undefined,
+                if False then values will be kept as original
+            sampling (str): Either 'bilinear' interpolation (default) or, 'nearest' for
+                nearest node. The latter can be useful for resampling discrete maps.
+
+        Example::
+
+            # map with 230x210 columns, rotation 20
+            surf1 = xtgeo.surface_from_file("some1.gri")
+            # map with 270x190 columns, rotation 0
+            surf2 = xtgeo.surface_from_file("some2.gri")
+            # will sample (interpolate) surf2's values to surf1
+            surf1.resample(surf2)
+
+        Returns:
+            Instance's surface values will be updated in-place.
+
+
+        .. versionchanged:: 2.9
+           Added ``mask`` keyword, default is True for backward compatibility.
+
+        .. versionchanged:: 2.21
+           Added ``sampling`` keyword option.
+
+        """
+        if not isinstance(other, RegularSurface):
+            raise ValueError("Argument not a RegularSurface instance")
+
+        logger.info("Do resampling...")
+
+        _regsurf_oper.resample(self, other, mask=mask, sampling=sampling)
+
+    # ==================================================================================
+    # Change a surface more fundamentally
+    # ==================================================================================
+
+    def unrotate(self, factor=2):
+        r"""Unrotete a map instance, and this will also change nrow, ncol, xinc, etc.
+
+        The default sampling (factor=2) makes a finer grid in order to
+        avoid artifacts, and this default can be used in most cases.
+
+        If an even finer grid is wanted, increase the factor. Theoretically the
+        new increment for factor=N is between :math:`\\frac{1}{N}` and
+        :math:`\\frac{1}{N}\\sqrt{2}` of the original increment,
+        dependent on the rotation of the original surface.
+
+        If the current instance already is unrotated, nothing is done.
+
+        Args:
+            factor (int): Refinement factor (>= 1)
+
+        """
+        if abs(self.rotation) < 0.00001:
+            logger.info("Surface has no rotation, nothing is done")
+            return
+
+        if factor < 1:
+            raise ValueError("Unrotate refinement factor cannot be be less than 1")
+
+        if not isinstance(factor, int):
+            raise ValueError("Refinementfactor must an integer")
+
+        scopy = self
+        if scopy._yflip < 0:
+            scopy = self.copy()
+            scopy.swapaxes()
+
+        xlen = scopy.xmax - scopy.xmin
+        ylen = scopy.ymax - scopy.ymin
+        ncol = scopy.ncol * factor
+        nrow = scopy.nrow * factor
+        xinc = xlen / (ncol - 1)  # node based, not cell center based
+        yinc = ylen / (nrow - 1)
+        vals = ma.zeros((ncol, nrow), order="C")
+
+        nonrot = RegularSurface(
+            xori=scopy.xmin,
+            yori=scopy.ymin,
+            xinc=xinc,
+            yinc=yinc,
+            ncol=ncol,
+            nrow=nrow,
+            values=vals,
+            yflip=1,
+        )
+        nonrot.resample(scopy)
+
+        self._values = nonrot.values
+        self._nrow = nonrot.nrow
+        self._ncol = nonrot.ncol
+        self._rotation = nonrot.rotation
+        self._xori = nonrot.xori
+        self._yori = nonrot.yori
+        self._xinc = nonrot.xinc
+        self._yinc = nonrot.yinc
+        self._yflip = nonrot.yflip
+        self._ilines = nonrot.ilines
+        self._xlines = nonrot.xlines
+
+    def refine(self, factor):
+        """Refine a surface with a factor.
+
+        Range for factor is 2 to 10.
+
+        Note that there may be some 'loss' of nodes at the edges of the
+        updated map, as only the 'inside' nodes in the updated map
+        versus the input map are applied.
+
+        Args:
+            factor (int): Refinement factor
+        """
+        logger.info("Do refining...")
+
+        if not isinstance(factor, int):
+            raise ValueError("Argument not a, Integer")
+
+        if factor < 2 or factor >= 10:
+            raise ValueError("Argument exceeds range 2 .. 10")
+
+        xlen = self._xinc * (self._ncol - 1)
+        ylen = self._yinc * (self._nrow - 1)
+
+        proxy = self.copy()
+        self._ncol = proxy.ncol * factor
+        self._nrow = proxy.nrow * factor
+        self._xinc = xlen / (self._ncol - 1)
+        self._yinc = ylen / (self._nrow - 1)
+
+        self._values = ma.zeros((self._ncol, self._nrow))
+
+        self._ilines = np.array(range(1, self._ncol + 1), dtype=np.int32)
+        self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
+
+        self.resample(proxy)
+
+        del proxy
+        logger.info("Do refining... done")
+
+    def coarsen(self, factor):
+        """Coarsen a surface with a factor.
+
+        Range for coarsening is 2 to 10, where e.g. 2 meaning half the number of
+        columns and rows.
+
+        Note that there may be some 'loss' of nodes at the edges of the
+        updated map, as only the 'inside' nodes in the updated map
+        versus the input map are applied.
+
+        Args:
+            factor (int): Coarsen factor (2 .. 10)
+
+        Raises:
+            ValueError: Coarsen is too large, giving too few nodes in result
+        """
+        logger.info("Do coarsening...")
+        if not isinstance(factor, int):
+            raise ValueError("Argument not a, Integer")
+
+        if factor < 2 or factor >= 10:
+            raise ValueError("Argument exceeds range 2 .. 10")
+
+        proxy = self.copy()
+        xlen = self._xinc * (self._ncol - 1)
+        ylen = self._yinc * (self._nrow - 1)
+
+        ncol = int(round(proxy._ncol / factor))
+        nrow = int(round(proxy._nrow / factor))
+
+        if ncol < 4 or nrow < 4:
+            raise ValueError(
+                "Coarsen is too large, giving ncol or nrow less than 4 nodes"
+            )
+
+        self._ncol = ncol
+        self._nrow = nrow
+
+        self._xinc = xlen / (self._ncol - 1)
+        self._yinc = ylen / (self._nrow - 1)
+
+        self._values = ma.zeros((self._ncol, self._nrow))
+
+        self._ilines = np.array(range(1, self._ncol + 1), dtype=np.int32)
+        self._xlines = np.array(range(1, self._nrow + 1), dtype=np.int32)
+
+        self.resample(proxy)
+
+        del proxy
+        logger.info("Do coarsening... done")
+
+    # ==================================================================================
+    # Interacion with a grid3d
+    # ==================================================================================
+
+    def slice_grid3d(self, grid, prop, zsurf=None, sbuffer=1):
+        """Slice the grid property and update the instance surface to sampled values.
+
+        Args:
+            grid (Grid): Instance of a Grid.
+            prop (GridProperty): Instance of a GridProperty, belongs to grid
+            zsurf (surface object): Instance of map, which is used a slicer.
+                If None, then the surface instance itself is used a slice
+                criteria. Note that zsurf must have same map defs as the
+                surface instance.
+            sbuffer (int): Default is 1; if "holes" after sampling
+                extend this to e.g. 3
+        Example::
+
+            >>> import xtgeo
+            >>> grd = xtgeo.grid_from_file(reek_dir + '/REEK.EGRID')
+            >>> prop = xtgeo.gridproperty_from_file(
+            ...     reek_dir + '/REEK.UNRST',
+            ...     name='PRESSURE',
+            ...     date="first",
+            ...     grid=grd,
+            ... )
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> # update surf to sample the 3D grid property:
+            >>> surf.slice_grid3d(grd, prop)
+
+        Raises:
+            Exception if maps have different definitions (topology)
+        """
+        if not isinstance(grid, xtgeo.grid3d.Grid):
+            raise ValueError("First argument must be a grid instance")
+
+        ier = _regsurf_grid3d.slice_grid3d(
+            self, grid, prop, zsurf=zsurf, sbuffer=sbuffer
+        )
+
+        if ier != 0:
+            raise RuntimeError(
+                "Wrong status from routine; something went wrong. Contact the author"
+            )
+
+    # ==================================================================================
+    # Interacion with a cube
+    # ==================================================================================
+
+    def slice_cube(
+        self,
+        cube,
+        zsurf=None,
+        sampling="nearest",
+        mask=True,
+        snapxy=False,
+        deadtraces=True,
+        algorithm=2,
+    ):
+        """Slice the cube and update the instance surface to sampled cube values.
+
+        Args:
+            cube (object): Instance of a Cube()
+            zsurf (surface object): Instance of a depth (or time) map, which
+                is the depth or time map (or...) that is used a slicer.
+                If None, then the surface instance itself is used a slice
+                criteria. Note that zsurf must have same map defs as the
+                surface instance.
+            sampling (str): 'nearest' for nearest node (default), or
+                'trilinear' for trilinear interpolation.
+            mask (bool): If True (default), then the map values outside
+                the cube will be undef. Otherwise, map will be kept as is.
+            snapxy (bool): If True (optional), then the map values will get
+                values at nearest Cube XY location. Only relevant to use if
+                surface is derived from seismic coordinates (e.g. Auto4D).
+            deadtraces (bool): If True (default) then dead cube traces
+                (given as value 2 in SEGY trace headers), are treated as
+                undefined, and map will become undefined at dead trace location.
+            algorithm (int): 1 for legacy method, 2 (default from 2.9) for
+                new method available in xtgeo from version 2.9
+
+        Example::
+
+            >>> import xtgeo
+            >>> cube = xtgeo.cube_from_file(cube_dir + "/ib_test_cube2.segy")
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> # update surf to sample cube values:
+            >>> surf.slice_cube(cube)
+
+        Raises:
+            Exception if maps have different definitions (topology)
+            RuntimeWarning if number of sampled nodes is less than 10%
+
+        .. versionchanged:: 2.9 Added ``algorithm`` keyword, default is 2
+        """
+        ier = _regsurf_cube.slice_cube(
+            self,
+            cube,
+            zsurf=zsurf,
+            sampling=sampling,
+            mask=mask,
+            snapxy=snapxy,
+            deadtraces=deadtraces,
+            algorithm=algorithm,
+        )
+
+        if ier == -4:
+            xtg.warnuser("Number of sampled surface nodes < 10 percent of Cube nodes")
+            print("Number of sampled surface nodes < 10 percent of Cube nodes")
+        elif ier == -5:
+            xtg.warn("No nodes sampled: map is 100 percent outside of cube?")
+
+    def slice_cube_window(
+        self,
+        cube,
+        zsurf=None,
+        other=None,
+        other_position="below",
+        sampling="nearest",
+        mask=True,
+        zrange=None,
+        ndiv=None,
+        attribute="max",
+        maskthreshold=0.1,
+        snapxy=False,
+        showprogress=False,
+        deadtraces=True,
+        algorithm=2,
+    ):
+        """Slice the cube within a vertical window and get the statistical attrubutes.
+
+        The statistical attributes can be min, max etc. Attributes are:
+
+        * 'max' for maximum
+
+        * 'min' for minimum
+
+        * 'rms' for root mean square
+
+        * 'mean' for expected value
+
+        * 'var' for variance (population var; https://en.wikipedia.org/wiki/Variance)
+
+        * 'maxpos' for maximum of positive values
+
+        * 'maxneg' for negative maximum of negative values
+
+        * 'maxabs' for maximum of absolute values
+
+        * 'sumpos' for sum of positive values using cube sampling resolution
+
+        * 'sumneg' for sum of negative values using cube sampling resolution
+
+        * 'meanabs' for mean of absolute values
+
+        * 'meanpos' for mean of positive values
+
+        * 'meanneg' for mean of negative values
+
+        Note that 'all' can be used to select all attributes that are currently
+        available.
+
+        Args:
+            cube (Cube): Instance of a Cube()
+            zsurf (RegularSurface): Instance of a depth (or time) map, which
+                is the depth or time map (or...) that is used a slicer.
+                If None, then the surface instance itself is used a slice
+                criteria. Note that zsurf must have same map defs as the
+                surface instance.
+            other (RegularSurface): Instance of other surface if window is
+                between surfaces instead of a static window. The zrange
+                input is then not applied.
+            sampling (str): 'nearest'/'trilinear'/'cube' for nearest node (default),
+                 or 'trilinear' for trilinear interpolation. The 'cube' option is
+                 only available with algorithm = 2 and will overrule ndiv and sample
+                 at the cube's Z increment resolution.
+            mask (bool): If True (default), then the map values outside
+                the cube will be undef, otherwise map will be kept as-is
+            zrange (float): The one-sided "radius" range of the window, e.g. 10
+                (10 is default) units (e.g. meters if in depth mode).
+                The full window is +- zrange (i.e. diameter).
+                If other surface is present, zrange is computed based on that.
+            ndiv (int): Number of intervals for sampling within zrange. None
+                means 'auto' sampling, using 0.5 of cube Z increment as basis. If
+                algorithm = 2 and sampling is 'cube', the cube Z increment
+                will be used.
+            attribute (str or list): The requested attribute(s), e.g.
+                'max' value. May also be a list of attributes, e.g.
+                ['min', 'rms', 'max']. By such, a dict of surface objects is
+                returned. Note 'all' will make a list of possible attributes
+            maskthreshold (float): Only if two surface; if isochore is less
+                than given value, the result will be masked.
+            snapxy (bool): If True (optional), then the map values will get
+                values at nearest Cube XY location. Only relevant to use if
+                surface is derived from seismic coordinates (e.g. Auto4D).
+            showprogress (bool): If True, then a progress is printed to stdout.
+            deadtraces (bool): If True (default) then dead cube traces
+                (given as value 2 in SEGY trace headers), are treated as
+                undefined, nad map will be undefined at dead trace location.
+            algorithm (int): 1 for legacy method, 2 (default) for new faster
+                and more precise method available from xtgeo version 2.9.
+
+        Example::
+
+            >>> import xtgeo
+            >>> cube = xtgeo.Cube(cube_dir + "/ib_test_cube2.segy")
+            >>> surf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> # update surf to sample cube values in a total range of 30 m:
+            >>> surf.slice_cube_window(cube, attribute='min', zrange=15.0)
+
+            >>> # Here a list is given instead:
+            >>> alst = ['min', 'max', 'rms']
+
+            >>> myattrs = surf.slice_cube_window(cube, attribute=alst, zrange=15.0)
+            >>> for attr in myattrs.keys():
+            ...     _ = myattrs[attr].to_file(
+            ...         outdir + '/myfile_' + attr + '.gri'
+            ...     )
+
+        Raises:
+            Exception if maps have different definitions (topology)
+            ValueError if attribute is invalid.
+
+        Returns:
+            If `attribute` is a string, then the instance is updated and
+            None is returned. If `attribute` is a list, then a dictionary
+            of surface objects is returned.
+
+        .. versionchanged:: 2.9 Added ``algorithm`` keyword, default is now 2,
+                            while 1 is the legacy version
+        """
+        if other is None and zrange is None:
+            zrange = 10
+
+        asurfs = _regsurf_cube_window.slice_cube_window(
+            self,
+            cube,
+            zsurf=zsurf,
+            other=other,
+            other_position=other_position,
+            sampling=sampling,
+            mask=mask,
+            zrange=zrange,
+            ndiv=ndiv,
+            attribute=attribute,
+            maskthreshold=maskthreshold,
+            snapxy=snapxy,
+            showprogress=showprogress,
+            deadtraces=deadtraces,
+            algorithm=algorithm,
+        )
+
+        return asurfs
+
+    # ==================================================================================
+    # Special methods
+    # ==================================================================================
+
+    def get_boundary_polygons(
+        self,
+        alpha_factor: Optional[float] = 1.0,
+        convex: Optional[bool] = False,
+        simplify: Optional[bool] = True,
+    ):
+        """Extract boundary polygons from the surface.
+
+        A regular surface may often contain areas of undefined (masked) entries which
+        makes the surface appear 'ragged' and/or 'patchy'.
+
+        This method extracts boundaries around the surface patches, and the
+        precision depends on the keyword settings. As default, the ``alpha_factor``
+        of 1 makes a precise boundary, while a larger alpha_factor makes more rough
+        polygons.
+
+        .. image:: images/regsurf_boundary_polygons.png
+           :width: 600
+           :align: center
+
+        |
+
+        Args:
+            alpha_factor: An alpha multiplier, where lowest allowed value is 1.0.
+                A higher number will produce smoother and less accurate polygons. Not
+                applied if convex is set to True.
+            convex: The default is False, which means that a "concave hull" algorithm
+                is used. If convex is True, the alpha factor is overridden to a large
+                number, producing a 'convex' shape boundary instead.
+            simplify: If True, a simplification is done in order to reduce the number
+                of points in the polygons, where tolerance is 0.1. Another
+                alternative to True is to input a Dict on the form
+                ``{"tolerance": 2.0, "preserve_topology": True}``, cf. the
+                :func:`Polygons.simplify()` method. For details on e.g. tolerance, see
+                Shapely's simplify() method.
+
+        Returns:
+            A XTGeo Polygons instance
+
+        Example::
+
+            surf = xtgeo.surface_from_file("mytop.gri")
+            # eliminate all values below a depth, e.g. a fluid contact
+            surf.values = np.ma.masked_greater(surf.values, 2100.0)
+            boundary = surf.get_boundary_polygons()
+            # the boundary may contain several smaller polygons; keep only the
+            # largest (first) polygon which is number 0:
+            boundary.filter_byid([0])  # polygon is updated in-place
+
+        See also:
+            The :func:`Polygons.boundary_from_points()` class method.
+
+        .. versionadded:: 3.1
+        """
+        return _regsurf_boundary.create_boundary(self, alpha_factor, convex, simplify)
+
+    def get_fence(
+        self, xyfence: np.ndarray, sampling: Optional[str] = "bilinear"
+    ) -> np.ma.MaskedArray:
+        """Sample the surface along X and Y positions (numpy arrays) and get Z.
+
+        .. versionchanged:: 2.14 Added keyword option `sampling`
+
+        Returns a masked numpy 2D array similar as input, but with updated
+        Z values, which are masked if undefined.
+
+        Args:
+            xyfence: A 2D numpy array with shape (N, 3) where columns
+                are (X, Y, Z). The Z will be updated to the map.
+            sampling: Use "bilinear" (default) for interpolation or "nearest" for
+                snapping to nearest node.
+
+        """
+        xyfence = _regsurf_oper.get_fence(self, xyfence, sampling=sampling)
+
+        return xyfence
+
+    def get_randomline(
+        self,
+        fencespec: Union[np.ndarray, object],
+        hincrement: Optional[Union[bool, float]] = None,
+        atleast: Optional[int] = 5,
+        nextend: Optional[int] = 2,
+        sampling: Optional[str] = "bilinear",
+    ) -> np.ndarray:
+        """Extract a line along a fencespec.
+
+        .. versionadded:: 2.1
+        .. versionchanged:: 2.14 Added keyword option `sampling`
+
+        Here, horizontal axis is "length" and vertical axis is sampled depth, and
+        this is used for fence plots.
+
+        The input fencespec is either a 2D numpy where each row is X, Y, Z, HLEN,
+        where X, Y are UTM coordinates, Z is depth/time, and HLEN is a
+        length along the fence, or a Polygons instance.
+
+        If input fencspec is a numpy 2D, it is important that the HLEN array
+        has a constant increment and ideally a sampling that is less than the
+        map resolution. If a Polygons() instance, this is automated if hincrement is
+        None, and ignored if hincrement is False.
+
+        Returns a ndarray with shape (:, 2).
+
+        Args:
+            fencespec:
+                2D numpy with X, Y, Z, HLEN as rows or a xtgeo Polygons() object.
+            hincrement: Resampling horizontally. This applies only
+                if the fencespec is a Polygons() instance. If None (default),
+                the distance will be deduced automatically. If False, then it assumes
+                the Polygons can be used as-is.
+            atleast: Minimum number of horizontal samples (only if
+                fencespec is a Polygons instance and hincrement != False)
+            nextend: Extend with nextend * hincrement in both ends (only if
+                fencespec is a Polygons instance and hincrement != False)
+            sampling: Use "bilinear" (default) for interpolation or "nearest" for
+                snapping to nearest node.
+
+
+        Example::
+
+            fence = xtgeo.Polygons("somefile.pol")
+            fspec = fence.get_fence(distance=20, nextend=5, asnumpy=True)
+            surf = xtgeo.RegularSurface("somefile.gri")
+
+            arr = surf.get_randomline(fspec)
+
+            distance = arr[:, 0]
+            zval = arr[:, 1]
+            # matplotlib...
+            plt.plot(distance, zval)
+
+        .. seealso::
+           Class :class:`~xtgeo.xyz.polygons.Polygons`
+              The method :meth:`~xtgeo.xyz.polygons.Polygons.get_fence()` which can be
+              used to pregenerate `fencespec`
+        """
+        xyfence = _regsurf_oper.get_randomline(
+            self,
+            fencespec,
+            hincrement=hincrement,
+            atleast=atleast,
+            nextend=nextend,
+            sampling=sampling,
+        )
+
+        return xyfence
+
+    def hc_thickness_from_3dprops(
+        self,
+        xprop=None,
+        yprop=None,
+        hcpfzprop=None,
+        zoneprop=None,
+        zone_minmax=None,
+        dzprop=None,
+        zone_avg=False,
+        coarsen=1,
+        mask_outside=False,
+    ):
+        """Make a thickness weighted HC thickness map.
+
+        Make a HC thickness map based on numpy arrays of properties
+        from a 3D grid. The numpy arrays here shall be ndarray,
+        not masked numpies (MaskedArray).
+
+        Note that the input hcpfzprop is hydrocarbon fraction multiplied
+        with thickness, which can be achieved by e.g.:
+        cpfz = dz*poro*ntg*shc or by hcpfz = dz*hcpv/vbulk
+
+        Args:
+            xprop (ndarray): 3D numpy array of X coordinates
+            yprop (ndarray): 3D numpy array of Y coordinates
+            hcpfzprop (ndarray): 3D numpy array of HC fraction multiplied
+                with DZ per cell.
+            zoneprop (ndarray): 3D numpy array indicating zonation
+                property, where 1 is the lowest (0 values can be used to
+                exclude parts of the grid)
+            dzprop (ndarray): 3D numpy array holding DZ thickness. Will
+                be applied in weighting if zone_avg is active.
+            zone_minmax (tuple): (optional) 2 element list indicating start
+                and stop zonation (both start and end spec are included)
+            zone_avg (bool): A zone averaging is done prior to map gridding.
+                This may speed up the process a lot, but result will be less
+                precise. Default is False.
+            coarsen (int): Select every N'th X Y point in the gridding. Will
+                speed up process, but less precise result. Default=1
+            mask_outside (bool): Will mask the result map undef where sum of DZ
+                is zero. Default is False as it costs some extra CPU.
+
+        Returns:
+            True if operation went OK (but check result!), False if not
+        """
+        for inum, myprop in enumerate([xprop, yprop, hcpfzprop, zoneprop]):
+            if isinstance(myprop, ma.MaskedArray):
+                raise ValueError(
+                    f"Property input {inum} with avg {myprop.mean()} to {__name__} "
+                    "is a masked array, not a plain numpy ndarray"
+                )
+
+        status = _regsurf_gridding.avgsum_from_3dprops_gridding(
+            self,
+            summing=True,
+            xprop=xprop,
+            yprop=yprop,
+            mprop=hcpfzprop,
+            dzprop=dzprop,
+            zoneprop=zoneprop,
+            zone_minmax=zone_minmax,
+            zone_avg=zone_avg,
+            coarsen=coarsen,
+            mask_outside=mask_outside,
+        )
+
+        if status is False:
+            raise RuntimeError("Failure from hc thickness calculation")
+
+    def avg_from_3dprop(
+        self,
+        xprop=None,
+        yprop=None,
+        mprop=None,
+        dzprop=None,
+        truncate_le=None,
+        zoneprop=None,
+        zone_minmax=None,
+        coarsen=1,
+        zone_avg=False,
+    ):
+        """Average map (DZ weighted) based on numpy arrays of properties from a 3D grid.
+
+        The 3D arrays mush be ordinary numpies of size (nx,ny,nz). Undef
+        entries must be given weights 0 by using DZ=0
+
+        Args:
+            xprop: 3D numpy of all X coordinates (also inactive cells)
+            yprop: 3D numpy of all Y coordinates (also inactive cells)
+            mprop: 3D numpy of requested property (e.g. porosity) all
+            dzprop: 3D numpy of dz values (for weighting)
+                NB zero for undef cells
+            truncate_le (float): Optional. Truncate value (mask) if
+                value is less
+            zoneprop: 3D numpy to a zone property
+            zone_minmax: a tuple with from-to zones to combine
+                (e.g. (1,3))
+
+        Returns:
+            Nothing explicit, but updates the surface object.
+        """
+        for inum, myprop in enumerate([xprop, yprop, mprop, dzprop, zoneprop]):
+            if isinstance(myprop, ma.MaskedArray):
+                raise ValueError(
+                    f"Property input {inum} with avg {myprop.mean()} to {__name__} "
+                    "is a masked array, not a plain numpy ndarray"
+                )
+
+        _regsurf_gridding.avgsum_from_3dprops_gridding(
+            self,
+            summing=False,
+            xprop=xprop,
+            yprop=yprop,
+            mprop=mprop,
+            dzprop=dzprop,
+            truncate_le=truncate_le,
+            zoneprop=zoneprop,
+            zone_minmax=zone_minmax,
+            coarsen=coarsen,
+            zone_avg=zone_avg,
+        )
+
+    def quickplot(
+        self,
+        filename=None,
+        title="QuickPlot for Surfaces",
+        subtitle=None,
+        infotext=None,
+        minmax=(None, None),
+        xlabelrotation=None,
+        colormap="rainbow",
+        colortable=None,
+        faults=None,
+        logarithmic=False,
+    ):
+        """Fast surface plot of maps using matplotlib.
+
+        Args:
+            filename (str): Name of plot file; None will plot to screen.
+            title (str): Title of plot
+            subtitle (str): Subtitle of plot
+            infotext (str): Additonal info on plot.
+            minmax (tuple): Tuple of min and max values to be plotted. Note
+                that values outside range will be set equal to range limits
+            xlabelrotation (float): Rotation in degrees of X labels.
+            colormap (str): Name of matplotlib or RMS file or XTGeo
+                colormap. Default is matplotlib's 'rainbow'
+            colortable (str): Deprecated, for backward compatibility! used
+                colormap instead.
+            faults (dict): If fault plot is wanted, a dictionary on the
+                form => {'faults': XTGeo Polygons object, 'color': 'k'}
+            logarithmic (bool): If True, a logarithmic contouring color scale
+                will be used.
+
+        """
+        # This is using the more versatile Map class in XTGeo. Most kwargs
+        # is just passed as is. Prefer using Map() directly in apps?
+
+        ncount = self.values.count()
+        if ncount < 5:
+            xtg.warn(f"None or too few map nodes for plotting. Skip output {filename}!")
+            return
+
+        mymap = xtgeo.plot.Map()
+
+        logger.info("Infotext is <%s>", infotext)
+        mymap.canvas(title=title, subtitle=subtitle, infotext=infotext)
+
+        minvalue = minmax[0]
+        maxvalue = minmax[1]
+
+        if colortable is not None:
+            xtg.warndeprecated(
+                "The colortable parameter is deprecated,"
+                "and will be removed in version 4.0. Use colormap instead."
+            )
+            colormap = colortable
+
+        mymap.colormap = colormap
+
+        mymap.plot_surface(
+            self,
+            minvalue=minvalue,
+            maxvalue=maxvalue,
+            xlabelrotation=xlabelrotation,
+            logarithmic=logarithmic,
+        )
+        if faults:
+            poly = faults.pop("faults")
+            mymap.plot_faults(poly, **faults)
+
+        if filename is None:
+            mymap.show()
+        else:
+            mymap.savefig(filename)
+
+    def distance_from_point(self, point=(0, 0), azimuth=0.0):
+        """Make map values as horizontal distance from a point with azimuth direction.
+
+        Args:
+            point (tuple): Point to measure from
+            azimuth (float): Angle from North (clockwise) in degrees
+
+        """
+        _regsurf_oper.distance_from_point(self, point=point, azimuth=azimuth)
+
+    def translate_coordinates(self, translate=(0, 0, 0)):
+        """Translate a map in X Y VALUE space.
+
+        Args:
+            translate (tuple): Translate (shift) distance in X Y Z
+
+        Example::
+
+            >>> import xtgeo
+            >>> mysurf = xtgeo.surface_from_file(surface_dir + '/topreek_rota.gri')
+            >>> print(mysurf.xori, mysurf.yori)
+            468895.125 5932889.5
+            >>> mysurf.translate_coordinates((300,500,0))
+            >>> print(mysurf.xori, mysurf.yori)
+            469195.125 5933389.5
+
+        """
+        xshift, yshift, zshift = translate
+
+        # just shift the xori and yori
+        self.xori = self.xori + xshift
+        self.yori = self.yori + yshift
+
+        # note the Z coordinates are perhaps not depth
+        # numpy operation:
+        self.values = self.values + zshift
+
+    # ==================================================================================
+    # Private
+    # ==================================================================================
+
+    def _ensure_correct_values(
+        self, values
+    ):  # pylint: disable=too-many-branches, too-many-statements
+        """Ensures that values is a 2D masked numpy (ncol, nrow), C order.
+
+        This is an improved but private version over ensure_correct_values
+
+        Args:
+            values (array-like or scalar): Values to process.
+
+        Return:
+            Nothing, self._values will be updated inplace
+
+        """
+        currentmask = None
+        if isinstance(self.values, ma.MaskedArray):
+            currentmask = ma.getmaskarray(self.values)
+
+        if isinstance(values, ma.MaskedArray):
+            newmask = ma.getmaskarray(values)
+            vals = values.astype(np.float64)
+            vals = ma.masked_greater(vals, self.undef_limit)
+            vals = ma.masked_invalid(vals)
+            if (
+                currentmask is not None
+                and np.array_equal(currentmask, newmask)
+                and self.values.shape == values.shape
+                and values.flags.c_contiguous is True
+            ):
+                self._values *= 0
+                self._values += vals
+            else:
+                vals = vals.reshape((self._ncol, self._nrow))
+                if not vals.flags.c_contiguous:
+                    mask = ma.getmaskarray(values)
+                    mask = np.asanyarray(mask, order="C")
+                    vals = np.asanyarray(vals, order="C")
+                    vals = ma.array(vals, mask=mask, order="C")
+                self._values = vals
+
+        elif isinstance(values, numbers.Number):
+            if currentmask is not None:
+                vals = np.ones(self.dimensions, dtype=np.float64) * values
+                vals = np.ma.array(vals, mask=currentmask)
+
+                # there maybe cases where values scalar input is some kind of UNDEF
+                # which will change the mask
+                vals = ma.masked_greater(vals, self.undef_limit, copy=False)
+                vals = ma.masked_invalid(vals, copy=False)
+                self._values *= 0
+                self._values += vals
+            else:
+                vals = ma.zeros((self.ncol, self.nrow), order="C", dtype=np.float64)
+                self._values = vals + float(values)
+
+        elif isinstance(values, (list, tuple, np.ndarray)):  # ie values ~ list-like
+            vals = ma.array(values, order="C", dtype=np.float64)
+            vals = ma.masked_greater(vals, self.undef_limit, copy=True)
+            vals = ma.masked_invalid(vals, copy=True)
+
+            if vals.shape != (self.ncol, self.nrow):
+                try:
+                    vals = ma.reshape(vals, (self.ncol, self.nrow), order="C")
+                except ValueError as emsg:
+                    raise ValueError(f"Cannot reshape array: {values}") from emsg
+
+            self._values = vals
+
+        elif isinstance(values, (list, tuple)):  # ie values ~ list-like
+            vals = ma.array(values, order="C", dtype=np.float64)
+            vals = ma.masked_greater(vals, self.undef_limit, copy=True)
+            vals = ma.masked_invalid(vals, copy=True)
+
+            if vals.shape != (self.ncol, self.nrow):
+                try:
+                    vals = ma.reshape(vals, (self.ncol, self.nrow), order="C")
+                except ValueError as emsg:
+                    raise ValueError(f"Cannot reshape array: {values}") from emsg
+
+            self._values = vals
+
+        else:
+            raise ValueError(f"Input values are in invalid format: {values}")
+
+        if self._values.mask is ma.nomask:
+            self._values = ma.array(self._values, mask=ma.getmaskarray(self._values))
```

## xtgeo/surface/surfaces.py

 * *Ordering differences only*

```diff
@@ -1,233 +1,233 @@
-# -*- coding: utf-8 -*-
-
-"""The surfaces module, which has the Surfaces class (collection of surface objects)."""
-from xtgeo import RegularSurface
-
-try:
-    from typing import Literal
-except ImportError:
-    from typing_extensions import Literal
-from typing import Optional, List
-import warnings
-import deprecation
-import numpy as np
-
-import xtgeo
-from . import _surfs_import
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def surfaces_from_grid(grid, subgrids=True, rfactor=1):
-    surf, subtype, order = _surfs_import.from_grid3d(grid, subgrids, rfactor)
-    return Surfaces(surfaces=surf, subtype=subtype, order=order)
-
-
-class Surfaces:
-    """Class for a collection of Surface objects, for operations that involves
-    a number of surfaces, such as statistical numbers.
-
-    A collection of surfaces can be different things:
-
-    * A list if surfaces in stratigraphic order
-    * A collection of different realisations of the same surface
-    * A collection of isochores
-
-    Args:
-        input (list, optional): A list of XTGeo objects and/or file names)
-        subtype (str): "tops", "isochores", or None (default)
-        order (str): Assummed order: "same", "stratigraphic", None(default)
-
-    .. seealso::
-       Class :class:`~xtgeo.surface.regular_surface.RegularSurface` class.
-
-    .. versionadded:: 2.1
-    """
-
-    def __init__(
-        self,
-        surfaces: Optional[List[RegularSurface]] = None,
-        subtype: Optional[Literal["tops", "isochores"]] = None,
-        order: Optional[Literal["same", "stratigraphic"]] = None,
-    ):
-        self._surfaces = []
-        if surfaces is not None:
-            self.append(surfaces)
-        self._subtype = subtype
-        self._order = order
-
-    @property
-    def surfaces(self):
-        """Get or set a list of individual surfaces"""
-        return self._surfaces
-
-    @surfaces.setter
-    def surfaces(self, slist):
-        if not isinstance(slist, list):
-            raise ValueError("Input not a list")
-
-        for elem in slist:
-            if not isinstance(elem, xtgeo.RegularSurface):
-                raise ValueError("Element in list not a valid type of Surface")
-
-        self._surfaces = slist
-
-    def append(self, slist):
-        """Append surfaces from either a list of RegularSurface objects,
-        a list of files, or a mix."""
-        for item in slist:
-            if isinstance(item, xtgeo.RegularSurface):
-                self.surfaces.append(item)
-            else:
-                try:
-                    sobj = xtgeo.surface_from_file(item, fformat="guess")
-                    self.surfaces.append(sobj)
-                except OSError:
-                    xtg.warnuser(f"Cannot read as file, skip: {item}")
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout"""
-
-        dsc = xtgeo.common.XTGDescription()
-        dsc.title(f"Description of {self.__class__.__name__} instance")
-        dsc.txt("Object ID", id(self))
-
-        for inum, surf in enumerate(self.surfaces):
-            dsc.txt("Surface:", inum, surf.name)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    def copy(self):
-        """Copy a Surfaces instance to a new unique instance (a deep copy)."""
-
-        new = Surfaces()
-
-        for surf in self._surfaces:
-            newsurf = surf.copy()
-            new._surfaces.append(newsurf)
-
-        new._order = self._order
-        new._subtype = self._subtype
-
-        return new
-
-    def get_surface(self, name):
-        """Get a RegularSurface() instance by name, or return None if name not found"""
-
-        logger.info("Asking for a surface with name %s", name)
-        for surf in self._surfaces:
-            if surf.name == name:
-                return surf
-        return None
-
-    @deprecation.deprecated(
-        deprecated_in="2.15",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.surface.surfaces_from_grid() instead",
-    )
-    def from_grid3d(self, grid, subgrids=True, rfactor=1):
-        """Derive surfaces from a 3D grid"""
-        self.surfaces, self._subtype, self._order = _surfs_import.from_grid3d(
-            grid, subgrids, rfactor
-        )
-
-    def apply(self, func, *args, **kwargs):
-        """Apply a function to the Surfaces array.
-
-        The return value of the function (numpy nan comptatible) will be a
-        numpy array of the same shape as the first surface.
-
-        E.g. surfs.apply(np.nanmean, axis=0) will return the mean surface.
-
-        Args:
-            func: Function to apply, e.g. np.nanmean
-            args: The function arguments
-            kwargs: The function keyword arguments
-
-        Raises:
-            ValueError: If surfaces differ in topology.
-
-        """
-        template = self.surfaces[0].copy()
-        slist = []
-        for surf in self.surfaces:
-            status = template.compare_topology(surf, strict=False)
-            if not status:
-                raise ValueError("Cannot do statistics, surfaces differ in topology")
-            slist.append(np.ma.filled(surf.values, fill_value=np.nan))
-
-        xlist = np.array(slist)
-
-        with warnings.catch_warnings():
-            warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
-            template.values = func(xlist, *args, **kwargs)
-
-        return template
-
-    def statistics(self, percentiles=None):
-        """Return statistical measures from the surfaces.
-
-        The statistics returned is:
-        * mean: the arithmetic mean surface
-        * std: the standard deviation surface (where ddof = 1)
-        * percentiles: on demand (such operations may be slow)
-
-        Currently this function expects that the surfaces all have the same
-        shape/topology.
-
-        Args:
-            percentiles (list of float): If defined, a list of perecentiles to evaluate
-                e.g. [10, 50, 90] for p10, p50, p90
-
-        Returns:
-            dict: A dictionary of statistical measures, see list above
-
-        Raises:
-            ValueError: If surfaces differ in topology.
-
-        Example::
-
-            surfs = Surfaces(mylist)  # mylist is a collection of files
-            stats = surfs.statistics()
-            # export the mean surface
-            stats["mean"].to_file("mymean.gri")
-
-        .. versionchanged:: 2.13 Added `percentile`
-        """
-        result = {}
-
-        template = self.surfaces[0].copy()
-
-        slist = []
-        for surf in self.surfaces:
-            status = template.compare_topology(surf, strict=False)
-            if not status:
-                raise ValueError("Cannot do statistics, surfaces differ in topology")
-            slist.append(np.ma.filled(surf.values, fill_value=np.nan).ravel())
-
-        xlist = np.array(slist)
-
-        template.values = np.ma.masked_invalid(xlist).mean(axis=0)
-        result["mean"] = template.copy()
-        template.values = np.ma.masked_invalid(xlist).std(axis=0, ddof=1)
-        result["std"] = template.copy()
-
-        if percentiles is not None:
-            # nan on a axis tends to give warnings that are not a worry; suppress:
-            with warnings.catch_warnings():
-                warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
-                res = np.nanpercentile(xlist, percentiles, axis=0)
-
-            for slice, prc in enumerate(percentiles):
-                template.values = res[slice, :]
-                result["p" + str(prc)] = template.copy()
-                if prc == 50:
-                    result["median"] = result["p50"]
-
-        return result
+# -*- coding: utf-8 -*-
+
+"""The surfaces module, which has the Surfaces class (collection of surface objects)."""
+from xtgeo import RegularSurface
+
+try:
+    from typing import Literal
+except ImportError:
+    from typing_extensions import Literal
+from typing import Optional, List
+import warnings
+import deprecation
+import numpy as np
+
+import xtgeo
+from . import _surfs_import
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def surfaces_from_grid(grid, subgrids=True, rfactor=1):
+    surf, subtype, order = _surfs_import.from_grid3d(grid, subgrids, rfactor)
+    return Surfaces(surfaces=surf, subtype=subtype, order=order)
+
+
+class Surfaces:
+    """Class for a collection of Surface objects, for operations that involves
+    a number of surfaces, such as statistical numbers.
+
+    A collection of surfaces can be different things:
+
+    * A list if surfaces in stratigraphic order
+    * A collection of different realisations of the same surface
+    * A collection of isochores
+
+    Args:
+        input (list, optional): A list of XTGeo objects and/or file names)
+        subtype (str): "tops", "isochores", or None (default)
+        order (str): Assummed order: "same", "stratigraphic", None(default)
+
+    .. seealso::
+       Class :class:`~xtgeo.surface.regular_surface.RegularSurface` class.
+
+    .. versionadded:: 2.1
+    """
+
+    def __init__(
+        self,
+        surfaces: Optional[List[RegularSurface]] = None,
+        subtype: Optional[Literal["tops", "isochores"]] = None,
+        order: Optional[Literal["same", "stratigraphic"]] = None,
+    ):
+        self._surfaces = []
+        if surfaces is not None:
+            self.append(surfaces)
+        self._subtype = subtype
+        self._order = order
+
+    @property
+    def surfaces(self):
+        """Get or set a list of individual surfaces"""
+        return self._surfaces
+
+    @surfaces.setter
+    def surfaces(self, slist):
+        if not isinstance(slist, list):
+            raise ValueError("Input not a list")
+
+        for elem in slist:
+            if not isinstance(elem, xtgeo.RegularSurface):
+                raise ValueError("Element in list not a valid type of Surface")
+
+        self._surfaces = slist
+
+    def append(self, slist):
+        """Append surfaces from either a list of RegularSurface objects,
+        a list of files, or a mix."""
+        for item in slist:
+            if isinstance(item, xtgeo.RegularSurface):
+                self.surfaces.append(item)
+            else:
+                try:
+                    sobj = xtgeo.surface_from_file(item, fformat="guess")
+                    self.surfaces.append(sobj)
+                except OSError:
+                    xtg.warnuser(f"Cannot read as file, skip: {item}")
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout"""
+
+        dsc = xtgeo.common.XTGDescription()
+        dsc.title(f"Description of {self.__class__.__name__} instance")
+        dsc.txt("Object ID", id(self))
+
+        for inum, surf in enumerate(self.surfaces):
+            dsc.txt("Surface:", inum, surf.name)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    def copy(self):
+        """Copy a Surfaces instance to a new unique instance (a deep copy)."""
+
+        new = Surfaces()
+
+        for surf in self._surfaces:
+            newsurf = surf.copy()
+            new._surfaces.append(newsurf)
+
+        new._order = self._order
+        new._subtype = self._subtype
+
+        return new
+
+    def get_surface(self, name):
+        """Get a RegularSurface() instance by name, or return None if name not found"""
+
+        logger.info("Asking for a surface with name %s", name)
+        for surf in self._surfaces:
+            if surf.name == name:
+                return surf
+        return None
+
+    @deprecation.deprecated(
+        deprecated_in="2.15",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.surface.surfaces_from_grid() instead",
+    )
+    def from_grid3d(self, grid, subgrids=True, rfactor=1):
+        """Derive surfaces from a 3D grid"""
+        self.surfaces, self._subtype, self._order = _surfs_import.from_grid3d(
+            grid, subgrids, rfactor
+        )
+
+    def apply(self, func, *args, **kwargs):
+        """Apply a function to the Surfaces array.
+
+        The return value of the function (numpy nan comptatible) will be a
+        numpy array of the same shape as the first surface.
+
+        E.g. surfs.apply(np.nanmean, axis=0) will return the mean surface.
+
+        Args:
+            func: Function to apply, e.g. np.nanmean
+            args: The function arguments
+            kwargs: The function keyword arguments
+
+        Raises:
+            ValueError: If surfaces differ in topology.
+
+        """
+        template = self.surfaces[0].copy()
+        slist = []
+        for surf in self.surfaces:
+            status = template.compare_topology(surf, strict=False)
+            if not status:
+                raise ValueError("Cannot do statistics, surfaces differ in topology")
+            slist.append(np.ma.filled(surf.values, fill_value=np.nan))
+
+        xlist = np.array(slist)
+
+        with warnings.catch_warnings():
+            warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
+            template.values = func(xlist, *args, **kwargs)
+
+        return template
+
+    def statistics(self, percentiles=None):
+        """Return statistical measures from the surfaces.
+
+        The statistics returned is:
+        * mean: the arithmetic mean surface
+        * std: the standard deviation surface (where ddof = 1)
+        * percentiles: on demand (such operations may be slow)
+
+        Currently this function expects that the surfaces all have the same
+        shape/topology.
+
+        Args:
+            percentiles (list of float): If defined, a list of perecentiles to evaluate
+                e.g. [10, 50, 90] for p10, p50, p90
+
+        Returns:
+            dict: A dictionary of statistical measures, see list above
+
+        Raises:
+            ValueError: If surfaces differ in topology.
+
+        Example::
+
+            surfs = Surfaces(mylist)  # mylist is a collection of files
+            stats = surfs.statistics()
+            # export the mean surface
+            stats["mean"].to_file("mymean.gri")
+
+        .. versionchanged:: 2.13 Added `percentile`
+        """
+        result = {}
+
+        template = self.surfaces[0].copy()
+
+        slist = []
+        for surf in self.surfaces:
+            status = template.compare_topology(surf, strict=False)
+            if not status:
+                raise ValueError("Cannot do statistics, surfaces differ in topology")
+            slist.append(np.ma.filled(surf.values, fill_value=np.nan).ravel())
+
+        xlist = np.array(slist)
+
+        template.values = np.ma.masked_invalid(xlist).mean(axis=0)
+        result["mean"] = template.copy()
+        template.values = np.ma.masked_invalid(xlist).std(axis=0, ddof=1)
+        result["std"] = template.copy()
+
+        if percentiles is not None:
+            # nan on a axis tends to give warnings that are not a worry; suppress:
+            with warnings.catch_warnings():
+                warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
+                res = np.nanpercentile(xlist, percentiles, axis=0)
+
+            for slice, prc in enumerate(percentiles):
+                template.values = res[slice, :]
+                result["p" + str(prc)] = template.copy()
+                if prc == 50:
+                    result["median"] = result["p50"]
+
+        return result
```

## xtgeo/well/__init__.py

 * *Ordering differences only*

```diff
@@ -1,10 +1,10 @@
-# -*- coding: utf-8 -*-
-"""The XTGeo well package"""
-
-
-# flake8: noqa
-from .well1 import Well
-from .wells import Wells
-
-from .blocked_well import BlockedWell
-from .blocked_wells import BlockedWells
+# -*- coding: utf-8 -*-
+"""The XTGeo well package"""
+
+
+# flake8: noqa
+from .well1 import Well
+from .wells import Wells
+
+from .blocked_well import BlockedWell
+from .blocked_wells import BlockedWells
```

## xtgeo/well/_blockedwell_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,205 +1,205 @@
-# -*- coding: utf-8 -*-
-"""Well input and output, private module for ROXAPI"""
-
-
-from collections import OrderedDict
-
-import numpy as np
-import numpy.ma as npma
-import pandas as pd
-
-from xtgeo.common import XTGeoDialog
-from xtgeo.roxutils import RoxUtils
-from xtgeo.common.exceptions import WellNotFoundError
-
-try:
-    import roxar
-except ImportError:
-    pass
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-# Import / export via ROX api
-
-
-def import_bwell_roxapi(
-    self, project, gname, bwname, wname, lognames=None, ijk=True, realisation=0
-):
-    """Private function for loading project and ROXAPI blockwell import"""
-
-    logger.info("Opening RMS project ...")
-    rox = RoxUtils(project, readonly=True)
-
-    _roxapi_import_bwell(self, rox, gname, bwname, wname, lognames, ijk, realisation)
-
-    rox.safe_close()
-
-
-def export_bwell_roxapi(
-    self, project, gname, bwname, wname, lognames="all", ijk=False, realisation=0
-):
-    """Private function for blockwell export (store in RMS) from XTGeo to RoxarAPI"""
-
-    logger.info("Opening RMS project ...")
-    rox = RoxUtils(project, readonly=False)
-
-    _roxapi_export_bwell(self, rox, gname, bwname, wname, lognames, ijk, realisation)
-
-    if rox._roxexternal:
-        rox.project.save()
-
-    rox.safe_close()
-
-
-def _roxapi_import_bwell(
-    self, rox, gname, bwname, wname, lognames, ijk, realisation
-):  # pylint: disable=too-many-statements
-    """Private function for ROXAPI well import (get well from Roxar)"""
-
-    if gname in rox.project.grid_models:
-        gmodel = rox.project.grid_models[gname]
-        logger.info("RMS grid model <%s> OK", gname)
-    else:
-        raise ValueError(f"No such grid name present: {gname}")
-
-    if bwname in gmodel.blocked_wells_set:
-        bwset = gmodel.blocked_wells_set[bwname]
-        logger.info("Blocked well set <%s> OK", bwname)
-    else:
-        raise ValueError(f"No such blocked well set: {bwname}")
-
-    if wname in bwset.get_well_names():
-        self._wname = wname
-    else:
-        raise WellNotFoundError(f"No such well in blocked well set: {wname}")
-
-    # pylint: disable=unnecessary-comprehension
-    bwprops = [item for item in bwset.properties]
-    bwnames = [item.name for item in bwset.properties]
-
-    bw_cellindices = bwset.get_cell_numbers()
-    dind = bwset.get_data_indices([wname])
-
-    cind = bw_cellindices[dind]
-    xyz = np.transpose(gmodel.get_grid(realisation=realisation).get_cell_centers(cind))
-
-    logs = OrderedDict()
-    logs["X_UTME"] = xyz[0].astype(np.float64)
-    logs["Y_UTMN"] = xyz[1].astype(np.float64)
-    logs["Z_TVDSS"] = xyz[2].astype(np.float64)
-    if ijk:
-        ijk = np.transpose(
-            gmodel.get_grid(realisation=realisation).grid_indexer.get_indices(cind)
-        )
-        logs["I_INDEX"] = ijk[0].astype(np.float64)
-        logs["J_INDEX"] = ijk[1].astype(np.float64)
-        logs["K_INDEX"] = ijk[2].astype(np.float64)
-
-    usenames = []
-    if lognames and lognames == "all":
-        usenames = bwnames
-    elif lognames:
-        usenames = lognames
-
-    for bwprop in bwprops:
-        lname = bwprop.name
-        if lname not in usenames:
-            continue
-        propvalues = bwprop.get_values(realisation=realisation)
-        tmplog = propvalues[dind].astype(np.float64)
-        tmplog = npma.filled(tmplog, fill_value=np.nan)
-        tmplog[tmplog == -999] = np.nan
-        if "discrete" in str(bwprop.type):
-            self._wlogtypes[lname] = "DISC"
-            self._wlogrecords[lname] = bwprop.code_names
-        else:
-            self._wlogtypes[lname] = "CONT"
-            self._wlogrecords[lname] = None
-
-        logs[lname] = tmplog
-
-    self._df = pd.DataFrame.from_dict(logs)
-    self._gname = gname
-    self._filesrc = None
-
-    # finally get some other metadata like RKB and topside X Y; as they
-    # seem to miss for the BW in RMS, try and get them from the
-    # well itself...
-
-    if wname in rox.project.wells:
-        self._rkb = rox.project.wells[wname].rkb
-        self._xpos, self._ypos = rox.project.wells[wname].wellhead
-    else:
-        self._rkb = None
-        self._xpos, self._ypos = self._df["X_UTME"][0], self._df["Y_UTMN"][0]
-
-
-def _roxapi_export_bwell(
-    self, rox, gname, bwname, wname, lognames, ijk, realisation
-):  # pylint: disable=too-many-statements
-    """Private function for ROXAPI well export (set well with updated logs to Roxar)"""
-
-    if gname in rox.project.grid_models:
-        gmodel = rox.project.grid_models[gname]
-        logger.info("RMS grid model <%s> OK", gname)
-    else:
-        raise ValueError(f"No such grid name present: {gname}")
-
-    if bwname in gmodel.blocked_wells_set:
-        bwset = gmodel.blocked_wells_set[bwname]
-        logger.info("Blocked well set <%s> OK", bwname)
-        bwprops = bwset.properties
-    else:
-        raise ValueError(f"No such blocked well set: {bwname}")
-
-    if wname in bwset.get_well_names():
-        self._wname = wname
-    else:
-        raise WellNotFoundError(f"No such well in blocked well set: {wname}")
-
-    bwnames = [item.name for item in bwset.properties]
-
-    # get the current indices for the well
-    dind = bwset.get_data_indices([self._wname])
-
-    for lname in self.lognames:
-        if not ijk and "_INDEX" in lname:
-            continue
-
-        if lognames != "all" and lname not in lognames:
-            continue
-
-        if lname not in bwnames:
-            if self._wlogtypes[lname] == "CONT":
-                print("Create CONT", lname, "for", wname)
-                bwlog = bwset.properties.create(
-                    lname, roxar.GridPropertyType.continuous, np.float32
-                )
-                bwprop = bwset.generate_values(discrete=False, fill_value=0.0)
-            else:
-                print("Create DISK", lname, "for", wname)
-                bwlog = bwset.properties.create(
-                    lname, roxar.GridPropertyType.discrete, np.int32
-                )
-                bwprop = bwset.generate_values(discrete=True, fill_value=0)
-
-        else:
-            bwlog = bwprops[lname]
-            bwprop = bwlog.get_values(realisation=realisation)
-
-        usedtype = bwprop.dtype
-        dind = bwset.get_data_indices([self._wname])
-
-        if self.dataframe[lname].values.size != dind.size:
-            raise ValueError(
-                "Dataframe is of wrong size, changing numbers of rows is not possible"
-            )
-
-        maskedvalues = np.ma.masked_invalid(self.dataframe[lname].values).astype(
-            usedtype
-        )
-
-        bwprop[dind] = maskedvalues
-        bwlog.set_values(bwprop)
+# -*- coding: utf-8 -*-
+"""Well input and output, private module for ROXAPI"""
+
+
+from collections import OrderedDict
+
+import numpy as np
+import numpy.ma as npma
+import pandas as pd
+
+from xtgeo.common import XTGeoDialog
+from xtgeo.roxutils import RoxUtils
+from xtgeo.common.exceptions import WellNotFoundError
+
+try:
+    import roxar
+except ImportError:
+    pass
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+# Import / export via ROX api
+
+
+def import_bwell_roxapi(
+    self, project, gname, bwname, wname, lognames=None, ijk=True, realisation=0
+):
+    """Private function for loading project and ROXAPI blockwell import"""
+
+    logger.info("Opening RMS project ...")
+    rox = RoxUtils(project, readonly=True)
+
+    _roxapi_import_bwell(self, rox, gname, bwname, wname, lognames, ijk, realisation)
+
+    rox.safe_close()
+
+
+def export_bwell_roxapi(
+    self, project, gname, bwname, wname, lognames="all", ijk=False, realisation=0
+):
+    """Private function for blockwell export (store in RMS) from XTGeo to RoxarAPI"""
+
+    logger.info("Opening RMS project ...")
+    rox = RoxUtils(project, readonly=False)
+
+    _roxapi_export_bwell(self, rox, gname, bwname, wname, lognames, ijk, realisation)
+
+    if rox._roxexternal:
+        rox.project.save()
+
+    rox.safe_close()
+
+
+def _roxapi_import_bwell(
+    self, rox, gname, bwname, wname, lognames, ijk, realisation
+):  # pylint: disable=too-many-statements
+    """Private function for ROXAPI well import (get well from Roxar)"""
+
+    if gname in rox.project.grid_models:
+        gmodel = rox.project.grid_models[gname]
+        logger.info("RMS grid model <%s> OK", gname)
+    else:
+        raise ValueError(f"No such grid name present: {gname}")
+
+    if bwname in gmodel.blocked_wells_set:
+        bwset = gmodel.blocked_wells_set[bwname]
+        logger.info("Blocked well set <%s> OK", bwname)
+    else:
+        raise ValueError(f"No such blocked well set: {bwname}")
+
+    if wname in bwset.get_well_names():
+        self._wname = wname
+    else:
+        raise WellNotFoundError(f"No such well in blocked well set: {wname}")
+
+    # pylint: disable=unnecessary-comprehension
+    bwprops = [item for item in bwset.properties]
+    bwnames = [item.name for item in bwset.properties]
+
+    bw_cellindices = bwset.get_cell_numbers()
+    dind = bwset.get_data_indices([wname])
+
+    cind = bw_cellindices[dind]
+    xyz = np.transpose(gmodel.get_grid(realisation=realisation).get_cell_centers(cind))
+
+    logs = OrderedDict()
+    logs["X_UTME"] = xyz[0].astype(np.float64)
+    logs["Y_UTMN"] = xyz[1].astype(np.float64)
+    logs["Z_TVDSS"] = xyz[2].astype(np.float64)
+    if ijk:
+        ijk = np.transpose(
+            gmodel.get_grid(realisation=realisation).grid_indexer.get_indices(cind)
+        )
+        logs["I_INDEX"] = ijk[0].astype(np.float64)
+        logs["J_INDEX"] = ijk[1].astype(np.float64)
+        logs["K_INDEX"] = ijk[2].astype(np.float64)
+
+    usenames = []
+    if lognames and lognames == "all":
+        usenames = bwnames
+    elif lognames:
+        usenames = lognames
+
+    for bwprop in bwprops:
+        lname = bwprop.name
+        if lname not in usenames:
+            continue
+        propvalues = bwprop.get_values(realisation=realisation)
+        tmplog = propvalues[dind].astype(np.float64)
+        tmplog = npma.filled(tmplog, fill_value=np.nan)
+        tmplog[tmplog == -999] = np.nan
+        if "discrete" in str(bwprop.type):
+            self._wlogtypes[lname] = "DISC"
+            self._wlogrecords[lname] = bwprop.code_names
+        else:
+            self._wlogtypes[lname] = "CONT"
+            self._wlogrecords[lname] = None
+
+        logs[lname] = tmplog
+
+    self._df = pd.DataFrame.from_dict(logs)
+    self._gname = gname
+    self._filesrc = None
+
+    # finally get some other metadata like RKB and topside X Y; as they
+    # seem to miss for the BW in RMS, try and get them from the
+    # well itself...
+
+    if wname in rox.project.wells:
+        self._rkb = rox.project.wells[wname].rkb
+        self._xpos, self._ypos = rox.project.wells[wname].wellhead
+    else:
+        self._rkb = None
+        self._xpos, self._ypos = self._df["X_UTME"][0], self._df["Y_UTMN"][0]
+
+
+def _roxapi_export_bwell(
+    self, rox, gname, bwname, wname, lognames, ijk, realisation
+):  # pylint: disable=too-many-statements
+    """Private function for ROXAPI well export (set well with updated logs to Roxar)"""
+
+    if gname in rox.project.grid_models:
+        gmodel = rox.project.grid_models[gname]
+        logger.info("RMS grid model <%s> OK", gname)
+    else:
+        raise ValueError(f"No such grid name present: {gname}")
+
+    if bwname in gmodel.blocked_wells_set:
+        bwset = gmodel.blocked_wells_set[bwname]
+        logger.info("Blocked well set <%s> OK", bwname)
+        bwprops = bwset.properties
+    else:
+        raise ValueError(f"No such blocked well set: {bwname}")
+
+    if wname in bwset.get_well_names():
+        self._wname = wname
+    else:
+        raise WellNotFoundError(f"No such well in blocked well set: {wname}")
+
+    bwnames = [item.name for item in bwset.properties]
+
+    # get the current indices for the well
+    dind = bwset.get_data_indices([self._wname])
+
+    for lname in self.lognames:
+        if not ijk and "_INDEX" in lname:
+            continue
+
+        if lognames != "all" and lname not in lognames:
+            continue
+
+        if lname not in bwnames:
+            if self._wlogtypes[lname] == "CONT":
+                print("Create CONT", lname, "for", wname)
+                bwlog = bwset.properties.create(
+                    lname, roxar.GridPropertyType.continuous, np.float32
+                )
+                bwprop = bwset.generate_values(discrete=False, fill_value=0.0)
+            else:
+                print("Create DISK", lname, "for", wname)
+                bwlog = bwset.properties.create(
+                    lname, roxar.GridPropertyType.discrete, np.int32
+                )
+                bwprop = bwset.generate_values(discrete=True, fill_value=0)
+
+        else:
+            bwlog = bwprops[lname]
+            bwprop = bwlog.get_values(realisation=realisation)
+
+        usedtype = bwprop.dtype
+        dind = bwset.get_data_indices([self._wname])
+
+        if self.dataframe[lname].values.size != dind.size:
+            raise ValueError(
+                "Dataframe is of wrong size, changing numbers of rows is not possible"
+            )
+
+        maskedvalues = np.ma.masked_invalid(self.dataframe[lname].values).astype(
+            usedtype
+        )
+
+        bwprop[dind] = maskedvalues
+        bwlog.set_values(bwprop)
```

## xtgeo/well/_blockedwells_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,69 +1,69 @@
-# -*- coding: utf-8 -*-
-"""Well input and output, private module for ROXAPI"""
-
-
-from xtgeo.common import XTGeoDialog
-from xtgeo.roxutils import RoxUtils
-from .blocked_well import BlockedWell
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-# Import from ROX api
-# --------------------------------------------------------------------------------------
-def import_bwells_roxapi(
-    self, project, gname, bwname, lognames=None, ijk=True, realisation=0
-):  # pragma: no cover
-    """Private function for loading project and ROXAPI blockwell import"""
-
-    logger.info("Opening RMS project ...")
-    rox = RoxUtils(project, readonly=True)
-
-    _roxapi_import_bwells(self, rox, gname, bwname, lognames, ijk, realisation)
-
-    rox.safe_close()
-
-
-def _roxapi_import_bwells(
-    self, rox, gname, bwname, lnames, ijk, realisation
-):  # pragma: no cover
-    """Private function for ROXAPI blocked wells import"""
-
-    if gname in rox.project.grid_models:
-        gmodel = rox.project.grid_models[gname]
-        logger.info("RMS grid model <%s> OK", gname)
-    else:
-        raise ValueError(f"No such grid name present: {gname}")
-
-    if bwname in gmodel.blocked_wells_set:
-        bwset = gmodel.blocked_wells_set[bwname]
-        logger.info("Blocked well set <%s> OK", bwname)
-    else:
-        raise ValueError(f"No such blocked well set: {bwname}")
-
-    wnames = bwset.get_well_names()
-
-    logger.info("Lognames are %s", lnames)
-
-    bwlist = []
-    logger.info("Loading wells ...")
-    for wname in wnames:
-        logger.info("Loading well %s", wname)
-        bwtmp = BlockedWell()
-        bwtmp.from_roxar(
-            rox.project,
-            gname,
-            bwname,
-            wname,
-            lognames=lnames,
-            ijk=ijk,
-            realisation=realisation,
-        )
-        bwlist.append(bwtmp)
-
-    self._wells = bwlist
-
-    if not self._wells:
-        xtg.warn("No wells imported to BlockedWells")
-        self._wells = None
+# -*- coding: utf-8 -*-
+"""Well input and output, private module for ROXAPI"""
+
+
+from xtgeo.common import XTGeoDialog
+from xtgeo.roxutils import RoxUtils
+from .blocked_well import BlockedWell
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+# Import from ROX api
+# --------------------------------------------------------------------------------------
+def import_bwells_roxapi(
+    self, project, gname, bwname, lognames=None, ijk=True, realisation=0
+):  # pragma: no cover
+    """Private function for loading project and ROXAPI blockwell import"""
+
+    logger.info("Opening RMS project ...")
+    rox = RoxUtils(project, readonly=True)
+
+    _roxapi_import_bwells(self, rox, gname, bwname, lognames, ijk, realisation)
+
+    rox.safe_close()
+
+
+def _roxapi_import_bwells(
+    self, rox, gname, bwname, lnames, ijk, realisation
+):  # pragma: no cover
+    """Private function for ROXAPI blocked wells import"""
+
+    if gname in rox.project.grid_models:
+        gmodel = rox.project.grid_models[gname]
+        logger.info("RMS grid model <%s> OK", gname)
+    else:
+        raise ValueError(f"No such grid name present: {gname}")
+
+    if bwname in gmodel.blocked_wells_set:
+        bwset = gmodel.blocked_wells_set[bwname]
+        logger.info("Blocked well set <%s> OK", bwname)
+    else:
+        raise ValueError(f"No such blocked well set: {bwname}")
+
+    wnames = bwset.get_well_names()
+
+    logger.info("Lognames are %s", lnames)
+
+    bwlist = []
+    logger.info("Loading wells ...")
+    for wname in wnames:
+        logger.info("Loading well %s", wname)
+        bwtmp = BlockedWell()
+        bwtmp.from_roxar(
+            rox.project,
+            gname,
+            bwname,
+            wname,
+            lognames=lnames,
+            ijk=ijk,
+            realisation=realisation,
+        )
+        bwlist.append(bwtmp)
+
+    self._wells = bwlist
+
+    if not self._wells:
+        xtg.warn("No wells imported to BlockedWells")
+        self._wells = None
```

## xtgeo/well/_well_io.py

 * *Ordering differences only*

```diff
@@ -1,330 +1,330 @@
-# -*- coding: utf-8 -*-
-"""Well input and ouput, private module"""
-import json
-from collections import OrderedDict
-from copy import deepcopy
-
-import numpy as np
-import pandas as pd
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_rms_ascii(
-    wfile,
-    mdlogname=None,
-    zonelogname=None,
-    strict=False,
-    lognames="all",
-    lognames_strict=False,
-):
-    """Import RMS ascii table well"""
-    # pylint: disable=too-many-locals, too-many-branches, too-many-statements
-    wlogtype = dict()
-    wlogrecords = dict()
-
-    xlognames_all = ["X_UTME", "Y_UTMN", "Z_TVDSS"]
-    xlognames = []
-
-    lnum = 1
-    with open(wfile.file, "r") as fwell:
-        for line in fwell:
-            if lnum == 1:
-                _ffver = line.strip()  # noqa, file version
-            elif lnum == 2:
-                _wtype = line.strip()  # noqa, well type
-            elif lnum == 3:
-                # usually 4 fields, but last (rkb) can be missing. A
-                # complication is that first field (well name) may have spaces,
-                # hence some clever guessing is needed. However, this cannot be
-                # 100% foolproof... if Ycoord < 1000 and last item of a well
-                # name with spaces is a number, then this may fail.
-                assume_rkb = False
-                row = line.strip().split()
-                newrow = []
-                if len(row) > 3:
-                    for item in row:
-                        try:
-                            item = float(item)
-                        except ValueError:
-                            item = str(item)
-                        newrow.append(item)
-                    if all(isinstance(var, float) for var in newrow[-3:]):
-                        if abs(newrow[-1] < 1000.0):
-                            assume_rkb = True
-
-                if assume_rkb:
-                    rkb = float(row.pop())
-                else:
-                    rkb = None
-                ypos = float(row.pop())
-                xpos = float(row.pop())
-                wname = " ".join(map(str, row))
-
-            elif lnum == 4:
-                nlogs = int(line)
-                nlogread = 1
-                logger.debug("Number of logs: %s", nlogs)
-
-            else:
-                row = line.strip().split()
-                lname = row[0]
-
-                # if i_index etc, make uppercase to I_INDEX
-                # however it is most practical to treat indexes as CONT logs
-                if "_index" in lname:
-                    lname = lname.upper()
-
-                ltype = row[1].upper()
-
-                rxv = row[2:]
-
-                xlognames_all.append(lname)
-                xlognames.append(lname)
-
-                wlogtype[lname] = ltype
-
-                logger.debug("Reading log name %s of type %s", lname, ltype)
-
-                if ltype == "DISC":
-                    xdict = {int(rxv[i]): rxv[i + 1] for i in range(0, len(rxv), 2)}
-                    wlogrecords[lname] = xdict
-                else:
-                    wlogrecords[lname] = rxv
-
-                nlogread += 1
-
-                if nlogread > nlogs:
-                    break
-
-            lnum += 1
-
-    # now import all logs as pandas framework
-
-    dfr = pd.read_csv(
-        wfile.file,
-        delim_whitespace=True,
-        skiprows=lnum,
-        header=None,
-        names=xlognames_all,
-        dtype=np.float64,
-        na_values=-999,
-    )
-
-    # undef values have a high float number? or keep Nan?
-    # df.fillna(Well.UNDEF, inplace=True)
-
-    dfr = _trim_on_lognames(dfr, lognames, lognames_strict, wname)
-    mdlogname, zonelogname = _check_special_logs(
-        dfr, mdlogname, zonelogname, strict, wname
-    )
-
-    return {
-        "wlogtypes": wlogtype,
-        "wlogrecords": wlogrecords,
-        "rkb": rkb,
-        "xpos": xpos,
-        "ypos": ypos,
-        "wname": wname,
-        "df": dfr,
-        "mdlogname": mdlogname,
-        "zonelogname": zonelogname,
-    }
-
-
-def _trim_on_lognames(dfr, lognames, lognames_strict, wname):
-    """Reduce the dataframe based on provided list of lognames"""
-    if lognames == "all":
-        return dfr
-
-    uselnames = ["X_UTME", "Y_UTMN", "Z_TVDSS"]
-    if isinstance(lognames, str):
-        uselnames.append(lognames)
-    elif isinstance(lognames, list):
-        uselnames.extend(lognames)
-
-    newdf = pd.DataFrame()
-    for lname in uselnames:
-        if lname in dfr.columns:
-            newdf[lname] = dfr[lname]
-        else:
-            if lognames_strict:
-                msg = f"Logname <{lname}> is not present for <{wname}>"
-                msg += " (required log under condition lognames_strict=True)"
-                raise ValueError(msg)
-
-    return newdf
-
-
-def _check_special_logs(dfr, mdlogname, zonelogname, strict, wname):
-    """Check for MD log and Zonelog, if requested"""
-
-    mname = mdlogname
-    zname = zonelogname
-
-    if mdlogname is not None:
-        if mdlogname not in dfr.columns:
-            msg = (
-                f"mdlogname={mdlogname} was requested but no such log found for "
-                f"well {wname}"
-            )
-            mname = None
-            if strict:
-                raise ValueError(msg)
-
-            logger.warning(msg)
-
-    # check for zone log:
-    if zonelogname is not None:
-        if zonelogname not in dfr.columns:
-            msg = (
-                f"zonelogname={zonelogname} was requested but no such log found "
-                f"for well {wname}"
-            )
-            zname = None
-            if strict:
-                raise ValueError(msg)
-
-            logger.warning(msg)
-
-    return mname, zname
-
-
-def export_rms_ascii(self, wfile, precision=4):
-    """Export to RMS well format."""
-
-    with open(wfile, "w") as fwell:
-        print("1.0", file=fwell)
-        print("Unknown", file=fwell)
-        if self._rkb is None:
-            print(f"{self._wname} {self._xpos} {self._ypos}", file=fwell)
-        else:
-            print(
-                f"{self._wname} {self._xpos} {self._ypos} {self._rkb}",
-                file=fwell,
-            )
-        print(f"{len(self.lognames)}", file=fwell)
-        for lname in self.lognames:
-            usewrec = "linear"
-            wrec = []
-            if isinstance(self._wlogrecords[lname], dict):
-                for key in self._wlogrecords[lname]:
-                    wrec.append(key)
-                    wrec.append(self._wlogrecords[lname][key])
-                usewrec = " ".join(str(x) for x in wrec)
-
-            print(f"{lname} {self._wlogtypes[lname]} {usewrec}", file=fwell)
-
-    # now export all logs as pandas framework
-    tmpdf = self._df.copy()
-    tmpdf.fillna(value=-999, inplace=True)
-
-    # make the disc as is np.int
-    for lname in self._wlogtypes:
-        if self._wlogtypes[lname] == "DISC":
-            tmpdf[[lname]] = tmpdf[[lname]].astype(int)
-
-    cformat = "%-." + str(precision) + "f"
-    tmpdf.to_csv(
-        wfile,
-        sep=" ",
-        header=False,
-        index=False,
-        float_format=cformat,
-        escapechar=" ",
-        mode="a",
-    )
-
-
-def export_hdf5_well(self, wfile, compression="lzf"):
-    """Save to HDF5 format."""
-    logger.info("Export to hdf5 format...")
-
-    self._ensure_consistency()
-
-    self.metadata.required = self
-
-    meta = self.metadata.get_metadata()
-    jmeta = json.dumps(meta)
-
-    complib = "zlib"  # same as default lzf
-    complevel = 5
-    if compression and compression == "blosc":
-        complib = "blosc"
-    else:
-        complevel = 0
-
-    with pd.HDFStore(wfile.file, "w", complevel=complevel, complib=complib) as store:
-        logger.info("export to HDF5 %s", wfile.name)
-        store.put("Well", self._df)
-        store.get_storer("Well").attrs["metadata"] = jmeta
-        store.get_storer("Well").attrs["provider"] = "xtgeo"
-        store.get_storer("Well").attrs["format_idcode"] = 1401
-
-    logger.info("Export to hdf5 format... done!")
-
-
-def import_wlogs(wlogs: OrderedDict):
-    """
-    This converts joined wlogtypes/wlogrecords such as found in
-    the hdf5 format to the format used in the Well object.
-
-    >>> import_wlogs(OrderedDict())
-    {'wlogtypes': {}, 'wlogrecords': {}}
-    >>> import_wlogs(OrderedDict([("X_UTME", ("CONT", None))]))
-    {'wlogtypes': {'X_UTME': 'CONT'}, 'wlogrecords': {'X_UTME': None}}
-
-    Returns:
-        dictionary with "wlogtypes" and "wlogrecords" as keys
-        and corresponding values.
-    """
-    wlogtypes = dict()
-    wlogrecords = dict()
-    for key in wlogs.keys():
-        typ, rec = wlogs[key]
-
-        if typ in {"DISC", "CONT"}:
-            wlogtypes[key] = deepcopy(typ)
-        else:
-            raise ValueError(f"Invalid log type found in input: {typ}")
-
-        if rec is None or isinstance(rec, dict):
-            wlogrecords[key] = deepcopy(rec)
-        else:
-            raise ValueError(f"Invalid log record found in input: {rec}")
-    return {"wlogtypes": wlogtypes, "wlogrecords": wlogrecords}
-
-
-def import_hdf5_well(wfile, **kwargs):
-    """Load from HDF5 format."""
-    logger.info("The kwargs may be unused: %s", kwargs)
-    reqattrs = xtgeo.MetaDataWell.REQUIRED
-
-    with pd.HDFStore(wfile.file, "r") as store:
-        data = store.get("Well")
-        wstore = store.get_storer("Well")
-        jmeta = wstore.attrs["metadata"]
-        # provider = wstore.attrs["provider"]
-        # format_idcode = wstore.attrs["format_idcode"]
-
-    if isinstance(jmeta, bytes):
-        jmeta = jmeta.decode()
-
-    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
-    req = meta["_required_"]
-    result = dict()
-    for myattr in reqattrs:
-        if myattr == "wlogs":
-            result.update(import_wlogs(req[myattr]))
-        elif myattr == "name":
-            result["wname"] = req[myattr]
-        else:
-            result[myattr] = req[myattr]
-
-    result["df"] = data
-    return result
+# -*- coding: utf-8 -*-
+"""Well input and ouput, private module"""
+import json
+from collections import OrderedDict
+from copy import deepcopy
+
+import numpy as np
+import pandas as pd
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_rms_ascii(
+    wfile,
+    mdlogname=None,
+    zonelogname=None,
+    strict=False,
+    lognames="all",
+    lognames_strict=False,
+):
+    """Import RMS ascii table well"""
+    # pylint: disable=too-many-locals, too-many-branches, too-many-statements
+    wlogtype = dict()
+    wlogrecords = dict()
+
+    xlognames_all = ["X_UTME", "Y_UTMN", "Z_TVDSS"]
+    xlognames = []
+
+    lnum = 1
+    with open(wfile.file, "r") as fwell:
+        for line in fwell:
+            if lnum == 1:
+                _ffver = line.strip()  # noqa, file version
+            elif lnum == 2:
+                _wtype = line.strip()  # noqa, well type
+            elif lnum == 3:
+                # usually 4 fields, but last (rkb) can be missing. A
+                # complication is that first field (well name) may have spaces,
+                # hence some clever guessing is needed. However, this cannot be
+                # 100% foolproof... if Ycoord < 1000 and last item of a well
+                # name with spaces is a number, then this may fail.
+                assume_rkb = False
+                row = line.strip().split()
+                newrow = []
+                if len(row) > 3:
+                    for item in row:
+                        try:
+                            item = float(item)
+                        except ValueError:
+                            item = str(item)
+                        newrow.append(item)
+                    if all(isinstance(var, float) for var in newrow[-3:]):
+                        if abs(newrow[-1] < 1000.0):
+                            assume_rkb = True
+
+                if assume_rkb:
+                    rkb = float(row.pop())
+                else:
+                    rkb = None
+                ypos = float(row.pop())
+                xpos = float(row.pop())
+                wname = " ".join(map(str, row))
+
+            elif lnum == 4:
+                nlogs = int(line)
+                nlogread = 1
+                logger.debug("Number of logs: %s", nlogs)
+
+            else:
+                row = line.strip().split()
+                lname = row[0]
+
+                # if i_index etc, make uppercase to I_INDEX
+                # however it is most practical to treat indexes as CONT logs
+                if "_index" in lname:
+                    lname = lname.upper()
+
+                ltype = row[1].upper()
+
+                rxv = row[2:]
+
+                xlognames_all.append(lname)
+                xlognames.append(lname)
+
+                wlogtype[lname] = ltype
+
+                logger.debug("Reading log name %s of type %s", lname, ltype)
+
+                if ltype == "DISC":
+                    xdict = {int(rxv[i]): rxv[i + 1] for i in range(0, len(rxv), 2)}
+                    wlogrecords[lname] = xdict
+                else:
+                    wlogrecords[lname] = rxv
+
+                nlogread += 1
+
+                if nlogread > nlogs:
+                    break
+
+            lnum += 1
+
+    # now import all logs as pandas framework
+
+    dfr = pd.read_csv(
+        wfile.file,
+        delim_whitespace=True,
+        skiprows=lnum,
+        header=None,
+        names=xlognames_all,
+        dtype=np.float64,
+        na_values=-999,
+    )
+
+    # undef values have a high float number? or keep Nan?
+    # df.fillna(Well.UNDEF, inplace=True)
+
+    dfr = _trim_on_lognames(dfr, lognames, lognames_strict, wname)
+    mdlogname, zonelogname = _check_special_logs(
+        dfr, mdlogname, zonelogname, strict, wname
+    )
+
+    return {
+        "wlogtypes": wlogtype,
+        "wlogrecords": wlogrecords,
+        "rkb": rkb,
+        "xpos": xpos,
+        "ypos": ypos,
+        "wname": wname,
+        "df": dfr,
+        "mdlogname": mdlogname,
+        "zonelogname": zonelogname,
+    }
+
+
+def _trim_on_lognames(dfr, lognames, lognames_strict, wname):
+    """Reduce the dataframe based on provided list of lognames"""
+    if lognames == "all":
+        return dfr
+
+    uselnames = ["X_UTME", "Y_UTMN", "Z_TVDSS"]
+    if isinstance(lognames, str):
+        uselnames.append(lognames)
+    elif isinstance(lognames, list):
+        uselnames.extend(lognames)
+
+    newdf = pd.DataFrame()
+    for lname in uselnames:
+        if lname in dfr.columns:
+            newdf[lname] = dfr[lname]
+        else:
+            if lognames_strict:
+                msg = f"Logname <{lname}> is not present for <{wname}>"
+                msg += " (required log under condition lognames_strict=True)"
+                raise ValueError(msg)
+
+    return newdf
+
+
+def _check_special_logs(dfr, mdlogname, zonelogname, strict, wname):
+    """Check for MD log and Zonelog, if requested"""
+
+    mname = mdlogname
+    zname = zonelogname
+
+    if mdlogname is not None:
+        if mdlogname not in dfr.columns:
+            msg = (
+                f"mdlogname={mdlogname} was requested but no such log found for "
+                f"well {wname}"
+            )
+            mname = None
+            if strict:
+                raise ValueError(msg)
+
+            logger.warning(msg)
+
+    # check for zone log:
+    if zonelogname is not None:
+        if zonelogname not in dfr.columns:
+            msg = (
+                f"zonelogname={zonelogname} was requested but no such log found "
+                f"for well {wname}"
+            )
+            zname = None
+            if strict:
+                raise ValueError(msg)
+
+            logger.warning(msg)
+
+    return mname, zname
+
+
+def export_rms_ascii(self, wfile, precision=4):
+    """Export to RMS well format."""
+
+    with open(wfile, "w") as fwell:
+        print("1.0", file=fwell)
+        print("Unknown", file=fwell)
+        if self._rkb is None:
+            print(f"{self._wname} {self._xpos} {self._ypos}", file=fwell)
+        else:
+            print(
+                f"{self._wname} {self._xpos} {self._ypos} {self._rkb}",
+                file=fwell,
+            )
+        print(f"{len(self.lognames)}", file=fwell)
+        for lname in self.lognames:
+            usewrec = "linear"
+            wrec = []
+            if isinstance(self._wlogrecords[lname], dict):
+                for key in self._wlogrecords[lname]:
+                    wrec.append(key)
+                    wrec.append(self._wlogrecords[lname][key])
+                usewrec = " ".join(str(x) for x in wrec)
+
+            print(f"{lname} {self._wlogtypes[lname]} {usewrec}", file=fwell)
+
+    # now export all logs as pandas framework
+    tmpdf = self._df.copy()
+    tmpdf.fillna(value=-999, inplace=True)
+
+    # make the disc as is np.int
+    for lname in self._wlogtypes:
+        if self._wlogtypes[lname] == "DISC":
+            tmpdf[[lname]] = tmpdf[[lname]].astype(int)
+
+    cformat = "%-." + str(precision) + "f"
+    tmpdf.to_csv(
+        wfile,
+        sep=" ",
+        header=False,
+        index=False,
+        float_format=cformat,
+        escapechar=" ",
+        mode="a",
+    )
+
+
+def export_hdf5_well(self, wfile, compression="lzf"):
+    """Save to HDF5 format."""
+    logger.info("Export to hdf5 format...")
+
+    self._ensure_consistency()
+
+    self.metadata.required = self
+
+    meta = self.metadata.get_metadata()
+    jmeta = json.dumps(meta)
+
+    complib = "zlib"  # same as default lzf
+    complevel = 5
+    if compression and compression == "blosc":
+        complib = "blosc"
+    else:
+        complevel = 0
+
+    with pd.HDFStore(wfile.file, "w", complevel=complevel, complib=complib) as store:
+        logger.info("export to HDF5 %s", wfile.name)
+        store.put("Well", self._df)
+        store.get_storer("Well").attrs["metadata"] = jmeta
+        store.get_storer("Well").attrs["provider"] = "xtgeo"
+        store.get_storer("Well").attrs["format_idcode"] = 1401
+
+    logger.info("Export to hdf5 format... done!")
+
+
+def import_wlogs(wlogs: OrderedDict):
+    """
+    This converts joined wlogtypes/wlogrecords such as found in
+    the hdf5 format to the format used in the Well object.
+
+    >>> import_wlogs(OrderedDict())
+    {'wlogtypes': {}, 'wlogrecords': {}}
+    >>> import_wlogs(OrderedDict([("X_UTME", ("CONT", None))]))
+    {'wlogtypes': {'X_UTME': 'CONT'}, 'wlogrecords': {'X_UTME': None}}
+
+    Returns:
+        dictionary with "wlogtypes" and "wlogrecords" as keys
+        and corresponding values.
+    """
+    wlogtypes = dict()
+    wlogrecords = dict()
+    for key in wlogs.keys():
+        typ, rec = wlogs[key]
+
+        if typ in {"DISC", "CONT"}:
+            wlogtypes[key] = deepcopy(typ)
+        else:
+            raise ValueError(f"Invalid log type found in input: {typ}")
+
+        if rec is None or isinstance(rec, dict):
+            wlogrecords[key] = deepcopy(rec)
+        else:
+            raise ValueError(f"Invalid log record found in input: {rec}")
+    return {"wlogtypes": wlogtypes, "wlogrecords": wlogrecords}
+
+
+def import_hdf5_well(wfile, **kwargs):
+    """Load from HDF5 format."""
+    logger.info("The kwargs may be unused: %s", kwargs)
+    reqattrs = xtgeo.MetaDataWell.REQUIRED
+
+    with pd.HDFStore(wfile.file, "r") as store:
+        data = store.get("Well")
+        wstore = store.get_storer("Well")
+        jmeta = wstore.attrs["metadata"]
+        # provider = wstore.attrs["provider"]
+        # format_idcode = wstore.attrs["format_idcode"]
+
+    if isinstance(jmeta, bytes):
+        jmeta = jmeta.decode()
+
+    meta = json.loads(jmeta, object_pairs_hook=OrderedDict)
+    req = meta["_required_"]
+    result = dict()
+    for myattr in reqattrs:
+        if myattr == "wlogs":
+            result.update(import_wlogs(req[myattr]))
+        elif myattr == "name":
+            result["wname"] = req[myattr]
+        else:
+            result[myattr] = req[myattr]
+
+    result["df"] = data
+    return result
```

## xtgeo/well/_well_oper.py

 * *Ordering differences only*

```diff
@@ -1,578 +1,578 @@
-"""Operations along a well, private module."""
-
-import copy
-
-import numpy as np
-import pandas as pd
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.common import constants as const
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def delete_log(self, lname):
-    """Delete/remove an existing log, or list of logs."""
-    self._ensure_consistency()
-
-    if not isinstance(lname, list):
-        lname = [lname]
-
-    lcount = 0
-    for logn in lname:
-        if logn not in self._wlognames:
-            logger.info("Log does no exist: %s", logn)
-            continue
-
-        logger.info("Log exist and will be deleted: %s", logn)
-        lcount += 1
-        del self._wlogtypes[logn]
-        del self._wlogrecords[logn]
-
-        self._df.drop(logn, axis=1, inplace=True)
-        self._ensure_consistency()
-
-        if self._mdlogname == logn:
-            self._mdlogname = None
-        if self._zonelogname == logn:
-            self._zonelogname = None
-
-    self._ensure_consistency()
-    return lcount
-
-
-def rescale(self, delta=0.15, tvdrange=None):
-    """Rescale by using a new MD increment.
-
-    The rescaling is technically done by interpolation in the Pandas dataframe
-    """
-    pdrows = pd.options.display.max_rows
-    pd.options.display.max_rows = 999
-
-    dfrcolumns0 = self._df.columns
-
-    if self.mdlogname is None:
-        self.geometrics()
-
-    dfrcolumns1 = self._df.columns
-    columnsadded = list(set(dfrcolumns1) - set(dfrcolumns0))  # new tmp columns, if any
-
-    dfr = self._df.copy().set_index(self.mdlogname)
-
-    logger.debug("Initial dataframe\n %s", dfr)
-
-    start = dfr.index[0]
-    stop = dfr.index[-1]
-    startt = start
-    stopt = stop
-
-    if tvdrange and isinstance(tvdrange, tuple) and len(tvdrange) == 2:
-        tvd1, tvd2 = tvdrange
-
-        try:
-            startt = dfr.index[dfr["Z_TVDSS"] >= tvd1][0]
-        except IndexError:
-            startt = start
-
-        try:
-            stopt = dfr.index[dfr["Z_TVDSS"] >= tvd2][0]
-        except IndexError:
-            stopt = stop
-
-    dfr1 = dfr[start:startt]
-    dfr2 = dfr[stopt:stop]
-
-    nentry = int(round((stopt - startt) / delta))
-
-    dfr = dfr.reindex(dfr.index.union(np.linspace(startt, stopt, num=nentry)))
-    dfr = dfr.interpolate("index", limit_area="inside").loc[
-        np.linspace(startt, stopt, num=nentry)
-    ]
-
-    dfr = pd.concat([dfr1, dfr, dfr2], sort=False)
-
-    dfr.drop_duplicates(inplace=True)
-    dfr[self.mdlogname] = dfr.index
-    dfr.reset_index(inplace=True, drop=True)
-
-    for lname in dfr.columns:
-        if lname in self._wlogtypes:
-            ltype = self._wlogtypes[lname]
-            if ltype == "DISC":
-                dfr = dfr.round({lname: 0})
-
-    logger.debug("Updated dataframe:\n%s", dfr)
-
-    pd.options.display.max_rows = pdrows  # reset
-
-    self._df = dfr
-    if columnsadded:
-        self.delete_log(columnsadded)
-
-
-def make_zone_qual_log(self, zqname):
-    """Make a flag log based on stratigraphic relations."""
-    if zqname in self.dataframe:
-        logger.warning("Quality log %s exists, will be overwritten", zqname)
-
-    if not self.zonelogname or self.zonelogname not in self.dataframe:
-        raise ValueError("Cannot find a zonelog")
-
-    dff = self.get_filled_dataframe()
-    dff["ztmp"] = dff[self.zonelogname]
-    dff["ztmp"] = (dff.ztmp != dff.ztmp.shift()).cumsum()
-
-    sgrp = dff.groupby("ztmp")
-
-    dff[zqname] = dff[self.zonelogname] * 0
-
-    idlist = list()
-    seq = list()
-    for idx, grp in sgrp:
-        izns = int(grp[self.zonelogname].mean())
-        seq.append(izns)
-        idlist.append(idx)
-
-    codes = {
-        0: "UNDETERMINED",
-        1: "INCREASE",
-        2: "DECREASE",
-        3: "U_TURN",
-        4: "INV_U_TURN",
-        9: "INCOMPLETE",
-    }
-
-    code = list()
-    for ind, iseq in enumerate(seq):
-        if ind in (0, len(seq) - 1):
-            code.append(0)
-        else:
-            prev_ = seq[ind - 1]
-            next_ = seq[ind + 1]
-            if prev_ > const.UNDEF_INT_LIMIT or next_ > const.UNDEF_INT_LIMIT:
-                code.append(9)
-            elif next_ > iseq > prev_:
-                code.append(1)
-            elif next_ < iseq < prev_:
-                code.append(2)
-            elif next_ < iseq > prev_:
-                code.append(3)
-            elif next_ > iseq < prev_:
-                code.append(4)
-    dcode = dict(zip(idlist, code))
-
-    # now create the new log
-    self.create_log(zqname, logtype="DISC", logrecord=codes)
-    for key, val in dcode.items():
-        self._df[zqname][dff["ztmp"] == key] = val
-
-    # set the metadata
-    self.set_logtype(zqname, "DISC")
-    self.set_logrecord(zqname, codes)
-
-    del dff
-
-
-def make_ijk_from_grid(self, grid, grid_id="", algorithm=1, activeonly=True):
-    """Make an IJK log from grid indices."""
-    logger.info("Using algorithm %s in %s", algorithm, __name__)
-
-    if algorithm == 1:
-        _make_ijk_from_grid_v1(self, grid, grid_id=grid_id)
-    else:
-        _make_ijk_from_grid_v2(self, grid, grid_id=grid_id, activeonly=activeonly)
-
-    logger.info("Using algorithm %s in %s done", algorithm, __name__)
-
-
-def _make_ijk_from_grid_v1(self, grid, grid_id=""):
-    """Getting IJK from a grid and make as well logs.
-
-    This is the first version, using _cxtgeo.grd3d_well_ijk from C
-    """
-    logger.info("Using algorithm 1 in %s", __name__)
-
-    wxarr = self.get_carray("X_UTME")
-    wyarr = self.get_carray("Y_UTMN")
-    wzarr = self.get_carray("Z_TVDSS")
-
-    nlen = self.nrow
-    wivec = _cxtgeo.new_intarray(nlen)
-    wjvec = _cxtgeo.new_intarray(nlen)
-    wkvec = _cxtgeo.new_intarray(nlen)
-
-    onelayergrid = grid.copy()
-    onelayergrid.reduce_to_one_layer()
-
-    cstatus = _cxtgeo.grd3d_well_ijk(
-        grid.ncol,
-        grid.nrow,
-        grid.nlay,
-        grid._coordsv,
-        grid._zcornsv,
-        grid._actnumsv,
-        onelayergrid._zcornsv,
-        onelayergrid._actnumsv,
-        self.nrow,
-        wxarr,
-        wyarr,
-        wzarr,
-        wivec,
-        wjvec,
-        wkvec,
-        0,
-    )
-
-    if cstatus != 0:
-        raise RuntimeError(f"Error from C routine, code is {cstatus}")
-
-    indarray = _cxtgeo.swig_carr_to_numpy_i1d(nlen, wivec).astype("float")
-    jndarray = _cxtgeo.swig_carr_to_numpy_i1d(nlen, wjvec).astype("float")
-    kndarray = _cxtgeo.swig_carr_to_numpy_i1d(nlen, wkvec).astype("float")
-
-    indarray[indarray == 0] = np.nan
-    jndarray[jndarray == 0] = np.nan
-    kndarray[kndarray == 0] = np.nan
-
-    icellname = "ICELL" + grid_id
-    jcellname = "JCELL" + grid_id
-    kcellname = "KCELL" + grid_id
-
-    self._df[icellname] = indarray
-    self._df[jcellname] = jndarray
-    self._df[kcellname] = kndarray
-
-    for cellname in [icellname, jcellname, kcellname]:
-        self._wlogtypes[cellname] = "DISC"
-
-    self._wlogrecords[icellname] = {ncel: str(ncel) for ncel in range(1, grid.ncol + 1)}
-    self._wlogrecords[jcellname] = {ncel: str(ncel) for ncel in range(1, grid.nrow + 1)}
-    self._wlogrecords[kcellname] = {ncel: str(ncel) for ncel in range(1, grid.nlay + 1)}
-
-    _cxtgeo.delete_intarray(wivec)
-    _cxtgeo.delete_intarray(wjvec)
-    _cxtgeo.delete_intarray(wkvec)
-    _cxtgeo.delete_doublearray(wxarr)
-    _cxtgeo.delete_doublearray(wyarr)
-    _cxtgeo.delete_doublearray(wzarr)
-
-    del onelayergrid
-
-
-def _make_ijk_from_grid_v2(self, grid, grid_id="", activeonly=True):
-    """Getting IJK from a grid and make as well logs.
-
-    This is a newer version, using grid.get_ijk_from_points which in turn
-    use the from C method x_chk_point_in_hexahedron, while v1 use the
-    x_chk_point_in_cell. This one is believed to be more precise!
-    """
-    # establish a Points instance and make points dataframe from well trajectory X Y Z
-    wpoints = xtgeo.Points()
-    wpdf = self.dataframe.loc[:, ["X_UTME", "Y_UTMN", "Z_TVDSS"]].copy()
-    wpoints.dataframe = wpdf
-    wpoints.dataframe.reset_index(inplace=True, drop=True)
-
-    # column names
-    cna = ("ICELL" + grid_id, "JCELL" + grid_id, "KCELL" + grid_id)
-
-    df = grid.get_ijk_from_points(
-        wpoints,
-        activeonly=activeonly,
-        zerobased=False,
-        dataframe=True,
-        includepoints=False,
-        columnnames=cna,
-        fmt="float",
-        undef=np.nan,
-    )
-
-    # The resulting df shall have same length as the well's dataframe,
-    # but the well index may not start from one. So first ignore index, then
-    # re-establish
-    wellindex = self.dataframe.index
-
-    newdf = pd.concat([self.dataframe.reset_index(drop=True), df], axis=1)
-    newdf.index = wellindex
-
-    self.dataframe = newdf
-
-
-def get_gridproperties(self, gridprops, grid=("ICELL", "JCELL", "KCELL"), prop_id=""):
-    """Getting gridproperties as logs."""
-    if not isinstance(gridprops, (xtgeo.GridProperty, xtgeo.GridProperties)):
-        raise ValueError('"gridprops" not a GridProperties or GridProperty instance')
-
-    if isinstance(gridprops, xtgeo.GridProperty):
-        gprops = xtgeo.GridProperties()
-        gprops.append_props([gridprops])
-    else:
-        gprops = gridprops
-
-    if isinstance(grid, tuple):
-        icl, jcl, kcl = grid
-    elif isinstance(grid, xtgeo.Grid):
-        self.make_ijk_from_grid(grid, grid_id="_tmp", algorithm=2)
-        icl, jcl, kcl = ("ICELL_tmp", "JCELL_tmp", "KCELL_tmp")
-    else:
-        raise ValueError('The "grid" is of wrong type, must be a tuple or ' "a Grid")
-
-    iind = self.dataframe[icl].values - 1
-    jind = self.dataframe[jcl].values - 1
-    kind = self.dataframe[kcl].values - 1
-
-    xind = iind.copy()
-
-    iind[np.isnan(iind)] = 0
-    jind[np.isnan(jind)] = 0
-    kind[np.isnan(kind)] = 0
-
-    #    iind = np.ma.masked_where(iind[~np.isnan(iind)].astype('int')
-    iind = iind.astype("int")
-    jind = jind.astype("int")
-    kind = kind.astype("int")
-
-    for prop in gprops.props:
-        arr = prop.values[iind, jind, kind].astype("float")
-        arr[np.isnan(xind)] = np.nan
-        pname = prop.name + prop_id
-        self.dataframe[pname] = arr
-        self._wlognames.append(pname)
-        if prop.isdiscrete:
-            self._wlogtypes[pname] = "DISC"
-            self._wlogrecords[pname] = copy.deepcopy(prop.codes)
-    self._ensure_consistency()
-    self.delete_logs(["ICELL_tmp", "JCELL_tmp", "KCELL_tmp"])
-
-
-def report_zonation_holes(self, threshold=5):
-    """Reports if well has holes in zonation, less or equal to N samples."""
-    # pylint: disable=too-many-branches, too-many-statements
-
-    if self.zonelogname is None:
-        raise RuntimeError("No zonelog present for well")
-
-    wellreport = []
-
-    zlog = self._df[self.zonelogname].values.copy()
-
-    mdlog = None
-    if self.mdlogname:
-        mdlog = self._df[self.mdlogname].values
-
-    xvv = self._df["X_UTME"].values
-    yvv = self._df["Y_UTMN"].values
-    zvv = self._df["Z_TVDSS"].values
-    zlog[np.isnan(zlog)] = const.UNDEF_INT
-
-    ncv = 0
-    first = True
-    hole = False
-    for ind, zone in np.ndenumerate(zlog):
-        ino = ind[0]
-        if zone > const.UNDEF_INT_LIMIT and first:
-            continue
-
-        if zone < const.UNDEF_INT_LIMIT and first:
-            first = False
-            continue
-
-        if zone > const.UNDEF_INT_LIMIT:
-            ncv += 1
-            hole = True
-
-        if zone > const.UNDEF_INT_LIMIT and ncv > threshold:
-            logger.info("Restart first (bigger hole)")
-            hole = False
-            first = True
-            ncv = 0
-            continue
-
-        if hole and zone < const.UNDEF_INT_LIMIT and ncv <= threshold:
-            # here we have a hole that fits criteria
-            if mdlog is not None:
-                entry = (
-                    ino,
-                    xvv[ino],
-                    yvv[ino],
-                    zvv[ino],
-                    int(zone),
-                    self.xwellname,
-                    mdlog[ino],
-                )
-            else:
-                entry = (ino, xvv[ino], yvv[ino], zvv[ino], int(zone), self.xwellname)
-
-            wellreport.append(entry)
-
-            # restart count
-            hole = False
-            ncv = 0
-
-        if hole and zone < const.UNDEF_INT_LIMIT and ncv > threshold:
-            hole = False
-            ncv = 0
-
-    if not wellreport:  # ie length is 0
-        return None
-
-    if mdlog is not None:
-        clm = ["INDEX", "X_UTME", "Y_UTMN", "Z_TVDSS", "Zone", "Well", "MD"]
-    else:
-        clm = ["INDEX", "X_UTME", "Y_UTMN", "Z_TVDSS", "Zone", "Well"]
-
-    return pd.DataFrame(wellreport, columns=clm)
-
-
-def mask_shoulderbeds(self, inputlogs, targetlogs, nsamples, strict):
-    """Mask targetlogs around discrete boundaries.
-
-    Returns True if inputlog(s) and targetlog(s) are present; otherwise False.
-    """
-    logger.info("Mask shoulderbeds for some logs...")
-
-    useinputs, usetargets, use_numeric = _mask_shoulderbeds_checks(
-        self, inputlogs, targetlogs, nsamples, strict
-    )
-
-    if not useinputs or not usetargets:
-        logger.info("Mask shoulderbeds for some logs... nothing done")
-        return False
-
-    for inlog in useinputs:
-        inseries = self._df[inlog]
-        if use_numeric:
-            bseries = _get_bseries(inseries, nsamples)
-        else:
-            mode, value = list(nsamples.items())[0]
-
-            depth = self._df["Z_TVDSS"]
-            if mode == "md" and self.mdlogname is not None:
-                depth = self._df[self.mdlogname]
-            elif mode == "md" and self.mdlogname is None:
-                raise ValueError("There is no mdlogname attribute present.")
-
-            bseries = _get_bseries_by_distance(depth, inseries, value)
-
-        for target in usetargets:
-            self._df.loc[bseries, target] = np.nan
-
-    logger.info("Mask shoulderbeds for some logs... done")
-    return True
-
-
-def _mask_shoulderbeds_checks(self, inputlogs, targetlogs, nsamples, strict):
-    """Checks/validates input for mask targetlogs around discrete boundaries."""
-    # check that inputlogs exists and that they are discrete, and targetlogs
-    useinputs = []
-    for inlog in inputlogs:
-        if inlog not in self._wlogtypes.keys() and strict is True:
-            raise ValueError(f"Input log {inlog} is missing and strict=True")
-        if inlog in self._wlogtypes.keys() and self._wlogtypes[inlog] != "DISC":
-            raise ValueError(f"Input log {inlog} is not of type DISC")
-        if inlog in self._wlogtypes.keys():
-            useinputs.append(inlog)
-
-    usetargets = []
-    for target in targetlogs:
-        if target not in self._wlogtypes.keys() and strict is True:
-            raise ValueError(f"Target log {target} is missing and strict=True")
-        if target in self._wlogtypes.keys():
-            usetargets.append(target)
-
-    use_numeric = True
-    if isinstance(nsamples, int):
-        maxlen = len(self._df) // 2
-        if nsamples < 1 or nsamples > maxlen:
-            raise ValueError(f"Keyword nsamples must be an int > 1 and < {maxlen}")
-    elif isinstance(nsamples, dict):
-        if len(nsamples) == 1 and any(key in nsamples.keys() for key in ["md", "tvd"]):
-            use_numeric = False
-        else:
-            raise ValueError(f"Keyword nsamples is incorrect in some way: {nsamples}")
-    else:
-        raise ValueError("Keyword nsamples is not an int or a dictionary")
-
-    # return a list of input logs to be used (useinputs), a list of target logs to
-    # be used (usetargets) and a use_numeric bool (True if nsamples is an int)
-    return useinputs, usetargets, use_numeric
-
-
-def _get_bseries(inseries, nsamples):
-    """Return a bool filter based on number of samples."""
-    if not isinstance(inseries, pd.Series):
-        raise RuntimeError("Bug, input must be a pandas Series() instance.")
-
-    if len(inseries) == 0:
-        return pd.Series([], dtype=bool)
-
-    # nsmaples < 1 or input series with <= 1 element will not be prosessed
-    if nsamples < 1 or len(inseries) <= 1:
-        return pd.Series(inseries, dtype=bool).replace(True, False)
-
-    def _growfilter(bseries, nleft):
-        if not nleft:
-            return bseries
-
-        return _growfilter(bseries | bseries.shift(-1) | bseries.shift(1), nleft - 1)
-
-    # make a tmp mask log (series) based input logs and use that for mask filterings
-    transitions = inseries.diff().abs() > 0
-    bseries = transitions | transitions.shift(-1)
-
-    return _growfilter(bseries, nsamples - 1)
-
-
-def _get_bseries_by_distance(depth, inseries, distance):
-    """Return a bool filter defined by distance to log breaks."""
-    if not isinstance(inseries, pd.Series):
-        raise RuntimeError("BUG: input must be a pandas Series() instance.")
-
-    if len(inseries) == 0:
-        return pd.Series([], dtype=bool)
-
-    # Input series with <= 1 element will not be prosessed
-    if len(inseries) <= 1:
-        return pd.Series(inseries, dtype=bool).replace(True, False)
-
-    bseries = pd.Series(np.zeros(inseries.values.size), dtype="int32").values
-    try:
-        inseries = np.nan_to_num(inseries.values, nan=xtgeo.UNDEF_INT).astype("int32")
-    except TypeError:
-        # for older numpy version
-        inseries = inseries.values
-        inseries[np.isnan(inseries)] = xtgeo.UNDEF_INT
-        inseries = inseries.astype("int32")
-
-    res = _cxtgeo.well_mask_shoulder(
-        depth.values.astype("float64"), inseries, bseries, distance
-    )
-
-    if res != 0:
-        raise RuntimeError("BUG: return from _cxtgeo.well_mask_shoulder not zero")
-
-    res = np.array(bseries, dtype=bool)
-    return res
-
-
-def create_surf_distance_log(self, surf, name):
-    """Create a log which is vertical distance from a RegularSurface."""
-    logger.info("Create a log which is distance to surface")
-
-    if not isinstance(surf, xtgeo.RegularSurface):
-        raise ValueError("Input surface is not a RegularSurface instance.")
-
-    # make a Points instance since points has the snap
-    zvalues = self.dataframe["Z_TVDSS"]
-    points = xtgeo.Points()
-    points.dataframe = self.dataframe.iloc[:, 0:3]
-    points.snap_surface(surf)
-    snapped = points.dataframe["Z_TVDSS"]
-    diff = snapped - zvalues
-
-    # create log (default is force overwrite if it exists)
-    self.create_log(name)
-    self.dataframe[name] = diff
+"""Operations along a well, private module."""
+
+import copy
+
+import numpy as np
+import pandas as pd
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.common import constants as const
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def delete_log(self, lname):
+    """Delete/remove an existing log, or list of logs."""
+    self._ensure_consistency()
+
+    if not isinstance(lname, list):
+        lname = [lname]
+
+    lcount = 0
+    for logn in lname:
+        if logn not in self._wlognames:
+            logger.info("Log does no exist: %s", logn)
+            continue
+
+        logger.info("Log exist and will be deleted: %s", logn)
+        lcount += 1
+        del self._wlogtypes[logn]
+        del self._wlogrecords[logn]
+
+        self._df.drop(logn, axis=1, inplace=True)
+        self._ensure_consistency()
+
+        if self._mdlogname == logn:
+            self._mdlogname = None
+        if self._zonelogname == logn:
+            self._zonelogname = None
+
+    self._ensure_consistency()
+    return lcount
+
+
+def rescale(self, delta=0.15, tvdrange=None):
+    """Rescale by using a new MD increment.
+
+    The rescaling is technically done by interpolation in the Pandas dataframe
+    """
+    pdrows = pd.options.display.max_rows
+    pd.options.display.max_rows = 999
+
+    dfrcolumns0 = self._df.columns
+
+    if self.mdlogname is None:
+        self.geometrics()
+
+    dfrcolumns1 = self._df.columns
+    columnsadded = list(set(dfrcolumns1) - set(dfrcolumns0))  # new tmp columns, if any
+
+    dfr = self._df.copy().set_index(self.mdlogname)
+
+    logger.debug("Initial dataframe\n %s", dfr)
+
+    start = dfr.index[0]
+    stop = dfr.index[-1]
+    startt = start
+    stopt = stop
+
+    if tvdrange and isinstance(tvdrange, tuple) and len(tvdrange) == 2:
+        tvd1, tvd2 = tvdrange
+
+        try:
+            startt = dfr.index[dfr["Z_TVDSS"] >= tvd1][0]
+        except IndexError:
+            startt = start
+
+        try:
+            stopt = dfr.index[dfr["Z_TVDSS"] >= tvd2][0]
+        except IndexError:
+            stopt = stop
+
+    dfr1 = dfr[start:startt]
+    dfr2 = dfr[stopt:stop]
+
+    nentry = int(round((stopt - startt) / delta))
+
+    dfr = dfr.reindex(dfr.index.union(np.linspace(startt, stopt, num=nentry)))
+    dfr = dfr.interpolate("index", limit_area="inside").loc[
+        np.linspace(startt, stopt, num=nentry)
+    ]
+
+    dfr = pd.concat([dfr1, dfr, dfr2], sort=False)
+
+    dfr.drop_duplicates(inplace=True)
+    dfr[self.mdlogname] = dfr.index
+    dfr.reset_index(inplace=True, drop=True)
+
+    for lname in dfr.columns:
+        if lname in self._wlogtypes:
+            ltype = self._wlogtypes[lname]
+            if ltype == "DISC":
+                dfr = dfr.round({lname: 0})
+
+    logger.debug("Updated dataframe:\n%s", dfr)
+
+    pd.options.display.max_rows = pdrows  # reset
+
+    self._df = dfr
+    if columnsadded:
+        self.delete_log(columnsadded)
+
+
+def make_zone_qual_log(self, zqname):
+    """Make a flag log based on stratigraphic relations."""
+    if zqname in self.dataframe:
+        logger.warning("Quality log %s exists, will be overwritten", zqname)
+
+    if not self.zonelogname or self.zonelogname not in self.dataframe:
+        raise ValueError("Cannot find a zonelog")
+
+    dff = self.get_filled_dataframe()
+    dff["ztmp"] = dff[self.zonelogname]
+    dff["ztmp"] = (dff.ztmp != dff.ztmp.shift()).cumsum()
+
+    sgrp = dff.groupby("ztmp")
+
+    dff[zqname] = dff[self.zonelogname] * 0
+
+    idlist = list()
+    seq = list()
+    for idx, grp in sgrp:
+        izns = int(grp[self.zonelogname].mean())
+        seq.append(izns)
+        idlist.append(idx)
+
+    codes = {
+        0: "UNDETERMINED",
+        1: "INCREASE",
+        2: "DECREASE",
+        3: "U_TURN",
+        4: "INV_U_TURN",
+        9: "INCOMPLETE",
+    }
+
+    code = list()
+    for ind, iseq in enumerate(seq):
+        if ind in (0, len(seq) - 1):
+            code.append(0)
+        else:
+            prev_ = seq[ind - 1]
+            next_ = seq[ind + 1]
+            if prev_ > const.UNDEF_INT_LIMIT or next_ > const.UNDEF_INT_LIMIT:
+                code.append(9)
+            elif next_ > iseq > prev_:
+                code.append(1)
+            elif next_ < iseq < prev_:
+                code.append(2)
+            elif next_ < iseq > prev_:
+                code.append(3)
+            elif next_ > iseq < prev_:
+                code.append(4)
+    dcode = dict(zip(idlist, code))
+
+    # now create the new log
+    self.create_log(zqname, logtype="DISC", logrecord=codes)
+    for key, val in dcode.items():
+        self._df[zqname][dff["ztmp"] == key] = val
+
+    # set the metadata
+    self.set_logtype(zqname, "DISC")
+    self.set_logrecord(zqname, codes)
+
+    del dff
+
+
+def make_ijk_from_grid(self, grid, grid_id="", algorithm=1, activeonly=True):
+    """Make an IJK log from grid indices."""
+    logger.info("Using algorithm %s in %s", algorithm, __name__)
+
+    if algorithm == 1:
+        _make_ijk_from_grid_v1(self, grid, grid_id=grid_id)
+    else:
+        _make_ijk_from_grid_v2(self, grid, grid_id=grid_id, activeonly=activeonly)
+
+    logger.info("Using algorithm %s in %s done", algorithm, __name__)
+
+
+def _make_ijk_from_grid_v1(self, grid, grid_id=""):
+    """Getting IJK from a grid and make as well logs.
+
+    This is the first version, using _cxtgeo.grd3d_well_ijk from C
+    """
+    logger.info("Using algorithm 1 in %s", __name__)
+
+    wxarr = self.get_carray("X_UTME")
+    wyarr = self.get_carray("Y_UTMN")
+    wzarr = self.get_carray("Z_TVDSS")
+
+    nlen = self.nrow
+    wivec = _cxtgeo.new_intarray(nlen)
+    wjvec = _cxtgeo.new_intarray(nlen)
+    wkvec = _cxtgeo.new_intarray(nlen)
+
+    onelayergrid = grid.copy()
+    onelayergrid.reduce_to_one_layer()
+
+    cstatus = _cxtgeo.grd3d_well_ijk(
+        grid.ncol,
+        grid.nrow,
+        grid.nlay,
+        grid._coordsv,
+        grid._zcornsv,
+        grid._actnumsv,
+        onelayergrid._zcornsv,
+        onelayergrid._actnumsv,
+        self.nrow,
+        wxarr,
+        wyarr,
+        wzarr,
+        wivec,
+        wjvec,
+        wkvec,
+        0,
+    )
+
+    if cstatus != 0:
+        raise RuntimeError(f"Error from C routine, code is {cstatus}")
+
+    indarray = _cxtgeo.swig_carr_to_numpy_i1d(nlen, wivec).astype("float")
+    jndarray = _cxtgeo.swig_carr_to_numpy_i1d(nlen, wjvec).astype("float")
+    kndarray = _cxtgeo.swig_carr_to_numpy_i1d(nlen, wkvec).astype("float")
+
+    indarray[indarray == 0] = np.nan
+    jndarray[jndarray == 0] = np.nan
+    kndarray[kndarray == 0] = np.nan
+
+    icellname = "ICELL" + grid_id
+    jcellname = "JCELL" + grid_id
+    kcellname = "KCELL" + grid_id
+
+    self._df[icellname] = indarray
+    self._df[jcellname] = jndarray
+    self._df[kcellname] = kndarray
+
+    for cellname in [icellname, jcellname, kcellname]:
+        self._wlogtypes[cellname] = "DISC"
+
+    self._wlogrecords[icellname] = {ncel: str(ncel) for ncel in range(1, grid.ncol + 1)}
+    self._wlogrecords[jcellname] = {ncel: str(ncel) for ncel in range(1, grid.nrow + 1)}
+    self._wlogrecords[kcellname] = {ncel: str(ncel) for ncel in range(1, grid.nlay + 1)}
+
+    _cxtgeo.delete_intarray(wivec)
+    _cxtgeo.delete_intarray(wjvec)
+    _cxtgeo.delete_intarray(wkvec)
+    _cxtgeo.delete_doublearray(wxarr)
+    _cxtgeo.delete_doublearray(wyarr)
+    _cxtgeo.delete_doublearray(wzarr)
+
+    del onelayergrid
+
+
+def _make_ijk_from_grid_v2(self, grid, grid_id="", activeonly=True):
+    """Getting IJK from a grid and make as well logs.
+
+    This is a newer version, using grid.get_ijk_from_points which in turn
+    use the from C method x_chk_point_in_hexahedron, while v1 use the
+    x_chk_point_in_cell. This one is believed to be more precise!
+    """
+    # establish a Points instance and make points dataframe from well trajectory X Y Z
+    wpoints = xtgeo.Points()
+    wpdf = self.dataframe.loc[:, ["X_UTME", "Y_UTMN", "Z_TVDSS"]].copy()
+    wpoints.dataframe = wpdf
+    wpoints.dataframe.reset_index(inplace=True, drop=True)
+
+    # column names
+    cna = ("ICELL" + grid_id, "JCELL" + grid_id, "KCELL" + grid_id)
+
+    df = grid.get_ijk_from_points(
+        wpoints,
+        activeonly=activeonly,
+        zerobased=False,
+        dataframe=True,
+        includepoints=False,
+        columnnames=cna,
+        fmt="float",
+        undef=np.nan,
+    )
+
+    # The resulting df shall have same length as the well's dataframe,
+    # but the well index may not start from one. So first ignore index, then
+    # re-establish
+    wellindex = self.dataframe.index
+
+    newdf = pd.concat([self.dataframe.reset_index(drop=True), df], axis=1)
+    newdf.index = wellindex
+
+    self.dataframe = newdf
+
+
+def get_gridproperties(self, gridprops, grid=("ICELL", "JCELL", "KCELL"), prop_id=""):
+    """Getting gridproperties as logs."""
+    if not isinstance(gridprops, (xtgeo.GridProperty, xtgeo.GridProperties)):
+        raise ValueError('"gridprops" not a GridProperties or GridProperty instance')
+
+    if isinstance(gridprops, xtgeo.GridProperty):
+        gprops = xtgeo.GridProperties()
+        gprops.append_props([gridprops])
+    else:
+        gprops = gridprops
+
+    if isinstance(grid, tuple):
+        icl, jcl, kcl = grid
+    elif isinstance(grid, xtgeo.Grid):
+        self.make_ijk_from_grid(grid, grid_id="_tmp", algorithm=2)
+        icl, jcl, kcl = ("ICELL_tmp", "JCELL_tmp", "KCELL_tmp")
+    else:
+        raise ValueError('The "grid" is of wrong type, must be a tuple or ' "a Grid")
+
+    iind = self.dataframe[icl].values - 1
+    jind = self.dataframe[jcl].values - 1
+    kind = self.dataframe[kcl].values - 1
+
+    xind = iind.copy()
+
+    iind[np.isnan(iind)] = 0
+    jind[np.isnan(jind)] = 0
+    kind[np.isnan(kind)] = 0
+
+    #    iind = np.ma.masked_where(iind[~np.isnan(iind)].astype('int')
+    iind = iind.astype("int")
+    jind = jind.astype("int")
+    kind = kind.astype("int")
+
+    for prop in gprops.props:
+        arr = prop.values[iind, jind, kind].astype("float")
+        arr[np.isnan(xind)] = np.nan
+        pname = prop.name + prop_id
+        self.dataframe[pname] = arr
+        self._wlognames.append(pname)
+        if prop.isdiscrete:
+            self._wlogtypes[pname] = "DISC"
+            self._wlogrecords[pname] = copy.deepcopy(prop.codes)
+    self._ensure_consistency()
+    self.delete_logs(["ICELL_tmp", "JCELL_tmp", "KCELL_tmp"])
+
+
+def report_zonation_holes(self, threshold=5):
+    """Reports if well has holes in zonation, less or equal to N samples."""
+    # pylint: disable=too-many-branches, too-many-statements
+
+    if self.zonelogname is None:
+        raise RuntimeError("No zonelog present for well")
+
+    wellreport = []
+
+    zlog = self._df[self.zonelogname].values.copy()
+
+    mdlog = None
+    if self.mdlogname:
+        mdlog = self._df[self.mdlogname].values
+
+    xvv = self._df["X_UTME"].values
+    yvv = self._df["Y_UTMN"].values
+    zvv = self._df["Z_TVDSS"].values
+    zlog[np.isnan(zlog)] = const.UNDEF_INT
+
+    ncv = 0
+    first = True
+    hole = False
+    for ind, zone in np.ndenumerate(zlog):
+        ino = ind[0]
+        if zone > const.UNDEF_INT_LIMIT and first:
+            continue
+
+        if zone < const.UNDEF_INT_LIMIT and first:
+            first = False
+            continue
+
+        if zone > const.UNDEF_INT_LIMIT:
+            ncv += 1
+            hole = True
+
+        if zone > const.UNDEF_INT_LIMIT and ncv > threshold:
+            logger.info("Restart first (bigger hole)")
+            hole = False
+            first = True
+            ncv = 0
+            continue
+
+        if hole and zone < const.UNDEF_INT_LIMIT and ncv <= threshold:
+            # here we have a hole that fits criteria
+            if mdlog is not None:
+                entry = (
+                    ino,
+                    xvv[ino],
+                    yvv[ino],
+                    zvv[ino],
+                    int(zone),
+                    self.xwellname,
+                    mdlog[ino],
+                )
+            else:
+                entry = (ino, xvv[ino], yvv[ino], zvv[ino], int(zone), self.xwellname)
+
+            wellreport.append(entry)
+
+            # restart count
+            hole = False
+            ncv = 0
+
+        if hole and zone < const.UNDEF_INT_LIMIT and ncv > threshold:
+            hole = False
+            ncv = 0
+
+    if not wellreport:  # ie length is 0
+        return None
+
+    if mdlog is not None:
+        clm = ["INDEX", "X_UTME", "Y_UTMN", "Z_TVDSS", "Zone", "Well", "MD"]
+    else:
+        clm = ["INDEX", "X_UTME", "Y_UTMN", "Z_TVDSS", "Zone", "Well"]
+
+    return pd.DataFrame(wellreport, columns=clm)
+
+
+def mask_shoulderbeds(self, inputlogs, targetlogs, nsamples, strict):
+    """Mask targetlogs around discrete boundaries.
+
+    Returns True if inputlog(s) and targetlog(s) are present; otherwise False.
+    """
+    logger.info("Mask shoulderbeds for some logs...")
+
+    useinputs, usetargets, use_numeric = _mask_shoulderbeds_checks(
+        self, inputlogs, targetlogs, nsamples, strict
+    )
+
+    if not useinputs or not usetargets:
+        logger.info("Mask shoulderbeds for some logs... nothing done")
+        return False
+
+    for inlog in useinputs:
+        inseries = self._df[inlog]
+        if use_numeric:
+            bseries = _get_bseries(inseries, nsamples)
+        else:
+            mode, value = list(nsamples.items())[0]
+
+            depth = self._df["Z_TVDSS"]
+            if mode == "md" and self.mdlogname is not None:
+                depth = self._df[self.mdlogname]
+            elif mode == "md" and self.mdlogname is None:
+                raise ValueError("There is no mdlogname attribute present.")
+
+            bseries = _get_bseries_by_distance(depth, inseries, value)
+
+        for target in usetargets:
+            self._df.loc[bseries, target] = np.nan
+
+    logger.info("Mask shoulderbeds for some logs... done")
+    return True
+
+
+def _mask_shoulderbeds_checks(self, inputlogs, targetlogs, nsamples, strict):
+    """Checks/validates input for mask targetlogs around discrete boundaries."""
+    # check that inputlogs exists and that they are discrete, and targetlogs
+    useinputs = []
+    for inlog in inputlogs:
+        if inlog not in self._wlogtypes.keys() and strict is True:
+            raise ValueError(f"Input log {inlog} is missing and strict=True")
+        if inlog in self._wlogtypes.keys() and self._wlogtypes[inlog] != "DISC":
+            raise ValueError(f"Input log {inlog} is not of type DISC")
+        if inlog in self._wlogtypes.keys():
+            useinputs.append(inlog)
+
+    usetargets = []
+    for target in targetlogs:
+        if target not in self._wlogtypes.keys() and strict is True:
+            raise ValueError(f"Target log {target} is missing and strict=True")
+        if target in self._wlogtypes.keys():
+            usetargets.append(target)
+
+    use_numeric = True
+    if isinstance(nsamples, int):
+        maxlen = len(self._df) // 2
+        if nsamples < 1 or nsamples > maxlen:
+            raise ValueError(f"Keyword nsamples must be an int > 1 and < {maxlen}")
+    elif isinstance(nsamples, dict):
+        if len(nsamples) == 1 and any(key in nsamples.keys() for key in ["md", "tvd"]):
+            use_numeric = False
+        else:
+            raise ValueError(f"Keyword nsamples is incorrect in some way: {nsamples}")
+    else:
+        raise ValueError("Keyword nsamples is not an int or a dictionary")
+
+    # return a list of input logs to be used (useinputs), a list of target logs to
+    # be used (usetargets) and a use_numeric bool (True if nsamples is an int)
+    return useinputs, usetargets, use_numeric
+
+
+def _get_bseries(inseries, nsamples):
+    """Return a bool filter based on number of samples."""
+    if not isinstance(inseries, pd.Series):
+        raise RuntimeError("Bug, input must be a pandas Series() instance.")
+
+    if len(inseries) == 0:
+        return pd.Series([], dtype=bool)
+
+    # nsmaples < 1 or input series with <= 1 element will not be prosessed
+    if nsamples < 1 or len(inseries) <= 1:
+        return pd.Series(inseries, dtype=bool).replace(True, False)
+
+    def _growfilter(bseries, nleft):
+        if not nleft:
+            return bseries
+
+        return _growfilter(bseries | bseries.shift(-1) | bseries.shift(1), nleft - 1)
+
+    # make a tmp mask log (series) based input logs and use that for mask filterings
+    transitions = inseries.diff().abs() > 0
+    bseries = transitions | transitions.shift(-1)
+
+    return _growfilter(bseries, nsamples - 1)
+
+
+def _get_bseries_by_distance(depth, inseries, distance):
+    """Return a bool filter defined by distance to log breaks."""
+    if not isinstance(inseries, pd.Series):
+        raise RuntimeError("BUG: input must be a pandas Series() instance.")
+
+    if len(inseries) == 0:
+        return pd.Series([], dtype=bool)
+
+    # Input series with <= 1 element will not be prosessed
+    if len(inseries) <= 1:
+        return pd.Series(inseries, dtype=bool).replace(True, False)
+
+    bseries = pd.Series(np.zeros(inseries.values.size), dtype="int32").values
+    try:
+        inseries = np.nan_to_num(inseries.values, nan=xtgeo.UNDEF_INT).astype("int32")
+    except TypeError:
+        # for older numpy version
+        inseries = inseries.values
+        inseries[np.isnan(inseries)] = xtgeo.UNDEF_INT
+        inseries = inseries.astype("int32")
+
+    res = _cxtgeo.well_mask_shoulder(
+        depth.values.astype("float64"), inseries, bseries, distance
+    )
+
+    if res != 0:
+        raise RuntimeError("BUG: return from _cxtgeo.well_mask_shoulder not zero")
+
+    res = np.array(bseries, dtype=bool)
+    return res
+
+
+def create_surf_distance_log(self, surf, name):
+    """Create a log which is vertical distance from a RegularSurface."""
+    logger.info("Create a log which is distance to surface")
+
+    if not isinstance(surf, xtgeo.RegularSurface):
+        raise ValueError("Input surface is not a RegularSurface instance.")
+
+    # make a Points instance since points has the snap
+    zvalues = self.dataframe["Z_TVDSS"]
+    points = xtgeo.Points()
+    points.dataframe = self.dataframe.iloc[:, 0:3]
+    points.snap_surface(surf)
+    snapped = points.dataframe["Z_TVDSS"]
+    diff = snapped - zvalues
+
+    # create log (default is force overwrite if it exists)
+    self.create_log(name)
+    self.dataframe[name] = diff
```

## xtgeo/well/_well_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,294 +1,294 @@
-# -*- coding: utf-8 -*-
-"""Well input and output, private module for ROXAPI."""
-
-
-from collections import OrderedDict
-
-import numpy as np
-import numpy.ma as npma
-import pandas as pd
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-from xtgeo.roxutils import RoxUtils
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-# Well() instance self = xwell1
-
-
-# Import from ROX api
-# --------------------------------------------------------------------------------------
-def import_well_roxapi(
-    project,
-    wname,
-    trajectory="Drilled trajectory",
-    logrun="log",
-    lognames="all",
-    lognames_strict=False,
-    inclmd=False,
-    inclsurvey=False,
-):  # pragma: no cover
-    """Private function for loading project and ROXAPI well import."""
-    rox = RoxUtils(project, readonly=True)
-
-    result = _roxapi_import_well(
-        rox,
-        wname,
-        trajectory,
-        logrun,
-        lognames,
-        lognames_strict,
-        inclmd,
-        inclsurvey,
-    )
-
-    rox.safe_close()
-    return result
-
-
-def _roxapi_import_well(
-    rox, wname, traj, lrun, lognames, lognames_strict, inclmd, inclsurvey
-):  # pragma: no cover
-    """Private function for ROXAPI well import."""
-    if wname in rox.project.wells:
-        roxwell = rox.project.wells[wname]
-    else:
-        raise ValueError(f"No such well name present: {wname}")
-
-    if traj in roxwell.wellbore.trajectories:
-        roxtraj = roxwell.wellbore.trajectories[traj]
-    else:
-        raise ValueError(f"No such well traj present for {wname}: {traj}")
-
-    if lrun in roxtraj.log_runs:
-        roxlrun = roxtraj.log_runs[lrun]
-    else:
-        raise ValueError(f"No such logrun present for {wname}: {lrun}")
-
-    wlogtypes = dict()
-    wlogrecords = dict()
-
-    # get logs repr trajecetry
-    mdlogname, logs = _roxapi_traj(
-        wlogtypes, wlogrecords, roxtraj, roxlrun, inclmd, inclsurvey
-    )
-
-    if lognames and lognames == "all":
-        for logcurv in roxlrun.log_curves:
-            lname = logcurv.name
-            logs[lname] = _get_roxlog(wlogtypes, wlogrecords, roxlrun, lname)
-    elif lognames:
-        for lname in lognames:
-            if lname in roxlrun.log_curves:
-                logs[lname] = _get_roxlog(wlogtypes, wlogrecords, roxlrun, lname)
-            else:
-                if lognames_strict:
-                    validlogs = [logname.name for logname in roxlrun.log_curves]
-                    raise ValueError(
-                        f"Could not get log name {lname}, validlogs are {validlogs}"
-                    )
-
-    return {
-        "rkb": roxwell.rkb,
-        "xpos": roxwell.wellhead[0],
-        "ypos": roxwell.wellhead[1],
-        "wname": wname,
-        "wlogtypes": wlogtypes,
-        "wlogrecords": wlogrecords,
-        "mdlogname": mdlogname,
-        "df": pd.DataFrame.from_dict(logs),
-    }
-
-
-def _roxapi_traj(
-    wlogtypes, wlogrecords, roxtraj, roxlrun, inclmd, inclsurvey
-):  # pragma: no cover
-    """Get trajectory in ROXAPI."""
-    # compute trajectory
-
-    surveyset = roxtraj.survey_point_series
-    measured_depths = roxlrun.get_measured_depths()
-
-    mds = measured_depths.tolist()
-
-    geo_array_shape = (len(measured_depths), 6)
-    geo_array = np.empty(geo_array_shape)
-
-    for ino, mdv in enumerate(mds):
-        try:
-            geo_array[ino] = surveyset.interpolate_survey_point(mdv)
-        except ValueError:
-            logger.warning("MD is %s, surveyinterpolation fails, " "CHECK RESULT!", mdv)
-            geo_array[ino] = geo_array[ino - 1]
-
-    logs = OrderedDict()
-    mdlogname = None
-
-    logs["X_UTME"] = geo_array[:, 3]
-    logs["Y_UTMN"] = geo_array[:, 4]
-    logs["Z_TVDSS"] = geo_array[:, 5]
-    if inclmd or inclsurvey:
-        logs["M_MDEPTH"] = geo_array[:, 0]
-        mdlogname = "M_MDEPTH"
-    if inclsurvey:
-        logs["M_INCL"] = geo_array[:, 1]
-        logs["M_AZI"] = geo_array[:, 2]
-
-    return mdlogname, logs
-
-
-def _get_roxlog(wlogtypes, wlogrecords, roxlrun, lname):  # pragma: no cover
-    roxcurve = roxlrun.log_curves[lname]
-    tmplog = roxcurve.get_values().astype(np.float64)
-    tmplog = npma.filled(tmplog, fill_value=np.nan)
-    tmplog[tmplog == -999] = np.nan
-    if roxcurve.is_discrete:
-        wlogtypes[lname] = "DISC"
-        wlogrecords[lname] = roxcurve.get_code_names()
-    else:
-        wlogtypes[lname] = "CONT"
-        wlogrecords[lname] = None
-
-    return tmplog
-
-
-def export_well_roxapi(
-    xwell1,
-    project,
-    wname,
-    lognames="all",
-    logrun="log",
-    trajectory="Drilled trajectory",
-    realisation=0,
-):
-    """Private function for well export (store in RMS) from XTGeo to RoxarAPI."""
-    logger.info("Opening RMS project ...")
-
-    rox = RoxUtils(project, readonly=False)
-
-    _roxapi_export_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation)
-
-    if rox._roxexternal:
-        rox.project.save()
-
-    rox.safe_close()
-
-
-def _roxapi_export_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation):
-    if wname in rox.project.wells:
-        _roxapi_update_well(
-            xwell1, rox, wname, lognames, logrun, trajectory, realisation
-        )
-    else:
-        _roxapi_create_well(
-            xwell1, rox, wname, lognames, logrun, trajectory, realisation
-        )
-
-
-def _roxapi_update_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation):
-    """Assume well is to updated only with logs, new or changed.
-
-    Also, the length of arrays should not change, at least not for now.
-
-    """
-    logger.info("Key realisation not in use: %s", realisation)
-
-    well = rox.project.wells[wname]
-    traj = well.wellbore.trajectories[trajectory]
-    lrun = traj.log_runs[logrun]
-
-    lrun.log_curves.clear()
-
-    if lognames == "all":
-        uselognames = xwell1.lognames
-    else:
-        uselognames = lognames
-
-    for lname in uselognames:
-        isdiscrete = False
-        xtglimit = xtgeo.UNDEF_LIMIT
-        if xwell1._wlogtypes[lname] == "DISC":
-            isdiscrete = True
-            xtglimit = xtgeo.UNDEF_INT_LIMIT
-
-        if isdiscrete:
-            thelog = lrun.log_curves.create_discrete(name=lname)
-        else:
-            thelog = lrun.log_curves.create(name=lname)
-
-        values = thelog.generate_values()
-
-        if values.size != xwell1.dataframe[lname].values.size:
-            raise ValueError("New logs have different sampling or size, not possible")
-
-        usedtype = values.dtype
-
-        vals = np.ma.masked_invalid(xwell1.dataframe[lname].values)
-        vals = np.ma.masked_greater(vals, xtglimit)
-        vals = vals.astype(usedtype)
-        thelog.set_values(vals)
-
-        if isdiscrete:
-            # roxarapi requires keys to int, while xtgeo can accept any, e.g. strings
-            if vals.mask.all():
-                codedict = {0: "unset"}
-            else:
-                codedict = {
-                    int(key): str(value)
-                    for key, value in xwell1._wlogrecords[lname].items()
-                }
-            thelog.set_code_names(codedict)
-
-
-def _roxapi_create_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation):
-    """Save Well() instance to a new well in RMS.
-
-    From version 2.15.
-    """
-    logger.debug("Key realisation is not supported: %s", realisation)
-
-    roxwell = rox.project.wells.create(wname)
-    roxwell.rkb = xwell1.rkb
-    roxwell.wellhead = (xwell1.xpos, xwell1.ypos)
-
-    traj = roxwell.wellbore.trajectories.create(trajectory)
-
-    series = traj.survey_point_series
-    east = xwell1.dataframe["X_UTME"].values
-    north = xwell1.dataframe["Y_UTMN"].values
-    tvd = xwell1.dataframe["Z_TVDSS"].values
-    values = np.array([east, north, tvd]).transpose()
-    series.set_points(values)
-
-    md = series.get_measured_depths_and_points()[:, 0]
-
-    lrun = traj.log_runs.create(logrun)
-    lrun.set_measured_depths(md)
-
-    # Add log curves
-    for curvename, curveprop in xwell1.get_wlogs().items():
-        if curvename not in xwell1.lognames:
-            continue  # skip X_UTME .. Z_TVDSS
-        if lognames and lognames != "all" and curvename not in lognames:
-            continue
-        if not lognames:
-            continue
-
-        cname = curvename
-        if curvename == "MD":
-            cname = "MD_imported"
-            xtg.warn(f"Logname MD is renamed to {cname}")
-
-        if curveprop[0] == "DISC":
-            lcurve = lrun.log_curves.create_discrete(cname)
-            cc = np.ma.masked_invalid(xwell1.dataframe[curvename].values)
-            lcurve.set_values(cc.astype(np.int32))
-            codedict = {int(key): str(value) for key, value in curveprop[1].items()}
-            lcurve.set_code_names(codedict)
-        else:
-            lcurve = lrun.log_curves.create(cname)
-            lcurve.set_values(xwell1.dataframe[curvename].values)
-
-        logger.info("Log curve created: %s", cname)
+# -*- coding: utf-8 -*-
+"""Well input and output, private module for ROXAPI."""
+
+
+from collections import OrderedDict
+
+import numpy as np
+import numpy.ma as npma
+import pandas as pd
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+from xtgeo.roxutils import RoxUtils
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+# Well() instance self = xwell1
+
+
+# Import from ROX api
+# --------------------------------------------------------------------------------------
+def import_well_roxapi(
+    project,
+    wname,
+    trajectory="Drilled trajectory",
+    logrun="log",
+    lognames="all",
+    lognames_strict=False,
+    inclmd=False,
+    inclsurvey=False,
+):  # pragma: no cover
+    """Private function for loading project and ROXAPI well import."""
+    rox = RoxUtils(project, readonly=True)
+
+    result = _roxapi_import_well(
+        rox,
+        wname,
+        trajectory,
+        logrun,
+        lognames,
+        lognames_strict,
+        inclmd,
+        inclsurvey,
+    )
+
+    rox.safe_close()
+    return result
+
+
+def _roxapi_import_well(
+    rox, wname, traj, lrun, lognames, lognames_strict, inclmd, inclsurvey
+):  # pragma: no cover
+    """Private function for ROXAPI well import."""
+    if wname in rox.project.wells:
+        roxwell = rox.project.wells[wname]
+    else:
+        raise ValueError(f"No such well name present: {wname}")
+
+    if traj in roxwell.wellbore.trajectories:
+        roxtraj = roxwell.wellbore.trajectories[traj]
+    else:
+        raise ValueError(f"No such well traj present for {wname}: {traj}")
+
+    if lrun in roxtraj.log_runs:
+        roxlrun = roxtraj.log_runs[lrun]
+    else:
+        raise ValueError(f"No such logrun present for {wname}: {lrun}")
+
+    wlogtypes = dict()
+    wlogrecords = dict()
+
+    # get logs repr trajecetry
+    mdlogname, logs = _roxapi_traj(
+        wlogtypes, wlogrecords, roxtraj, roxlrun, inclmd, inclsurvey
+    )
+
+    if lognames and lognames == "all":
+        for logcurv in roxlrun.log_curves:
+            lname = logcurv.name
+            logs[lname] = _get_roxlog(wlogtypes, wlogrecords, roxlrun, lname)
+    elif lognames:
+        for lname in lognames:
+            if lname in roxlrun.log_curves:
+                logs[lname] = _get_roxlog(wlogtypes, wlogrecords, roxlrun, lname)
+            else:
+                if lognames_strict:
+                    validlogs = [logname.name for logname in roxlrun.log_curves]
+                    raise ValueError(
+                        f"Could not get log name {lname}, validlogs are {validlogs}"
+                    )
+
+    return {
+        "rkb": roxwell.rkb,
+        "xpos": roxwell.wellhead[0],
+        "ypos": roxwell.wellhead[1],
+        "wname": wname,
+        "wlogtypes": wlogtypes,
+        "wlogrecords": wlogrecords,
+        "mdlogname": mdlogname,
+        "df": pd.DataFrame.from_dict(logs),
+    }
+
+
+def _roxapi_traj(
+    wlogtypes, wlogrecords, roxtraj, roxlrun, inclmd, inclsurvey
+):  # pragma: no cover
+    """Get trajectory in ROXAPI."""
+    # compute trajectory
+
+    surveyset = roxtraj.survey_point_series
+    measured_depths = roxlrun.get_measured_depths()
+
+    mds = measured_depths.tolist()
+
+    geo_array_shape = (len(measured_depths), 6)
+    geo_array = np.empty(geo_array_shape)
+
+    for ino, mdv in enumerate(mds):
+        try:
+            geo_array[ino] = surveyset.interpolate_survey_point(mdv)
+        except ValueError:
+            logger.warning("MD is %s, surveyinterpolation fails, " "CHECK RESULT!", mdv)
+            geo_array[ino] = geo_array[ino - 1]
+
+    logs = OrderedDict()
+    mdlogname = None
+
+    logs["X_UTME"] = geo_array[:, 3]
+    logs["Y_UTMN"] = geo_array[:, 4]
+    logs["Z_TVDSS"] = geo_array[:, 5]
+    if inclmd or inclsurvey:
+        logs["M_MDEPTH"] = geo_array[:, 0]
+        mdlogname = "M_MDEPTH"
+    if inclsurvey:
+        logs["M_INCL"] = geo_array[:, 1]
+        logs["M_AZI"] = geo_array[:, 2]
+
+    return mdlogname, logs
+
+
+def _get_roxlog(wlogtypes, wlogrecords, roxlrun, lname):  # pragma: no cover
+    roxcurve = roxlrun.log_curves[lname]
+    tmplog = roxcurve.get_values().astype(np.float64)
+    tmplog = npma.filled(tmplog, fill_value=np.nan)
+    tmplog[tmplog == -999] = np.nan
+    if roxcurve.is_discrete:
+        wlogtypes[lname] = "DISC"
+        wlogrecords[lname] = roxcurve.get_code_names()
+    else:
+        wlogtypes[lname] = "CONT"
+        wlogrecords[lname] = None
+
+    return tmplog
+
+
+def export_well_roxapi(
+    xwell1,
+    project,
+    wname,
+    lognames="all",
+    logrun="log",
+    trajectory="Drilled trajectory",
+    realisation=0,
+):
+    """Private function for well export (store in RMS) from XTGeo to RoxarAPI."""
+    logger.info("Opening RMS project ...")
+
+    rox = RoxUtils(project, readonly=False)
+
+    _roxapi_export_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation)
+
+    if rox._roxexternal:
+        rox.project.save()
+
+    rox.safe_close()
+
+
+def _roxapi_export_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation):
+    if wname in rox.project.wells:
+        _roxapi_update_well(
+            xwell1, rox, wname, lognames, logrun, trajectory, realisation
+        )
+    else:
+        _roxapi_create_well(
+            xwell1, rox, wname, lognames, logrun, trajectory, realisation
+        )
+
+
+def _roxapi_update_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation):
+    """Assume well is to updated only with logs, new or changed.
+
+    Also, the length of arrays should not change, at least not for now.
+
+    """
+    logger.info("Key realisation not in use: %s", realisation)
+
+    well = rox.project.wells[wname]
+    traj = well.wellbore.trajectories[trajectory]
+    lrun = traj.log_runs[logrun]
+
+    lrun.log_curves.clear()
+
+    if lognames == "all":
+        uselognames = xwell1.lognames
+    else:
+        uselognames = lognames
+
+    for lname in uselognames:
+        isdiscrete = False
+        xtglimit = xtgeo.UNDEF_LIMIT
+        if xwell1._wlogtypes[lname] == "DISC":
+            isdiscrete = True
+            xtglimit = xtgeo.UNDEF_INT_LIMIT
+
+        if isdiscrete:
+            thelog = lrun.log_curves.create_discrete(name=lname)
+        else:
+            thelog = lrun.log_curves.create(name=lname)
+
+        values = thelog.generate_values()
+
+        if values.size != xwell1.dataframe[lname].values.size:
+            raise ValueError("New logs have different sampling or size, not possible")
+
+        usedtype = values.dtype
+
+        vals = np.ma.masked_invalid(xwell1.dataframe[lname].values)
+        vals = np.ma.masked_greater(vals, xtglimit)
+        vals = vals.astype(usedtype)
+        thelog.set_values(vals)
+
+        if isdiscrete:
+            # roxarapi requires keys to int, while xtgeo can accept any, e.g. strings
+            if vals.mask.all():
+                codedict = {0: "unset"}
+            else:
+                codedict = {
+                    int(key): str(value)
+                    for key, value in xwell1._wlogrecords[lname].items()
+                }
+            thelog.set_code_names(codedict)
+
+
+def _roxapi_create_well(xwell1, rox, wname, lognames, logrun, trajectory, realisation):
+    """Save Well() instance to a new well in RMS.
+
+    From version 2.15.
+    """
+    logger.debug("Key realisation is not supported: %s", realisation)
+
+    roxwell = rox.project.wells.create(wname)
+    roxwell.rkb = xwell1.rkb
+    roxwell.wellhead = (xwell1.xpos, xwell1.ypos)
+
+    traj = roxwell.wellbore.trajectories.create(trajectory)
+
+    series = traj.survey_point_series
+    east = xwell1.dataframe["X_UTME"].values
+    north = xwell1.dataframe["Y_UTMN"].values
+    tvd = xwell1.dataframe["Z_TVDSS"].values
+    values = np.array([east, north, tvd]).transpose()
+    series.set_points(values)
+
+    md = series.get_measured_depths_and_points()[:, 0]
+
+    lrun = traj.log_runs.create(logrun)
+    lrun.set_measured_depths(md)
+
+    # Add log curves
+    for curvename, curveprop in xwell1.get_wlogs().items():
+        if curvename not in xwell1.lognames:
+            continue  # skip X_UTME .. Z_TVDSS
+        if lognames and lognames != "all" and curvename not in lognames:
+            continue
+        if not lognames:
+            continue
+
+        cname = curvename
+        if curvename == "MD":
+            cname = "MD_imported"
+            xtg.warn(f"Logname MD is renamed to {cname}")
+
+        if curveprop[0] == "DISC":
+            lcurve = lrun.log_curves.create_discrete(cname)
+            cc = np.ma.masked_invalid(xwell1.dataframe[curvename].values)
+            lcurve.set_values(cc.astype(np.int32))
+            codedict = {int(key): str(value) for key, value in curveprop[1].items()}
+            lcurve.set_code_names(codedict)
+        else:
+            lcurve = lrun.log_curves.create(cname)
+            lcurve.set_values(xwell1.dataframe[curvename].values)
+
+        logger.info("Log curve created: %s", cname)
```

## xtgeo/well/_wellmarkers.py

 * *Ordering differences only*

```diff
@@ -1,477 +1,477 @@
-# -*- coding: utf-8 -*-
-"""Well marker data; private module"""
-
-
-from collections import OrderedDict
-import numpy as np
-import pandas as pd
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-import xtgeo.common.constants as const
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def get_zonation_points(self, tops, incl_limit, top_prefix, zonelist, use_undef):
-    """
-    Getting zonation tops (private routine)
-
-    Args, see calling routine
-    """
-    # get the relevant logs:
-
-    self.geometrics()  # note the caller has made a copy of the true self
-
-    # as zlog is float64; need to convert to int array with high
-    # number as undef
-    if self.zonelogname is not None:
-        if use_undef:
-            self._df.dropna(subset=[self.zonelogname], inplace=True)
-        zlog = self._df[self.zonelogname].values
-        zlog[np.isnan(zlog)] = const.UNDEF_INT
-        zlog = np.rint(zlog).astype(int)
-    else:
-        return None
-
-    xvv = self._df["X_UTME"].values
-    yvv = self._df["Y_UTMN"].values
-    zvv = self._df["Z_TVDSS"].values
-    incl = self._df["Q_INCL"].values
-    mdv = self._df["Q_MDEPTH"].values
-
-    if self.mdlogname is not None:
-        mdv = self._df[self.mdlogname].values
-
-    if zonelist is None:
-        # need to declare as list; otherwise Py3 will get dict.keys
-        zonelist = list(self.get_logrecord(self.zonelogname).keys())
-
-    logger.info("Find values for %s", zonelist)
-
-    ztops, ztopnames, zisos, zisonames = _extract_ztops(
-        self,
-        zonelist,
-        xvv,
-        yvv,
-        zvv,
-        zlog,
-        mdv,
-        incl,
-        tops=tops,
-        incl_limit=incl_limit,
-        prefix=top_prefix,
-        use_undef=use_undef,
-    )
-
-    if tops:
-        zlist = ztops
-    else:
-        zlist = zisos
-
-    logger.debug(zlist)
-
-    if tops:
-        dfr = pd.DataFrame(zlist, columns=ztopnames)
-    else:
-        dfr = pd.DataFrame(zlist, columns=zisonames)
-
-    return dfr
-
-
-def _extract_ztops(
-    self,
-    zonelist,
-    xcv,
-    ycv,
-    zcv,
-    zlog,
-    mdv,
-    incl,
-    tops=True,
-    incl_limit=80,
-    prefix="Top",
-    use_undef=False,
-):
-    """Extract a list of tops for a zone.
-
-    Args:
-        zonelist (list-like): The zonelog list numbers to apply; either
-            as a list, or a tuple; 2 entries forms a range [start, stop]
-        xcv (np): X Position numpy array
-        ycv (np): Y Position numpy array
-        zcv (np): Z Position numpy array
-        zlog (np): Zonelog array
-        mdv (np): MDepth log numpy array
-        incl (np): Inclination log numpy array
-        tops (bool): Compute tops or thickness (zone) points (default True)
-        incl_limit (float): Limitation of zone computation (angle, degrees)
-        use_undef (bool): If True, then transition from UNDEF is also
-            used.
-    """
-    # pylint: disable=too-many-locals, too-many-branches, too-many-statements
-
-    # The wellpoints will be a list of tuples (one tuple per hit)
-    wpts = []
-    zlogname = self.zonelogname
-
-    if not tops and incl_limit is None:
-        incl_limit = 80
-
-    azi = -999.0  # tmp so far
-
-    if isinstance(zonelist, tuple):
-        if len(zonelist) != 2:
-            raise ValueError(
-                f"zonelist given as tuple must be of length 2, was {len(zonelist)}"
-            )
-        usezonerange = range(zonelist[0], zonelist[1] + 1)
-    elif isinstance(zonelist, list):
-        if len(zonelist) == 1:
-            raise ValueError(
-                f"zonelist given as list must contain two or more"
-                f" elements, had 1: {zonelist}"
-            )
-        usezonerange = zonelist
-    else:
-        raise TypeError(
-            f"zonelist must be either list (of two or more elements) or "
-            f"a tuple (with two elements representing start and stop), was"
-            f"{type(zonelist)}"
-        )
-
-    # check if increasing monotonic and with no jumps:
-    if not all(i + 1 == j for i, j in zip(usezonerange, usezonerange[1:])):
-        raise ValueError(
-            f"zonelist must be strictly increasing with increment of one,"
-            f" was {usezonerange}"
-        )
-
-    iundef = const.UNDEF_INT
-    iundeflimit = const.UNDEF_INT_LIMIT
-    pzone = iundef
-
-    if use_undef:
-        pzone = usezonerange[0] - 1
-
-    for ind, zone in np.ndenumerate(zlog):
-        ino = ind[0]  # since ind is a tuple...
-
-        if pzone != zone and pzone < iundeflimit and zone < iundeflimit:
-            logger.debug("Found break in zonation")
-            if pzone < zone:
-                logger.debug(
-                    "Found match, increasing zonation at %s < %s (MD %s)",
-                    pzone,
-                    zone,
-                    mdv[ino],
-                )
-                for kzv in range(pzone + 1, zone + 1):
-                    if kzv in usezonerange:
-                        zname = self.get_logrecord_codename(zlogname, kzv)
-                        zname = prefix + zname
-                        ztop = (
-                            xcv[ino],
-                            ycv[ino],
-                            zcv[ino],
-                            mdv[ino],
-                            incl[ino],
-                            azi,
-                            kzv,
-                            zname,
-                            self.xwellname,
-                        )
-                        wpts.append(ztop)
-            if pzone > zone and ino > 0:
-                logger.debug(
-                    "Found match, decreasing zonation at %s > %s (MD %s)",
-                    pzone,
-                    zone,
-                    mdv[ino - 1],
-                )
-                for kzv in range(pzone, zone, -1):
-                    if kzv in usezonerange:
-                        zname = self.get_logrecord_codename(zlogname, kzv)
-                        zname = prefix + zname
-                        ztop = (
-                            xcv[ino - 1],
-                            ycv[ino - 1],
-                            zcv[ino - 1],
-                            mdv[ino - 1],
-                            incl[ino - 1],
-                            azi,
-                            kzv,
-                            zname,
-                            self.xwellname,
-                        )
-                        wpts.append(ztop)
-        pzone = zone
-
-    wpts_names = [
-        "X_UTME",
-        "Y_UTMN",
-        "Z_TVDSS",
-        self.mdlogname,
-        "Q_INCL",
-        "Q_AZI",
-        "Zone",
-        "TopName",
-        "WellName",
-    ]
-
-    if tops:
-        return wpts, wpts_names, None, None
-
-    # next get a MIDPOINT zthickness (DZ)
-    llen = len(wpts) - 1
-
-    zwpts_names = [
-        "X_UTME",
-        "Y_UTMN",
-        "Z_TVDSS",
-        self.mdlogname + "_AVG",
-        "Q_MD1",
-        "Q_MD2",
-        "Q_INCL",
-        "Q_AZI",
-        "Zone",
-        "ZoneName",
-        "WellName",
-    ]
-
-    zwpts = []
-    for ino in range(llen):
-        i1v = ino
-        i2v = ino + 1
-        xx1, yy1, zz1, md1, incl1, _azi1, zk1, zn1, wn1 = wpts[i1v]
-        xx2, yy2, zz2, md2, incl2, _azi2, zk2, zn2, _wn2 = wpts[i2v]
-
-        # mid point
-        xx_avg = (xx1 + xx2) / 2
-        yy_avg = (yy1 + yy2) / 2
-        md_avg = (md1 + md2) / 2
-        incl_avg = (incl1 + incl2) / 2
-
-        azi_avg = -999.0  # to be fixed later
-
-        zzp = round(abs(zz2 - zz1), 4)
-
-        useok = False
-
-        if incl_avg < incl_limit:
-            useok = True
-
-        if useok and zk2 != zk1:
-            usezk = zk1
-            usezn = zn1
-            if zk1 > zk2:
-                usezk = zk2
-                usezn = zn2
-            usezn = usezn[len(prefix) :]
-
-            zzone = (
-                xx_avg,
-                yy_avg,
-                zzp,
-                md_avg,
-                md1,
-                md2,
-                incl_avg,
-                azi_avg,
-                usezk,
-                usezn,
-                wn1,
-            )
-            zwpts.append(zzone)
-
-    return wpts, wpts_names, zwpts, zwpts_names
-
-
-def get_fraction_per_zone(
-    self,
-    dlogname,
-    dvalues,
-    zonelist=None,
-    incl_limit=80,
-    count_limit=3,
-    zonelogname=None,
-):  # pylint: disable=too-many-branches, too-many-statements
-    """Fraction of e.g. a facies in a zone segment.
-
-        X_UTME       Y_UTMN    Z_TVDSS  Zonelog  Facies  M_INCL
-    464011.719  5931757.257  1663.1079      3.0     1.0    10.2
-    464011.751  5931757.271  1663.6084      3.0     1.0    10.3
-    464011.783  5931757.285  1664.1090      3.0     2.0    11.2
-    464011.815  5931757.299  1664.6097      3.0     2.0    11.4
-    464011.847  5931757.313  1665.1105      3.0     2.0    11.5
-    464011.879  5931757.326  1665.6114      3.0     2.0    12.0
-    464011.911  5931757.340  1666.1123      3.0     1.0    12.2
-    464011.943  5931757.354  1666.6134      3.0     1.0    13.4
-
-    Count fraction of one or more facies (dvalues list)
-    filtered on a zone, given that Inclination is below limit all over.
-    Since a zone can be repeated, it is important to split
-    into segments by POLY_ID. When fraction is determined, the
-    AVG X Y coord is applied.
-
-    If there are one or more occurences of undef for the dlogname
-    in that interval, no value shall be computed.
-
-    Args:
-        dlogname (str): Name of discrete log e.g. Facies
-        dvalues (list): List of codes to sum fraction upon
-        zonelist (list): List of zones to compute over
-        incl_limit (float): Skip if max inclination found > incl_limit
-        count_limit (int): Minimum number of samples required per segment
-
-    Returns:
-        A dataframe with relevant data...
-
-    """
-    logger.info("The zonelist is %s", zonelist)
-    logger.info("The dlogname is %s", dlogname)
-    logger.info("The dvalues are %s", dvalues)
-
-    if zonelogname is not None:
-        usezonelogname = zonelogname
-        self.zonelogname = zonelogname
-    else:
-        usezonelogname = self.zonelogname
-
-    if usezonelogname is None:
-        raise RuntimeError("Stop, zonelogname is None")
-
-    self.make_zone_qual_log("_QFLAG")
-
-    if zonelist is None:
-        # need to declare as list; otherwise Py3 will get dict.keys
-        zonelist = list(self.get_logrecord(self.zonelogname).keys())
-
-    useinclname = "Q_INCL"
-    if "M_INCL" in self._df:
-        useinclname = "M_INCL"
-    else:
-        self.geometrics()
-
-    result = OrderedDict()
-    result["X_UTME"] = []
-    result["Y_UTMN"] = []
-    result["DFRAC"] = []
-    result["Q_INCL"] = []
-    result["ZONE"] = []
-    result["WELLNAME"] = []
-    result[dlogname] = []
-
-    svalues = str(dvalues).rstrip("]").lstrip("[").replace(", ", "+")
-
-    xtralogs = [dlogname, useinclname, "_QFLAG"]
-    for izon in zonelist:
-        logger.info("The zone number is %s", izon)
-        logger.info("The extralogs are %s", xtralogs)
-
-        dfr = self.get_zone_interval(izon, extralogs=xtralogs)
-
-        if dfr is None:
-            continue
-
-        dfrx = dfr.groupby("POLY_ID")
-
-        for _polyid, dframe in dfrx:
-            qinclmax = dframe["Q_INCL"].max()
-            qinclavg = dframe["Q_INCL"].mean()
-            qflag = dframe["_QFLAG"].mean()
-            dseries = dframe[dlogname]
-            if qflag < 0.5 or qflag > 2.5:  # 1 or 2 is OK
-                logger.debug("Skipped due to zone %s", qflag)
-                continue
-            if qinclmax > incl_limit:
-                logger.debug("Skipped due to max inclination %s", qinclmax)
-                continue
-            if dseries.size < count_limit:  # interval too short for fraction
-                logger.debug("Skipped due to too few values %s", dseries.size)
-                continue
-            if dseries.max() > const.UNDEF_INT_LIMIT:
-                logger.debug("Skipped due to too missing/undef value(s)")
-                continue
-
-            xavg = dframe["X_UTME"].mean()
-            yavg = dframe["Y_UTMN"].mean()
-
-            dfrac = 0.0
-            for dval in dvalues:
-                if any(dseries.isin([dval])):
-                    dfrac += dseries.value_counts(normalize=True)[dval]
-
-            result["X_UTME"].append(xavg)
-            result["Y_UTMN"].append(yavg)
-            result["DFRAC"].append(dfrac)
-            result["Q_INCL"].append(qinclavg)
-            result["ZONE"].append(izon)
-            result["WELLNAME"].append(self.xwellname)
-            result[dlogname].append(svalues)
-
-    # make the dataframe and return it
-    if result["X_UTME"]:
-        return pd.DataFrame.from_dict(result)
-
-    self.delete_log("_QFLAG")
-
-    return None
-
-
-def get_surface_picks(self, surf):
-    """get Surface picks"""
-
-    xcor = self._df["X_UTME"].values
-    ycor = self._df["Y_UTMN"].values
-    zcor = self._df["Z_TVDSS"].values
-
-    if self.mdlogname:
-        mcor = self._df[self.mdlogname].values
-    else:
-        mcor = np.zeros(xcor.size, dtype=np.float64) + xtgeo.UNDEF
-
-    nval, xres, yres, zres, mres, dres = _cxtgeo.well_surf_picks(
-        xcor,
-        ycor,
-        zcor,
-        mcor,
-        surf.ncol,
-        surf.nrow,
-        surf.xori,
-        surf.yori,
-        surf.xinc,
-        surf.yinc,
-        surf.yflip,
-        surf.rotation,
-        surf.npvalues1d,
-        xcor.size,
-        xcor.size,
-        xcor.size,
-        xcor.size,
-        xcor.size,
-    )
-
-    if nval > 0:
-        poi = xtgeo.Points()
-
-        mres[mres > xtgeo.UNDEF_LIMIT] = np.nan
-
-        res = OrderedDict()
-        res[poi.xname] = xres[:nval]
-        res[poi.yname] = yres[:nval]
-        res[poi.zname] = zres[:nval]
-        if self.mdlogname:
-            res[self.mdlogname] = mres[:nval]
-        res["DIRECTION"] = dres[:nval]
-        res["WELLNAME"] = self.wellname
-
-        poi.dataframe = pd.DataFrame.from_dict(res)
-
-        return poi
-
-    return None
-
-    # return a xtgeo Poinst() object with points as dataframe, given that nval > 0
+# -*- coding: utf-8 -*-
+"""Well marker data; private module"""
+
+
+from collections import OrderedDict
+import numpy as np
+import pandas as pd
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+import xtgeo.common.constants as const
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def get_zonation_points(self, tops, incl_limit, top_prefix, zonelist, use_undef):
+    """
+    Getting zonation tops (private routine)
+
+    Args, see calling routine
+    """
+    # get the relevant logs:
+
+    self.geometrics()  # note the caller has made a copy of the true self
+
+    # as zlog is float64; need to convert to int array with high
+    # number as undef
+    if self.zonelogname is not None:
+        if use_undef:
+            self._df.dropna(subset=[self.zonelogname], inplace=True)
+        zlog = self._df[self.zonelogname].values
+        zlog[np.isnan(zlog)] = const.UNDEF_INT
+        zlog = np.rint(zlog).astype(int)
+    else:
+        return None
+
+    xvv = self._df["X_UTME"].values
+    yvv = self._df["Y_UTMN"].values
+    zvv = self._df["Z_TVDSS"].values
+    incl = self._df["Q_INCL"].values
+    mdv = self._df["Q_MDEPTH"].values
+
+    if self.mdlogname is not None:
+        mdv = self._df[self.mdlogname].values
+
+    if zonelist is None:
+        # need to declare as list; otherwise Py3 will get dict.keys
+        zonelist = list(self.get_logrecord(self.zonelogname).keys())
+
+    logger.info("Find values for %s", zonelist)
+
+    ztops, ztopnames, zisos, zisonames = _extract_ztops(
+        self,
+        zonelist,
+        xvv,
+        yvv,
+        zvv,
+        zlog,
+        mdv,
+        incl,
+        tops=tops,
+        incl_limit=incl_limit,
+        prefix=top_prefix,
+        use_undef=use_undef,
+    )
+
+    if tops:
+        zlist = ztops
+    else:
+        zlist = zisos
+
+    logger.debug(zlist)
+
+    if tops:
+        dfr = pd.DataFrame(zlist, columns=ztopnames)
+    else:
+        dfr = pd.DataFrame(zlist, columns=zisonames)
+
+    return dfr
+
+
+def _extract_ztops(
+    self,
+    zonelist,
+    xcv,
+    ycv,
+    zcv,
+    zlog,
+    mdv,
+    incl,
+    tops=True,
+    incl_limit=80,
+    prefix="Top",
+    use_undef=False,
+):
+    """Extract a list of tops for a zone.
+
+    Args:
+        zonelist (list-like): The zonelog list numbers to apply; either
+            as a list, or a tuple; 2 entries forms a range [start, stop]
+        xcv (np): X Position numpy array
+        ycv (np): Y Position numpy array
+        zcv (np): Z Position numpy array
+        zlog (np): Zonelog array
+        mdv (np): MDepth log numpy array
+        incl (np): Inclination log numpy array
+        tops (bool): Compute tops or thickness (zone) points (default True)
+        incl_limit (float): Limitation of zone computation (angle, degrees)
+        use_undef (bool): If True, then transition from UNDEF is also
+            used.
+    """
+    # pylint: disable=too-many-locals, too-many-branches, too-many-statements
+
+    # The wellpoints will be a list of tuples (one tuple per hit)
+    wpts = []
+    zlogname = self.zonelogname
+
+    if not tops and incl_limit is None:
+        incl_limit = 80
+
+    azi = -999.0  # tmp so far
+
+    if isinstance(zonelist, tuple):
+        if len(zonelist) != 2:
+            raise ValueError(
+                f"zonelist given as tuple must be of length 2, was {len(zonelist)}"
+            )
+        usezonerange = range(zonelist[0], zonelist[1] + 1)
+    elif isinstance(zonelist, list):
+        if len(zonelist) == 1:
+            raise ValueError(
+                f"zonelist given as list must contain two or more"
+                f" elements, had 1: {zonelist}"
+            )
+        usezonerange = zonelist
+    else:
+        raise TypeError(
+            f"zonelist must be either list (of two or more elements) or "
+            f"a tuple (with two elements representing start and stop), was"
+            f"{type(zonelist)}"
+        )
+
+    # check if increasing monotonic and with no jumps:
+    if not all(i + 1 == j for i, j in zip(usezonerange, usezonerange[1:])):
+        raise ValueError(
+            f"zonelist must be strictly increasing with increment of one,"
+            f" was {usezonerange}"
+        )
+
+    iundef = const.UNDEF_INT
+    iundeflimit = const.UNDEF_INT_LIMIT
+    pzone = iundef
+
+    if use_undef:
+        pzone = usezonerange[0] - 1
+
+    for ind, zone in np.ndenumerate(zlog):
+        ino = ind[0]  # since ind is a tuple...
+
+        if pzone != zone and pzone < iundeflimit and zone < iundeflimit:
+            logger.debug("Found break in zonation")
+            if pzone < zone:
+                logger.debug(
+                    "Found match, increasing zonation at %s < %s (MD %s)",
+                    pzone,
+                    zone,
+                    mdv[ino],
+                )
+                for kzv in range(pzone + 1, zone + 1):
+                    if kzv in usezonerange:
+                        zname = self.get_logrecord_codename(zlogname, kzv)
+                        zname = prefix + zname
+                        ztop = (
+                            xcv[ino],
+                            ycv[ino],
+                            zcv[ino],
+                            mdv[ino],
+                            incl[ino],
+                            azi,
+                            kzv,
+                            zname,
+                            self.xwellname,
+                        )
+                        wpts.append(ztop)
+            if pzone > zone and ino > 0:
+                logger.debug(
+                    "Found match, decreasing zonation at %s > %s (MD %s)",
+                    pzone,
+                    zone,
+                    mdv[ino - 1],
+                )
+                for kzv in range(pzone, zone, -1):
+                    if kzv in usezonerange:
+                        zname = self.get_logrecord_codename(zlogname, kzv)
+                        zname = prefix + zname
+                        ztop = (
+                            xcv[ino - 1],
+                            ycv[ino - 1],
+                            zcv[ino - 1],
+                            mdv[ino - 1],
+                            incl[ino - 1],
+                            azi,
+                            kzv,
+                            zname,
+                            self.xwellname,
+                        )
+                        wpts.append(ztop)
+        pzone = zone
+
+    wpts_names = [
+        "X_UTME",
+        "Y_UTMN",
+        "Z_TVDSS",
+        self.mdlogname,
+        "Q_INCL",
+        "Q_AZI",
+        "Zone",
+        "TopName",
+        "WellName",
+    ]
+
+    if tops:
+        return wpts, wpts_names, None, None
+
+    # next get a MIDPOINT zthickness (DZ)
+    llen = len(wpts) - 1
+
+    zwpts_names = [
+        "X_UTME",
+        "Y_UTMN",
+        "Z_TVDSS",
+        self.mdlogname + "_AVG",
+        "Q_MD1",
+        "Q_MD2",
+        "Q_INCL",
+        "Q_AZI",
+        "Zone",
+        "ZoneName",
+        "WellName",
+    ]
+
+    zwpts = []
+    for ino in range(llen):
+        i1v = ino
+        i2v = ino + 1
+        xx1, yy1, zz1, md1, incl1, _azi1, zk1, zn1, wn1 = wpts[i1v]
+        xx2, yy2, zz2, md2, incl2, _azi2, zk2, zn2, _wn2 = wpts[i2v]
+
+        # mid point
+        xx_avg = (xx1 + xx2) / 2
+        yy_avg = (yy1 + yy2) / 2
+        md_avg = (md1 + md2) / 2
+        incl_avg = (incl1 + incl2) / 2
+
+        azi_avg = -999.0  # to be fixed later
+
+        zzp = round(abs(zz2 - zz1), 4)
+
+        useok = False
+
+        if incl_avg < incl_limit:
+            useok = True
+
+        if useok and zk2 != zk1:
+            usezk = zk1
+            usezn = zn1
+            if zk1 > zk2:
+                usezk = zk2
+                usezn = zn2
+            usezn = usezn[len(prefix) :]
+
+            zzone = (
+                xx_avg,
+                yy_avg,
+                zzp,
+                md_avg,
+                md1,
+                md2,
+                incl_avg,
+                azi_avg,
+                usezk,
+                usezn,
+                wn1,
+            )
+            zwpts.append(zzone)
+
+    return wpts, wpts_names, zwpts, zwpts_names
+
+
+def get_fraction_per_zone(
+    self,
+    dlogname,
+    dvalues,
+    zonelist=None,
+    incl_limit=80,
+    count_limit=3,
+    zonelogname=None,
+):  # pylint: disable=too-many-branches, too-many-statements
+    """Fraction of e.g. a facies in a zone segment.
+
+        X_UTME       Y_UTMN    Z_TVDSS  Zonelog  Facies  M_INCL
+    464011.719  5931757.257  1663.1079      3.0     1.0    10.2
+    464011.751  5931757.271  1663.6084      3.0     1.0    10.3
+    464011.783  5931757.285  1664.1090      3.0     2.0    11.2
+    464011.815  5931757.299  1664.6097      3.0     2.0    11.4
+    464011.847  5931757.313  1665.1105      3.0     2.0    11.5
+    464011.879  5931757.326  1665.6114      3.0     2.0    12.0
+    464011.911  5931757.340  1666.1123      3.0     1.0    12.2
+    464011.943  5931757.354  1666.6134      3.0     1.0    13.4
+
+    Count fraction of one or more facies (dvalues list)
+    filtered on a zone, given that Inclination is below limit all over.
+    Since a zone can be repeated, it is important to split
+    into segments by POLY_ID. When fraction is determined, the
+    AVG X Y coord is applied.
+
+    If there are one or more occurences of undef for the dlogname
+    in that interval, no value shall be computed.
+
+    Args:
+        dlogname (str): Name of discrete log e.g. Facies
+        dvalues (list): List of codes to sum fraction upon
+        zonelist (list): List of zones to compute over
+        incl_limit (float): Skip if max inclination found > incl_limit
+        count_limit (int): Minimum number of samples required per segment
+
+    Returns:
+        A dataframe with relevant data...
+
+    """
+    logger.info("The zonelist is %s", zonelist)
+    logger.info("The dlogname is %s", dlogname)
+    logger.info("The dvalues are %s", dvalues)
+
+    if zonelogname is not None:
+        usezonelogname = zonelogname
+        self.zonelogname = zonelogname
+    else:
+        usezonelogname = self.zonelogname
+
+    if usezonelogname is None:
+        raise RuntimeError("Stop, zonelogname is None")
+
+    self.make_zone_qual_log("_QFLAG")
+
+    if zonelist is None:
+        # need to declare as list; otherwise Py3 will get dict.keys
+        zonelist = list(self.get_logrecord(self.zonelogname).keys())
+
+    useinclname = "Q_INCL"
+    if "M_INCL" in self._df:
+        useinclname = "M_INCL"
+    else:
+        self.geometrics()
+
+    result = OrderedDict()
+    result["X_UTME"] = []
+    result["Y_UTMN"] = []
+    result["DFRAC"] = []
+    result["Q_INCL"] = []
+    result["ZONE"] = []
+    result["WELLNAME"] = []
+    result[dlogname] = []
+
+    svalues = str(dvalues).rstrip("]").lstrip("[").replace(", ", "+")
+
+    xtralogs = [dlogname, useinclname, "_QFLAG"]
+    for izon in zonelist:
+        logger.info("The zone number is %s", izon)
+        logger.info("The extralogs are %s", xtralogs)
+
+        dfr = self.get_zone_interval(izon, extralogs=xtralogs)
+
+        if dfr is None:
+            continue
+
+        dfrx = dfr.groupby("POLY_ID")
+
+        for _polyid, dframe in dfrx:
+            qinclmax = dframe["Q_INCL"].max()
+            qinclavg = dframe["Q_INCL"].mean()
+            qflag = dframe["_QFLAG"].mean()
+            dseries = dframe[dlogname]
+            if qflag < 0.5 or qflag > 2.5:  # 1 or 2 is OK
+                logger.debug("Skipped due to zone %s", qflag)
+                continue
+            if qinclmax > incl_limit:
+                logger.debug("Skipped due to max inclination %s", qinclmax)
+                continue
+            if dseries.size < count_limit:  # interval too short for fraction
+                logger.debug("Skipped due to too few values %s", dseries.size)
+                continue
+            if dseries.max() > const.UNDEF_INT_LIMIT:
+                logger.debug("Skipped due to too missing/undef value(s)")
+                continue
+
+            xavg = dframe["X_UTME"].mean()
+            yavg = dframe["Y_UTMN"].mean()
+
+            dfrac = 0.0
+            for dval in dvalues:
+                if any(dseries.isin([dval])):
+                    dfrac += dseries.value_counts(normalize=True)[dval]
+
+            result["X_UTME"].append(xavg)
+            result["Y_UTMN"].append(yavg)
+            result["DFRAC"].append(dfrac)
+            result["Q_INCL"].append(qinclavg)
+            result["ZONE"].append(izon)
+            result["WELLNAME"].append(self.xwellname)
+            result[dlogname].append(svalues)
+
+    # make the dataframe and return it
+    if result["X_UTME"]:
+        return pd.DataFrame.from_dict(result)
+
+    self.delete_log("_QFLAG")
+
+    return None
+
+
+def get_surface_picks(self, surf):
+    """get Surface picks"""
+
+    xcor = self._df["X_UTME"].values
+    ycor = self._df["Y_UTMN"].values
+    zcor = self._df["Z_TVDSS"].values
+
+    if self.mdlogname:
+        mcor = self._df[self.mdlogname].values
+    else:
+        mcor = np.zeros(xcor.size, dtype=np.float64) + xtgeo.UNDEF
+
+    nval, xres, yres, zres, mres, dres = _cxtgeo.well_surf_picks(
+        xcor,
+        ycor,
+        zcor,
+        mcor,
+        surf.ncol,
+        surf.nrow,
+        surf.xori,
+        surf.yori,
+        surf.xinc,
+        surf.yinc,
+        surf.yflip,
+        surf.rotation,
+        surf.npvalues1d,
+        xcor.size,
+        xcor.size,
+        xcor.size,
+        xcor.size,
+        xcor.size,
+    )
+
+    if nval > 0:
+        poi = xtgeo.Points()
+
+        mres[mres > xtgeo.UNDEF_LIMIT] = np.nan
+
+        res = OrderedDict()
+        res[poi.xname] = xres[:nval]
+        res[poi.yname] = yres[:nval]
+        res[poi.zname] = zres[:nval]
+        if self.mdlogname:
+            res[self.mdlogname] = mres[:nval]
+        res["DIRECTION"] = dres[:nval]
+        res["WELLNAME"] = self.wellname
+
+        poi.dataframe = pd.DataFrame.from_dict(res)
+
+        return poi
+
+    return None
+
+    # return a xtgeo Poinst() object with points as dataframe, given that nval > 0
```

## xtgeo/well/_wells_utils.py

 * *Ordering differences only*

```diff
@@ -1,147 +1,147 @@
-# -*- coding: utf-8 -*-
-"""Utilities for Wells class"""
-
-
-import logging
-import numpy as np
-import pandas as pd
-import shapely.geometry as sg
-
-from xtgeo.common import XTGeoDialog
-from xtgeo.common import XTGShowProgress
-
-logger = logging.getLogger(__name__)
-logger.addHandler(logging.NullHandler())
-
-xtg = XTGeoDialog()
-
-
-def wellintersections(
-    self, wfilter=None, showprogress=False
-):  # pylint: disable=too-many-locals, too-many-branches, too-many-statements
-    """Get intersections between wells, return as dataframe table.
-
-    This routine is using "shapely" functions!
-
-    Some actions are done in order to filter away the part of the trajectories
-    that are paralell.
-
-    """
-
-    xpoints = []
-
-    # make a dict if nocrossings
-    nox = {}
-
-    wlen = len(self.wells)
-
-    progress = XTGShowProgress(wlen, show=showprogress, leadtext="progress: ", skip=5)
-
-    for iwell, well in enumerate(self.wells):
-        progress.flush(iwell)
-
-        logger.info("Work with %s", well.name)
-        try:
-            well.geometrics()
-        except ValueError:
-            logger.info("Skip %s (cannot compute geometrics)", well.name)
-            continue
-
-        welldfr = well.dataframe.copy()
-
-        xcor = welldfr["X_UTME"].values
-        ycor = welldfr["Y_UTMN"].values
-        mcor = welldfr[well.mdlogname].values
-        logger.info("The mdlogname property is: %s", well.mdlogname)
-
-        if xcor.size < 2:
-            continue
-
-        thisline1 = sg.LineString(np.stack([xcor, ycor], axis=1))
-        thisline2 = sg.LineString(np.stack([xcor, ycor, mcor], axis=1))
-
-        nox[well.name] = list()
-        # loop over other wells
-        for other in self.wells:
-            if other.name == well.name:
-                continue  # same well
-
-            if not well.may_overlap(other):
-                nox[well.name].append(other.name)
-                continue  # a quick check; no chance for overlap
-
-            logger.info("Consider crossing with %s ...", other.name)
-
-            # try to be smart to skip entries that earlier have beenn tested
-            # for crossing. If other does not cross well, then well does not
-            # cross other...
-            if other.name in nox.keys() and well.name in nox[other.name]:
-                continue
-
-            # truncate away the paralell part on a copy
-            owell = other.copy()
-
-            # wfilter = None
-            if wfilter is not None and "parallel" in wfilter:
-                xtol = wfilter["parallel"].get("xtol")
-                ytol = wfilter["parallel"].get("ytol")
-                ztol = wfilter["parallel"].get("ztol")
-                itol = wfilter["parallel"].get("itol")
-                atol = wfilter["parallel"].get("atol")
-                owell.truncate_parallel_path(
-                    well, xtol=xtol, ytol=ytol, ztol=ztol, itol=itol, atol=atol
-                )
-
-            xcorc = owell.dataframe["X_UTME"].values
-            ycorc = owell.dataframe["Y_UTMN"].values
-            zcorc = owell.dataframe["Z_TVDSS"].values
-
-            if xcorc.size < 2:
-                continue
-
-            otherline = sg.LineString(np.stack([xcorc, ycorc, zcorc], axis=1))
-
-            if not thisline1.crosses(otherline):
-                nox[well.name].append(other.name)
-                continue
-
-            ixx = thisline1.intersection(otherline)
-
-            if ixx.is_empty:
-                nox[well.name].append(other.name)
-                continue
-
-            # need this trick to get mdepth
-            other2 = sg.LineString(np.stack([xcorc, ycorc], axis=1))
-            ixx2 = thisline2.intersection(other2)
-
-            logger.debug("==> Intersects with %s", other.name)
-
-            if isinstance(ixx, sg.Point):
-                xcor, ycor, zcor = ixx.coords[0]
-                _x, _y, mcor = ixx2.coords[0]
-                xpoints.append([well.name, mcor, other.name, xcor, ycor, zcor])
-
-            elif isinstance(ixx, sg.MultiPoint):
-                pxx2 = list(ixx2)
-                for ino, pxx in enumerate(list(ixx)):
-                    xcor, ycor, zcor = pxx.coords[0]
-                    _x, _y, mcor = pxx2[ino].coords[0]
-                    xpoints.append([well.name, mcor, other.name, xcor, ycor, zcor])
-
-            elif isinstance(ixx, sg.GeometryCollection):
-                gxx2 = list(ixx2)
-                for ino, gxx in enumerate(list(ixx)):
-                    if isinstance(gxx, sg.Point):
-                        xcor, ycor, zcor = gxx.coords[0]
-                        _x, _y, mcor = gxx2[ino].coords[0]
-                        xpoints.append([well.name, mcor, other.name, xcor, ycor, zcor])
-
-    dfr = pd.DataFrame(
-        xpoints, columns=["WELL", "MDEPTH", "CWELL", "X_UTME", "Y_UTMN", "Z_TVDSS"]
-    )
-
-    progress.finished()
-
-    logger.info("All intersections found!")
-    return dfr
+# -*- coding: utf-8 -*-
+"""Utilities for Wells class"""
+
+
+import logging
+import numpy as np
+import pandas as pd
+import shapely.geometry as sg
+
+from xtgeo.common import XTGeoDialog
+from xtgeo.common import XTGShowProgress
+
+logger = logging.getLogger(__name__)
+logger.addHandler(logging.NullHandler())
+
+xtg = XTGeoDialog()
+
+
+def wellintersections(
+    self, wfilter=None, showprogress=False
+):  # pylint: disable=too-many-locals, too-many-branches, too-many-statements
+    """Get intersections between wells, return as dataframe table.
+
+    This routine is using "shapely" functions!
+
+    Some actions are done in order to filter away the part of the trajectories
+    that are paralell.
+
+    """
+
+    xpoints = []
+
+    # make a dict if nocrossings
+    nox = {}
+
+    wlen = len(self.wells)
+
+    progress = XTGShowProgress(wlen, show=showprogress, leadtext="progress: ", skip=5)
+
+    for iwell, well in enumerate(self.wells):
+        progress.flush(iwell)
+
+        logger.info("Work with %s", well.name)
+        try:
+            well.geometrics()
+        except ValueError:
+            logger.info("Skip %s (cannot compute geometrics)", well.name)
+            continue
+
+        welldfr = well.dataframe.copy()
+
+        xcor = welldfr["X_UTME"].values
+        ycor = welldfr["Y_UTMN"].values
+        mcor = welldfr[well.mdlogname].values
+        logger.info("The mdlogname property is: %s", well.mdlogname)
+
+        if xcor.size < 2:
+            continue
+
+        thisline1 = sg.LineString(np.stack([xcor, ycor], axis=1))
+        thisline2 = sg.LineString(np.stack([xcor, ycor, mcor], axis=1))
+
+        nox[well.name] = list()
+        # loop over other wells
+        for other in self.wells:
+            if other.name == well.name:
+                continue  # same well
+
+            if not well.may_overlap(other):
+                nox[well.name].append(other.name)
+                continue  # a quick check; no chance for overlap
+
+            logger.info("Consider crossing with %s ...", other.name)
+
+            # try to be smart to skip entries that earlier have beenn tested
+            # for crossing. If other does not cross well, then well does not
+            # cross other...
+            if other.name in nox.keys() and well.name in nox[other.name]:
+                continue
+
+            # truncate away the paralell part on a copy
+            owell = other.copy()
+
+            # wfilter = None
+            if wfilter is not None and "parallel" in wfilter:
+                xtol = wfilter["parallel"].get("xtol")
+                ytol = wfilter["parallel"].get("ytol")
+                ztol = wfilter["parallel"].get("ztol")
+                itol = wfilter["parallel"].get("itol")
+                atol = wfilter["parallel"].get("atol")
+                owell.truncate_parallel_path(
+                    well, xtol=xtol, ytol=ytol, ztol=ztol, itol=itol, atol=atol
+                )
+
+            xcorc = owell.dataframe["X_UTME"].values
+            ycorc = owell.dataframe["Y_UTMN"].values
+            zcorc = owell.dataframe["Z_TVDSS"].values
+
+            if xcorc.size < 2:
+                continue
+
+            otherline = sg.LineString(np.stack([xcorc, ycorc, zcorc], axis=1))
+
+            if not thisline1.crosses(otherline):
+                nox[well.name].append(other.name)
+                continue
+
+            ixx = thisline1.intersection(otherline)
+
+            if ixx.is_empty:
+                nox[well.name].append(other.name)
+                continue
+
+            # need this trick to get mdepth
+            other2 = sg.LineString(np.stack([xcorc, ycorc], axis=1))
+            ixx2 = thisline2.intersection(other2)
+
+            logger.debug("==> Intersects with %s", other.name)
+
+            if isinstance(ixx, sg.Point):
+                xcor, ycor, zcor = ixx.coords[0]
+                _x, _y, mcor = ixx2.coords[0]
+                xpoints.append([well.name, mcor, other.name, xcor, ycor, zcor])
+
+            elif isinstance(ixx, sg.MultiPoint):
+                pxx2 = list(ixx2)
+                for ino, pxx in enumerate(list(ixx)):
+                    xcor, ycor, zcor = pxx.coords[0]
+                    _x, _y, mcor = pxx2[ino].coords[0]
+                    xpoints.append([well.name, mcor, other.name, xcor, ycor, zcor])
+
+            elif isinstance(ixx, sg.GeometryCollection):
+                gxx2 = list(ixx2)
+                for ino, gxx in enumerate(list(ixx)):
+                    if isinstance(gxx, sg.Point):
+                        xcor, ycor, zcor = gxx.coords[0]
+                        _x, _y, mcor = gxx2[ino].coords[0]
+                        xpoints.append([well.name, mcor, other.name, xcor, ycor, zcor])
+
+    dfr = pd.DataFrame(
+        xpoints, columns=["WELL", "MDEPTH", "CWELL", "X_UTME", "Y_UTMN", "Z_TVDSS"]
+    )
+
+    progress.finished()
+
+    logger.info("All intersections found!")
+    return dfr
```

## xtgeo/well/blocked_well.py

 * *Ordering differences only*

```diff
@@ -1,238 +1,238 @@
-# -*- coding: utf-8 -*-
-"""XTGeo blockedwell module"""
-
-
-import deprecation
-import xtgeo
-
-from . import _blockedwell_roxapi
-from .well1 import Well
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-# =============================================================================
-# METHODS as wrappers to class init + import
-
-
-def blockedwell_from_file(
-    bwfile, fformat="rms_ascii", mdlogname=None, zonelogname=None, strict=False
-):
-    """Make an instance of a BlockedWell directly from file import.
-
-    Args:
-        bmfile (str): Name of file
-        fformat (str): See :meth:`Well.from_file`
-        mdlogname (str): See :meth:`Well.from_file`
-        zonelogname (str): See :meth:`Well.from_file`
-        strict (bool): See :meth:`Well.from_file`
-
-    Example::
-
-        >>> import xtgeo
-        >>> well3 = xtgeo.blockedwell_from_file(well_dir + '/OP_1.bw')
-    """
-
-    return Well._read_file(
-        bwfile,
-        fformat=fformat,
-        mdlogname=mdlogname,
-        zonelogname=zonelogname,
-        strict=strict,
-    )
-
-    # return obj
-
-
-def blockedwell_from_roxar(project, gname, bwname, wname, lognames=None, ijk=True):
-    """This makes an instance of a BlockedWell directly from Roxar RMS.
-
-    For arguments, see :meth:`BlockedWell.from_roxar`.
-
-    Example::
-
-        # inside RMS:
-        import xtgeo
-        mylogs = ['ZONELOG', 'GR', 'Facies']
-        mybw = xtgeo.blockedwell_from_roxar(project, 'Simgrid', 'BW', '31_3-1',
-                                            lognames=mylogs)
-
-    """
-
-    obj = BlockedWell()
-
-    obj.from_roxar(project, gname, bwname, wname, ijk=ijk, lognames=lognames)
-
-    return obj
-
-
-# =============================================================================
-# CLASS
-
-
-class BlockedWell(Well):
-    """Class for a blocked well in the XTGeo framework, subclassed from the
-    Well class.
-
-    Similar to Wells, the blocked well logs are stored as Pandas dataframe,
-    which make manipulation easy and fast.
-
-    For blocked well logs, the numbers of rows cannot be changed if you want to
-    save the result in RMS, as this is derived from the grid. Also the blocked well
-    icon must exist before save.
-
-    The well trajectory are here represented as logs, and XYZ have magic names:
-    X_UTME, Y_UTMN, Z_TVDSS, which are the three first Pandas columns.
-
-    Other geometry logs has also 'semi-magic' names:
-
-    M_MDEPTH or Q_MDEPTH: Measured depth, either real/true (M...) or
-    quasi computed/estimated (Q...). The Quasi may be incorrect for
-    all uses, but sufficient for some computations.
-
-    Similar for M_INCL, Q_INCL, M_AZI, Q_ASI.
-
-    I_INDEX, J_INDEX, K_INDEX: They are grid indices. For practical reasons
-    they are treated as a CONT logs, since the min/max grid indices usually are
-    unknown, and hence making a code index is not trivial.
-
-    All Pandas values (yes, discrete also!) are stored as float64
-    format, and undefined values are Nan. Integers are stored as Float due
-    to the lacking support for 'Integer Nan' (currently lacking in Pandas,
-    but may come in later Pandas versions).
-
-    Note there is a method that can return a dataframe (copy) with Integer
-    and Float columns, see :meth:`get_filled_dataframe`.
-
-    The instance can be made either from file or::
-
-        >>> well1 = BlockedWell(well_dir + '/OP_1.bw')  # assume RMS ascii well
-        >>> well2 = BlockedWell(well_dir + '/OP_1.bw', fformat='rms_ascii')
-        >>> well3 = xtgeo.blockedwell_from_file(well_dir + '/OP_1.bw')
-
-    If in RMS, instance can be made also from RMS icon::
-
-        well4 = xtgeo.blockedwell_from_roxar(
-            project,
-            'gridname',
-            'bwname',
-            'wellname',
-        )
-
-    For arguments, see method under :meth:`from_file`.
-
-    """
-
-    VALID_LOGTYPES = {"DISC", "CONT"}
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-
-        self._gridname = None
-
-    @property
-    def gridname(self):
-        """Returns or set (rename) the grid name that the blocked wells
-        belongs to."""
-        return self._gridname
-
-    @gridname.setter
-    def gridname(self, newname):
-        self._gridname = newname
-
-    def copy(self):
-        newbw = super().copy()
-
-        newbw._gridname = self._gridname
-
-        return newbw
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.blockedwell_from_roxar() instead",
-    )
-    def from_roxar(self, *args, **kwargs):
-        """Import (retrieve) a single blocked well from roxar project.
-
-        Note this method works only when inside RMS, or when RMS license is
-        activated.
-
-        Args:
-            project (str): Magic string `project` or file path to project
-            gname (str): Name of GridModel icon in RMS
-            bwname (str): Name of Blocked Well icon in RMS, usually 'BW'
-            wname (str): Name of well, as shown in RMS.
-            lognames (list): List of lognames to include, or use 'all' for
-                all current blocked logs for this well. Default is 'all'.
-            realisation (int): Realisation index (0 is default)
-            ijk (bool): If True, then make additional logs with grid IJK as I_INDEX,
-                etc, default is False
-        """
-        project = args[0]
-        gname = args[1]
-        bwname = args[2]
-        wname = args[3]
-        lognames = kwargs.get("lognames", "all")
-        ijk = kwargs.get("ijk", False)
-        realisation = kwargs.get("realisation", 0)
-
-        _blockedwell_roxapi.import_bwell_roxapi(
-            self,
-            project,
-            gname,
-            bwname,
-            wname,
-            lognames=lognames,
-            ijk=ijk,
-            realisation=realisation,
-        )
-
-        self._ensure_consistency()
-
-    def to_roxar(self, *args, **kwargs):
-        """Set (export) a single blocked well item inside roxar project.
-
-        Note this method works only when inside RMS, or when RMS license is
-        activated.
-
-        Note:
-           When project is file path (direct access, outside RMS) then
-           ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-           will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str or object): Magic object 'project' or file path to project
-            gname (str): Name of GridModel icon in RMS
-            bwname (str): Name of Blocked Well icon in RMS, usually 'BW'
-            wname (str): Name of well, as shown in RMS.
-            lognames (list or "all"): List of lognames to include, or use 'all' for
-                all current blocked logs for this well (except index logs). Default is
-                "all".
-            realisation (int): Realisation index (0 is default)
-            ijk (bool): If True, then also write special index logs if they exist,
-                such as I_INDEX, J_INDEX, K_INDEX, etc. Default is False
-
-        .. versionadded: 2.12
-
-        """
-        project = args[0]
-        gname = args[1]
-        bwname = args[2]
-        wname = args[3]
-        lognames = kwargs.get("lognames", "all")
-        ijk = kwargs.get("ijk", False)
-        realisation = kwargs.get("realisation", 0)
-
-        _blockedwell_roxapi.export_bwell_roxapi(
-            self,
-            project,
-            gname,
-            bwname,
-            wname,
-            lognames=lognames,
-            ijk=ijk,
-            realisation=realisation,
-        )
+# -*- coding: utf-8 -*-
+"""XTGeo blockedwell module"""
+
+
+import deprecation
+import xtgeo
+
+from . import _blockedwell_roxapi
+from .well1 import Well
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+# =============================================================================
+# METHODS as wrappers to class init + import
+
+
+def blockedwell_from_file(
+    bwfile, fformat="rms_ascii", mdlogname=None, zonelogname=None, strict=False
+):
+    """Make an instance of a BlockedWell directly from file import.
+
+    Args:
+        bmfile (str): Name of file
+        fformat (str): See :meth:`Well.from_file`
+        mdlogname (str): See :meth:`Well.from_file`
+        zonelogname (str): See :meth:`Well.from_file`
+        strict (bool): See :meth:`Well.from_file`
+
+    Example::
+
+        >>> import xtgeo
+        >>> well3 = xtgeo.blockedwell_from_file(well_dir + '/OP_1.bw')
+    """
+
+    return Well._read_file(
+        bwfile,
+        fformat=fformat,
+        mdlogname=mdlogname,
+        zonelogname=zonelogname,
+        strict=strict,
+    )
+
+    # return obj
+
+
+def blockedwell_from_roxar(project, gname, bwname, wname, lognames=None, ijk=True):
+    """This makes an instance of a BlockedWell directly from Roxar RMS.
+
+    For arguments, see :meth:`BlockedWell.from_roxar`.
+
+    Example::
+
+        # inside RMS:
+        import xtgeo
+        mylogs = ['ZONELOG', 'GR', 'Facies']
+        mybw = xtgeo.blockedwell_from_roxar(project, 'Simgrid', 'BW', '31_3-1',
+                                            lognames=mylogs)
+
+    """
+
+    obj = BlockedWell()
+
+    obj.from_roxar(project, gname, bwname, wname, ijk=ijk, lognames=lognames)
+
+    return obj
+
+
+# =============================================================================
+# CLASS
+
+
+class BlockedWell(Well):
+    """Class for a blocked well in the XTGeo framework, subclassed from the
+    Well class.
+
+    Similar to Wells, the blocked well logs are stored as Pandas dataframe,
+    which make manipulation easy and fast.
+
+    For blocked well logs, the numbers of rows cannot be changed if you want to
+    save the result in RMS, as this is derived from the grid. Also the blocked well
+    icon must exist before save.
+
+    The well trajectory are here represented as logs, and XYZ have magic names:
+    X_UTME, Y_UTMN, Z_TVDSS, which are the three first Pandas columns.
+
+    Other geometry logs has also 'semi-magic' names:
+
+    M_MDEPTH or Q_MDEPTH: Measured depth, either real/true (M...) or
+    quasi computed/estimated (Q...). The Quasi may be incorrect for
+    all uses, but sufficient for some computations.
+
+    Similar for M_INCL, Q_INCL, M_AZI, Q_ASI.
+
+    I_INDEX, J_INDEX, K_INDEX: They are grid indices. For practical reasons
+    they are treated as a CONT logs, since the min/max grid indices usually are
+    unknown, and hence making a code index is not trivial.
+
+    All Pandas values (yes, discrete also!) are stored as float64
+    format, and undefined values are Nan. Integers are stored as Float due
+    to the lacking support for 'Integer Nan' (currently lacking in Pandas,
+    but may come in later Pandas versions).
+
+    Note there is a method that can return a dataframe (copy) with Integer
+    and Float columns, see :meth:`get_filled_dataframe`.
+
+    The instance can be made either from file or::
+
+        >>> well1 = BlockedWell(well_dir + '/OP_1.bw')  # assume RMS ascii well
+        >>> well2 = BlockedWell(well_dir + '/OP_1.bw', fformat='rms_ascii')
+        >>> well3 = xtgeo.blockedwell_from_file(well_dir + '/OP_1.bw')
+
+    If in RMS, instance can be made also from RMS icon::
+
+        well4 = xtgeo.blockedwell_from_roxar(
+            project,
+            'gridname',
+            'bwname',
+            'wellname',
+        )
+
+    For arguments, see method under :meth:`from_file`.
+
+    """
+
+    VALID_LOGTYPES = {"DISC", "CONT"}
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+
+        self._gridname = None
+
+    @property
+    def gridname(self):
+        """Returns or set (rename) the grid name that the blocked wells
+        belongs to."""
+        return self._gridname
+
+    @gridname.setter
+    def gridname(self, newname):
+        self._gridname = newname
+
+    def copy(self):
+        newbw = super().copy()
+
+        newbw._gridname = self._gridname
+
+        return newbw
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.blockedwell_from_roxar() instead",
+    )
+    def from_roxar(self, *args, **kwargs):
+        """Import (retrieve) a single blocked well from roxar project.
+
+        Note this method works only when inside RMS, or when RMS license is
+        activated.
+
+        Args:
+            project (str): Magic string `project` or file path to project
+            gname (str): Name of GridModel icon in RMS
+            bwname (str): Name of Blocked Well icon in RMS, usually 'BW'
+            wname (str): Name of well, as shown in RMS.
+            lognames (list): List of lognames to include, or use 'all' for
+                all current blocked logs for this well. Default is 'all'.
+            realisation (int): Realisation index (0 is default)
+            ijk (bool): If True, then make additional logs with grid IJK as I_INDEX,
+                etc, default is False
+        """
+        project = args[0]
+        gname = args[1]
+        bwname = args[2]
+        wname = args[3]
+        lognames = kwargs.get("lognames", "all")
+        ijk = kwargs.get("ijk", False)
+        realisation = kwargs.get("realisation", 0)
+
+        _blockedwell_roxapi.import_bwell_roxapi(
+            self,
+            project,
+            gname,
+            bwname,
+            wname,
+            lognames=lognames,
+            ijk=ijk,
+            realisation=realisation,
+        )
+
+        self._ensure_consistency()
+
+    def to_roxar(self, *args, **kwargs):
+        """Set (export) a single blocked well item inside roxar project.
+
+        Note this method works only when inside RMS, or when RMS license is
+        activated.
+
+        Note:
+           When project is file path (direct access, outside RMS) then
+           ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+           will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str or object): Magic object 'project' or file path to project
+            gname (str): Name of GridModel icon in RMS
+            bwname (str): Name of Blocked Well icon in RMS, usually 'BW'
+            wname (str): Name of well, as shown in RMS.
+            lognames (list or "all"): List of lognames to include, or use 'all' for
+                all current blocked logs for this well (except index logs). Default is
+                "all".
+            realisation (int): Realisation index (0 is default)
+            ijk (bool): If True, then also write special index logs if they exist,
+                such as I_INDEX, J_INDEX, K_INDEX, etc. Default is False
+
+        .. versionadded: 2.12
+
+        """
+        project = args[0]
+        gname = args[1]
+        bwname = args[2]
+        wname = args[3]
+        lognames = kwargs.get("lognames", "all")
+        ijk = kwargs.get("ijk", False)
+        realisation = kwargs.get("realisation", 0)
+
+        _blockedwell_roxapi.export_bwell_roxapi(
+            self,
+            project,
+            gname,
+            bwname,
+            wname,
+            lognames=lognames,
+            ijk=ijk,
+            realisation=realisation,
+        )
```

## xtgeo/well/blocked_wells.py

 * *Ordering differences only*

```diff
@@ -1,188 +1,188 @@
-# -*- coding: utf-8 -*-
-"""BlockedWells module, (collection of BlockedWell objects)"""
-
-# ======================================================================================
-
-
-import deprecation
-
-import xtgeo
-
-from . import _blockedwells_roxapi
-from .blocked_well import BlockedWell
-from .wells import Wells
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def blockedwells_from_files(
-    filelist,
-    fformat="rms_ascii",
-    mdlogname=None,
-    zonelogname=None,
-    strict=True,
-):
-    """Import blocked wells from a list of files (filelist).
-
-    Args:
-        filelist (list of str): List with file names
-        fformat (str): File format, rms_ascii (rms well) is
-            currently supported and default format.
-        mdlogname (str): Name of measured depth log, if any
-        zonelogname (str): Name of zonation log, if any
-        strict (bool): If True, then import will fail if
-            zonelogname or mdlogname are asked for but not present
-            in wells.
-
-    Example:
-        Here the from_file method is used to initiate the object
-        directly::
-
-          mywells = BlockedWells(['31_2-6.w', '31_2-7.w', '31_2-8.w'])
-    """
-    return BlockedWells(
-        [
-            xtgeo.blockedwell_from_file(
-                wfile,
-                fformat=fformat,
-                mdlogname=mdlogname,
-                zonelogname=zonelogname,
-                strict=strict,
-            )
-            for wfile in filelist
-        ]
-    )
-
-
-def blockedwells_from_roxar(
-    project, gname, bwname, lognames=None, ijk=True
-):  # pragma: no cover
-    """This makes an instance of a BlockedWells directly from Roxar RMS.
-
-    For arguments, see :meth:`BlockedWells.from_roxar`.
-
-    Note the difference between classes BlockedWell and BlockedWells.
-
-    Example::
-
-        # inside RMS:
-        import xtgeo
-        mylogs = ['ZONELOG', 'GR', 'Facies']
-        mybws = xtgeo.blockedwells_from_roxar(project, 'Simgrid', 'BW',
-                                            lognames=mylogs)
-
-    """
-
-    obj = BlockedWells()
-
-    obj.from_roxar(project, gname, bwname, ijk=ijk, lognames=lognames)
-
-    return obj
-
-
-class BlockedWells(Wells):
-    """Class for a collection of BlockedWell objects, for operations that
-    involves a number of wells.
-
-    See also the :class:`xtgeo.well.BlockedWell` class.
-    """
-
-    def copy(self):
-        """Copy a BlockedWells instance to a new unique instance."""
-
-        return BlockedWells([w.copy() for w in self._wells])
-
-    def get_blocked_well(self, name):
-        """Get a BlockedWell() instance by name, or None"""
-        logger.info("Calling super...")
-        return super().get_well(name)
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.blockedwells_from_files() instead",
-    )
-    def from_files(
-        self,
-        filelist,
-        fformat="rms_ascii",
-        mdlogname=None,
-        zonelogname=None,
-        strict=True,
-        append=True,
-    ):
-        """Import blocked wells from a list of files (filelist).
-
-        Args:
-            filelist (list of str): List with file names
-            fformat (str): File format, rms_ascii (rms well) is
-                currently supported and default format.
-            mdlogname (str): Name of measured depth log, if any
-            zonelogname (str): Name of zonation log, if any
-            strict (bool): If True, then import will fail if
-                zonelogname or mdlogname are asked for but not present
-                in wells.
-            append (bool): If True, new wells will be added to existing
-                wells.
-
-        Example:
-            Here the from_file method is used to initiate the object
-            directly::
-
-              mywells = BlockedWells(['31_2-6.w', '31_2-7.w', '31_2-8.w'])
-        """
-
-        if not append:
-            self._wells = []
-
-        # file checks are done within the Well() class
-        for wfile in filelist:
-            try:
-                wll = BlockedWell(
-                    wfile,
-                    fformat=fformat,
-                    mdlogname=mdlogname,
-                    zonelogname=zonelogname,
-                    strict=strict,
-                )
-                self._wells.append(wll)
-            except ValueError as err:
-                xtg.warn(f"SKIP this well: {err}")
-                continue
-        if not self._wells:
-            xtg.warn("No wells imported!")
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.blockedwells_from_roxar() instead",
-    )
-    def from_roxar(self, *args, **kwargs):  # pragma: no cover
-        """Import (retrieve) blocked wells from roxar project.
-
-        Note this method works only when inside RMS, or when RMS license is
-        activated.
-
-        All the wells present in the bwname icon will be imported.
-
-        Args:
-            project (str): Magic string 'project' or file path to project
-            gname (str): Name of GridModel icon in RMS
-            bwname (str): Name of Blocked Well icon in RMS, usually 'BW'
-            lognames (list): List of lognames to include, or use 'all' for
-                all current blocked logs for this well.
-            ijk (bool): If True, then logs with grid IJK as I_INDEX, etc
-            realisation (int): Realisation index (0 is default)
-        """
-        project = args[0]
-        gname = args[1]
-        bwname = args[2]
-        lognames = kwargs.get("lognames", None)
-        ijk = kwargs.get("ijk", True)
-
-        _blockedwells_roxapi.import_bwells_roxapi(
-            self, project, gname, bwname, lognames=lognames, ijk=ijk
-        )
+# -*- coding: utf-8 -*-
+"""BlockedWells module, (collection of BlockedWell objects)"""
+
+# ======================================================================================
+
+
+import deprecation
+
+import xtgeo
+
+from . import _blockedwells_roxapi
+from .blocked_well import BlockedWell
+from .wells import Wells
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def blockedwells_from_files(
+    filelist,
+    fformat="rms_ascii",
+    mdlogname=None,
+    zonelogname=None,
+    strict=True,
+):
+    """Import blocked wells from a list of files (filelist).
+
+    Args:
+        filelist (list of str): List with file names
+        fformat (str): File format, rms_ascii (rms well) is
+            currently supported and default format.
+        mdlogname (str): Name of measured depth log, if any
+        zonelogname (str): Name of zonation log, if any
+        strict (bool): If True, then import will fail if
+            zonelogname or mdlogname are asked for but not present
+            in wells.
+
+    Example:
+        Here the from_file method is used to initiate the object
+        directly::
+
+          mywells = BlockedWells(['31_2-6.w', '31_2-7.w', '31_2-8.w'])
+    """
+    return BlockedWells(
+        [
+            xtgeo.blockedwell_from_file(
+                wfile,
+                fformat=fformat,
+                mdlogname=mdlogname,
+                zonelogname=zonelogname,
+                strict=strict,
+            )
+            for wfile in filelist
+        ]
+    )
+
+
+def blockedwells_from_roxar(
+    project, gname, bwname, lognames=None, ijk=True
+):  # pragma: no cover
+    """This makes an instance of a BlockedWells directly from Roxar RMS.
+
+    For arguments, see :meth:`BlockedWells.from_roxar`.
+
+    Note the difference between classes BlockedWell and BlockedWells.
+
+    Example::
+
+        # inside RMS:
+        import xtgeo
+        mylogs = ['ZONELOG', 'GR', 'Facies']
+        mybws = xtgeo.blockedwells_from_roxar(project, 'Simgrid', 'BW',
+                                            lognames=mylogs)
+
+    """
+
+    obj = BlockedWells()
+
+    obj.from_roxar(project, gname, bwname, ijk=ijk, lognames=lognames)
+
+    return obj
+
+
+class BlockedWells(Wells):
+    """Class for a collection of BlockedWell objects, for operations that
+    involves a number of wells.
+
+    See also the :class:`xtgeo.well.BlockedWell` class.
+    """
+
+    def copy(self):
+        """Copy a BlockedWells instance to a new unique instance."""
+
+        return BlockedWells([w.copy() for w in self._wells])
+
+    def get_blocked_well(self, name):
+        """Get a BlockedWell() instance by name, or None"""
+        logger.info("Calling super...")
+        return super().get_well(name)
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.blockedwells_from_files() instead",
+    )
+    def from_files(
+        self,
+        filelist,
+        fformat="rms_ascii",
+        mdlogname=None,
+        zonelogname=None,
+        strict=True,
+        append=True,
+    ):
+        """Import blocked wells from a list of files (filelist).
+
+        Args:
+            filelist (list of str): List with file names
+            fformat (str): File format, rms_ascii (rms well) is
+                currently supported and default format.
+            mdlogname (str): Name of measured depth log, if any
+            zonelogname (str): Name of zonation log, if any
+            strict (bool): If True, then import will fail if
+                zonelogname or mdlogname are asked for but not present
+                in wells.
+            append (bool): If True, new wells will be added to existing
+                wells.
+
+        Example:
+            Here the from_file method is used to initiate the object
+            directly::
+
+              mywells = BlockedWells(['31_2-6.w', '31_2-7.w', '31_2-8.w'])
+        """
+
+        if not append:
+            self._wells = []
+
+        # file checks are done within the Well() class
+        for wfile in filelist:
+            try:
+                wll = BlockedWell(
+                    wfile,
+                    fformat=fformat,
+                    mdlogname=mdlogname,
+                    zonelogname=zonelogname,
+                    strict=strict,
+                )
+                self._wells.append(wll)
+            except ValueError as err:
+                xtg.warn(f"SKIP this well: {err}")
+                continue
+        if not self._wells:
+            xtg.warn("No wells imported!")
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.blockedwells_from_roxar() instead",
+    )
+    def from_roxar(self, *args, **kwargs):  # pragma: no cover
+        """Import (retrieve) blocked wells from roxar project.
+
+        Note this method works only when inside RMS, or when RMS license is
+        activated.
+
+        All the wells present in the bwname icon will be imported.
+
+        Args:
+            project (str): Magic string 'project' or file path to project
+            gname (str): Name of GridModel icon in RMS
+            bwname (str): Name of Blocked Well icon in RMS, usually 'BW'
+            lognames (list): List of lognames to include, or use 'all' for
+                all current blocked logs for this well.
+            ijk (bool): If True, then logs with grid IJK as I_INDEX, etc
+            realisation (int): Realisation index (0 is default)
+        """
+        project = args[0]
+        gname = args[1]
+        bwname = args[2]
+        lognames = kwargs.get("lognames", None)
+        ijk = kwargs.get("ijk", True)
+
+        _blockedwells_roxapi.import_bwells_roxapi(
+            self, project, gname, bwname, lognames=lognames, ijk=ijk
+        )
```

## xtgeo/well/well1.py

 * *Ordering differences only*

```diff
@@ -1,1729 +1,1729 @@
-# -*- coding: utf-8 -*-
-"""XTGeo well module, working with one single well."""
-
-import functools
-import io
-import math
-import warnings
-from collections import OrderedDict
-from copy import deepcopy
-from pathlib import Path
-from typing import Dict, List, Optional, Union
-
-import deprecation
-import numpy as np
-import pandas as pd
-
-import xtgeo
-import xtgeo.common.constants as const
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo import XTGeoCLibError
-
-from . import _well_io, _well_oper, _well_roxapi, _wellmarkers
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-# pylint: disable=too-many-public-methods
-
-
-# ======================================================================================
-# METHODS as wrappers to class init + import
-
-
-def _data_reader_factory(file_format):
-    if file_format in ["rmswell", "irap_ascii"]:
-        return _well_io.import_rms_ascii
-    if file_format == "hdf":
-        return _well_io.import_hdf5_well
-    raise ValueError(
-        f"Unknown file format {file_format}, supported formats are "
-        "'rmswell', 'irap_ascii' and 'hdf'"
-    )
-
-
-def well_from_file(
-    wfile: Union[str, Path],
-    fformat: Optional[str] = "rms_ascii",
-    mdlogname: Optional[str] = None,
-    zonelogname: Optional[str] = None,
-    lognames: Optional[Union[str, List[str]]] = "all",
-    lognames_strict: Optional[bool] = False,
-    strict: Optional[bool] = False,
-) -> "Well":
-    """Make an instance of a Well directly from file import.
-
-    Note:
-
-      rms_ascii is the only correct for wells from RMS. Irap did not have this
-      format. For maps and points, the formats from the old Irap tool is
-      applied in RMS, hence "irap_ascii" and "rms_ascii" are there the same.
-
-    Args:
-        wfile: File path, either a string or a pathlib.Path instance
-        fformat: See :meth:`Well.from_file`
-        mdlogname: Name of Measured Depth log if any
-        zonelogname: Name of Zonelog, if any
-        lognames: Name or list of lognames to import, default is "all"
-        lognames_strict: If True, all lognames must be present.
-        strict: If True, then import will fail if zonelogname or mdlogname are asked
-            for but not present in wells.
-
-    Example::
-
-        >>> import xtgeo
-        >>> mywell = xtgeo.well_from_file(well_dir + "/OP_1.w")
-
-    .. versionchanged:: 2.1 Added ``lognames`` and ``lognames_strict``
-    .. versionchanged:: 2.1 ``strict`` now defaults to False
-    """
-    return Well._read_file(
-        wfile,
-        fformat=fformat,
-        mdlogname=mdlogname,
-        zonelogname=zonelogname,
-        strict=strict,
-        lognames=lognames,
-        lognames_strict=lognames_strict,
-    )
-
-
-def well_from_roxar(
-    project: Union[str, object],
-    name: str,
-    trajectory: Optional[str] = "Drilled trajectory",
-    logrun: Optional[str] = "log",
-    lognames: Optional[Union[str, List[str]]] = "all",
-    lognames_strict: Optional[bool] = False,
-    inclmd: Optional[bool] = False,
-    inclsurvey: Optional[bool] = False,
-) -> "Well":
-    """This makes an instance of a Well directly from Roxar RMS.
-
-
-    Note this method works only when inside RMS, or when RMS license is
-    activated.
-
-    Args:
-        project: Path to project or magic ``project`` variable in RMS.
-        name: Name of Well, as shown in RMS.
-        trajectory: Name of trajectory in RMS.
-        logrun: Name of logrun in RMS.
-        lognames: List of lognames to import or use 'all' for all present logs
-        lognames_strict: If True and log is not in lognames is a list, an Exception will
-            be raised.
-        inclmd: If True, a Measured Depth log will be included.
-        inclsurvey: If True, logs for azimuth and deviation will be included.
-
-    Returns:
-        Well instance.
-
-    Example::
-
-        # inside RMS:
-        import xtgeo
-        mylogs = ['ZONELOG', 'GR', 'Facies']
-        mywell = xtgeo.well_from_roxar(
-            project, "31_3-1", trajectory="Drilled", logrun="log", lognames=mylogs
-        )
-
-    .. versionchanged:: 2.1 lognames defaults to "all", not None
-    """
-    return Well._read_roxar(
-        project,
-        name,
-        trajectory=trajectory,
-        logrun=logrun,
-        lognames=lognames,
-        lognames_strict=lognames_strict,
-        inclmd=inclmd,
-        inclsurvey=inclsurvey,
-    )
-
-
-def allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the
-    # construction of Well and should be deleted once the deprecation period
-    # has expired, the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(self, *args, **kwargs):
-        if not args and not kwargs:
-            warnings.warn(
-                "Initializing empty well is deprecated, please provide "
-                "non-defaulted values, or use mywell = "
-                "xtgeo.well_from_file('filename')",
-                DeprecationWarning,
-            )
-            return func(
-                self,
-                *([0.0] * 3),
-                "",
-                pd.DataFrame({"X_UTME": [], "Y_UTMN": [], "Z_TVDSS": []}),
-            )
-
-        # Checking if we are doing an initialization from file and raise a
-        # deprecation warning if we are.
-        if "wfile" in kwargs or (
-            len(args) >= 1 and isinstance(args[0], (str, Path, xtgeo._XTGeoFile))
-        ):
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "mywell = xtgeo.well_from_file('filename') instead",
-                DeprecationWarning,
-            )
-            if len(args) >= 1:
-                wfile = args[0]
-                args = args[1:]
-            else:
-                wfile = kwargs.pop("wfile", None)
-            if len(args) >= 1:
-                fformat = args[0]
-                args = args[1:]
-            else:
-                fformat = kwargs.pop("fformat", None)
-
-            mfile = xtgeo._XTGeoFile(wfile)
-            if fformat is None or fformat == "guess":
-                fformat = mfile.detect_fformat()
-            else:
-                fformat = mfile.generic_format_by_proposal(fformat)
-            kwargs = _data_reader_factory(fformat)(mfile, *args, **kwargs)
-            kwargs["filesrc"] = mfile.file
-            return func(self, **kwargs)
-        return func(self, *args, **kwargs)
-
-    return wrapper
-
-
-class Well:
-    """Class for a well in the XTGeo framework.
-
-    The well logs are stored in a Pandas dataframe, which make manipulation
-    easy and fast.
-
-    The well trajectory are here represented as logs, and XYZ have magic names:
-    ``X_UTME``, ``Y_UTMN``, ``Z_TVDSS``, which are the three first Pandas columns.
-
-    Other geometry logs has also 'semi-magic' names:
-
-    M_MDEPTH or Q_MDEPTH: Measured depth, either real/true (M_xx) or
-    quasi computed/estimated (Q_xx). The Quasi may be incorrect for
-    all uses, but sufficient for some computations.
-
-    Similar for M_INCL, Q_INCL, M_AZI, Q_ASI.
-
-    All Pandas values (yes, discrete also!) are currently stored as float64
-    format, and undefined values are Nan. Integers are stored as Float due
-    to the (historic) lacking support for 'Integer Nan'. In coming versions,
-    use of ``pandas.NA`` (available from Pandas version 1.0) may be implemented.
-
-    Note there is a method that can return a dataframe (copy) with Integer
-    and Float columns, see :meth:`get_filled_dataframe`.
-
-    The instance can be made either from file or (todo!) by specification::
-
-        >>> well1 = Well(well_dir + '/OP_1.w')  # assume RMS ascii well
-        >>> well2 = Well(well_dir + '/OP_1.w', fformat='rms_ascii')
-        >>> well3 = xtgeo.well_from_file(well_dir + '/OP_1.w')
-
-    Args:
-        rkb: well RKB height
-        xpos: well head X pos
-        ypos: well head Y pos
-        wname: well name
-        df: pandas dataframe with log values, expects columns to include
-          'X_UTME', 'Y_UTMN', 'Z_TVDSS' for x, y and z coordinates.
-          Other columns should be log values.
-        filesrc: source file if any
-        mdlogname: Name of Measured Depth log if any.
-        zonelogname: Name of Zonelog, if any
-        wlogtypes: dictionary of log types, 'DISC' or 'CONT', defaults to
-                to 'CONT'.
-        wlogrecords: dictionary of codes for 'DISC' logs, None for no codes given,
-            defaults to None.
-    """
-
-    VALID_LOGTYPES = {"DISC", "CONT"}
-
-    @allow_deprecated_init
-    def __init__(
-        self,
-        rkb: float,
-        xpos: float,
-        ypos: float,
-        wname: str,
-        df: pd.DataFrame,
-        mdlogname: str = None,
-        zonelogname: str = None,
-        wlogtypes: Dict[str, str] = None,
-        wlogrecords: Dict[str, str] = None,
-        filesrc: Optional[Union[str, Path]] = None,
-    ):
-        if not all(
-            coordinate in df.columns for coordinate in ("X_UTME", "Y_UTMN", "Z_TVDSS")
-        ):
-            raise ValueError(
-                "Well dataframe must include 'X_UTME',"
-                f" 'Y_UTMN' and 'Z_TVDSS', got {df.columns}"
-            )
-        self._reset(
-            rkb,
-            xpos,
-            ypos,
-            wname,
-            df,
-            filesrc,
-            mdlogname,
-            zonelogname,
-            wlogtypes,
-            wlogrecords,
-        )
-
-    def _reset(
-        self,
-        rkb: float = None,
-        xpos: float = None,
-        ypos: float = None,
-        wname: str = None,
-        df: pd.DataFrame = None,
-        filesrc: Optional[Union[str, Path]] = None,
-        mdlogname: str = None,
-        zonelogname: str = None,
-        wlogtypes: Dict[str, str] = None,
-        wlogrecords: Dict[str, str] = None,
-    ):
-        if wlogtypes is None:
-            wlogtypes = dict()
-        if wlogrecords is None:
-            wlogrecords = dict()
-
-        self._rkb = rkb
-        self._xpos = xpos
-        self._ypos = ypos
-        self._wname = wname
-        self._filesrc = filesrc
-        self._mdlogname = mdlogname
-        self._zonelogname = zonelogname
-
-        self._wlogtypes = wlogtypes
-        self._wlogrecords = wlogrecords
-
-        self._df = df
-
-        self._wlognames = list(self._df.columns)
-
-        self._metadata = xtgeo.MetaDataWell()
-        self._metadata.required = self
-
-        self._ensure_consistency()
-
-    def __repr__(self):  # noqa: D105
-        # should be able to newobject = eval(repr(thisobject))
-        myrp = (
-            f"{self.__class__.__name__} (filesrc={self._filesrc!r}, "
-            f"name={self._wname!r},  ID={id(self)})"
-        )
-        return myrp
-
-    def __str__(self):  # noqa: D105
-        # user friendly print
-        return self.describe(flush=False)
-
-    def _ensure_consistency(self):  # pragma: no coverage
-        """Ensure consistency within an object (private function).
-
-        Consistency checking. As well log names are columns in the Pandas DF,
-        there are additional attributes per log that have to be "in sync".
-        """
-        if self._df is None:
-            return
-
-        self._wlognames = list(self._df.columns)
-
-        for logname in self._wlognames:
-            if logname not in self._wlogtypes:
-                self._wlogtypes[logname] = "CONT"  # continuous as default
-                self._wlogrecords[logname] = None  # None as default
-            else:
-                if self._wlogtypes[logname] not in self.VALID_LOGTYPES:
-                    self._wlogtypes[logname] = "CONT"
-                    self._wlogrecords[logname] = None  # None as default
-
-            if logname not in self._wlogrecords:
-                if self._wlogtypes[logname] == "DISC":
-                    # it is a discrete log with missing record; try to find
-                    # a default one based on current values...
-                    lvalues = self._df[logname].values.round(decimals=0)
-                    lmin = int(lvalues.min())
-                    lmax = int(lvalues.max())
-
-                    lvalues = lvalues.astype("int")
-                    codes = {}
-                    for lval in range(lmin, lmax + 1):
-                        if lval in lvalues:
-                            codes[lval] = str(lval)
-
-                    self._wlogrecords = codes
-
-    # ==================================================================================
-    # Properties
-    # ==================================================================================
-
-    @property
-    def metadata(self):
-        """Return metadata object instance of type MetaDataRegularSurface."""
-        return self._metadata
-
-    @metadata.setter
-    def metadata(self, obj):
-        # The current metadata object can be replaced. This is a bit dangerous so
-        # further check must be done to validate. TODO.
-        if not isinstance(obj, xtgeo.MetaDataWell):
-            raise ValueError("Input obj not an instance of MetaDataRegularCube")
-
-        self._metadata = obj
-
-    @property
-    def rkb(self):
-        """Returns RKB height for the well (read only)."""
-        return self._rkb
-
-    @property
-    def xpos(self):
-        """Returns well header X position (read only)."""
-        return self._xpos
-
-    @property
-    def ypos(self) -> float:
-        """Returns well header Y position (read only)."""
-        return self._ypos
-
-    @property
-    def wellname(self):
-        """str: Returns well name, read only."""
-        return self._wname
-
-    @property
-    def name(self):
-        """Returns or set (rename) a well name."""
-        return self._wname
-
-    @name.setter
-    def name(self, newname):
-        self._wname = newname
-
-    # alias
-    wname = name
-
-    @property
-    def safewellname(self):
-        """Get well name on syntax safe form; '/' and spaces replaced with '_'."""
-        xname = self._wname
-        xname = xname.replace("/", "_")
-        xname = xname.replace(" ", "_")
-        return xname
-
-    @property
-    def xwellname(self):
-        """See safewellname."""
-        return self.safewellname
-
-    @property
-    def shortwellname(self):
-        """str: Well name on a short form where blockname/spaces removed (read only).
-
-        This should cope with both North Sea style and Haltenbanken style.
-
-        E.g.: '31/2-G-5 AH' -> 'G-5AH', '6472_11-F-23_AH_T2' -> 'F-23AHT2'
-
-        """
-        return self.get_short_wellname(self.wellname)
-
-    @property
-    def truewellname(self):
-        """Returns well name on the assummed form aka '31/2-E-4 AH2'."""
-        xname = self.xwellname
-        if "/" not in xname:
-            xname = xname.replace("_", "/", 1)
-            xname = xname.replace("_", " ")
-        return xname
-
-    @property
-    def mdlogname(self):
-        """str: Returns name of MD log, if any (None if missing)."""
-        return self._mdlogname
-
-    @mdlogname.setter
-    def mdlogname(self, mname):
-        if mname in self._wlognames:
-            self._mdlogname = mname
-        else:
-            self._mdlogname = None
-
-    @property
-    def zonelogname(self):
-        """str: Returns or sets name of zone log, return None if missing."""
-        return self._zonelogname
-
-    @zonelogname.setter
-    def zonelogname(self, zname):
-        if zname in self._wlognames:
-            self._zonelogname = zname
-        else:
-            self._zonelogname = None
-
-    @property
-    def dataframe(self):
-        """Returns or set the Pandas dataframe object for all logs."""
-        return self._df
-
-    @dataframe.setter
-    def dataframe(self, dfr):
-        self._df = dfr.copy()
-        self._ensure_consistency()
-
-    @property
-    def nrow(self):
-        """int: Returns the Pandas dataframe object number of rows."""
-        return len(self._df.index)
-
-    @property
-    def ncol(self):
-        """int: Returns the Pandas dataframe object number of columns."""
-        return len(self._df.columns)
-
-    @property
-    def nlogs(self):
-        """int: Returns the Pandas dataframe object number of columns."""
-        return len(self._df.columns) - 3
-
-    @property
-    def lognames_all(self):
-        """list: Returns dataframe column names as list, including mandatory coords."""
-        self._ensure_consistency()
-        return self._wlognames
-
-    @property
-    def lognames(self):
-        """list: Returns the Pandas dataframe column as list excluding coords."""
-        return list(self._df)[3:]
-
-    # ==================================================================================
-    # Methods
-    # ==================================================================================
-
-    @staticmethod
-    def get_short_wellname(wellname):
-        """Well name on a short name form where blockname and spaces are removed.
-
-        This should cope with both North Sea style and Haltenbanken style.
-        E.g.: '31/2-G-5 AH' -> 'G-5AH', '6472_11-F-23_AH_T2' -> 'F-23AHT2'
-        """
-        newname = []
-        first1 = False
-        first2 = False
-        for letter in wellname:
-            if first1 and first2:
-                newname.append(letter)
-                continue
-            if letter in ("_", "/"):
-                first1 = True
-                continue
-            if first1 and letter == "-":
-                first2 = True
-                continue
-
-        xname = "".join(newname)
-        xname = xname.replace("_", "")
-        xname = xname.replace(" ", "")
-        return xname
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout."""
-        dsc = xtgeo.common.XTGDescription()
-
-        dsc.title("Description of Well instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("File source", self._filesrc)
-        dsc.txt("Well name", self._wname)
-        dsc.txt("RKB", self._rkb)
-        dsc.txt("Well head", self._xpos, self._ypos)
-        dsc.txt("Name of all columns", self.lognames_all)
-        dsc.txt("Name of log columns", self.lognames)
-        for wlog in self.lognames:
-            rec = self.get_logrecord(wlog)
-            if rec is not None and len(rec) > 3:
-                string = "("
-                nlen = len(rec)
-                for idx, (code, val) in enumerate(rec.items()):
-                    if idx < 2:
-                        string += f"{code}: {val} "
-                    elif idx == nlen - 1:
-                        string += f"...  {code}: {val})"
-            else:
-                string = f"{rec}"
-            dsc.txt("Logname", wlog, self.get_logtype(wlog), string)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.well_from_file() instead",
-    )
-    def from_file(
-        self,
-        wfile,
-        fformat="rms_ascii",
-        **kwargs,
-    ):
-        """Deprecated, see :meth:`xtgeo.well_from_file`"""
-
-        wfile = xtgeo._XTGeoFile(wfile)
-        if fformat is None or fformat == "guess":
-            fformat = wfile.detect_fformat()
-        else:
-            fformat = wfile.generic_format_by_proposal(fformat)  # default
-
-        kwargs = _data_reader_factory(fformat)(wfile, **kwargs)
-        self._reset(**kwargs)
-        return self
-
-    @classmethod
-    def _read_file(
-        cls,
-        wfile,
-        fformat="rms_ascii",
-        **kwargs,
-    ):
-        """Import well from file.
-
-        Args:
-            wfile (str): Name of file as string or pathlib.Path
-            fformat (str): File format, rms_ascii (rms well) is
-                currently supported and default format.
-            mdlogname (str): Name of measured depth log, if any
-            zonelogname (str): Name of zonation log, if any
-            strict (bool): If True, then import will fail if
-                zonelogname or mdlogname are asked for but not present
-                in wells. If False, and e.g. zonelogname is not present, the
-                attribute ``zonelogname`` will be set to None.
-            lognames (str or list): Name or list of lognames to import, default is "all"
-            lognames_strict (bool): Flag to require all logs in lognames (unless "all")
-                or to just accept that subset that is present. Default is `False`.
-
-
-        Returns:
-            Object instance (optionally)
-
-        Example:
-            Here the from_file method is used to initiate the object
-            directly::
-
-            >>> mywell = Well().from_file(well_dir + '/OP_1.w')
-
-        .. versionchanged:: 2.1 ``lognames`` and ``lognames_strict`` added
-        .. versionchanged:: 2.1 ``strict`` now defaults to False
-        """
-
-        wfile = xtgeo._XTGeoFile(wfile)
-
-        if fformat is None or fformat == "guess":
-            fformat = wfile.detect_fformat()
-        else:
-            fformat = wfile.generic_format_by_proposal(fformat)  # default
-
-        kwargs = _data_reader_factory(fformat)(wfile, **kwargs)
-        return cls(**kwargs)
-
-    def to_file(
-        self,
-        wfile: Union[str, Path, io.BytesIO],
-        fformat: Optional[str] = "rms_ascii",
-    ):
-        """Export well to file or memory stream.
-
-        Args:
-            wfile: File name or stream.
-            fformat: File format ('rms_ascii'/'rmswell', 'hdf/hdf5/h5').
-
-        Example::
-
-            >>> xwell = Well(well_dir + '/OP_1.w')
-            >>> xwell.dataframe['Poro'] += 0.1
-            >>> filename = xwell.to_file(outdir + "/somefile_copy.rmswell")
-
-        """
-        wfile = xtgeo._XTGeoFile(wfile, mode="wb", obj=self)
-
-        wfile.check_folder(raiseerror=OSError)
-
-        self._ensure_consistency()
-
-        if fformat in (None, "rms_ascii", "rms_asc", "rmsasc", "rmswell"):
-            _well_io.export_rms_ascii(self, wfile.name)
-
-        elif fformat in ("hdf", "hdf5", "h5"):
-            self.to_hdf(wfile)
-
-        return wfile.file
-
-    def from_hdf(
-        self,
-        wfile: Union[str, Path],
-    ):
-        """Deprecated, use :meth:`xtgeo.well_from_file()`"""
-        return self.from_file(wfile, fformat="hdf")
-
-    def to_hdf(
-        self,
-        wfile: Union[str, Path],
-        compression: Optional[str] = "lzf",
-    ) -> Path:
-        """Export well to HDF based file.
-
-        Warning:
-            This implementation is currently experimental and only recommended
-            for testing.
-
-        Args:
-            wfile: HDF File name to write to export to.
-
-        Returns:
-            A Path instance to actual file applied.
-
-        .. versionadded:: 2.14
-        """
-        wfile = xtgeo._XTGeoFile(wfile, mode="wb", obj=self)
-
-        wfile.check_folder(raiseerror=OSError)
-
-        _well_io.export_hdf5_well(self, wfile, compression=compression)
-
-        return wfile.file
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.well_from_roxar() instead",
-    )
-    def from_roxar(
-        self,
-        project: Union[str, object],
-        name: str,
-        trajectory: Optional[str] = "Drilled trajectory",
-        logrun: Optional[str] = "log",
-        lognames: Optional[Union[str, List[str]]] = "all",
-        lognames_strict: Optional[bool] = False,
-        inclmd: Optional[bool] = False,
-        inclsurvey: Optional[bool] = False,
-    ):
-        """Deprecated, use :meth:`xtgeo.well_from_roxar()`"""
-        kwargs = _well_roxapi.import_well_roxapi(
-            project,
-            name,
-            trajectory=trajectory,
-            logrun=logrun,
-            lognames=lognames,
-            lognames_strict=lognames_strict,
-            inclmd=inclmd,
-            inclsurvey=inclsurvey,
-        )
-        self._reset(**kwargs)
-        return self
-
-    @classmethod
-    def _read_roxar(
-        cls,
-        project: Union[str, object],
-        name: str,
-        trajectory: Optional[str] = "Drilled trajectory",
-        logrun: Optional[str] = "log",
-        lognames: Optional[Union[str, List[str]]] = "all",
-        lognames_strict: Optional[bool] = False,
-        inclmd: Optional[bool] = False,
-        inclsurvey: Optional[bool] = False,
-    ):
-        kwargs = _well_roxapi.import_well_roxapi(
-            project,
-            name,
-            trajectory=trajectory,
-            logrun=logrun,
-            lognames=lognames,
-            lognames_strict=lognames_strict,
-            inclmd=inclmd,
-            inclsurvey=inclsurvey,
-        )
-        return cls(**kwargs)
-
-    def to_roxar(self, *args, **kwargs):
-        """Export (save/store) a well to a roxar project.
-
-        Note this method works only when inside RMS, or when RMS license is
-        activated.
-
-        The current implementation will either update existing well names
-        (then well log array size must not change), or it will make a new well in RMS.
-
-        Note:
-           When project is file path (direct access, outside RMS) then
-           ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-           will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str): Magic string 'project' or file path to project
-            wname (str): Name of well, as shown in RMS.
-            lognames (:obj:list or :obj:str): List of lognames to save, or
-                use simply 'all' for current logs for this well. Default is 'all'
-            realisation (int): Currently inactive
-            trajectory (str): Name of trajectory in RMS
-            logrun (str): Name of logrun in RMS
-
-        .. versionadded:: 2.12
-        .. versionchanged:: 2.15
-            Saving to new wells enabled (earlier only modifying existing)
-
-        """
-        # use *args, **kwargs since this method is overrided in blocked_well, and
-        # signature should be the same
-
-        project = args[0]
-        wname = args[1]
-        lognames = kwargs.get("lognames", "all")
-        trajectory = kwargs.get("trajectory", "Drilled trajectory")
-        logrun = kwargs.get("logrun", "log")
-        realisation = kwargs.get("realisation", 0)
-
-        logger.debug("Not in use: realisation %s", realisation)
-
-        _well_roxapi.export_well_roxapi(
-            self,
-            project,
-            wname,
-            lognames=lognames,
-            trajectory=trajectory,
-            logrun=logrun,
-            realisation=realisation,
-        )
-
-    def get_wlogs(self) -> OrderedDict:
-        """Get a compound dictionary with well log metadata.
-
-        The result will be an Ordered dict on the form:
-
-        ``{"X_UTME": ["CONT", None], ... "Facies": ["DISC", {1: "BG", 2: "SAND"}]}``
-        """
-        res = OrderedDict()
-
-        for key in self._wlognames:
-            wtype = "CONT"
-            wrecord = None
-            if key in self._wlogtypes:
-                wtype = self._wlogtypes[key]
-            if key in self._wlogrecords:
-                wrecord = self._wlogrecords[key]
-
-            res[key] = [wtype, wrecord]
-
-        return res
-
-    def set_wlogs(self, wlogs: OrderedDict):
-        """Set a compound dictionary with well log metadata.
-
-        This operation is somewhat risky as it may lead to inconsistency, so use with
-        care! Typically, one will use :meth:`get_wlogs` first and then modify some
-        attributes.
-
-        Args:
-            wlogs: Input data dictionary
-
-        Raises:
-            ValueError: Invalid log type found in input:
-            ValueError: Invalid log record found in input:
-            ValueError: Invalid input key found:
-            ValueError: Invalid log record found in input:
-
-        """
-        for key in self._wlognames:
-            if key in wlogs.keys():
-                typ, rec = wlogs[key]
-
-                if typ in Well.VALID_LOGTYPES:
-                    self._wlogtypes[key] = deepcopy(typ)
-                else:
-                    raise ValueError(f"Invalid log type found in input: {typ}")
-
-                if rec is None or isinstance(rec, dict):
-                    self._wlogrecords[key] = deepcopy(rec)
-                else:
-                    raise ValueError(f"Invalid log record found in input: {rec}")
-
-            else:
-                raise ValueError(f"Key for column not found in input: {key}")
-
-        for key in wlogs.keys():
-            if key not in self._wlognames:
-                raise ValueError(f"Invalid input key found: {key}")
-
-        self._ensure_consistency()
-
-    def isdiscrete(self, logname):
-        """Return True of log is discrete, otherwise False.
-
-        Args:
-            logname (str): Name of log to check if discrete or not
-
-        .. versionadded:: 2.2.0
-        """
-        if logname in self._wlognames and self.get_logtype(logname) == "DISC":
-            return True
-        return False
-
-    def copy(self):
-        """Copy a Well instance to a new unique Well instance."""
-        return Well(
-            self.rkb,
-            self.xpos,
-            self.ypos,
-            self.wname,
-            self._df.copy(),
-            self.mdlogname,
-            self.zonelogname,
-            deepcopy(self._wlogtypes),
-            deepcopy(self._wlogrecords),
-            self._filesrc,
-        )
-
-    def rename_log(self, lname, newname):
-        """Rename a log, e.g. Poro to PORO."""
-        self._ensure_consistency()
-
-        if lname not in self.lognames:
-            raise ValueError("Input log does not exist")
-
-        if newname in self.lognames:
-            raise ValueError("New log name exists already")
-
-        self._wlogtypes[newname] = self._wlogtypes.pop(lname)
-        self._wlogrecords[newname] = self._wlogrecords.pop(lname)
-
-        # rename in dataframe
-        self._df.rename(index=str, columns={lname: newname}, inplace=True)
-
-        if self._mdlogname == lname:
-            self._mdlogname = newname
-
-        if self._zonelogname == lname:
-            self._zonelogname = newname
-
-    def create_log(self, lname, logtype="CONT", logrecord=None, value=0.0, force=True):
-        """Create a new log with initial values.
-
-        If the logname already exists, it will be silently overwritten, unless
-        the option force=False.
-
-        Args:
-            lname (str): name of new log
-            logtype (str): Must be 'CONT' (default) or 'DISC' (discrete)
-            logrecord (dict): A dictionary of key: values for 'DISC' logs
-            value (float): initia value to set_index
-            force (bool): If True, and lname exists, it will be overwritten, if
-               False, no new log will be made. Will return False.
-
-        Returns:
-            True ff a new log is made (either new or force overwrite an
-            existing) or False if the new log already exists,
-            and ``force=False``.
-
-        """
-        if lname in self.lognames and force is False:
-            return False
-
-        self._wlogtypes[lname] = logtype
-        self._wlogrecords[lname] = logrecord
-
-        # make a new column
-        self._df[lname] = float(value)
-        self._ensure_consistency()
-        return True
-
-    def delete_log(self, lname):
-        """Delete/remove an existing log, or list of logs.
-
-        Will continue silently if a log does not exist.
-
-        Args:
-            lname(str or list): A logname or a list of lognames
-
-        Returns:
-            Number of logs deleted
-        """
-        return _well_oper.delete_log(self, lname)
-
-    delete_logs = delete_log  # alias function
-
-    def get_logtype(self, lname):
-        """Returns the type of a give log (e.g. DISC or CONT)."""
-        self._ensure_consistency()
-
-        if lname in self._wlogtypes:
-            return self._wlogtypes[lname]
-        return None
-
-    def set_logtype(self, lname, ltype):
-        """Sets the type of a give log (e.g. DISC or CONT)."""
-        self._ensure_consistency()
-
-        valid = {"DISC", "CONT"}
-
-        if ltype in valid:
-            self._wlogtypes[lname] = ltype
-        else:
-            raise ValueError(f"Try to set invalid log type: {ltype}")
-
-    def get_logrecord(self, lname):
-        """Returns the record (dict) of a given log name, None if not exists."""
-        if lname in self._wlogtypes:
-            return self._wlogrecords[lname]
-
-        return None
-
-    def set_logrecord(self, lname, newdict):
-        """Sets the record (dict) of a given discrete log."""
-        self._ensure_consistency()
-        if lname not in self.lognames:
-            raise ValueError(f"No such logname: {lname}")
-
-        if self._wlogtypes[lname] == "CONT":
-            raise ValueError("Cannot set a log record for a continuous log")
-
-        if not isinstance(newdict, dict):
-            raise ValueError("Input is not a dictionary")
-
-        self._wlogrecords[lname] = newdict
-
-    def get_logrecord_codename(self, lname, key):
-        """Returns the name entry of a log record, for a given key.
-
-        Example::
-
-            # get the name for zonelog entry no 4:
-            zname = well.get_logrecord_codename('ZONELOG', 4)
-        """
-        zlogdict = self.get_logrecord(lname)
-        if key in zlogdict:
-            return zlogdict[key]
-
-        return None
-
-    def get_carray(self, lname):
-        """Returns the C array pointer (via SWIG) for a given log.
-
-        Type conversion is double if float64, int32 if DISC log.
-        Returns None of log does not exist.
-        """
-        if lname in self._df:
-            np_array = self._df[lname].values
-        else:
-            return None
-
-        if self.get_logtype(lname) == "DISC":
-            carr = self._convert_np_carr_int(np_array)
-        else:
-            carr = self._convert_np_carr_double(np_array)
-
-        return carr
-
-    def get_filled_dataframe(
-        self, fill_value=const.UNDEF, fill_value_int=const.UNDEF_INT
-    ):
-        """Fill the Nan's in the dataframe with real UNDEF values.
-
-        This module returns a copy of the dataframe in the object; it
-        does not change the instance.
-
-        Note that DISC logs will be casted to columns with integer
-        as datatype.
-
-        Returns:
-            A pandas dataframe where Nan er replaces with preset
-                high XTGeo UNDEF values, or user defined values.
-
-        """
-        lnames = self.lognames
-
-        newdf = self._df.copy()
-
-        # make a dictionary of datatypes
-        dtype = {"X_UTME": "float64", "Y_UTMN": "float64", "Z_TVDSS": "float64"}
-
-        dfill = {"X_UTME": const.UNDEF, "Y_UTMN": const.UNDEF, "Z_TVDSS": const.UNDEF}
-
-        for lname in lnames:
-            if self.get_logtype(lname) == "DISC":
-                dtype[lname] = np.int32
-                dfill[lname] = fill_value_int
-            else:
-                dtype[lname] = np.float64
-                dfill[lname] = fill_value
-
-        # now first fill Nan's (because int cannot be converted if Nan)
-        newdf = newdf.fillna(dfill)
-        newdf = newdf.astype(dtype)
-
-        return newdf
-
-    def create_relative_hlen(self):
-        """Make a relative length of a well, as a log.
-
-        The first well og entry defines zero, then the horizontal length
-        is computed relative to that by simple geometric methods.
-        """
-        # extract numpies from XYZ trajectory logs
-        xv = self._df["X_UTME"].values
-        yv = self._df["Y_UTMN"].values
-
-        distance = []
-        previous_x, previous_y = xv[0], yv[0]
-        for i, (x, y) in enumerate(zip(xv, yv)):
-            distance.append(math.hypot((previous_x - x), (y - previous_y)))
-            previous_x, previous_y = x, y
-
-        self._df["R_HLEN"] = pd.Series(np.cumsum(distance), index=self._df.index)
-
-    def geometrics(self):
-        """Compute some well geometrical arrays MD, INCL, AZI, as logs.
-
-        These are kind of quasi measurements hence the logs will named
-        with a Q in front as Q_MDEPTH, Q_INCL, and Q_AZI.
-
-        These logs will be added to the dataframe. If the mdlogname
-        attribute does not exist in advance, it will be set to 'Q_MDEPTH'.
-
-        Returns:
-            False if geometrics cannot be computed
-
-        """
-        if self._df.shape[0] < 3:
-            raise ValueError(
-                f"Cannot compute geometrics for {self.name}. Not enough "
-                f"trajectory points (need >3, have: {self.dataframe.shape[0]})"
-            )
-
-        # extract numpies from XYZ trajetory logs
-        ptr_xv = self.get_carray("X_UTME")
-        ptr_yv = self.get_carray("Y_UTMN")
-        ptr_zv = self.get_carray("Z_TVDSS")
-
-        # get number of rows in pandas
-        nlen = self.nrow
-
-        ptr_md = _cxtgeo.new_doublearray(nlen)
-        ptr_incl = _cxtgeo.new_doublearray(nlen)
-        ptr_az = _cxtgeo.new_doublearray(nlen)
-
-        ier = _cxtgeo.well_geometrics(
-            nlen, ptr_xv, ptr_yv, ptr_zv, ptr_md, ptr_incl, ptr_az, 0
-        )
-
-        if ier != 0:
-            raise XTGeoCLibError(f"well_geometrics failed with error code: {ier}")
-
-        dnumpy = self._convert_carr_double_np(ptr_md)
-        self._df["Q_MDEPTH"] = pd.Series(dnumpy, index=self._df.index)
-
-        dnumpy = self._convert_carr_double_np(ptr_incl)
-        self._df["Q_INCL"] = pd.Series(dnumpy, index=self._df.index)
-
-        dnumpy = self._convert_carr_double_np(ptr_az)
-        self._df["Q_AZI"] = pd.Series(dnumpy, index=self._df.index)
-
-        if not self._mdlogname:
-            self._mdlogname = "Q_MDEPTH"
-
-        # delete tmp pointers
-        _cxtgeo.delete_doublearray(ptr_xv)
-        _cxtgeo.delete_doublearray(ptr_yv)
-        _cxtgeo.delete_doublearray(ptr_zv)
-        _cxtgeo.delete_doublearray(ptr_md)
-        _cxtgeo.delete_doublearray(ptr_incl)
-        _cxtgeo.delete_doublearray(ptr_az)
-
-        return True
-
-    def truncate_parallel_path(
-        self, other, xtol=None, ytol=None, ztol=None, itol=None, atol=None
-    ):
-        """Truncate the part of the well trajectory that is ~parallel with other.
-
-        Args:
-            other (Well): Other well to compare with
-            xtol (float): Tolerance in X (East) coord for measuring unit
-            ytol (float): Tolerance in Y (North) coord for measuring unit
-            ztol (float): Tolerance in Z (TVD) coord for measuring unit
-            itol (float): Tolerance in inclination (degrees)
-            atol (float): Tolerance in azimuth (degrees)
-        """
-        if xtol is None:
-            xtol = 0.0
-        if ytol is None:
-            ytol = 0.0
-        if ztol is None:
-            ztol = 0.0
-        if itol is None:
-            itol = 0.0
-        if atol is None:
-            atol = 0.0
-
-        if self.dataframe.shape[0] < 3 or other.dataframe.shape[0] < 3:
-            raise ValueError(
-                f"Too few points to truncate parallel path, was {self._df.size} and "
-                f"{other._df.size}, must be >3"
-            )
-
-        # extract numpies from XYZ trajectory logs
-        xv1 = self._df["X_UTME"].values
-        yv1 = self._df["Y_UTMN"].values
-        zv1 = self._df["Z_TVDSS"].values
-
-        xv2 = other._df["X_UTME"].values
-        yv2 = other._df["Y_UTMN"].values
-        zv2 = other._df["Z_TVDSS"].values
-
-        ier = _cxtgeo.well_trunc_parallel(
-            xv1, yv1, zv1, xv2, yv2, zv2, xtol, ytol, ztol, itol, atol, 0
-        )
-
-        if ier != 0:
-            raise RuntimeError("Unexpected error")
-
-        self._df = self._df[self._df["X_UTME"] < const.UNDEF_LIMIT]
-        self._df.reset_index(drop=True, inplace=True)
-
-    def may_overlap(self, other):
-        """Consider if well overlap in X Y coordinates with other well, True/False."""
-        if self._df.size < 2 or other._df.size < 2:
-            return False
-
-        # extract numpies from XYZ trajectory logs
-        xmin1 = np.nanmin(self.dataframe["X_UTME"].values)
-        xmax1 = np.nanmax(self.dataframe["X_UTME"].values)
-        ymin1 = np.nanmin(self.dataframe["Y_UTMN"].values)
-        ymax1 = np.nanmax(self.dataframe["Y_UTMN"].values)
-
-        xmin2 = np.nanmin(other.dataframe["X_UTME"].values)
-        xmax2 = np.nanmax(other.dataframe["X_UTME"].values)
-        ymin2 = np.nanmin(other.dataframe["Y_UTMN"].values)
-        ymax2 = np.nanmax(other.dataframe["Y_UTMN"].values)
-
-        if xmin1 > xmax2 or ymin1 > ymax2:
-            return False
-        if xmin2 > xmax1 or ymin2 > ymax1:
-            return False
-
-        return True
-
-    def limit_tvd(self, tvdmin, tvdmax):
-        """Truncate the part of the well that is outside tvdmin, tvdmax.
-
-        Range will be in tvdmin <= tvd <= tvdmax.
-
-        Args:
-            tvdmin (float): Minimum TVD
-            tvdmax (float): Maximum TVD
-        """
-        self._df = self._df[self._df["Z_TVDSS"] >= tvdmin]
-        self._df = self._df[self._df["Z_TVDSS"] <= tvdmax]
-
-        self._df.reset_index(drop=True, inplace=True)
-
-    def downsample(self, interval=4, keeplast=True):
-        """Downsample by sampling every N'th element (coarsen only).
-
-        Args:
-            interval (int): Sampling interval.
-            keeplast (bool): If True, the last element from the original
-                dataframe is kept, to avoid that the well is shortened.
-        """
-        if self._df.size < 2 * interval:
-            return
-
-        dfr = self._df[::interval]
-
-        if keeplast:
-            dfr = pd.concat([dfr, self._df.iloc[-1:]], ignore_index=True)
-
-        self._df = dfr.reset_index(drop=True)
-
-    def rescale(self, delta=0.15, tvdrange=None):
-        """Rescale (refine or coarse) by sampling a delta along the trajectory, in MD.
-
-        Args:
-            delta (float): Step length
-            tvdrange (tuple of floats): Resampling can be limited to TVD interval
-
-        .. versionchanged:: 2.2 Added tvdrange
-        """
-        _well_oper.rescale(self, delta=delta, tvdrange=tvdrange)
-
-    def get_polygons(self, skipname=False):
-        """Return a Polygons object from the well trajectory.
-
-        Args:
-            skipname (bool): If True then name column is omitted
-
-        .. versionadded:: 2.1
-        .. versionchanged:: 2.13 Added `skipname` key
-        """
-        dfr = self._df.copy()
-
-        keep = ("X_UTME", "Y_UTMN", "Z_TVDSS")
-        for col in dfr.columns:
-            if col not in keep:
-                dfr.drop(labels=col, axis=1, inplace=True)
-        dfr["POLY_ID"] = 1
-
-        if not skipname:
-            dfr["NAME"] = self.xwellname
-        poly = xtgeo.Polygons()
-        poly.dataframe = dfr
-        poly.name = self.xwellname
-
-        return poly
-
-    def get_fence_polyline(self, sampling=20, nextend=2, tvdmin=None, asnumpy=True):
-        """Return a fence polyline as a numpy array or a Polygons object.
-
-        The result will aim for a regular sampling interval, useful for extracting
-        fence plots (cross-sections).
-
-        Args:
-            sampling (float): Sampling interval i.e. horizonal distance (input)
-            nextend (int): Number if sampling to extend; e.g. 2 * 20
-            tvdmin (float): Minimum TVD starting point.
-            as_numpy (bool): If True, a numpy array, otherwise a Polygons
-                object with 5 columns where the 2 last are HLEN and POLY_ID
-                and the POLY_ID will be set to 0.
-
-        Returns:
-            A numpy array of shape (NLEN, 5) in F order,
-            Or a Polygons object with 5 columns
-            If not possible, return False
-
-        .. versionchanged:: 2.1 improved algorithm
-        """
-        poly = self.get_polygons()
-
-        if tvdmin is not None:
-            poly.dataframe = poly.dataframe[poly.dataframe[poly.zname] >= tvdmin]
-            poly.dataframe.reset_index(drop=True, inplace=True)
-
-        return poly.get_fence(distance=sampling, nextend=nextend, asnumpy=asnumpy)
-
-    def create_surf_distance_log(
-        self,
-        surf: object,
-        name: Optional[str] = "DIST_SURF",
-    ):
-        """Make a log that is vertical distance to a regular surface.
-
-        If the trajectory is above the surface (i.e. more shallow), then the
-        distance sign is positive.
-
-        Args:
-            surf: The RegularSurface instance.
-            name: The name of the new log. If it exists it will be overwritten.
-
-        Example::
-
-            mywell.rescale()  # optional
-            thesurf = xtgeo.RegularSurface("some.gri")
-            mywell.create_surf_distance_log(thesurf, name="sdiff")
-
-        """
-        _well_oper.create_surf_distance_log(self, surf, name)
-
-    def report_zonation_holes(self, threshold=5):
-        """Reports if well has holes in zonation, less or equal to N samples.
-
-        Zonation may have holes due to various reasons, and
-        usually a few undef samples indicates that something is wrong.
-        This method reports well and start interval of the "holes"
-
-        The well shall have zonelog from import (via zonelogname attribute) and
-        preferly a MD log (via mdlogname attribute); however if the
-        latter is not present, a report withou MD values will be present.
-
-        Args:
-            threshold (int): Number of samples (max.) that defines a hole, e.g.
-                5 means that undef samples in the range [1, 5] (including 5) is
-                applied
-
-        Returns:
-            A Pandas dataframe as a report. None if no list is made.
-
-        Raises:
-            RuntimeError if zonelog is not present
-        """
-        dfr = _well_oper.report_zonation_holes(self, threshold=threshold)
-
-        return dfr
-
-    def get_zonation_points(
-        self, tops=True, incl_limit=80, top_prefix="Top", zonelist=None, use_undef=False
-    ):
-        """Extract zonation points from Zonelog and make a marker list.
-
-        Currently it is either 'Tops' or 'Zone' (thicknesses); default
-        is tops (i.e. tops=True).
-
-        The `zonelist` can be a list of zones, or a tuple with two members specifying
-        first and last member. Note however that the zonation shall be without jumps
-        and increasing. E.g.::
-
-            zonelist=(1, 5)  # meaning [1, 2, 3, 4, 5]
-            # or
-            zonelist=[1, 2, 3, 4]
-            # while _not_ legal:
-            zonelist=[1, 4, 8]
-
-        Zone numbers less than 0 are not accepted
-
-        Args:
-            tops (bool): If True then compute tops, else (thickness) points.
-            incl_limit (float): If given, and usezone is True, the max
-                angle of inclination to be  used as input to zonation points.
-            top_prefix (str): As well logs usually have isochore (zone) name,
-                this prefix could be Top, e.g. 'SO43' --> 'TopSO43'
-            zonelist (list of int or tuple): Zones to use
-            use_undef (bool): If True, then transition from UNDEF is also
-                used.
-
-
-        Returns:
-            A pandas dataframe (ready for the xyz/Points class), None
-            if a zonelog is missing
-        """
-        # make a copy of the well instance as some tmp well logs are made
-        scopy = self.copy()
-
-        dfr = _wellmarkers.get_zonation_points(
-            scopy, tops, incl_limit, top_prefix, zonelist, use_undef
-        )
-
-        del scopy
-
-        return dfr
-
-    def get_zone_interval(self, zonevalue, resample=1, extralogs=None):
-        """Extract the X Y Z ID line (polyline) segment for a given zonevalue.
-
-        Args:
-            zonevalue (int): The zone value to extract
-            resample (int): If given, downsample every N'th sample to make
-                polylines smaller in terms of bit and bytes.
-                1 = No downsampling.
-            extralogs (list of str): List of extra log names to include
-
-
-        Returns:
-            A pandas dataframe X Y Z ID (ready for the xyz/Polygon class),
-            None if a zonelog is missing or actual zone does dot
-            exist in the well.
-        """
-        if resample < 1 or not isinstance(resample, int):
-            raise KeyError("Key resample of wrong type (must be int >= 1)")
-
-        dff = self.get_filled_dataframe()
-
-        # the technical solution here is to make a tmp column which
-        # will add one number for each time the actual segment is repeated,
-        # not straightforward... (thanks to H. Berland for tip)
-
-        dff["ztmp"] = dff[self.zonelogname]
-        dff["ztmp"] = (dff[self.zonelogname] != zonevalue).astype(int)
-
-        dff["ztmp"] = (dff.ztmp != dff.ztmp.shift()).cumsum()
-
-        dff = dff[dff[self.zonelogname] == zonevalue]
-
-        m1v = dff["ztmp"].min()
-        m2v = dff["ztmp"].max()
-        if np.isnan(m1v):
-            logger.debug("Returns (no data)")
-            return None
-
-        df2 = dff.copy()
-
-        dflist = []
-        for mvv in range(m1v, m2v + 1):
-            dff9 = df2.copy()
-            dff9 = df2[df2["ztmp"] == mvv]
-            if dff9.index.shape[0] > 0:
-                dflist.append(dff9)
-
-        dxlist = []
-
-        useloglist = ["X_UTME", "Y_UTMN", "Z_TVDSS", "POLY_ID"]
-        if extralogs is not None:
-            useloglist.extend(extralogs)
-
-        # pylint: disable=consider-using-enumerate
-        for ivv in range(len(dflist)):
-            dxf = dflist[ivv]
-            dxf = dxf.rename(columns={"ztmp": "POLY_ID"})
-            cols = [xxx for xxx in dxf.columns if xxx not in useloglist]
-
-            dxf = dxf.drop(cols, axis=1)
-
-            # now (down) resample every N'th
-            if resample > 1:
-                dxf = pd.concat([dxf.iloc[::resample, :], dxf.tail(1)])
-
-            dxlist.append(dxf)
-
-        dff = pd.concat(dxlist)
-        dff.reset_index(inplace=True, drop=True)
-
-        logger.debug("Dataframe from well:\n%s", dff)
-        return dff
-
-    def get_fraction_per_zone(
-        self,
-        dlogname,
-        dcodes,
-        zonelist=None,
-        incl_limit=80,
-        count_limit=3,
-        zonelogname=None,
-    ):
-        """Get fraction of a discrete parameter, e.g. a facies, per zone.
-
-        It can be constrained by an inclination.
-
-        Also, it needs to be evaluated only of ZONE is complete; either
-        INCREASE or DECREASE ; hence a quality flag is made and applied.
-
-        Args:
-            dlogname (str): Name of discrete log, e.g. 'FACIES'
-            dnames (list of int): Codes of facies (or similar) to report for
-            zonelist (list of int): Zones to use
-            incl_limit (float): Inclination limit for well path.
-            count_limit (int): Minimum number of counts required per segment
-                for valid calculations
-            zonelogname (str). If None, the Well().zonelogname attribute is
-                applied
-
-        Returns:
-            A pandas dataframe (ready for the xyz/Points class), None
-            if a zonelog is missing or or dlogname is missing,
-            list is zero length for any reason.
-        """
-        dfr = _wellmarkers.get_fraction_per_zone(
-            self,
-            dlogname,
-            dcodes,
-            zonelist=zonelist,
-            incl_limit=incl_limit,
-            count_limit=count_limit,
-            zonelogname=zonelogname,
-        )
-
-        return dfr
-
-    def mask_shoulderbeds(
-        self,
-        inputlogs: List[str],
-        targetlogs: List[str],
-        nsamples: Optional[Union[int, Dict[str, float]]] = 2,
-        strict: Optional[bool] = False,
-    ) -> bool:
-        """Mask data around zone boundaries or other discrete log boundaries.
-
-        This operates on number of samples, hence the actual distance which is masked
-        depends on the sampling interval (ie. count) or on distance measures.
-        Distance measures are TVD (true vertical depth) or MD (measured depth).
-
-        .. image:: images/wells-mask-shoulderbeds.png
-           :width: 300
-           :align: center
-
-        Args:
-            inputlogs: List of input logs, must be of discrete type.
-            targetlogs: List of logs where mask is applied.
-            nsamples: Number of samples around boundaries to filter, per side, i.e.
-                value 2 means 2 above and 2 below, in total 4 samples.
-                As alternative specify nsamples indirectly with a relative distance,
-                as a dictionary with one record, as {"tvd": 0.5} or {"md": 0.7}.
-            strict: If True, will raise Exception of any of the input or target log
-                names are missing.
-
-        Returns:
-            True if any operation has been done. False in case nothing has been done,
-                 e.g. no targetlogs for this particular well and ``strict`` is False.
-
-        Raises:
-            ValueError: Various messages when wrong or inconsistent input.
-
-        Example:
-            >>> mywell1 = Well(well_dir + '/OP_1.w')
-            >>> mywell2 = Well(well_dir + '/OP_2.w')
-            >>> did_succeed = mywell1.mask_shoulderbeds(["Zonelog", "Facies"], ["Perm"])
-            >>> did_succeed = mywell2.mask_shoulderbeds(
-            ...     ["Zonelog"],
-            ...     ["Perm"],
-            ...     nsamples={"tvd": 0.8}
-            ... )
-
-        """
-        return _well_oper.mask_shoulderbeds(
-            self, inputlogs, targetlogs, nsamples, strict
-        )
-
-    def get_surface_picks(self, surf):
-        """Return :class:`.Points` obj where well crosses the surface (horizon picks).
-
-        There may be several points in the Points() dataframe attribute.
-        Also a ``DIRECTION`` column will show 1 if surface is penetrated from
-        above, and -1 if penetrated from below.
-
-        Args:
-            surf (RegularSurface): The surface instance
-
-        Returns:
-            A :class:`.Points` instance, or None if no crossing points
-
-        .. versionadded:: 2.8
-
-        """
-        return _wellmarkers.get_surface_picks(self, surf)
-
-    def make_ijk_from_grid(self, grid, grid_id="", algorithm=2, activeonly=True):
-        """Look through a Grid and add grid I J K as discrete logs.
-
-        Note that the the grid counting has base 1 (first row is 1 etc).
-
-        By default, log (i.e. column names in the dataframe) will be
-        ICELL, JCELL, KCELL, but you can add a tag (ID) to that name.
-
-        Args:
-            grid (Grid): A XTGeo Grid instance
-            grid_id (str): Add a tag (optional) to the current log name
-            algorithm (int): Which interbal algorithm to use, default is 2 (expert
-                setting)
-            activeonly (bool): If True, only active cells are applied (algorithm 2 only)
-
-        Raises:
-            RuntimeError: 'Error from C routine, code is ...'
-
-        .. versionchanged:: 2.9 Added keys for and `activeonly`
-        """
-        _well_oper.make_ijk_from_grid(
-            self, grid, grid_id=grid_id, algorithm=algorithm, activeonly=activeonly
-        )
-
-    def make_zone_qual_log(self, zqname):
-        """Create a zone quality/indicator (flag) log.
-
-        This routine looks through to zone log and flag intervals according
-        to neighbouring zones:
-
-        * 0: Undetermined flag
-
-        * 1: Zonelog interval numbering increases,
-             e.g. for zone 2: 1 1 1 1 2 2 2 2 2 5 5 5 5 5
-
-        * 2: Zonelog interval numbering decreases,
-             e.g. for zone 2: 6 6 6 2 2 2 2 1 1 1
-
-        * 3: Interval is a U turning point, e.g. 0 0 0 2 2 2 1 1 1
-
-        * 4: Interval is a inverse U turning point, 3 3 3 2 2 2 5 5
-
-        * 9: Interval is bounded by one or more missing sections,
-             e.g. 1 1 1 2 2 2 -999 -999
-
-        If a log with the name exists, it will be silently replaced
-
-        Args:
-            zqname (str): Name of quality log
-        """
-        _well_oper.make_zone_qual_log(self, zqname)
-
-    def get_gridproperties(
-        self, gridprops, grid=("ICELL", "JCELL", "KCELL"), prop_id="_model"
-    ):
-        """Look through a Grid and add a set of grid properties as logs.
-
-        The name of the logs will ...
-
-        This can be done to sample model properties along a well.
-
-        Args:
-            gridprops (Grid): A XTGeo GridProperties instance (a collection
-                of properties) or a single GridProperty instance
-            grid (Grid or tuple): A XTGeo Grid instance or a reference
-                via tuple. If this is tuple with log names,
-                it states that these logs already contains
-                the gridcell IJK numbering.
-            prop_id (str): Add a tag (optional) to the current log name, e.g
-                as PORO_model, where _model is the tag.
-
-        Raises:
-            None
-
-        .. versionadded:: 2.1
-
-        """
-        _well_oper.get_gridproperties(self, gridprops, grid=grid, prop_id=prop_id)
-
-    # ==================================================================================
-    # PRIVATE METHODS
-    # should not be applied outside the class
-    # ==================================================================================
-
-    # ----------------------------------------------------------------------------------
-    # Import/Export methods for various formats
-    # ----------------------------------------------------------------------------------
-
-    # ----------------------------------------------------------------------------------
-    # Special methods for nerds, todo is to move to private module
-    # ----------------------------------------------------------------------------------
-
-    def _convert_np_carr_int(self, np_array):
-        """Convert numpy 1D array to C array, assuming int type.
-
-        The numpy is always a double (float64), so need to convert first
-        """
-        carr = _cxtgeo.new_intarray(self.nrow)
-
-        np_array = np_array.astype(np.int32)
-
-        _cxtgeo.swig_numpy_to_carr_i1d(np_array, carr)
-
-        return carr
-
-    def _convert_np_carr_double(self, np_array):
-        """Convert numpy 1D array to C array, assuming double type."""
-        carr = _cxtgeo.new_doublearray(self.nrow)
-
-        _cxtgeo.swig_numpy_to_carr_1d(np_array, carr)
-
-        return carr
-
-    def _convert_carr_double_np(self, carray, nlen=None):
-        """Convert a C array to numpy, assuming double type."""
-        if nlen is None:
-            nlen = len(self._df.index)
-
-        nparray = _cxtgeo.swig_carr_to_numpy_1d(nlen, carray)
-
-        return nparray
+# -*- coding: utf-8 -*-
+"""XTGeo well module, working with one single well."""
+
+import functools
+import io
+import math
+import warnings
+from collections import OrderedDict
+from copy import deepcopy
+from pathlib import Path
+from typing import Dict, List, Optional, Union
+
+import deprecation
+import numpy as np
+import pandas as pd
+
+import xtgeo
+import xtgeo.common.constants as const
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo import XTGeoCLibError
+
+from . import _well_io, _well_oper, _well_roxapi, _wellmarkers
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+# pylint: disable=too-many-public-methods
+
+
+# ======================================================================================
+# METHODS as wrappers to class init + import
+
+
+def _data_reader_factory(file_format):
+    if file_format in ["rmswell", "irap_ascii"]:
+        return _well_io.import_rms_ascii
+    if file_format == "hdf":
+        return _well_io.import_hdf5_well
+    raise ValueError(
+        f"Unknown file format {file_format}, supported formats are "
+        "'rmswell', 'irap_ascii' and 'hdf'"
+    )
+
+
+def well_from_file(
+    wfile: Union[str, Path],
+    fformat: Optional[str] = "rms_ascii",
+    mdlogname: Optional[str] = None,
+    zonelogname: Optional[str] = None,
+    lognames: Optional[Union[str, List[str]]] = "all",
+    lognames_strict: Optional[bool] = False,
+    strict: Optional[bool] = False,
+) -> "Well":
+    """Make an instance of a Well directly from file import.
+
+    Note:
+
+      rms_ascii is the only correct for wells from RMS. Irap did not have this
+      format. For maps and points, the formats from the old Irap tool is
+      applied in RMS, hence "irap_ascii" and "rms_ascii" are there the same.
+
+    Args:
+        wfile: File path, either a string or a pathlib.Path instance
+        fformat: See :meth:`Well.from_file`
+        mdlogname: Name of Measured Depth log if any
+        zonelogname: Name of Zonelog, if any
+        lognames: Name or list of lognames to import, default is "all"
+        lognames_strict: If True, all lognames must be present.
+        strict: If True, then import will fail if zonelogname or mdlogname are asked
+            for but not present in wells.
+
+    Example::
+
+        >>> import xtgeo
+        >>> mywell = xtgeo.well_from_file(well_dir + "/OP_1.w")
+
+    .. versionchanged:: 2.1 Added ``lognames`` and ``lognames_strict``
+    .. versionchanged:: 2.1 ``strict`` now defaults to False
+    """
+    return Well._read_file(
+        wfile,
+        fformat=fformat,
+        mdlogname=mdlogname,
+        zonelogname=zonelogname,
+        strict=strict,
+        lognames=lognames,
+        lognames_strict=lognames_strict,
+    )
+
+
+def well_from_roxar(
+    project: Union[str, object],
+    name: str,
+    trajectory: Optional[str] = "Drilled trajectory",
+    logrun: Optional[str] = "log",
+    lognames: Optional[Union[str, List[str]]] = "all",
+    lognames_strict: Optional[bool] = False,
+    inclmd: Optional[bool] = False,
+    inclsurvey: Optional[bool] = False,
+) -> "Well":
+    """This makes an instance of a Well directly from Roxar RMS.
+
+
+    Note this method works only when inside RMS, or when RMS license is
+    activated.
+
+    Args:
+        project: Path to project or magic ``project`` variable in RMS.
+        name: Name of Well, as shown in RMS.
+        trajectory: Name of trajectory in RMS.
+        logrun: Name of logrun in RMS.
+        lognames: List of lognames to import or use 'all' for all present logs
+        lognames_strict: If True and log is not in lognames is a list, an Exception will
+            be raised.
+        inclmd: If True, a Measured Depth log will be included.
+        inclsurvey: If True, logs for azimuth and deviation will be included.
+
+    Returns:
+        Well instance.
+
+    Example::
+
+        # inside RMS:
+        import xtgeo
+        mylogs = ['ZONELOG', 'GR', 'Facies']
+        mywell = xtgeo.well_from_roxar(
+            project, "31_3-1", trajectory="Drilled", logrun="log", lognames=mylogs
+        )
+
+    .. versionchanged:: 2.1 lognames defaults to "all", not None
+    """
+    return Well._read_roxar(
+        project,
+        name,
+        trajectory=trajectory,
+        logrun=logrun,
+        lognames=lognames,
+        lognames_strict=lognames_strict,
+        inclmd=inclmd,
+        inclsurvey=inclsurvey,
+    )
+
+
+def allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the
+    # construction of Well and should be deleted once the deprecation period
+    # has expired, the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(self, *args, **kwargs):
+        if not args and not kwargs:
+            warnings.warn(
+                "Initializing empty well is deprecated, please provide "
+                "non-defaulted values, or use mywell = "
+                "xtgeo.well_from_file('filename')",
+                DeprecationWarning,
+            )
+            return func(
+                self,
+                *([0.0] * 3),
+                "",
+                pd.DataFrame({"X_UTME": [], "Y_UTMN": [], "Z_TVDSS": []}),
+            )
+
+        # Checking if we are doing an initialization from file and raise a
+        # deprecation warning if we are.
+        if "wfile" in kwargs or (
+            len(args) >= 1 and isinstance(args[0], (str, Path, xtgeo._XTGeoFile))
+        ):
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "mywell = xtgeo.well_from_file('filename') instead",
+                DeprecationWarning,
+            )
+            if len(args) >= 1:
+                wfile = args[0]
+                args = args[1:]
+            else:
+                wfile = kwargs.pop("wfile", None)
+            if len(args) >= 1:
+                fformat = args[0]
+                args = args[1:]
+            else:
+                fformat = kwargs.pop("fformat", None)
+
+            mfile = xtgeo._XTGeoFile(wfile)
+            if fformat is None or fformat == "guess":
+                fformat = mfile.detect_fformat()
+            else:
+                fformat = mfile.generic_format_by_proposal(fformat)
+            kwargs = _data_reader_factory(fformat)(mfile, *args, **kwargs)
+            kwargs["filesrc"] = mfile.file
+            return func(self, **kwargs)
+        return func(self, *args, **kwargs)
+
+    return wrapper
+
+
+class Well:
+    """Class for a well in the XTGeo framework.
+
+    The well logs are stored in a Pandas dataframe, which make manipulation
+    easy and fast.
+
+    The well trajectory are here represented as logs, and XYZ have magic names:
+    ``X_UTME``, ``Y_UTMN``, ``Z_TVDSS``, which are the three first Pandas columns.
+
+    Other geometry logs has also 'semi-magic' names:
+
+    M_MDEPTH or Q_MDEPTH: Measured depth, either real/true (M_xx) or
+    quasi computed/estimated (Q_xx). The Quasi may be incorrect for
+    all uses, but sufficient for some computations.
+
+    Similar for M_INCL, Q_INCL, M_AZI, Q_ASI.
+
+    All Pandas values (yes, discrete also!) are currently stored as float64
+    format, and undefined values are Nan. Integers are stored as Float due
+    to the (historic) lacking support for 'Integer Nan'. In coming versions,
+    use of ``pandas.NA`` (available from Pandas version 1.0) may be implemented.
+
+    Note there is a method that can return a dataframe (copy) with Integer
+    and Float columns, see :meth:`get_filled_dataframe`.
+
+    The instance can be made either from file or (todo!) by specification::
+
+        >>> well1 = Well(well_dir + '/OP_1.w')  # assume RMS ascii well
+        >>> well2 = Well(well_dir + '/OP_1.w', fformat='rms_ascii')
+        >>> well3 = xtgeo.well_from_file(well_dir + '/OP_1.w')
+
+    Args:
+        rkb: well RKB height
+        xpos: well head X pos
+        ypos: well head Y pos
+        wname: well name
+        df: pandas dataframe with log values, expects columns to include
+          'X_UTME', 'Y_UTMN', 'Z_TVDSS' for x, y and z coordinates.
+          Other columns should be log values.
+        filesrc: source file if any
+        mdlogname: Name of Measured Depth log if any.
+        zonelogname: Name of Zonelog, if any
+        wlogtypes: dictionary of log types, 'DISC' or 'CONT', defaults to
+                to 'CONT'.
+        wlogrecords: dictionary of codes for 'DISC' logs, None for no codes given,
+            defaults to None.
+    """
+
+    VALID_LOGTYPES = {"DISC", "CONT"}
+
+    @allow_deprecated_init
+    def __init__(
+        self,
+        rkb: float,
+        xpos: float,
+        ypos: float,
+        wname: str,
+        df: pd.DataFrame,
+        mdlogname: str = None,
+        zonelogname: str = None,
+        wlogtypes: Dict[str, str] = None,
+        wlogrecords: Dict[str, str] = None,
+        filesrc: Optional[Union[str, Path]] = None,
+    ):
+        if not all(
+            coordinate in df.columns for coordinate in ("X_UTME", "Y_UTMN", "Z_TVDSS")
+        ):
+            raise ValueError(
+                "Well dataframe must include 'X_UTME',"
+                f" 'Y_UTMN' and 'Z_TVDSS', got {df.columns}"
+            )
+        self._reset(
+            rkb,
+            xpos,
+            ypos,
+            wname,
+            df,
+            filesrc,
+            mdlogname,
+            zonelogname,
+            wlogtypes,
+            wlogrecords,
+        )
+
+    def _reset(
+        self,
+        rkb: float = None,
+        xpos: float = None,
+        ypos: float = None,
+        wname: str = None,
+        df: pd.DataFrame = None,
+        filesrc: Optional[Union[str, Path]] = None,
+        mdlogname: str = None,
+        zonelogname: str = None,
+        wlogtypes: Dict[str, str] = None,
+        wlogrecords: Dict[str, str] = None,
+    ):
+        if wlogtypes is None:
+            wlogtypes = dict()
+        if wlogrecords is None:
+            wlogrecords = dict()
+
+        self._rkb = rkb
+        self._xpos = xpos
+        self._ypos = ypos
+        self._wname = wname
+        self._filesrc = filesrc
+        self._mdlogname = mdlogname
+        self._zonelogname = zonelogname
+
+        self._wlogtypes = wlogtypes
+        self._wlogrecords = wlogrecords
+
+        self._df = df
+
+        self._wlognames = list(self._df.columns)
+
+        self._metadata = xtgeo.MetaDataWell()
+        self._metadata.required = self
+
+        self._ensure_consistency()
+
+    def __repr__(self):  # noqa: D105
+        # should be able to newobject = eval(repr(thisobject))
+        myrp = (
+            f"{self.__class__.__name__} (filesrc={self._filesrc!r}, "
+            f"name={self._wname!r},  ID={id(self)})"
+        )
+        return myrp
+
+    def __str__(self):  # noqa: D105
+        # user friendly print
+        return self.describe(flush=False)
+
+    def _ensure_consistency(self):  # pragma: no coverage
+        """Ensure consistency within an object (private function).
+
+        Consistency checking. As well log names are columns in the Pandas DF,
+        there are additional attributes per log that have to be "in sync".
+        """
+        if self._df is None:
+            return
+
+        self._wlognames = list(self._df.columns)
+
+        for logname in self._wlognames:
+            if logname not in self._wlogtypes:
+                self._wlogtypes[logname] = "CONT"  # continuous as default
+                self._wlogrecords[logname] = None  # None as default
+            else:
+                if self._wlogtypes[logname] not in self.VALID_LOGTYPES:
+                    self._wlogtypes[logname] = "CONT"
+                    self._wlogrecords[logname] = None  # None as default
+
+            if logname not in self._wlogrecords:
+                if self._wlogtypes[logname] == "DISC":
+                    # it is a discrete log with missing record; try to find
+                    # a default one based on current values...
+                    lvalues = self._df[logname].values.round(decimals=0)
+                    lmin = int(lvalues.min())
+                    lmax = int(lvalues.max())
+
+                    lvalues = lvalues.astype("int")
+                    codes = {}
+                    for lval in range(lmin, lmax + 1):
+                        if lval in lvalues:
+                            codes[lval] = str(lval)
+
+                    self._wlogrecords = codes
+
+    # ==================================================================================
+    # Properties
+    # ==================================================================================
+
+    @property
+    def metadata(self):
+        """Return metadata object instance of type MetaDataRegularSurface."""
+        return self._metadata
+
+    @metadata.setter
+    def metadata(self, obj):
+        # The current metadata object can be replaced. This is a bit dangerous so
+        # further check must be done to validate. TODO.
+        if not isinstance(obj, xtgeo.MetaDataWell):
+            raise ValueError("Input obj not an instance of MetaDataRegularCube")
+
+        self._metadata = obj
+
+    @property
+    def rkb(self):
+        """Returns RKB height for the well (read only)."""
+        return self._rkb
+
+    @property
+    def xpos(self):
+        """Returns well header X position (read only)."""
+        return self._xpos
+
+    @property
+    def ypos(self) -> float:
+        """Returns well header Y position (read only)."""
+        return self._ypos
+
+    @property
+    def wellname(self):
+        """str: Returns well name, read only."""
+        return self._wname
+
+    @property
+    def name(self):
+        """Returns or set (rename) a well name."""
+        return self._wname
+
+    @name.setter
+    def name(self, newname):
+        self._wname = newname
+
+    # alias
+    wname = name
+
+    @property
+    def safewellname(self):
+        """Get well name on syntax safe form; '/' and spaces replaced with '_'."""
+        xname = self._wname
+        xname = xname.replace("/", "_")
+        xname = xname.replace(" ", "_")
+        return xname
+
+    @property
+    def xwellname(self):
+        """See safewellname."""
+        return self.safewellname
+
+    @property
+    def shortwellname(self):
+        """str: Well name on a short form where blockname/spaces removed (read only).
+
+        This should cope with both North Sea style and Haltenbanken style.
+
+        E.g.: '31/2-G-5 AH' -> 'G-5AH', '6472_11-F-23_AH_T2' -> 'F-23AHT2'
+
+        """
+        return self.get_short_wellname(self.wellname)
+
+    @property
+    def truewellname(self):
+        """Returns well name on the assummed form aka '31/2-E-4 AH2'."""
+        xname = self.xwellname
+        if "/" not in xname:
+            xname = xname.replace("_", "/", 1)
+            xname = xname.replace("_", " ")
+        return xname
+
+    @property
+    def mdlogname(self):
+        """str: Returns name of MD log, if any (None if missing)."""
+        return self._mdlogname
+
+    @mdlogname.setter
+    def mdlogname(self, mname):
+        if mname in self._wlognames:
+            self._mdlogname = mname
+        else:
+            self._mdlogname = None
+
+    @property
+    def zonelogname(self):
+        """str: Returns or sets name of zone log, return None if missing."""
+        return self._zonelogname
+
+    @zonelogname.setter
+    def zonelogname(self, zname):
+        if zname in self._wlognames:
+            self._zonelogname = zname
+        else:
+            self._zonelogname = None
+
+    @property
+    def dataframe(self):
+        """Returns or set the Pandas dataframe object for all logs."""
+        return self._df
+
+    @dataframe.setter
+    def dataframe(self, dfr):
+        self._df = dfr.copy()
+        self._ensure_consistency()
+
+    @property
+    def nrow(self):
+        """int: Returns the Pandas dataframe object number of rows."""
+        return len(self._df.index)
+
+    @property
+    def ncol(self):
+        """int: Returns the Pandas dataframe object number of columns."""
+        return len(self._df.columns)
+
+    @property
+    def nlogs(self):
+        """int: Returns the Pandas dataframe object number of columns."""
+        return len(self._df.columns) - 3
+
+    @property
+    def lognames_all(self):
+        """list: Returns dataframe column names as list, including mandatory coords."""
+        self._ensure_consistency()
+        return self._wlognames
+
+    @property
+    def lognames(self):
+        """list: Returns the Pandas dataframe column as list excluding coords."""
+        return list(self._df)[3:]
+
+    # ==================================================================================
+    # Methods
+    # ==================================================================================
+
+    @staticmethod
+    def get_short_wellname(wellname):
+        """Well name on a short name form where blockname and spaces are removed.
+
+        This should cope with both North Sea style and Haltenbanken style.
+        E.g.: '31/2-G-5 AH' -> 'G-5AH', '6472_11-F-23_AH_T2' -> 'F-23AHT2'
+        """
+        newname = []
+        first1 = False
+        first2 = False
+        for letter in wellname:
+            if first1 and first2:
+                newname.append(letter)
+                continue
+            if letter in ("_", "/"):
+                first1 = True
+                continue
+            if first1 and letter == "-":
+                first2 = True
+                continue
+
+        xname = "".join(newname)
+        xname = xname.replace("_", "")
+        xname = xname.replace(" ", "")
+        return xname
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout."""
+        dsc = xtgeo.common.XTGDescription()
+
+        dsc.title("Description of Well instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("File source", self._filesrc)
+        dsc.txt("Well name", self._wname)
+        dsc.txt("RKB", self._rkb)
+        dsc.txt("Well head", self._xpos, self._ypos)
+        dsc.txt("Name of all columns", self.lognames_all)
+        dsc.txt("Name of log columns", self.lognames)
+        for wlog in self.lognames:
+            rec = self.get_logrecord(wlog)
+            if rec is not None and len(rec) > 3:
+                string = "("
+                nlen = len(rec)
+                for idx, (code, val) in enumerate(rec.items()):
+                    if idx < 2:
+                        string += f"{code}: {val} "
+                    elif idx == nlen - 1:
+                        string += f"...  {code}: {val})"
+            else:
+                string = f"{rec}"
+            dsc.txt("Logname", wlog, self.get_logtype(wlog), string)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.well_from_file() instead",
+    )
+    def from_file(
+        self,
+        wfile,
+        fformat="rms_ascii",
+        **kwargs,
+    ):
+        """Deprecated, see :meth:`xtgeo.well_from_file`"""
+
+        wfile = xtgeo._XTGeoFile(wfile)
+        if fformat is None or fformat == "guess":
+            fformat = wfile.detect_fformat()
+        else:
+            fformat = wfile.generic_format_by_proposal(fformat)  # default
+
+        kwargs = _data_reader_factory(fformat)(wfile, **kwargs)
+        self._reset(**kwargs)
+        return self
+
+    @classmethod
+    def _read_file(
+        cls,
+        wfile,
+        fformat="rms_ascii",
+        **kwargs,
+    ):
+        """Import well from file.
+
+        Args:
+            wfile (str): Name of file as string or pathlib.Path
+            fformat (str): File format, rms_ascii (rms well) is
+                currently supported and default format.
+            mdlogname (str): Name of measured depth log, if any
+            zonelogname (str): Name of zonation log, if any
+            strict (bool): If True, then import will fail if
+                zonelogname or mdlogname are asked for but not present
+                in wells. If False, and e.g. zonelogname is not present, the
+                attribute ``zonelogname`` will be set to None.
+            lognames (str or list): Name or list of lognames to import, default is "all"
+            lognames_strict (bool): Flag to require all logs in lognames (unless "all")
+                or to just accept that subset that is present. Default is `False`.
+
+
+        Returns:
+            Object instance (optionally)
+
+        Example:
+            Here the from_file method is used to initiate the object
+            directly::
+
+            >>> mywell = Well().from_file(well_dir + '/OP_1.w')
+
+        .. versionchanged:: 2.1 ``lognames`` and ``lognames_strict`` added
+        .. versionchanged:: 2.1 ``strict`` now defaults to False
+        """
+
+        wfile = xtgeo._XTGeoFile(wfile)
+
+        if fformat is None or fformat == "guess":
+            fformat = wfile.detect_fformat()
+        else:
+            fformat = wfile.generic_format_by_proposal(fformat)  # default
+
+        kwargs = _data_reader_factory(fformat)(wfile, **kwargs)
+        return cls(**kwargs)
+
+    def to_file(
+        self,
+        wfile: Union[str, Path, io.BytesIO],
+        fformat: Optional[str] = "rms_ascii",
+    ):
+        """Export well to file or memory stream.
+
+        Args:
+            wfile: File name or stream.
+            fformat: File format ('rms_ascii'/'rmswell', 'hdf/hdf5/h5').
+
+        Example::
+
+            >>> xwell = Well(well_dir + '/OP_1.w')
+            >>> xwell.dataframe['Poro'] += 0.1
+            >>> filename = xwell.to_file(outdir + "/somefile_copy.rmswell")
+
+        """
+        wfile = xtgeo._XTGeoFile(wfile, mode="wb", obj=self)
+
+        wfile.check_folder(raiseerror=OSError)
+
+        self._ensure_consistency()
+
+        if fformat in (None, "rms_ascii", "rms_asc", "rmsasc", "rmswell"):
+            _well_io.export_rms_ascii(self, wfile.name)
+
+        elif fformat in ("hdf", "hdf5", "h5"):
+            self.to_hdf(wfile)
+
+        return wfile.file
+
+    def from_hdf(
+        self,
+        wfile: Union[str, Path],
+    ):
+        """Deprecated, use :meth:`xtgeo.well_from_file()`"""
+        return self.from_file(wfile, fformat="hdf")
+
+    def to_hdf(
+        self,
+        wfile: Union[str, Path],
+        compression: Optional[str] = "lzf",
+    ) -> Path:
+        """Export well to HDF based file.
+
+        Warning:
+            This implementation is currently experimental and only recommended
+            for testing.
+
+        Args:
+            wfile: HDF File name to write to export to.
+
+        Returns:
+            A Path instance to actual file applied.
+
+        .. versionadded:: 2.14
+        """
+        wfile = xtgeo._XTGeoFile(wfile, mode="wb", obj=self)
+
+        wfile.check_folder(raiseerror=OSError)
+
+        _well_io.export_hdf5_well(self, wfile, compression=compression)
+
+        return wfile.file
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.well_from_roxar() instead",
+    )
+    def from_roxar(
+        self,
+        project: Union[str, object],
+        name: str,
+        trajectory: Optional[str] = "Drilled trajectory",
+        logrun: Optional[str] = "log",
+        lognames: Optional[Union[str, List[str]]] = "all",
+        lognames_strict: Optional[bool] = False,
+        inclmd: Optional[bool] = False,
+        inclsurvey: Optional[bool] = False,
+    ):
+        """Deprecated, use :meth:`xtgeo.well_from_roxar()`"""
+        kwargs = _well_roxapi.import_well_roxapi(
+            project,
+            name,
+            trajectory=trajectory,
+            logrun=logrun,
+            lognames=lognames,
+            lognames_strict=lognames_strict,
+            inclmd=inclmd,
+            inclsurvey=inclsurvey,
+        )
+        self._reset(**kwargs)
+        return self
+
+    @classmethod
+    def _read_roxar(
+        cls,
+        project: Union[str, object],
+        name: str,
+        trajectory: Optional[str] = "Drilled trajectory",
+        logrun: Optional[str] = "log",
+        lognames: Optional[Union[str, List[str]]] = "all",
+        lognames_strict: Optional[bool] = False,
+        inclmd: Optional[bool] = False,
+        inclsurvey: Optional[bool] = False,
+    ):
+        kwargs = _well_roxapi.import_well_roxapi(
+            project,
+            name,
+            trajectory=trajectory,
+            logrun=logrun,
+            lognames=lognames,
+            lognames_strict=lognames_strict,
+            inclmd=inclmd,
+            inclsurvey=inclsurvey,
+        )
+        return cls(**kwargs)
+
+    def to_roxar(self, *args, **kwargs):
+        """Export (save/store) a well to a roxar project.
+
+        Note this method works only when inside RMS, or when RMS license is
+        activated.
+
+        The current implementation will either update existing well names
+        (then well log array size must not change), or it will make a new well in RMS.
+
+        Note:
+           When project is file path (direct access, outside RMS) then
+           ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+           will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str): Magic string 'project' or file path to project
+            wname (str): Name of well, as shown in RMS.
+            lognames (:obj:list or :obj:str): List of lognames to save, or
+                use simply 'all' for current logs for this well. Default is 'all'
+            realisation (int): Currently inactive
+            trajectory (str): Name of trajectory in RMS
+            logrun (str): Name of logrun in RMS
+
+        .. versionadded:: 2.12
+        .. versionchanged:: 2.15
+            Saving to new wells enabled (earlier only modifying existing)
+
+        """
+        # use *args, **kwargs since this method is overrided in blocked_well, and
+        # signature should be the same
+
+        project = args[0]
+        wname = args[1]
+        lognames = kwargs.get("lognames", "all")
+        trajectory = kwargs.get("trajectory", "Drilled trajectory")
+        logrun = kwargs.get("logrun", "log")
+        realisation = kwargs.get("realisation", 0)
+
+        logger.debug("Not in use: realisation %s", realisation)
+
+        _well_roxapi.export_well_roxapi(
+            self,
+            project,
+            wname,
+            lognames=lognames,
+            trajectory=trajectory,
+            logrun=logrun,
+            realisation=realisation,
+        )
+
+    def get_wlogs(self) -> OrderedDict:
+        """Get a compound dictionary with well log metadata.
+
+        The result will be an Ordered dict on the form:
+
+        ``{"X_UTME": ["CONT", None], ... "Facies": ["DISC", {1: "BG", 2: "SAND"}]}``
+        """
+        res = OrderedDict()
+
+        for key in self._wlognames:
+            wtype = "CONT"
+            wrecord = None
+            if key in self._wlogtypes:
+                wtype = self._wlogtypes[key]
+            if key in self._wlogrecords:
+                wrecord = self._wlogrecords[key]
+
+            res[key] = [wtype, wrecord]
+
+        return res
+
+    def set_wlogs(self, wlogs: OrderedDict):
+        """Set a compound dictionary with well log metadata.
+
+        This operation is somewhat risky as it may lead to inconsistency, so use with
+        care! Typically, one will use :meth:`get_wlogs` first and then modify some
+        attributes.
+
+        Args:
+            wlogs: Input data dictionary
+
+        Raises:
+            ValueError: Invalid log type found in input:
+            ValueError: Invalid log record found in input:
+            ValueError: Invalid input key found:
+            ValueError: Invalid log record found in input:
+
+        """
+        for key in self._wlognames:
+            if key in wlogs.keys():
+                typ, rec = wlogs[key]
+
+                if typ in Well.VALID_LOGTYPES:
+                    self._wlogtypes[key] = deepcopy(typ)
+                else:
+                    raise ValueError(f"Invalid log type found in input: {typ}")
+
+                if rec is None or isinstance(rec, dict):
+                    self._wlogrecords[key] = deepcopy(rec)
+                else:
+                    raise ValueError(f"Invalid log record found in input: {rec}")
+
+            else:
+                raise ValueError(f"Key for column not found in input: {key}")
+
+        for key in wlogs.keys():
+            if key not in self._wlognames:
+                raise ValueError(f"Invalid input key found: {key}")
+
+        self._ensure_consistency()
+
+    def isdiscrete(self, logname):
+        """Return True of log is discrete, otherwise False.
+
+        Args:
+            logname (str): Name of log to check if discrete or not
+
+        .. versionadded:: 2.2.0
+        """
+        if logname in self._wlognames and self.get_logtype(logname) == "DISC":
+            return True
+        return False
+
+    def copy(self):
+        """Copy a Well instance to a new unique Well instance."""
+        return Well(
+            self.rkb,
+            self.xpos,
+            self.ypos,
+            self.wname,
+            self._df.copy(),
+            self.mdlogname,
+            self.zonelogname,
+            deepcopy(self._wlogtypes),
+            deepcopy(self._wlogrecords),
+            self._filesrc,
+        )
+
+    def rename_log(self, lname, newname):
+        """Rename a log, e.g. Poro to PORO."""
+        self._ensure_consistency()
+
+        if lname not in self.lognames:
+            raise ValueError("Input log does not exist")
+
+        if newname in self.lognames:
+            raise ValueError("New log name exists already")
+
+        self._wlogtypes[newname] = self._wlogtypes.pop(lname)
+        self._wlogrecords[newname] = self._wlogrecords.pop(lname)
+
+        # rename in dataframe
+        self._df.rename(index=str, columns={lname: newname}, inplace=True)
+
+        if self._mdlogname == lname:
+            self._mdlogname = newname
+
+        if self._zonelogname == lname:
+            self._zonelogname = newname
+
+    def create_log(self, lname, logtype="CONT", logrecord=None, value=0.0, force=True):
+        """Create a new log with initial values.
+
+        If the logname already exists, it will be silently overwritten, unless
+        the option force=False.
+
+        Args:
+            lname (str): name of new log
+            logtype (str): Must be 'CONT' (default) or 'DISC' (discrete)
+            logrecord (dict): A dictionary of key: values for 'DISC' logs
+            value (float): initia value to set_index
+            force (bool): If True, and lname exists, it will be overwritten, if
+               False, no new log will be made. Will return False.
+
+        Returns:
+            True ff a new log is made (either new or force overwrite an
+            existing) or False if the new log already exists,
+            and ``force=False``.
+
+        """
+        if lname in self.lognames and force is False:
+            return False
+
+        self._wlogtypes[lname] = logtype
+        self._wlogrecords[lname] = logrecord
+
+        # make a new column
+        self._df[lname] = float(value)
+        self._ensure_consistency()
+        return True
+
+    def delete_log(self, lname):
+        """Delete/remove an existing log, or list of logs.
+
+        Will continue silently if a log does not exist.
+
+        Args:
+            lname(str or list): A logname or a list of lognames
+
+        Returns:
+            Number of logs deleted
+        """
+        return _well_oper.delete_log(self, lname)
+
+    delete_logs = delete_log  # alias function
+
+    def get_logtype(self, lname):
+        """Returns the type of a give log (e.g. DISC or CONT)."""
+        self._ensure_consistency()
+
+        if lname in self._wlogtypes:
+            return self._wlogtypes[lname]
+        return None
+
+    def set_logtype(self, lname, ltype):
+        """Sets the type of a give log (e.g. DISC or CONT)."""
+        self._ensure_consistency()
+
+        valid = {"DISC", "CONT"}
+
+        if ltype in valid:
+            self._wlogtypes[lname] = ltype
+        else:
+            raise ValueError(f"Try to set invalid log type: {ltype}")
+
+    def get_logrecord(self, lname):
+        """Returns the record (dict) of a given log name, None if not exists."""
+        if lname in self._wlogtypes:
+            return self._wlogrecords[lname]
+
+        return None
+
+    def set_logrecord(self, lname, newdict):
+        """Sets the record (dict) of a given discrete log."""
+        self._ensure_consistency()
+        if lname not in self.lognames:
+            raise ValueError(f"No such logname: {lname}")
+
+        if self._wlogtypes[lname] == "CONT":
+            raise ValueError("Cannot set a log record for a continuous log")
+
+        if not isinstance(newdict, dict):
+            raise ValueError("Input is not a dictionary")
+
+        self._wlogrecords[lname] = newdict
+
+    def get_logrecord_codename(self, lname, key):
+        """Returns the name entry of a log record, for a given key.
+
+        Example::
+
+            # get the name for zonelog entry no 4:
+            zname = well.get_logrecord_codename('ZONELOG', 4)
+        """
+        zlogdict = self.get_logrecord(lname)
+        if key in zlogdict:
+            return zlogdict[key]
+
+        return None
+
+    def get_carray(self, lname):
+        """Returns the C array pointer (via SWIG) for a given log.
+
+        Type conversion is double if float64, int32 if DISC log.
+        Returns None of log does not exist.
+        """
+        if lname in self._df:
+            np_array = self._df[lname].values
+        else:
+            return None
+
+        if self.get_logtype(lname) == "DISC":
+            carr = self._convert_np_carr_int(np_array)
+        else:
+            carr = self._convert_np_carr_double(np_array)
+
+        return carr
+
+    def get_filled_dataframe(
+        self, fill_value=const.UNDEF, fill_value_int=const.UNDEF_INT
+    ):
+        """Fill the Nan's in the dataframe with real UNDEF values.
+
+        This module returns a copy of the dataframe in the object; it
+        does not change the instance.
+
+        Note that DISC logs will be casted to columns with integer
+        as datatype.
+
+        Returns:
+            A pandas dataframe where Nan er replaces with preset
+                high XTGeo UNDEF values, or user defined values.
+
+        """
+        lnames = self.lognames
+
+        newdf = self._df.copy()
+
+        # make a dictionary of datatypes
+        dtype = {"X_UTME": "float64", "Y_UTMN": "float64", "Z_TVDSS": "float64"}
+
+        dfill = {"X_UTME": const.UNDEF, "Y_UTMN": const.UNDEF, "Z_TVDSS": const.UNDEF}
+
+        for lname in lnames:
+            if self.get_logtype(lname) == "DISC":
+                dtype[lname] = np.int32
+                dfill[lname] = fill_value_int
+            else:
+                dtype[lname] = np.float64
+                dfill[lname] = fill_value
+
+        # now first fill Nan's (because int cannot be converted if Nan)
+        newdf = newdf.fillna(dfill)
+        newdf = newdf.astype(dtype)
+
+        return newdf
+
+    def create_relative_hlen(self):
+        """Make a relative length of a well, as a log.
+
+        The first well og entry defines zero, then the horizontal length
+        is computed relative to that by simple geometric methods.
+        """
+        # extract numpies from XYZ trajectory logs
+        xv = self._df["X_UTME"].values
+        yv = self._df["Y_UTMN"].values
+
+        distance = []
+        previous_x, previous_y = xv[0], yv[0]
+        for i, (x, y) in enumerate(zip(xv, yv)):
+            distance.append(math.hypot((previous_x - x), (y - previous_y)))
+            previous_x, previous_y = x, y
+
+        self._df["R_HLEN"] = pd.Series(np.cumsum(distance), index=self._df.index)
+
+    def geometrics(self):
+        """Compute some well geometrical arrays MD, INCL, AZI, as logs.
+
+        These are kind of quasi measurements hence the logs will named
+        with a Q in front as Q_MDEPTH, Q_INCL, and Q_AZI.
+
+        These logs will be added to the dataframe. If the mdlogname
+        attribute does not exist in advance, it will be set to 'Q_MDEPTH'.
+
+        Returns:
+            False if geometrics cannot be computed
+
+        """
+        if self._df.shape[0] < 3:
+            raise ValueError(
+                f"Cannot compute geometrics for {self.name}. Not enough "
+                f"trajectory points (need >3, have: {self.dataframe.shape[0]})"
+            )
+
+        # extract numpies from XYZ trajetory logs
+        ptr_xv = self.get_carray("X_UTME")
+        ptr_yv = self.get_carray("Y_UTMN")
+        ptr_zv = self.get_carray("Z_TVDSS")
+
+        # get number of rows in pandas
+        nlen = self.nrow
+
+        ptr_md = _cxtgeo.new_doublearray(nlen)
+        ptr_incl = _cxtgeo.new_doublearray(nlen)
+        ptr_az = _cxtgeo.new_doublearray(nlen)
+
+        ier = _cxtgeo.well_geometrics(
+            nlen, ptr_xv, ptr_yv, ptr_zv, ptr_md, ptr_incl, ptr_az, 0
+        )
+
+        if ier != 0:
+            raise XTGeoCLibError(f"well_geometrics failed with error code: {ier}")
+
+        dnumpy = self._convert_carr_double_np(ptr_md)
+        self._df["Q_MDEPTH"] = pd.Series(dnumpy, index=self._df.index)
+
+        dnumpy = self._convert_carr_double_np(ptr_incl)
+        self._df["Q_INCL"] = pd.Series(dnumpy, index=self._df.index)
+
+        dnumpy = self._convert_carr_double_np(ptr_az)
+        self._df["Q_AZI"] = pd.Series(dnumpy, index=self._df.index)
+
+        if not self._mdlogname:
+            self._mdlogname = "Q_MDEPTH"
+
+        # delete tmp pointers
+        _cxtgeo.delete_doublearray(ptr_xv)
+        _cxtgeo.delete_doublearray(ptr_yv)
+        _cxtgeo.delete_doublearray(ptr_zv)
+        _cxtgeo.delete_doublearray(ptr_md)
+        _cxtgeo.delete_doublearray(ptr_incl)
+        _cxtgeo.delete_doublearray(ptr_az)
+
+        return True
+
+    def truncate_parallel_path(
+        self, other, xtol=None, ytol=None, ztol=None, itol=None, atol=None
+    ):
+        """Truncate the part of the well trajectory that is ~parallel with other.
+
+        Args:
+            other (Well): Other well to compare with
+            xtol (float): Tolerance in X (East) coord for measuring unit
+            ytol (float): Tolerance in Y (North) coord for measuring unit
+            ztol (float): Tolerance in Z (TVD) coord for measuring unit
+            itol (float): Tolerance in inclination (degrees)
+            atol (float): Tolerance in azimuth (degrees)
+        """
+        if xtol is None:
+            xtol = 0.0
+        if ytol is None:
+            ytol = 0.0
+        if ztol is None:
+            ztol = 0.0
+        if itol is None:
+            itol = 0.0
+        if atol is None:
+            atol = 0.0
+
+        if self.dataframe.shape[0] < 3 or other.dataframe.shape[0] < 3:
+            raise ValueError(
+                f"Too few points to truncate parallel path, was {self._df.size} and "
+                f"{other._df.size}, must be >3"
+            )
+
+        # extract numpies from XYZ trajectory logs
+        xv1 = self._df["X_UTME"].values
+        yv1 = self._df["Y_UTMN"].values
+        zv1 = self._df["Z_TVDSS"].values
+
+        xv2 = other._df["X_UTME"].values
+        yv2 = other._df["Y_UTMN"].values
+        zv2 = other._df["Z_TVDSS"].values
+
+        ier = _cxtgeo.well_trunc_parallel(
+            xv1, yv1, zv1, xv2, yv2, zv2, xtol, ytol, ztol, itol, atol, 0
+        )
+
+        if ier != 0:
+            raise RuntimeError("Unexpected error")
+
+        self._df = self._df[self._df["X_UTME"] < const.UNDEF_LIMIT]
+        self._df.reset_index(drop=True, inplace=True)
+
+    def may_overlap(self, other):
+        """Consider if well overlap in X Y coordinates with other well, True/False."""
+        if self._df.size < 2 or other._df.size < 2:
+            return False
+
+        # extract numpies from XYZ trajectory logs
+        xmin1 = np.nanmin(self.dataframe["X_UTME"].values)
+        xmax1 = np.nanmax(self.dataframe["X_UTME"].values)
+        ymin1 = np.nanmin(self.dataframe["Y_UTMN"].values)
+        ymax1 = np.nanmax(self.dataframe["Y_UTMN"].values)
+
+        xmin2 = np.nanmin(other.dataframe["X_UTME"].values)
+        xmax2 = np.nanmax(other.dataframe["X_UTME"].values)
+        ymin2 = np.nanmin(other.dataframe["Y_UTMN"].values)
+        ymax2 = np.nanmax(other.dataframe["Y_UTMN"].values)
+
+        if xmin1 > xmax2 or ymin1 > ymax2:
+            return False
+        if xmin2 > xmax1 or ymin2 > ymax1:
+            return False
+
+        return True
+
+    def limit_tvd(self, tvdmin, tvdmax):
+        """Truncate the part of the well that is outside tvdmin, tvdmax.
+
+        Range will be in tvdmin <= tvd <= tvdmax.
+
+        Args:
+            tvdmin (float): Minimum TVD
+            tvdmax (float): Maximum TVD
+        """
+        self._df = self._df[self._df["Z_TVDSS"] >= tvdmin]
+        self._df = self._df[self._df["Z_TVDSS"] <= tvdmax]
+
+        self._df.reset_index(drop=True, inplace=True)
+
+    def downsample(self, interval=4, keeplast=True):
+        """Downsample by sampling every N'th element (coarsen only).
+
+        Args:
+            interval (int): Sampling interval.
+            keeplast (bool): If True, the last element from the original
+                dataframe is kept, to avoid that the well is shortened.
+        """
+        if self._df.size < 2 * interval:
+            return
+
+        dfr = self._df[::interval]
+
+        if keeplast:
+            dfr = pd.concat([dfr, self._df.iloc[-1:]], ignore_index=True)
+
+        self._df = dfr.reset_index(drop=True)
+
+    def rescale(self, delta=0.15, tvdrange=None):
+        """Rescale (refine or coarse) by sampling a delta along the trajectory, in MD.
+
+        Args:
+            delta (float): Step length
+            tvdrange (tuple of floats): Resampling can be limited to TVD interval
+
+        .. versionchanged:: 2.2 Added tvdrange
+        """
+        _well_oper.rescale(self, delta=delta, tvdrange=tvdrange)
+
+    def get_polygons(self, skipname=False):
+        """Return a Polygons object from the well trajectory.
+
+        Args:
+            skipname (bool): If True then name column is omitted
+
+        .. versionadded:: 2.1
+        .. versionchanged:: 2.13 Added `skipname` key
+        """
+        dfr = self._df.copy()
+
+        keep = ("X_UTME", "Y_UTMN", "Z_TVDSS")
+        for col in dfr.columns:
+            if col not in keep:
+                dfr.drop(labels=col, axis=1, inplace=True)
+        dfr["POLY_ID"] = 1
+
+        if not skipname:
+            dfr["NAME"] = self.xwellname
+        poly = xtgeo.Polygons()
+        poly.dataframe = dfr
+        poly.name = self.xwellname
+
+        return poly
+
+    def get_fence_polyline(self, sampling=20, nextend=2, tvdmin=None, asnumpy=True):
+        """Return a fence polyline as a numpy array or a Polygons object.
+
+        The result will aim for a regular sampling interval, useful for extracting
+        fence plots (cross-sections).
+
+        Args:
+            sampling (float): Sampling interval i.e. horizonal distance (input)
+            nextend (int): Number if sampling to extend; e.g. 2 * 20
+            tvdmin (float): Minimum TVD starting point.
+            as_numpy (bool): If True, a numpy array, otherwise a Polygons
+                object with 5 columns where the 2 last are HLEN and POLY_ID
+                and the POLY_ID will be set to 0.
+
+        Returns:
+            A numpy array of shape (NLEN, 5) in F order,
+            Or a Polygons object with 5 columns
+            If not possible, return False
+
+        .. versionchanged:: 2.1 improved algorithm
+        """
+        poly = self.get_polygons()
+
+        if tvdmin is not None:
+            poly.dataframe = poly.dataframe[poly.dataframe[poly.zname] >= tvdmin]
+            poly.dataframe.reset_index(drop=True, inplace=True)
+
+        return poly.get_fence(distance=sampling, nextend=nextend, asnumpy=asnumpy)
+
+    def create_surf_distance_log(
+        self,
+        surf: object,
+        name: Optional[str] = "DIST_SURF",
+    ):
+        """Make a log that is vertical distance to a regular surface.
+
+        If the trajectory is above the surface (i.e. more shallow), then the
+        distance sign is positive.
+
+        Args:
+            surf: The RegularSurface instance.
+            name: The name of the new log. If it exists it will be overwritten.
+
+        Example::
+
+            mywell.rescale()  # optional
+            thesurf = xtgeo.RegularSurface("some.gri")
+            mywell.create_surf_distance_log(thesurf, name="sdiff")
+
+        """
+        _well_oper.create_surf_distance_log(self, surf, name)
+
+    def report_zonation_holes(self, threshold=5):
+        """Reports if well has holes in zonation, less or equal to N samples.
+
+        Zonation may have holes due to various reasons, and
+        usually a few undef samples indicates that something is wrong.
+        This method reports well and start interval of the "holes"
+
+        The well shall have zonelog from import (via zonelogname attribute) and
+        preferly a MD log (via mdlogname attribute); however if the
+        latter is not present, a report withou MD values will be present.
+
+        Args:
+            threshold (int): Number of samples (max.) that defines a hole, e.g.
+                5 means that undef samples in the range [1, 5] (including 5) is
+                applied
+
+        Returns:
+            A Pandas dataframe as a report. None if no list is made.
+
+        Raises:
+            RuntimeError if zonelog is not present
+        """
+        dfr = _well_oper.report_zonation_holes(self, threshold=threshold)
+
+        return dfr
+
+    def get_zonation_points(
+        self, tops=True, incl_limit=80, top_prefix="Top", zonelist=None, use_undef=False
+    ):
+        """Extract zonation points from Zonelog and make a marker list.
+
+        Currently it is either 'Tops' or 'Zone' (thicknesses); default
+        is tops (i.e. tops=True).
+
+        The `zonelist` can be a list of zones, or a tuple with two members specifying
+        first and last member. Note however that the zonation shall be without jumps
+        and increasing. E.g.::
+
+            zonelist=(1, 5)  # meaning [1, 2, 3, 4, 5]
+            # or
+            zonelist=[1, 2, 3, 4]
+            # while _not_ legal:
+            zonelist=[1, 4, 8]
+
+        Zone numbers less than 0 are not accepted
+
+        Args:
+            tops (bool): If True then compute tops, else (thickness) points.
+            incl_limit (float): If given, and usezone is True, the max
+                angle of inclination to be  used as input to zonation points.
+            top_prefix (str): As well logs usually have isochore (zone) name,
+                this prefix could be Top, e.g. 'SO43' --> 'TopSO43'
+            zonelist (list of int or tuple): Zones to use
+            use_undef (bool): If True, then transition from UNDEF is also
+                used.
+
+
+        Returns:
+            A pandas dataframe (ready for the xyz/Points class), None
+            if a zonelog is missing
+        """
+        # make a copy of the well instance as some tmp well logs are made
+        scopy = self.copy()
+
+        dfr = _wellmarkers.get_zonation_points(
+            scopy, tops, incl_limit, top_prefix, zonelist, use_undef
+        )
+
+        del scopy
+
+        return dfr
+
+    def get_zone_interval(self, zonevalue, resample=1, extralogs=None):
+        """Extract the X Y Z ID line (polyline) segment for a given zonevalue.
+
+        Args:
+            zonevalue (int): The zone value to extract
+            resample (int): If given, downsample every N'th sample to make
+                polylines smaller in terms of bit and bytes.
+                1 = No downsampling.
+            extralogs (list of str): List of extra log names to include
+
+
+        Returns:
+            A pandas dataframe X Y Z ID (ready for the xyz/Polygon class),
+            None if a zonelog is missing or actual zone does dot
+            exist in the well.
+        """
+        if resample < 1 or not isinstance(resample, int):
+            raise KeyError("Key resample of wrong type (must be int >= 1)")
+
+        dff = self.get_filled_dataframe()
+
+        # the technical solution here is to make a tmp column which
+        # will add one number for each time the actual segment is repeated,
+        # not straightforward... (thanks to H. Berland for tip)
+
+        dff["ztmp"] = dff[self.zonelogname]
+        dff["ztmp"] = (dff[self.zonelogname] != zonevalue).astype(int)
+
+        dff["ztmp"] = (dff.ztmp != dff.ztmp.shift()).cumsum()
+
+        dff = dff[dff[self.zonelogname] == zonevalue]
+
+        m1v = dff["ztmp"].min()
+        m2v = dff["ztmp"].max()
+        if np.isnan(m1v):
+            logger.debug("Returns (no data)")
+            return None
+
+        df2 = dff.copy()
+
+        dflist = []
+        for mvv in range(m1v, m2v + 1):
+            dff9 = df2.copy()
+            dff9 = df2[df2["ztmp"] == mvv]
+            if dff9.index.shape[0] > 0:
+                dflist.append(dff9)
+
+        dxlist = []
+
+        useloglist = ["X_UTME", "Y_UTMN", "Z_TVDSS", "POLY_ID"]
+        if extralogs is not None:
+            useloglist.extend(extralogs)
+
+        # pylint: disable=consider-using-enumerate
+        for ivv in range(len(dflist)):
+            dxf = dflist[ivv]
+            dxf = dxf.rename(columns={"ztmp": "POLY_ID"})
+            cols = [xxx for xxx in dxf.columns if xxx not in useloglist]
+
+            dxf = dxf.drop(cols, axis=1)
+
+            # now (down) resample every N'th
+            if resample > 1:
+                dxf = pd.concat([dxf.iloc[::resample, :], dxf.tail(1)])
+
+            dxlist.append(dxf)
+
+        dff = pd.concat(dxlist)
+        dff.reset_index(inplace=True, drop=True)
+
+        logger.debug("Dataframe from well:\n%s", dff)
+        return dff
+
+    def get_fraction_per_zone(
+        self,
+        dlogname,
+        dcodes,
+        zonelist=None,
+        incl_limit=80,
+        count_limit=3,
+        zonelogname=None,
+    ):
+        """Get fraction of a discrete parameter, e.g. a facies, per zone.
+
+        It can be constrained by an inclination.
+
+        Also, it needs to be evaluated only of ZONE is complete; either
+        INCREASE or DECREASE ; hence a quality flag is made and applied.
+
+        Args:
+            dlogname (str): Name of discrete log, e.g. 'FACIES'
+            dnames (list of int): Codes of facies (or similar) to report for
+            zonelist (list of int): Zones to use
+            incl_limit (float): Inclination limit for well path.
+            count_limit (int): Minimum number of counts required per segment
+                for valid calculations
+            zonelogname (str). If None, the Well().zonelogname attribute is
+                applied
+
+        Returns:
+            A pandas dataframe (ready for the xyz/Points class), None
+            if a zonelog is missing or or dlogname is missing,
+            list is zero length for any reason.
+        """
+        dfr = _wellmarkers.get_fraction_per_zone(
+            self,
+            dlogname,
+            dcodes,
+            zonelist=zonelist,
+            incl_limit=incl_limit,
+            count_limit=count_limit,
+            zonelogname=zonelogname,
+        )
+
+        return dfr
+
+    def mask_shoulderbeds(
+        self,
+        inputlogs: List[str],
+        targetlogs: List[str],
+        nsamples: Optional[Union[int, Dict[str, float]]] = 2,
+        strict: Optional[bool] = False,
+    ) -> bool:
+        """Mask data around zone boundaries or other discrete log boundaries.
+
+        This operates on number of samples, hence the actual distance which is masked
+        depends on the sampling interval (ie. count) or on distance measures.
+        Distance measures are TVD (true vertical depth) or MD (measured depth).
+
+        .. image:: images/wells-mask-shoulderbeds.png
+           :width: 300
+           :align: center
+
+        Args:
+            inputlogs: List of input logs, must be of discrete type.
+            targetlogs: List of logs where mask is applied.
+            nsamples: Number of samples around boundaries to filter, per side, i.e.
+                value 2 means 2 above and 2 below, in total 4 samples.
+                As alternative specify nsamples indirectly with a relative distance,
+                as a dictionary with one record, as {"tvd": 0.5} or {"md": 0.7}.
+            strict: If True, will raise Exception of any of the input or target log
+                names are missing.
+
+        Returns:
+            True if any operation has been done. False in case nothing has been done,
+                 e.g. no targetlogs for this particular well and ``strict`` is False.
+
+        Raises:
+            ValueError: Various messages when wrong or inconsistent input.
+
+        Example:
+            >>> mywell1 = Well(well_dir + '/OP_1.w')
+            >>> mywell2 = Well(well_dir + '/OP_2.w')
+            >>> did_succeed = mywell1.mask_shoulderbeds(["Zonelog", "Facies"], ["Perm"])
+            >>> did_succeed = mywell2.mask_shoulderbeds(
+            ...     ["Zonelog"],
+            ...     ["Perm"],
+            ...     nsamples={"tvd": 0.8}
+            ... )
+
+        """
+        return _well_oper.mask_shoulderbeds(
+            self, inputlogs, targetlogs, nsamples, strict
+        )
+
+    def get_surface_picks(self, surf):
+        """Return :class:`.Points` obj where well crosses the surface (horizon picks).
+
+        There may be several points in the Points() dataframe attribute.
+        Also a ``DIRECTION`` column will show 1 if surface is penetrated from
+        above, and -1 if penetrated from below.
+
+        Args:
+            surf (RegularSurface): The surface instance
+
+        Returns:
+            A :class:`.Points` instance, or None if no crossing points
+
+        .. versionadded:: 2.8
+
+        """
+        return _wellmarkers.get_surface_picks(self, surf)
+
+    def make_ijk_from_grid(self, grid, grid_id="", algorithm=2, activeonly=True):
+        """Look through a Grid and add grid I J K as discrete logs.
+
+        Note that the the grid counting has base 1 (first row is 1 etc).
+
+        By default, log (i.e. column names in the dataframe) will be
+        ICELL, JCELL, KCELL, but you can add a tag (ID) to that name.
+
+        Args:
+            grid (Grid): A XTGeo Grid instance
+            grid_id (str): Add a tag (optional) to the current log name
+            algorithm (int): Which interbal algorithm to use, default is 2 (expert
+                setting)
+            activeonly (bool): If True, only active cells are applied (algorithm 2 only)
+
+        Raises:
+            RuntimeError: 'Error from C routine, code is ...'
+
+        .. versionchanged:: 2.9 Added keys for and `activeonly`
+        """
+        _well_oper.make_ijk_from_grid(
+            self, grid, grid_id=grid_id, algorithm=algorithm, activeonly=activeonly
+        )
+
+    def make_zone_qual_log(self, zqname):
+        """Create a zone quality/indicator (flag) log.
+
+        This routine looks through to zone log and flag intervals according
+        to neighbouring zones:
+
+        * 0: Undetermined flag
+
+        * 1: Zonelog interval numbering increases,
+             e.g. for zone 2: 1 1 1 1 2 2 2 2 2 5 5 5 5 5
+
+        * 2: Zonelog interval numbering decreases,
+             e.g. for zone 2: 6 6 6 2 2 2 2 1 1 1
+
+        * 3: Interval is a U turning point, e.g. 0 0 0 2 2 2 1 1 1
+
+        * 4: Interval is a inverse U turning point, 3 3 3 2 2 2 5 5
+
+        * 9: Interval is bounded by one or more missing sections,
+             e.g. 1 1 1 2 2 2 -999 -999
+
+        If a log with the name exists, it will be silently replaced
+
+        Args:
+            zqname (str): Name of quality log
+        """
+        _well_oper.make_zone_qual_log(self, zqname)
+
+    def get_gridproperties(
+        self, gridprops, grid=("ICELL", "JCELL", "KCELL"), prop_id="_model"
+    ):
+        """Look through a Grid and add a set of grid properties as logs.
+
+        The name of the logs will ...
+
+        This can be done to sample model properties along a well.
+
+        Args:
+            gridprops (Grid): A XTGeo GridProperties instance (a collection
+                of properties) or a single GridProperty instance
+            grid (Grid or tuple): A XTGeo Grid instance or a reference
+                via tuple. If this is tuple with log names,
+                it states that these logs already contains
+                the gridcell IJK numbering.
+            prop_id (str): Add a tag (optional) to the current log name, e.g
+                as PORO_model, where _model is the tag.
+
+        Raises:
+            None
+
+        .. versionadded:: 2.1
+
+        """
+        _well_oper.get_gridproperties(self, gridprops, grid=grid, prop_id=prop_id)
+
+    # ==================================================================================
+    # PRIVATE METHODS
+    # should not be applied outside the class
+    # ==================================================================================
+
+    # ----------------------------------------------------------------------------------
+    # Import/Export methods for various formats
+    # ----------------------------------------------------------------------------------
+
+    # ----------------------------------------------------------------------------------
+    # Special methods for nerds, todo is to move to private module
+    # ----------------------------------------------------------------------------------
+
+    def _convert_np_carr_int(self, np_array):
+        """Convert numpy 1D array to C array, assuming int type.
+
+        The numpy is always a double (float64), so need to convert first
+        """
+        carr = _cxtgeo.new_intarray(self.nrow)
+
+        np_array = np_array.astype(np.int32)
+
+        _cxtgeo.swig_numpy_to_carr_i1d(np_array, carr)
+
+        return carr
+
+    def _convert_np_carr_double(self, np_array):
+        """Convert numpy 1D array to C array, assuming double type."""
+        carr = _cxtgeo.new_doublearray(self.nrow)
+
+        _cxtgeo.swig_numpy_to_carr_1d(np_array, carr)
+
+        return carr
+
+    def _convert_carr_double_np(self, carray, nlen=None):
+        """Convert a C array to numpy, assuming double type."""
+        if nlen is None:
+            nlen = len(self._df.index)
+
+        nparray = _cxtgeo.swig_carr_to_numpy_1d(nlen, carray)
+
+        return nparray
```

## xtgeo/well/wells.py

 * *Ordering differences only*

```diff
@@ -1,298 +1,298 @@
-# -*- coding: utf-8 -*-
-
-"""Wells module, which has the Wells class (collection of Well objects)"""
-
-
-import functools
-import warnings
-from typing import List
-
-import deprecation
-import pandas as pd
-
-import xtgeo
-
-from . import _wells_utils
-from .well1 import Well
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def wells_from_files(filelist, *args, **kwargs):
-    """Import wells from a list of files (filelist).
-
-    Creates a Wells object from a list of filenames. Remaining arguments are
-        the same as :func:`xtgeo.well_from_file`.
-
-    Args:
-        filelist (list of filenames): List with file names
-
-    Example:
-        Here the from_file method is used to initiate the object
-        directly::
-
-            >>> mywells = Wells(
-            ...     [well_dir + '/OP_1.w', well_dir + '/OP_2.w']
-            ... )
-    """
-    return Wells([xtgeo.well_from_file(wfile, *args, **kwargs) for wfile in filelist])
-
-
-def allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of Wells and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        # Checking if we are doing an initialization
-        # from file and raise a deprecation warning if
-        # we are.
-        if args and args[0] and not isinstance(args[0][0], Well):
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "mywells = xtgeo.wells_from_files(['some_name.w']) instead",
-                DeprecationWarning,
-            )
-            return func(xtgeo.wells_from_files(*args, **kwargs))
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-class Wells:
-    """Class for a collection of Well objects, for operations that involves
-    a number of wells.
-
-    See also the :class:`xtgeo.well.Well` class.
-
-    Args:
-        wells: The list of Well objects.
-    """
-
-    @allow_deprecated_init
-    def __init__(self, wells: List[Well] = None):
-        if wells is None:
-            self._wells = []
-        else:
-            self._wells = wells
-
-    @property
-    def names(self):
-        """Returns a list of well names (read only).
-
-        Example::
-
-            namelist = wells.names
-            for prop in namelist:
-                print ('Well name is {}'.format(name))
-
-        """
-        return [w.name for w in self._wells]
-
-    @property
-    def wells(self):
-        """Returns or sets a list of XTGeo Well objects, None if empty."""
-        if not self._wells:
-            return None
-
-        return self._wells
-
-    @wells.setter
-    def wells(self, well_list):
-        for well in well_list:
-            if not isinstance(well, xtgeo.well.Well):
-                raise ValueError("Well in list not valid Well object")
-
-        self._wells = well_list
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout"""
-
-        dsc = xtgeo.common.XTGDescription()
-        dsc.title(f"Description of {self.__class__.__name__} instance")
-        dsc.txt("Object ID", id(self))
-
-        dsc.txt("Wells", self.names)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    def __iter__(self):
-        return iter(self.wells)
-
-    def copy(self):
-        """Copy a Wells instance to a new unique instance (a deep copy)."""
-
-        return Wells(self._wells.copy())
-
-    def get_well(self, name):
-        """Get a Well() instance by name, or None"""
-
-        logger.info("Asking for a well with name %s", name)
-        for well in self._wells:
-            if well.name == name:
-                return well
-        return None
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.wells_from_files() instead",
-    )
-    def from_files(
-        self,
-        filelist,
-        fformat="rms_ascii",
-        mdlogname=None,
-        zonelogname=None,
-        strict=True,
-        append=True,
-    ):
-        """Deprecated see :func:`wells_from_files`"""
-
-        if not append:
-            self._wells = []
-
-        # file checks are done within the Well() class
-        for wfile in filelist:
-            try:
-                self._wells.append(
-                    xtgeo.well_from_file(
-                        wfile,
-                        fformat=fformat,
-                        mdlogname=mdlogname,
-                        zonelogname=zonelogname,
-                        strict=strict,
-                    )
-                )
-            except ValueError as err:
-                xtg.warn(f"SKIP this well: {err}")
-                continue
-        if not self._wells:
-            xtg.warn("No wells imported!")
-
-    def from_roxar(self, *args, **kwargs):
-        """Import (retrieve) all wells (or based on a filter) from
-        roxar project.
-
-        Note this method works only when inside RMS, or when RMS license is
-        activated.
-
-        All the wells present in the bwname icon will be imported.
-
-        Args:
-            project (str): Magic string 'project' or file path to project
-            lognames (list): List of lognames to include, or use 'all' for
-                all current blocked logs for this well.
-            wfilter (str): This is a regular expression to tell which wells
-                that shall be included.
-            ijk (bool): If True, then logs with grid IJK as I_INDEX, etc
-            realisation (int): Realisation index (0 is default)
-
-        Example::
-
-            import xtgeo
-            mywells = xtgeo.Wells()
-            mywells.from_roxar(project, lognames='all', wfilter='31.*')
-
-        """
-        raise NotImplementedError("In prep...")
-
-    # not having this as property but a get_ .. is intended, for flexibility
-    def get_dataframe(self, filled=False, fill_value1=-999, fill_value2=-9999):
-        """Get a big dataframe for all wells or blocked wells in instance,
-        with well name as first column
-
-        Args:
-            filled (bool): If True, then NaN's are replaces with values
-            fill_value1 (int): Only applied if filled=True, for logs that
-                have missing values
-            fill_value2 (int): Only applied if filled=True, when logs
-                are missing completely for that well.
-        """
-        logger.info("Ask for big dataframe for all wells")
-
-        bigdflist = []
-        for well in self._wells:
-            dfr = well.dataframe.copy()
-            dfr["WELLNAME"] = well.name
-            logger.info(well.name)
-            if filled:
-                dfr = dfr.fillna(fill_value1)
-            bigdflist.append(dfr)
-
-        dfr = pd.concat(bigdflist, ignore_index=True, sort=True)
-        # the concat itself may lead to NaN's:
-        if filled:
-            dfr = dfr.fillna(fill_value2)
-
-        spec_order = ["WELLNAME", "X_UTME", "Y_UTMN", "Z_TVDSS"]
-        return dfr[spec_order + [col for col in dfr if col not in spec_order]]
-
-    def quickplot(self, filename=None, title="QuickPlot"):
-        """Fast plot of wells using matplotlib.
-
-        Args:
-            filename (str): Name of plot file; None will plot to screen.
-            title (str): Title of plot
-
-        """
-
-        mymap = xtgeo.plot.Map()
-
-        mymap.canvas(title=title)
-
-        mymap.plot_wells(self)
-
-        if filename is None:
-            mymap.show()
-        else:
-            mymap.savefig(filename)
-
-    def limit_tvd(self, tvdmin, tvdmax):
-        """Limit TVD to be in range tvdmin, tvdmax for all wells"""
-        for well in self.wells:
-            well.limit_tvd(tvdmin, tvdmax)
-
-    def downsample(self, interval=4, keeplast=True):
-        """Downsample by sampling every N'th element (coarsen only), all
-        wells.
-        """
-
-        for well in self.wells:
-            well.downsample(interval=interval, keeplast=keeplast)
-
-    def wellintersections(self, wfilter=None, showprogress=False):
-        """Get intersections between wells, return as dataframe table.
-
-        Notes on wfilter: A wfilter is settings to improve result. In
-        particular to remove parts of trajectories that are parallel.
-
-        wfilter = {'parallel': {'xtol': 4.0, 'ytol': 4.0, 'ztol':2.0,
-                                'itol':10, 'atol':2}}
-
-        Here xtol is tolerance in X coordinate; further Y tolerance,
-        Z tolerance, (I)nclination tolerance, and (A)zimuth tolerance.
-
-        Args:
-            tvdrange (tuple of floats): Search interval. One is often just
-                interested in the reservoir section.
-            wfilter (dict): A dictionrary for filter options, in order to
-                improve result. See example above.
-            showprogress (bool): Will show progress to screen if enabled.
-
-        Returns:
-            A Pandas dataframe object, with columns WELL, CWELL and UTMX UTMY
-                TVD coordinates for CWELL where CWELL crosses WELL,
-                and also MDEPTH for the WELL.
-        """
-
-        return _wells_utils.wellintersections(
-            self, wfilter=wfilter, showprogress=showprogress
-        )
+# -*- coding: utf-8 -*-
+
+"""Wells module, which has the Wells class (collection of Well objects)"""
+
+
+import functools
+import warnings
+from typing import List
+
+import deprecation
+import pandas as pd
+
+import xtgeo
+
+from . import _wells_utils
+from .well1 import Well
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def wells_from_files(filelist, *args, **kwargs):
+    """Import wells from a list of files (filelist).
+
+    Creates a Wells object from a list of filenames. Remaining arguments are
+        the same as :func:`xtgeo.well_from_file`.
+
+    Args:
+        filelist (list of filenames): List with file names
+
+    Example:
+        Here the from_file method is used to initiate the object
+        directly::
+
+            >>> mywells = Wells(
+            ...     [well_dir + '/OP_1.w', well_dir + '/OP_2.w']
+            ... )
+    """
+    return Wells([xtgeo.well_from_file(wfile, *args, **kwargs) for wfile in filelist])
+
+
+def allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of Wells and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        # Checking if we are doing an initialization
+        # from file and raise a deprecation warning if
+        # we are.
+        if args and args[0] and not isinstance(args[0][0], Well):
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "mywells = xtgeo.wells_from_files(['some_name.w']) instead",
+                DeprecationWarning,
+            )
+            return func(xtgeo.wells_from_files(*args, **kwargs))
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+class Wells:
+    """Class for a collection of Well objects, for operations that involves
+    a number of wells.
+
+    See also the :class:`xtgeo.well.Well` class.
+
+    Args:
+        wells: The list of Well objects.
+    """
+
+    @allow_deprecated_init
+    def __init__(self, wells: List[Well] = None):
+        if wells is None:
+            self._wells = []
+        else:
+            self._wells = wells
+
+    @property
+    def names(self):
+        """Returns a list of well names (read only).
+
+        Example::
+
+            namelist = wells.names
+            for prop in namelist:
+                print ('Well name is {}'.format(name))
+
+        """
+        return [w.name for w in self._wells]
+
+    @property
+    def wells(self):
+        """Returns or sets a list of XTGeo Well objects, None if empty."""
+        if not self._wells:
+            return None
+
+        return self._wells
+
+    @wells.setter
+    def wells(self, well_list):
+        for well in well_list:
+            if not isinstance(well, xtgeo.well.Well):
+                raise ValueError("Well in list not valid Well object")
+
+        self._wells = well_list
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout"""
+
+        dsc = xtgeo.common.XTGDescription()
+        dsc.title(f"Description of {self.__class__.__name__} instance")
+        dsc.txt("Object ID", id(self))
+
+        dsc.txt("Wells", self.names)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    def __iter__(self):
+        return iter(self.wells)
+
+    def copy(self):
+        """Copy a Wells instance to a new unique instance (a deep copy)."""
+
+        return Wells(self._wells.copy())
+
+    def get_well(self, name):
+        """Get a Well() instance by name, or None"""
+
+        logger.info("Asking for a well with name %s", name)
+        for well in self._wells:
+            if well.name == name:
+                return well
+        return None
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.wells_from_files() instead",
+    )
+    def from_files(
+        self,
+        filelist,
+        fformat="rms_ascii",
+        mdlogname=None,
+        zonelogname=None,
+        strict=True,
+        append=True,
+    ):
+        """Deprecated see :func:`wells_from_files`"""
+
+        if not append:
+            self._wells = []
+
+        # file checks are done within the Well() class
+        for wfile in filelist:
+            try:
+                self._wells.append(
+                    xtgeo.well_from_file(
+                        wfile,
+                        fformat=fformat,
+                        mdlogname=mdlogname,
+                        zonelogname=zonelogname,
+                        strict=strict,
+                    )
+                )
+            except ValueError as err:
+                xtg.warn(f"SKIP this well: {err}")
+                continue
+        if not self._wells:
+            xtg.warn("No wells imported!")
+
+    def from_roxar(self, *args, **kwargs):
+        """Import (retrieve) all wells (or based on a filter) from
+        roxar project.
+
+        Note this method works only when inside RMS, or when RMS license is
+        activated.
+
+        All the wells present in the bwname icon will be imported.
+
+        Args:
+            project (str): Magic string 'project' or file path to project
+            lognames (list): List of lognames to include, or use 'all' for
+                all current blocked logs for this well.
+            wfilter (str): This is a regular expression to tell which wells
+                that shall be included.
+            ijk (bool): If True, then logs with grid IJK as I_INDEX, etc
+            realisation (int): Realisation index (0 is default)
+
+        Example::
+
+            import xtgeo
+            mywells = xtgeo.Wells()
+            mywells.from_roxar(project, lognames='all', wfilter='31.*')
+
+        """
+        raise NotImplementedError("In prep...")
+
+    # not having this as property but a get_ .. is intended, for flexibility
+    def get_dataframe(self, filled=False, fill_value1=-999, fill_value2=-9999):
+        """Get a big dataframe for all wells or blocked wells in instance,
+        with well name as first column
+
+        Args:
+            filled (bool): If True, then NaN's are replaces with values
+            fill_value1 (int): Only applied if filled=True, for logs that
+                have missing values
+            fill_value2 (int): Only applied if filled=True, when logs
+                are missing completely for that well.
+        """
+        logger.info("Ask for big dataframe for all wells")
+
+        bigdflist = []
+        for well in self._wells:
+            dfr = well.dataframe.copy()
+            dfr["WELLNAME"] = well.name
+            logger.info(well.name)
+            if filled:
+                dfr = dfr.fillna(fill_value1)
+            bigdflist.append(dfr)
+
+        dfr = pd.concat(bigdflist, ignore_index=True, sort=True)
+        # the concat itself may lead to NaN's:
+        if filled:
+            dfr = dfr.fillna(fill_value2)
+
+        spec_order = ["WELLNAME", "X_UTME", "Y_UTMN", "Z_TVDSS"]
+        return dfr[spec_order + [col for col in dfr if col not in spec_order]]
+
+    def quickplot(self, filename=None, title="QuickPlot"):
+        """Fast plot of wells using matplotlib.
+
+        Args:
+            filename (str): Name of plot file; None will plot to screen.
+            title (str): Title of plot
+
+        """
+
+        mymap = xtgeo.plot.Map()
+
+        mymap.canvas(title=title)
+
+        mymap.plot_wells(self)
+
+        if filename is None:
+            mymap.show()
+        else:
+            mymap.savefig(filename)
+
+    def limit_tvd(self, tvdmin, tvdmax):
+        """Limit TVD to be in range tvdmin, tvdmax for all wells"""
+        for well in self.wells:
+            well.limit_tvd(tvdmin, tvdmax)
+
+    def downsample(self, interval=4, keeplast=True):
+        """Downsample by sampling every N'th element (coarsen only), all
+        wells.
+        """
+
+        for well in self.wells:
+            well.downsample(interval=interval, keeplast=keeplast)
+
+    def wellintersections(self, wfilter=None, showprogress=False):
+        """Get intersections between wells, return as dataframe table.
+
+        Notes on wfilter: A wfilter is settings to improve result. In
+        particular to remove parts of trajectories that are parallel.
+
+        wfilter = {'parallel': {'xtol': 4.0, 'ytol': 4.0, 'ztol':2.0,
+                                'itol':10, 'atol':2}}
+
+        Here xtol is tolerance in X coordinate; further Y tolerance,
+        Z tolerance, (I)nclination tolerance, and (A)zimuth tolerance.
+
+        Args:
+            tvdrange (tuple of floats): Search interval. One is often just
+                interested in the reservoir section.
+            wfilter (dict): A dictionrary for filter options, in order to
+                improve result. See example above.
+            showprogress (bool): Will show progress to screen if enabled.
+
+        Returns:
+            A Pandas dataframe object, with columns WELL, CWELL and UTMX UTMY
+                TVD coordinates for CWELL where CWELL crosses WELL,
+                and also MDEPTH for the WELL.
+        """
+
+        return _wells_utils.wellintersections(
+            self, wfilter=wfilter, showprogress=showprogress
+        )
```

## xtgeo/xyz/__init__.py

 * *Ordering differences only*

```diff
@@ -1,8 +1,8 @@
-# -*- coding: utf-8 -*-
-"""The XTGeo xyz (points and polygons) package."""
-
-
-# flake8: noqa
-from xtgeo.xyz._xyz import XYZ
-from xtgeo.xyz.points import Points
-from xtgeo.xyz.polygons import Polygons
+# -*- coding: utf-8 -*-
+"""The XTGeo xyz (points and polygons) package."""
+
+
+# flake8: noqa
+from xtgeo.xyz._xyz import XYZ
+from xtgeo.xyz.points import Points
+from xtgeo.xyz.polygons import Polygons
```

## xtgeo/xyz/_polygons_oper.py

 * *Ordering differences only*

```diff
@@ -1,282 +1,282 @@
-"""Various operations dedicated to Polygons class.
-
-First order functions here are:
-
-* boundary_from_points (input to class method)
-* simplify_polygons (instance method)
-
-
-Functions starting with '_' are local helper functions
-"""
-from math import ceil
-
-import numpy as np
-import pandas as pd
-import shapely.geometry as sg
-from scipy.spatial import Delaunay, cKDTree
-
-import xtgeo
-
-xtg = xtgeo.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-MINIMUM_NUMBER_POINTS = 4
-
-
-class BoundaryError(ValueError):
-    """Get error on not able to create a boundary."""
-
-
-def boundary_from_points(points, alpha_factor=1.0, alpha=None, concave=False):
-    """From a Point instance, make boundary polygons (generic)."""
-    if not isinstance(points, xtgeo.Points):
-        raise ValueError("The input points is not an instance of xtgeo.Points")
-
-    if points.nrow < MINIMUM_NUMBER_POINTS:
-        logger.info("No. points is %s, need >= %s", points.nrow, MINIMUM_NUMBER_POINTS)
-        raise ValueError(
-            "Too few points to derive a boundary, present is "
-            f"{points.nrow}, need at least {MINIMUM_NUMBER_POINTS}"
-        )
-
-    if alpha_factor <= 0.0:
-        raise ValueError("The alpha_factor value must be greater than 0.0")
-
-    if alpha is not None and alpha <= 0.0:
-        raise ValueError("The alpha value must be greater than 0.0")
-
-    if concave:
-        alpha_factor = 0
-        alpha = 999  # dummy
-
-    usepoints = points.copy()  # make a copy since points may be filtered
-
-    xvec = usepoints.dataframe[usepoints.xname].values
-    yvec = usepoints.dataframe[usepoints.yname].values
-    zvec = usepoints.dataframe[usepoints.zname].values
-
-    if alpha is None:
-        # use scipy to detect average distance; 30 points should be sufficient
-        xyv = np.column_stack((xvec, yvec))
-        use_npoints = 30 if xvec.size >= 30 else xvec.size
-        kdtree = cKDTree(xyv)
-        dist, _ = kdtree.query(xyv, k=use_npoints)
-        auto_alpha = np.mean(dist)
-        logger.info("Proposed auto alpha is %s", auto_alpha)
-
-        # now try this auto_alpha, and iterate to a gradually larger alpha proposal
-        # if still no success in creating a boundary
-        try:
-            return_values = _create_boundary_polygon(
-                xvec, yvec, zvec, auto_alpha * alpha_factor
-            )
-        except BoundaryError as berr:
-            # iterate and propose a new alpha value while raising an exception
-            propose_alpha = _propose_new_alpha(
-                xvec, yvec, zvec, alpha_factor, auto_alpha
-            )
-            propose_alpha_factor = alpha_factor * propose_alpha / auto_alpha
-            msg = (
-                "Your alpha_factor or alpha is too low. Set the alpha_factor to "
-                f"approx. {propose_alpha_factor:.3f}\n"
-            )
-            raise BoundaryError(msg) from berr
-    else:
-        return_values = _create_boundary_polygon(xvec, yvec, zvec, alpha * alpha_factor)
-
-    # return the class parameters to populate the Polygons instance
-    if return_values is None:
-        raise BoundaryError("Cannot create a a boundary, too few points?")
-
-    return return_values
-
-
-def _propose_new_alpha(xvec, yvec, zvec, alpha_factor, alpha):
-    """The current combination of alpha_factor and alpha did not get produce a result.
-
-    Do an iteration here and propose a new alpha_value, given that the user insists
-    on keeping the current alpha_factor.
-    """
-    trials = 0
-    max_trials = 1000
-    multiplier = 1.1
-
-    proposed_alpha = alpha * multiplier
-    while trials < max_trials:
-        trials += 1
-        logger.info("Proposed alpha is %s in trial %s", proposed_alpha, trials)
-        try:
-            _create_boundary_polygon(xvec, yvec, zvec, proposed_alpha * alpha_factor)
-            print("Trial number:", trials)
-            return ceil(proposed_alpha)
-        except BoundaryError:
-            proposed_alpha *= multiplier
-
-    raise RuntimeError(f"Not able to estimate an alpha in {max_trials} iterations!")
-
-
-def _create_boundary_polygon(
-    xvec: np.ndarray,
-    yvec: np.ndarray,
-    zvec: np.ndarray,
-    alpha: float,
-):
-    xy = np.column_stack((xvec, yvec))
-    coords = np.column_stack((xy, zvec))
-
-    # return None if too few points
-    if len(xy) < MINIMUM_NUMBER_POINTS:
-        return None
-
-    edges = _alpha_shape(xy, alpha=alpha)
-    if not edges:
-        raise BoundaryError(
-            "Your alpha or alpha_factor value is too low, try increasing it!"
-        )
-
-    # sort edges and group into separate polygons
-    sorted_edges = _sort_edges_and_split_in_polygons(edges)
-
-    data = []
-    for pol, pol_edges in enumerate(sorted_edges):
-        for edg_start, edg_stop in pol_edges:
-            data.append(list(coords[edg_start]) + [pol])
-            data.append(list(coords[edg_stop]) + [pol])
-
-    return data
-
-
-def _alpha_shape(points, alpha):
-    """Compute the alpha shape (concave hull) of a set of points.
-
-    Args:
-        points: np.array of shape (n,2) points.
-        alpha: alpha value.
-        only_outer TODO?: boolean value to specify if we keep only the outer border
-            or also inner edges.
-    Returns:
-        Set of (i,j) pairs representing edges of the alpha-shape. (i,j) are
-            the indices in the points array.
-    """
-
-    assert points.shape[0] >= MINIMUM_NUMBER_POINTS, "Need >= 4 pts to derive boundary"
-
-    def add_edge(edges, icv, jcv):
-        """Add an edge between the i-th and j-th points, if not in the list already."""
-        if (icv, jcv) in edges or (jcv, icv) in edges:
-            # if both neighboring triangles are in shape, it is not a boundary edge
-            edges.remove((jcv, icv))
-            return
-        edges.add((icv, jcv))
-
-    tri = Delaunay(points)
-
-    edges = set()
-
-    # Loop over triangles: ia, ib, ic = indices of corner points of the triangle
-    for ia, ib, ic in tri.simplices:
-        pa, pb, pc = points[ia], points[ib], points[ic]
-
-        # Computing radius of triangle circumcircle
-        # www.mathalino.com/reviewer/derivation-of-formulas/derivation-of-formula-\
-        # for-radius-of-circumcircle
-        avv = np.sqrt((pa[0] - pb[0]) ** 2 + (pa[1] - pb[1]) ** 2)
-        bvv = np.sqrt((pb[0] - pc[0]) ** 2 + (pb[1] - pc[1]) ** 2)
-        cvv = np.sqrt((pc[0] - pa[0]) ** 2 + (pc[1] - pa[1]) ** 2)
-
-        svv = (avv + bvv + cvv) / 2.0
-        partial = svv * (svv - avv) * (svv - bvv) * (svv - cvv)
-        partial = partial[partial > 0.0]  # to avoid sqrt of negative number
-        area = np.sqrt(partial)
-
-        # radius of circumcircle
-        circum_r = avv * bvv * cvv / (4.0 * area) if area.size > 0 else 0
-
-        # if radius less then alpha then add outer edge
-        if circum_r < alpha or alpha == 0:
-            add_edge(edges, ia, ib)
-            add_edge(edges, ib, ic)
-            add_edge(edges, ic, ia)
-
-    return edges
-
-
-def _sort_edges_and_split_in_polygons(edges):
-    """Divide the edges list into polygons and sort them to be connected."""
-    edges = list(edges)
-    sorted_pol_lines = {}
-    pol_nr = 1
-
-    while edges:
-        iter_edges = iter(edges)
-
-        if pol_nr not in sorted_pol_lines:
-            sorted_pol_lines[pol_nr] = []
-            edge = edges[-1]
-        else:
-            prev_edgepoint = sorted_pol_lines[pol_nr][-1][1]
-            edge = next((x for x in iter_edges if x[0] == prev_edgepoint), None)
-
-        # if edge is None it belongs to a new polygon
-        if edge is not None:
-            edges.remove(edge)
-            sorted_pol_lines[pol_nr].append(edge)
-        else:
-            pol_nr += 1
-
-    # return list of polygons sorted on the length of points
-    return sorted(sorted_pol_lines.values(), key=len, reverse=True)
-
-
-def simplify_polygons(self, tolerance: float, preserve_topology: bool) -> bool:
-    """Use Shapely's 'simplify' method to reduce points.
-
-    Note that attributes are not yet supported (perhaps impossible?)
-
-    For Args, see Shapely
-    """
-    try:
-        if self.attributes:
-            raise UserWarning(
-                "Attributes are present, but they will be lost when simplifying"
-            )
-    except AttributeError:
-        pass
-
-    recompute_hlen = True if self.hname in self.dataframe else False
-    recompute_tlen = True if self.tname in self.dataframe else False
-
-    orig_len = len(self.dataframe)
-
-    idgroups = self.dataframe.groupby(self.pname)
-    dfrlist = []
-    for idx, grp in idgroups:
-        if len(grp.index) < 2:
-            logger.warning("Cannot simplify polygons with less than two points. Skip")
-            continue
-
-        pxcor = grp[self.xname].values
-        pycor = grp[self.yname].values
-        pzcor = grp[self.zname].values
-        spoly = sg.LineString(np.stack([pxcor, pycor, pzcor], axis=1))
-
-        new_spoly = spoly.simplify(tolerance, preserve_topology=preserve_topology)
-        dfr = pd.DataFrame(
-            np.array(new_spoly.coords), columns=[self.xname, self.yname, self.zname]
-        )
-
-        dfr[self.pname] = idx
-        dfrlist.append(dfr)
-
-    dfr = pd.concat(dfrlist)
-    self.dataframe = dfr.reset_index(drop=True)
-
-    if recompute_hlen:
-        self.hlen()
-    if recompute_tlen:
-        self.tlen()
-
-    new_len = len(self.dataframe)
-
-    return True if new_len < orig_len else False
+"""Various operations dedicated to Polygons class.
+
+First order functions here are:
+
+* boundary_from_points (input to class method)
+* simplify_polygons (instance method)
+
+
+Functions starting with '_' are local helper functions
+"""
+from math import ceil
+
+import numpy as np
+import pandas as pd
+import shapely.geometry as sg
+from scipy.spatial import Delaunay, cKDTree
+
+import xtgeo
+
+xtg = xtgeo.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+MINIMUM_NUMBER_POINTS = 4
+
+
+class BoundaryError(ValueError):
+    """Get error on not able to create a boundary."""
+
+
+def boundary_from_points(points, alpha_factor=1.0, alpha=None, concave=False):
+    """From a Point instance, make boundary polygons (generic)."""
+    if not isinstance(points, xtgeo.Points):
+        raise ValueError("The input points is not an instance of xtgeo.Points")
+
+    if points.nrow < MINIMUM_NUMBER_POINTS:
+        logger.info("No. points is %s, need >= %s", points.nrow, MINIMUM_NUMBER_POINTS)
+        raise ValueError(
+            "Too few points to derive a boundary, present is "
+            f"{points.nrow}, need at least {MINIMUM_NUMBER_POINTS}"
+        )
+
+    if alpha_factor <= 0.0:
+        raise ValueError("The alpha_factor value must be greater than 0.0")
+
+    if alpha is not None and alpha <= 0.0:
+        raise ValueError("The alpha value must be greater than 0.0")
+
+    if concave:
+        alpha_factor = 0
+        alpha = 999  # dummy
+
+    usepoints = points.copy()  # make a copy since points may be filtered
+
+    xvec = usepoints.dataframe[usepoints.xname].values
+    yvec = usepoints.dataframe[usepoints.yname].values
+    zvec = usepoints.dataframe[usepoints.zname].values
+
+    if alpha is None:
+        # use scipy to detect average distance; 30 points should be sufficient
+        xyv = np.column_stack((xvec, yvec))
+        use_npoints = 30 if xvec.size >= 30 else xvec.size
+        kdtree = cKDTree(xyv)
+        dist, _ = kdtree.query(xyv, k=use_npoints)
+        auto_alpha = np.mean(dist)
+        logger.info("Proposed auto alpha is %s", auto_alpha)
+
+        # now try this auto_alpha, and iterate to a gradually larger alpha proposal
+        # if still no success in creating a boundary
+        try:
+            return_values = _create_boundary_polygon(
+                xvec, yvec, zvec, auto_alpha * alpha_factor
+            )
+        except BoundaryError as berr:
+            # iterate and propose a new alpha value while raising an exception
+            propose_alpha = _propose_new_alpha(
+                xvec, yvec, zvec, alpha_factor, auto_alpha
+            )
+            propose_alpha_factor = alpha_factor * propose_alpha / auto_alpha
+            msg = (
+                "Your alpha_factor or alpha is too low. Set the alpha_factor to "
+                f"approx. {propose_alpha_factor:.3f}\n"
+            )
+            raise BoundaryError(msg) from berr
+    else:
+        return_values = _create_boundary_polygon(xvec, yvec, zvec, alpha * alpha_factor)
+
+    # return the class parameters to populate the Polygons instance
+    if return_values is None:
+        raise BoundaryError("Cannot create a a boundary, too few points?")
+
+    return return_values
+
+
+def _propose_new_alpha(xvec, yvec, zvec, alpha_factor, alpha):
+    """The current combination of alpha_factor and alpha did not get produce a result.
+
+    Do an iteration here and propose a new alpha_value, given that the user insists
+    on keeping the current alpha_factor.
+    """
+    trials = 0
+    max_trials = 1000
+    multiplier = 1.1
+
+    proposed_alpha = alpha * multiplier
+    while trials < max_trials:
+        trials += 1
+        logger.info("Proposed alpha is %s in trial %s", proposed_alpha, trials)
+        try:
+            _create_boundary_polygon(xvec, yvec, zvec, proposed_alpha * alpha_factor)
+            print("Trial number:", trials)
+            return ceil(proposed_alpha)
+        except BoundaryError:
+            proposed_alpha *= multiplier
+
+    raise RuntimeError(f"Not able to estimate an alpha in {max_trials} iterations!")
+
+
+def _create_boundary_polygon(
+    xvec: np.ndarray,
+    yvec: np.ndarray,
+    zvec: np.ndarray,
+    alpha: float,
+):
+    xy = np.column_stack((xvec, yvec))
+    coords = np.column_stack((xy, zvec))
+
+    # return None if too few points
+    if len(xy) < MINIMUM_NUMBER_POINTS:
+        return None
+
+    edges = _alpha_shape(xy, alpha=alpha)
+    if not edges:
+        raise BoundaryError(
+            "Your alpha or alpha_factor value is too low, try increasing it!"
+        )
+
+    # sort edges and group into separate polygons
+    sorted_edges = _sort_edges_and_split_in_polygons(edges)
+
+    data = []
+    for pol, pol_edges in enumerate(sorted_edges):
+        for edg_start, edg_stop in pol_edges:
+            data.append(list(coords[edg_start]) + [pol])
+            data.append(list(coords[edg_stop]) + [pol])
+
+    return data
+
+
+def _alpha_shape(points, alpha):
+    """Compute the alpha shape (concave hull) of a set of points.
+
+    Args:
+        points: np.array of shape (n,2) points.
+        alpha: alpha value.
+        only_outer TODO?: boolean value to specify if we keep only the outer border
+            or also inner edges.
+    Returns:
+        Set of (i,j) pairs representing edges of the alpha-shape. (i,j) are
+            the indices in the points array.
+    """
+
+    assert points.shape[0] >= MINIMUM_NUMBER_POINTS, "Need >= 4 pts to derive boundary"
+
+    def add_edge(edges, icv, jcv):
+        """Add an edge between the i-th and j-th points, if not in the list already."""
+        if (icv, jcv) in edges or (jcv, icv) in edges:
+            # if both neighboring triangles are in shape, it is not a boundary edge
+            edges.remove((jcv, icv))
+            return
+        edges.add((icv, jcv))
+
+    tri = Delaunay(points)
+
+    edges = set()
+
+    # Loop over triangles: ia, ib, ic = indices of corner points of the triangle
+    for ia, ib, ic in tri.simplices:
+        pa, pb, pc = points[ia], points[ib], points[ic]
+
+        # Computing radius of triangle circumcircle
+        # www.mathalino.com/reviewer/derivation-of-formulas/derivation-of-formula-\
+        # for-radius-of-circumcircle
+        avv = np.sqrt((pa[0] - pb[0]) ** 2 + (pa[1] - pb[1]) ** 2)
+        bvv = np.sqrt((pb[0] - pc[0]) ** 2 + (pb[1] - pc[1]) ** 2)
+        cvv = np.sqrt((pc[0] - pa[0]) ** 2 + (pc[1] - pa[1]) ** 2)
+
+        svv = (avv + bvv + cvv) / 2.0
+        partial = svv * (svv - avv) * (svv - bvv) * (svv - cvv)
+        partial = partial[partial > 0.0]  # to avoid sqrt of negative number
+        area = np.sqrt(partial)
+
+        # radius of circumcircle
+        circum_r = avv * bvv * cvv / (4.0 * area) if area.size > 0 else 0
+
+        # if radius less then alpha then add outer edge
+        if circum_r < alpha or alpha == 0:
+            add_edge(edges, ia, ib)
+            add_edge(edges, ib, ic)
+            add_edge(edges, ic, ia)
+
+    return edges
+
+
+def _sort_edges_and_split_in_polygons(edges):
+    """Divide the edges list into polygons and sort them to be connected."""
+    edges = list(edges)
+    sorted_pol_lines = {}
+    pol_nr = 1
+
+    while edges:
+        iter_edges = iter(edges)
+
+        if pol_nr not in sorted_pol_lines:
+            sorted_pol_lines[pol_nr] = []
+            edge = edges[-1]
+        else:
+            prev_edgepoint = sorted_pol_lines[pol_nr][-1][1]
+            edge = next((x for x in iter_edges if x[0] == prev_edgepoint), None)
+
+        # if edge is None it belongs to a new polygon
+        if edge is not None:
+            edges.remove(edge)
+            sorted_pol_lines[pol_nr].append(edge)
+        else:
+            pol_nr += 1
+
+    # return list of polygons sorted on the length of points
+    return sorted(sorted_pol_lines.values(), key=len, reverse=True)
+
+
+def simplify_polygons(self, tolerance: float, preserve_topology: bool) -> bool:
+    """Use Shapely's 'simplify' method to reduce points.
+
+    Note that attributes are not yet supported (perhaps impossible?)
+
+    For Args, see Shapely
+    """
+    try:
+        if self.attributes:
+            raise UserWarning(
+                "Attributes are present, but they will be lost when simplifying"
+            )
+    except AttributeError:
+        pass
+
+    recompute_hlen = True if self.hname in self.dataframe else False
+    recompute_tlen = True if self.tname in self.dataframe else False
+
+    orig_len = len(self.dataframe)
+
+    idgroups = self.dataframe.groupby(self.pname)
+    dfrlist = []
+    for idx, grp in idgroups:
+        if len(grp.index) < 2:
+            logger.warning("Cannot simplify polygons with less than two points. Skip")
+            continue
+
+        pxcor = grp[self.xname].values
+        pycor = grp[self.yname].values
+        pzcor = grp[self.zname].values
+        spoly = sg.LineString(np.stack([pxcor, pycor, pzcor], axis=1))
+
+        new_spoly = spoly.simplify(tolerance, preserve_topology=preserve_topology)
+        dfr = pd.DataFrame(
+            np.array(new_spoly.coords), columns=[self.xname, self.yname, self.zname]
+        )
+
+        dfr[self.pname] = idx
+        dfrlist.append(dfr)
+
+    dfr = pd.concat(dfrlist)
+    self.dataframe = dfr.reset_index(drop=True)
+
+    if recompute_hlen:
+        self.hlen()
+    if recompute_tlen:
+        self.tlen()
+
+    new_len = len(self.dataframe)
+
+    return True if new_len < orig_len else False
```

## xtgeo/xyz/_xyz.py

 * *Ordering differences only*

```diff
@@ -1,393 +1,393 @@
-# -*- coding: utf-8 -*-
-"""XTGeo XYZ module (abstract base class)"""
-from abc import ABC, abstractmethod
-
-import numpy as np
-import pandas as pd
-
-from xtgeo.common import XTGDescription, XTGeoDialog
-
-from . import _xyz_oper
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-class XYZ(ABC):
-    """Abstract base class for XYZ objects, i.e. Points and Polygons in XTGeo.
-
-    The XYZ base class has common methods and properties for Points and Polygons. The
-    underlying data storage is a Pandas dataframe with minimal 3 (Points) or 4
-    (Polygons) columns, where the two first represent X and Y coordinates.
-
-    The third column is a number, which may represent the depth, thickness, or other
-    property. For Polygons, there is a 4'th column which is an integer representing
-    poly-line ID, which is handled in the Polygons class. Similarly, Points and Polygons
-    can have additional columns called `attributes`.
-
-    Note:
-        You cannot use the XYZ class directly. Use the :class:`Points` or
-        :class:`Polygons` classes!
-    """
-
-    def __init__(
-        self,
-        xname: str = "X_UTME",
-        yname: str = "Y_UTMN",
-        zname: str = "Z_TVDSS",
-    ):
-        """Concrete initialisation for base class _XYZ."""
-        self._xname = xname
-        self._yname = yname
-        self._zname = zname
-
-    def _reset(
-        self,
-        xname: str = "X_UTME",
-        yname: str = "Y_UTMN",
-        zname: str = "Z_TVDSS",
-    ):
-        """Used in deprecated methods."""
-        self._xname = xname
-        self._yname = yname
-        self._zname = zname
-
-    @property
-    def xname(self):
-        """Returns or set the name of the X column."""
-        return self._xname
-
-    @xname.setter
-    def xname(self, newname):
-        self._df_column_rename(newname, self._xname)
-        self._xname = newname
-
-    @property
-    def yname(self):
-        """Returns or set the name of the Y column."""
-        return self._yname
-
-    @yname.setter
-    def yname(self, newname):
-        self._df_column_rename(newname, self._yname)
-        self._yname = newname
-
-    @property
-    def zname(self):
-        """Returns or set the name of the Z column."""
-        return self._zname
-
-    @zname.setter
-    def zname(self, newname):
-        self._df_column_rename(newname, self._zname)
-        self._zname = newname
-
-    @property
-    @abstractmethod
-    def dataframe(self) -> pd.DataFrame:
-        """Return or set the Pandas dataframe object."""
-        ...
-
-    @property
-    def nrow(self):
-        """Returns the Pandas dataframe object number of rows."""
-        if self.dataframe is None:
-            return 0
-        return len(self.dataframe.index)
-
-    def _df_column_rename(self, newname, oldname):
-        if isinstance(newname, str):
-            if oldname and self.dataframe is not None:
-                self.dataframe.rename(columns={oldname: newname}, inplace=True)
-        else:
-            raise ValueError(f"Wrong type of input to {newname}; must be string")
-
-    def _check_name(self, value):
-        if not isinstance(value, str):
-            raise ValueError(f"Wrong type of input; must be string, was {type(value)}")
-
-        if value not in self.dataframe.columns:
-            raise ValueError(
-                f"{value} does not exist as a column name, must be "
-                f"one of: f{self.dataframe.columns}"
-            )
-
-    @abstractmethod
-    def copy(self):
-        """Returns a deep copy of an instance"""
-        ...
-
-    def describe(self, flush=True):
-        """Describe an instance by printing to stdout"""
-
-        dsc = XTGDescription()
-        dsc.title(f"Description of {self.__class__.__name__} instance")
-        dsc.txt("Object ID", id(self))
-        dsc.txt("xname, yname, zname", self._xname, self._yname, self._zname)
-
-        if flush:
-            dsc.flush()
-            return None
-
-        return dsc.astext()
-
-    @abstractmethod
-    def from_file(self, pfile, fformat="xyz"):
-        """Import Points or Polygons from a file (deprecated).
-
-        Supported import formats (fformat):
-
-        * 'xyz' or 'poi' or 'pol': Simple XYZ format
-
-        * 'zmap': ZMAP line format as exported from RMS (e.g. fault lines)
-
-        * 'rms_attr': RMS points formats with attributes (extra columns)
-
-        * 'guess': Try to choose file format based on extension
-
-        Args:
-            pfile (str): Name of file or pathlib.Path instance
-            fformat (str): File format, see list above
-
-        Returns:
-            Object instance (needed optionally)
-
-        Raises:
-            OSError: if file is not present or wrong permissions.
-
-        .. deprecated:: 2.16
-           Use e.g. xtgeo.points_from_file()
-        """
-        ...
-
-    @abstractmethod
-    def from_list(self, plist):
-        """Create Points or Polygons from a list-like input (deprecated).
-
-        This method is deprecated in favor of using e.g. xtgeo.Points(plist)
-        or xtgeo.Polygons(plist) instead.
-
-        The following inputs are possible:
-
-        * List of tuples [(x1, y1, z1, <id1>), (x2, y2, z2, <id2>), ...].
-        * List of lists  [[x1, y1, z1, <id1>], [x2, y2, z2, <id2>], ...].
-        * List of numpy arrays  [nparr1, nparr2, ...] where nparr1 is first row.
-        * A numpy array with shape [??1, ??2] ...
-        * An existing pandas dataframe
-
-        It is currently not much error checking that lists/tuples are consistent, e.g.
-        if there always is either 3 or 4 elements per tuple, or that 4 number is
-        an integer.
-
-        Args:
-            plist (str): List of tuples, each tuple is length 3 or 4.
-
-        Raises:
-            ValueError: If something is wrong with input
-
-        .. versionadded:: 2.6
-        .. versionchanged:: 2.16
-        .. deprecated:: 2.16
-           Use e.g. xtgeo.Points(list_like).
-        """
-        ...
-
-    def protected_columns(self):
-        """
-        Returns:
-            Columns not deleted by :meth:`delete_columns`, for
-            instance the coordinate columns.
-        """
-        return [self.xname, self.yname, self.zname, self.pname]
-
-    def geometry_columns(self):
-        """
-        Returns:
-            Columns can be deleted silently by :meth:`delete_columns`
-        """
-        return [self.hname, self.dhname, self.tname, self.dtname]
-
-    def delete_columns(self, clist, strict=False):
-        """Delete one or more columns by name.
-
-        Note that the columns returned by :meth:`protected_columns(self)` (for
-        instance, the coordinate columns) will not be deleted.
-
-        Args:
-            self (obj): Points or Polygons
-            clist (list): Name of columns
-            strict (bool): I False, will not trigger exception if a column is not
-                found. Otherways a ValueError will be raised.
-
-        Raises:
-            ValueError: If strict is True and columnname not present
-
-        Example::
-            mypoly.delete_columns(["WELL_ID", mypoly.hname, mypoly.dhname])
-
-        .. versionadded:: 2.1
-        """
-        for cname in clist:
-            if cname in self.protected_columns():
-                xtg.warnuser(
-                    f"The column {cname} is protected and will not be deleted."
-                )
-                continue
-
-            if cname in self.geometry_columns():
-                if strict:
-                    raise ValueError(f"The column {cname} is not present.")
-                else:
-                    logger.info(
-                        "The column %s is a geometry and will not be deleted.", cname
-                    )
-                continue
-
-            if cname not in self.dataframe:
-                if strict:
-                    raise ValueError(f"The column {cname} is not present.")
-                else:
-                    logger.info("Trying to delete %s, but it is not present.", cname)
-                    # xtg.warnuser(f"Trying to delete {cname}, but it is not present.")
-            else:
-                self.dataframe.drop(cname, axis=1, inplace=True)
-
-    def get_boundary(self):
-        """Get the square XYZ window (boundaries) of the instance.
-
-        Returns:
-            (xmin, xmax, ymin, ymax, zmin, zmax)
-
-        See also:
-            The class method :func:`Polygons.boundary_from_points()`
-
-        """
-        xmin = np.nanmin(self.dataframe[self.xname].values)
-        xmax = np.nanmax(self.dataframe[self.xname].values)
-        ymin = np.nanmin(self.dataframe[self.yname].values)
-        ymax = np.nanmax(self.dataframe[self.yname].values)
-        zmin = np.nanmin(self.dataframe[self.zname].values)
-        zmax = np.nanmax(self.dataframe[self.zname].values)
-
-        return (xmin, xmax, ymin, ymax, zmin, zmax)
-
-    def operation_polygons(self, poly, value, opname="add", inside=True):
-        """A generic function for operations restricted to inside or outside polygon(s).
-
-        The operations are performed on the Z values, while the 'inside' or 'outside'
-        of polygons are purely based on X and Y values (typically X is East and Y in
-        North coordinates).
-
-        The operations are XYZ generic i.e. done on the points that defines the
-        Polygon or the point in Points, depending on the calling instance.
-
-        Possible ``opname`` strings:
-
-        * ``add``: add the value
-        * ``sub``: substract the value
-        * ``mul``: multiply the value
-        * ``div``: divide the value
-        * ``set``: replace current values with value
-        * ``eli``: eliminate; here value is not applied
-
-        Args:
-            poly (Polygons): A XTGeo Polygons instance
-            value(float): Value to add, subtract etc
-            opname (str): Name of operation... 'add', 'sub', etc
-            inside (bool): If True do operation inside polygons; else outside. Note
-                that boundary is treated as 'inside'
-
-        Note:
-            This function works only intuitively when using one single polygon
-            in the ``poly`` instance. When having several polygons the
-            operation is done sequentially per polygon which may
-            lead to surprising results. For instance, using "add inside"
-            into two overlapping polygons, the addition will be doubled in the
-            overlapping part. Similarly using e.g. "eli, outside" will completely
-            remove all points of two non-overlapping polygons are given as input.
-        """
-        _xyz_oper.operation_polygons(self, poly, value, opname=opname, inside=inside)
-
-    def add_inside(self, poly, value):
-        """Add a value (scalar) to points inside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="add", inside=True)
-
-    def add_outside(self, poly, value):
-        """Add a value (scalar) to points outside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="add", inside=False)
-
-    def sub_inside(self, poly, value):
-        """Subtract a value (scalar) to points inside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="sub", inside=True)
-
-    def sub_outside(self, poly, value):
-        """Subtract a value (scalar) to points outside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="sub", inside=False)
-
-    def mul_inside(self, poly, value):
-        """Multiply a value (scalar) to points inside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="mul", inside=True)
-
-    def mul_outside(self, poly, value):
-        """Multiply a value (scalar) to points outside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="mul", inside=False)
-
-    def div_inside(self, poly, value):
-        """Divide a value (scalar) to points inside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="div", inside=True)
-
-    def div_outside(self, poly, value):
-        """Divide a value (scalar) outside polygons (value 0.0 will give result 0).
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="div", inside=False)
-
-    def set_inside(self, poly, value):
-        """Set a value (scalar) to points inside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="set", inside=True)
-
-    def set_outside(self, poly, value):
-        """Set a value (scalar) to points outside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, value, opname="set", inside=False)
-
-    def eli_inside(self, poly):
-        """Eliminate current points inside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, 0, opname="eli", inside=True)
-
-    def eli_outside(self, poly):
-        """Eliminate current points outside polygons.
-
-        See notes under :meth:`operation_polygons`.
-        """
-        self.operation_polygons(poly, 0, opname="eli", inside=False)
+# -*- coding: utf-8 -*-
+"""XTGeo XYZ module (abstract base class)"""
+from abc import ABC, abstractmethod
+
+import numpy as np
+import pandas as pd
+
+from xtgeo.common import XTGDescription, XTGeoDialog
+
+from . import _xyz_oper
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+class XYZ(ABC):
+    """Abstract base class for XYZ objects, i.e. Points and Polygons in XTGeo.
+
+    The XYZ base class has common methods and properties for Points and Polygons. The
+    underlying data storage is a Pandas dataframe with minimal 3 (Points) or 4
+    (Polygons) columns, where the two first represent X and Y coordinates.
+
+    The third column is a number, which may represent the depth, thickness, or other
+    property. For Polygons, there is a 4'th column which is an integer representing
+    poly-line ID, which is handled in the Polygons class. Similarly, Points and Polygons
+    can have additional columns called `attributes`.
+
+    Note:
+        You cannot use the XYZ class directly. Use the :class:`Points` or
+        :class:`Polygons` classes!
+    """
+
+    def __init__(
+        self,
+        xname: str = "X_UTME",
+        yname: str = "Y_UTMN",
+        zname: str = "Z_TVDSS",
+    ):
+        """Concrete initialisation for base class _XYZ."""
+        self._xname = xname
+        self._yname = yname
+        self._zname = zname
+
+    def _reset(
+        self,
+        xname: str = "X_UTME",
+        yname: str = "Y_UTMN",
+        zname: str = "Z_TVDSS",
+    ):
+        """Used in deprecated methods."""
+        self._xname = xname
+        self._yname = yname
+        self._zname = zname
+
+    @property
+    def xname(self):
+        """Returns or set the name of the X column."""
+        return self._xname
+
+    @xname.setter
+    def xname(self, newname):
+        self._df_column_rename(newname, self._xname)
+        self._xname = newname
+
+    @property
+    def yname(self):
+        """Returns or set the name of the Y column."""
+        return self._yname
+
+    @yname.setter
+    def yname(self, newname):
+        self._df_column_rename(newname, self._yname)
+        self._yname = newname
+
+    @property
+    def zname(self):
+        """Returns or set the name of the Z column."""
+        return self._zname
+
+    @zname.setter
+    def zname(self, newname):
+        self._df_column_rename(newname, self._zname)
+        self._zname = newname
+
+    @property
+    @abstractmethod
+    def dataframe(self) -> pd.DataFrame:
+        """Return or set the Pandas dataframe object."""
+        ...
+
+    @property
+    def nrow(self):
+        """Returns the Pandas dataframe object number of rows."""
+        if self.dataframe is None:
+            return 0
+        return len(self.dataframe.index)
+
+    def _df_column_rename(self, newname, oldname):
+        if isinstance(newname, str):
+            if oldname and self.dataframe is not None:
+                self.dataframe.rename(columns={oldname: newname}, inplace=True)
+        else:
+            raise ValueError(f"Wrong type of input to {newname}; must be string")
+
+    def _check_name(self, value):
+        if not isinstance(value, str):
+            raise ValueError(f"Wrong type of input; must be string, was {type(value)}")
+
+        if value not in self.dataframe.columns:
+            raise ValueError(
+                f"{value} does not exist as a column name, must be "
+                f"one of: f{self.dataframe.columns}"
+            )
+
+    @abstractmethod
+    def copy(self):
+        """Returns a deep copy of an instance"""
+        ...
+
+    def describe(self, flush=True):
+        """Describe an instance by printing to stdout"""
+
+        dsc = XTGDescription()
+        dsc.title(f"Description of {self.__class__.__name__} instance")
+        dsc.txt("Object ID", id(self))
+        dsc.txt("xname, yname, zname", self._xname, self._yname, self._zname)
+
+        if flush:
+            dsc.flush()
+            return None
+
+        return dsc.astext()
+
+    @abstractmethod
+    def from_file(self, pfile, fformat="xyz"):
+        """Import Points or Polygons from a file (deprecated).
+
+        Supported import formats (fformat):
+
+        * 'xyz' or 'poi' or 'pol': Simple XYZ format
+
+        * 'zmap': ZMAP line format as exported from RMS (e.g. fault lines)
+
+        * 'rms_attr': RMS points formats with attributes (extra columns)
+
+        * 'guess': Try to choose file format based on extension
+
+        Args:
+            pfile (str): Name of file or pathlib.Path instance
+            fformat (str): File format, see list above
+
+        Returns:
+            Object instance (needed optionally)
+
+        Raises:
+            OSError: if file is not present or wrong permissions.
+
+        .. deprecated:: 2.16
+           Use e.g. xtgeo.points_from_file()
+        """
+        ...
+
+    @abstractmethod
+    def from_list(self, plist):
+        """Create Points or Polygons from a list-like input (deprecated).
+
+        This method is deprecated in favor of using e.g. xtgeo.Points(plist)
+        or xtgeo.Polygons(plist) instead.
+
+        The following inputs are possible:
+
+        * List of tuples [(x1, y1, z1, <id1>), (x2, y2, z2, <id2>), ...].
+        * List of lists  [[x1, y1, z1, <id1>], [x2, y2, z2, <id2>], ...].
+        * List of numpy arrays  [nparr1, nparr2, ...] where nparr1 is first row.
+        * A numpy array with shape [??1, ??2] ...
+        * An existing pandas dataframe
+
+        It is currently not much error checking that lists/tuples are consistent, e.g.
+        if there always is either 3 or 4 elements per tuple, or that 4 number is
+        an integer.
+
+        Args:
+            plist (str): List of tuples, each tuple is length 3 or 4.
+
+        Raises:
+            ValueError: If something is wrong with input
+
+        .. versionadded:: 2.6
+        .. versionchanged:: 2.16
+        .. deprecated:: 2.16
+           Use e.g. xtgeo.Points(list_like).
+        """
+        ...
+
+    def protected_columns(self):
+        """
+        Returns:
+            Columns not deleted by :meth:`delete_columns`, for
+            instance the coordinate columns.
+        """
+        return [self.xname, self.yname, self.zname, self.pname]
+
+    def geometry_columns(self):
+        """
+        Returns:
+            Columns can be deleted silently by :meth:`delete_columns`
+        """
+        return [self.hname, self.dhname, self.tname, self.dtname]
+
+    def delete_columns(self, clist, strict=False):
+        """Delete one or more columns by name.
+
+        Note that the columns returned by :meth:`protected_columns(self)` (for
+        instance, the coordinate columns) will not be deleted.
+
+        Args:
+            self (obj): Points or Polygons
+            clist (list): Name of columns
+            strict (bool): I False, will not trigger exception if a column is not
+                found. Otherways a ValueError will be raised.
+
+        Raises:
+            ValueError: If strict is True and columnname not present
+
+        Example::
+            mypoly.delete_columns(["WELL_ID", mypoly.hname, mypoly.dhname])
+
+        .. versionadded:: 2.1
+        """
+        for cname in clist:
+            if cname in self.protected_columns():
+                xtg.warnuser(
+                    f"The column {cname} is protected and will not be deleted."
+                )
+                continue
+
+            if cname in self.geometry_columns():
+                if strict:
+                    raise ValueError(f"The column {cname} is not present.")
+                else:
+                    logger.info(
+                        "The column %s is a geometry and will not be deleted.", cname
+                    )
+                continue
+
+            if cname not in self.dataframe:
+                if strict:
+                    raise ValueError(f"The column {cname} is not present.")
+                else:
+                    logger.info("Trying to delete %s, but it is not present.", cname)
+                    # xtg.warnuser(f"Trying to delete {cname}, but it is not present.")
+            else:
+                self.dataframe.drop(cname, axis=1, inplace=True)
+
+    def get_boundary(self):
+        """Get the square XYZ window (boundaries) of the instance.
+
+        Returns:
+            (xmin, xmax, ymin, ymax, zmin, zmax)
+
+        See also:
+            The class method :func:`Polygons.boundary_from_points()`
+
+        """
+        xmin = np.nanmin(self.dataframe[self.xname].values)
+        xmax = np.nanmax(self.dataframe[self.xname].values)
+        ymin = np.nanmin(self.dataframe[self.yname].values)
+        ymax = np.nanmax(self.dataframe[self.yname].values)
+        zmin = np.nanmin(self.dataframe[self.zname].values)
+        zmax = np.nanmax(self.dataframe[self.zname].values)
+
+        return (xmin, xmax, ymin, ymax, zmin, zmax)
+
+    def operation_polygons(self, poly, value, opname="add", inside=True):
+        """A generic function for operations restricted to inside or outside polygon(s).
+
+        The operations are performed on the Z values, while the 'inside' or 'outside'
+        of polygons are purely based on X and Y values (typically X is East and Y in
+        North coordinates).
+
+        The operations are XYZ generic i.e. done on the points that defines the
+        Polygon or the point in Points, depending on the calling instance.
+
+        Possible ``opname`` strings:
+
+        * ``add``: add the value
+        * ``sub``: substract the value
+        * ``mul``: multiply the value
+        * ``div``: divide the value
+        * ``set``: replace current values with value
+        * ``eli``: eliminate; here value is not applied
+
+        Args:
+            poly (Polygons): A XTGeo Polygons instance
+            value(float): Value to add, subtract etc
+            opname (str): Name of operation... 'add', 'sub', etc
+            inside (bool): If True do operation inside polygons; else outside. Note
+                that boundary is treated as 'inside'
+
+        Note:
+            This function works only intuitively when using one single polygon
+            in the ``poly`` instance. When having several polygons the
+            operation is done sequentially per polygon which may
+            lead to surprising results. For instance, using "add inside"
+            into two overlapping polygons, the addition will be doubled in the
+            overlapping part. Similarly using e.g. "eli, outside" will completely
+            remove all points of two non-overlapping polygons are given as input.
+        """
+        _xyz_oper.operation_polygons(self, poly, value, opname=opname, inside=inside)
+
+    def add_inside(self, poly, value):
+        """Add a value (scalar) to points inside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="add", inside=True)
+
+    def add_outside(self, poly, value):
+        """Add a value (scalar) to points outside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="add", inside=False)
+
+    def sub_inside(self, poly, value):
+        """Subtract a value (scalar) to points inside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="sub", inside=True)
+
+    def sub_outside(self, poly, value):
+        """Subtract a value (scalar) to points outside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="sub", inside=False)
+
+    def mul_inside(self, poly, value):
+        """Multiply a value (scalar) to points inside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="mul", inside=True)
+
+    def mul_outside(self, poly, value):
+        """Multiply a value (scalar) to points outside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="mul", inside=False)
+
+    def div_inside(self, poly, value):
+        """Divide a value (scalar) to points inside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="div", inside=True)
+
+    def div_outside(self, poly, value):
+        """Divide a value (scalar) outside polygons (value 0.0 will give result 0).
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="div", inside=False)
+
+    def set_inside(self, poly, value):
+        """Set a value (scalar) to points inside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="set", inside=True)
+
+    def set_outside(self, poly, value):
+        """Set a value (scalar) to points outside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, value, opname="set", inside=False)
+
+    def eli_inside(self, poly):
+        """Eliminate current points inside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, 0, opname="eli", inside=True)
+
+    def eli_outside(self, poly):
+        """Eliminate current points outside polygons.
+
+        See notes under :meth:`operation_polygons`.
+        """
+        self.operation_polygons(poly, 0, opname="eli", inside=False)
```

## xtgeo/xyz/_xyz_io.py

 * *Ordering differences only*

```diff
@@ -1,481 +1,481 @@
-# -*- coding: utf-8 -*-
-"""Private import and export routines for XYZ stuff."""
-
-from collections import OrderedDict
-
-import numpy as np
-import pandas as pd
-
-import xtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def import_xyz(pfile, zname="Z_TVDSS"):
-    """Simple X Y Z file. All points as Pandas framework."""
-    return {
-        "zname": zname,
-        "xname": "X_UTME",
-        "yname": "Y_UTMN",
-        "values": pd.read_csv(
-            pfile.file,
-            delim_whitespace=True,
-            skiprows=0,
-            header=None,
-            names=["X_UTME", "Y_UTMN", zname],
-            dtype=np.float64,
-            na_values=999.00,
-        ),
-    }
-
-
-def import_zmap(pfile, zname="Z_TVDSS"):
-    """The zmap ascii polygon format; not sure about all details."""
-    # ...seems that I just
-    # take in the columns starting from @<blank> line as is.
-    # Potential here to improve...
-
-    #
-    # !
-    # ! File exported from RMS.
-    # !
-    # ! Project:
-    # ! Date:          2017-11-07T17:22:30
-    # !
-    # ! Polygons/points Z-MAP file generated for ''.
-    # ! Coordinate system is ''.
-    # !
-    # !------------------------------------------------------------------
-    # @FREE POINT        , DATA, 80, 1
-    # X (EASTING)        , 1, 1,  1,      1, 20,,    1.0E+30,,,   4, 0
-    # Y (NORTHING)       , 2, 2,  1,     21, 40,,    1.0E+30,,,   4, 0
-    # Z VALUE            , 3, 3,  1,     41, 60,,    1.0E+30,,,   4, 0
-    # SEG I.D.           , 4, 35, 1,     61, 70,,    1.0E+30,,,   0, 0
-    # @
-    #    457357.781250      6782685.500000      1744.463379         0
-    #    457359.343750      6782676.000000      1744.482056         0
-    #    457370.906250      6782606.000000      1744.619507         0
-    #    457370.468750      6782568.500000      1745.868286         0
-
-    xname = "X_UTME"
-    yname = "Y_UTMN"
-    pname = "POLY_ID"
-
-    dtype = {
-        xname: np.float64,
-        yname: np.float64,
-        zname: np.float64,
-        pname: np.int32,
-    }
-
-    df = pd.read_csv(
-        pfile.file,
-        delim_whitespace=True,
-        skiprows=16,
-        header=None,
-        names=[xname, yname, zname, pname],
-        dtype=dtype,
-        na_values=1.0e30,
-    )
-
-    return {"xname": xname, "yname": yname, "zname": zname, "values": df}
-
-
-def import_rms_attr(pfile, zname="Z_TVDSS"):
-    """The RMS ascii file Points format with attributes.
-
-    It appears that the the RMS attributes format is supported for Points only,
-    hence Polygons is not admitted.
-
-    Example::
-
-       Discrete  FaultBlock
-       String    FaultTag
-       Float     VerticalSep
-       519427.941  6733887.914  1968.988     6  UNDEF  UNDEF
-       519446.363  6732037.910  1806.782    19  UNDEF  UNDEF
-       519446.379  6732137.910  1795.707    19  UNDEF  UNDEF
-
-    Returns a kwargs list with the following items:
-        xname
-        yname
-        zname
-        values as a valid dataframe
-        attributes
-
-    Important notes from RMS manual and reverse engineering:
-
-    * For discrete numbers use 'Discrete' or 'Integer', not 'Int'
-    * For Discrete/Integer/Float both UNDEF and -999 will mark as undefined
-    * For Discrete/Integer, numbers less than -999 seems to accepted by RMS
-    * For String, use UNDEF only as undefined
-    """
-
-    kwargs = {}
-    _xn = kwargs["xname"] = "X_UTME"
-    _yn = kwargs["yname"] = "Y_UTMN"
-    _zn = kwargs["zname"] = zname
-
-    dtypes = {_xn: np.float64, _yn: np.float64, _zn: np.float64}
-
-    names = list(dtypes.keys())
-    _attrs = OrderedDict()
-
-    # parse header
-    skiprows = 0
-    with open(pfile.file, "r") as rmsfile:
-        for iline in range(20):
-            fields = rmsfile.readline().split()
-            if len(fields) != 2:
-                skiprows = iline
-                break
-
-            dty, cname = fields
-            dtyx = None
-
-            # note that Pandas treats dtype str as object, cf:
-            # https://stackoverflow.com/questions/34881079
-            if dty == "Discrete":
-                dtyx = "int"
-            elif dty == "String":
-                dtyx = "str"
-            elif dty == "Float":
-                dtyx = "float"
-            elif dty == "Int":
-                dtyx = "int"
-            else:
-                dtyx = "str"
-            names.append(cname)
-            _attrs[cname] = dtyx
-
-    dfr = pd.read_csv(
-        pfile.file,
-        delim_whitespace=True,
-        skiprows=skiprows,
-        header=None,
-        names=names,
-        dtype=dtypes,
-    )
-    for col in dfr.columns[3:]:
-        if col in _attrs:
-            if _attrs[col] == "float":
-                dfr[col].replace("UNDEF", xtgeo.UNDEF, inplace=True)
-            elif _attrs[col] == "int":
-                dfr[col].replace("UNDEF", xtgeo.UNDEF_INT, inplace=True)
-            # cast to numerical if possible
-        dfr[col] = pd.to_numeric(dfr[col], errors="ignore")
-
-    kwargs["values"] = dfr
-    kwargs["attributes"] = _attrs
-
-    return kwargs
-
-
-def to_file(
-    xyz,
-    pfile,
-    fformat="xyz",
-    attributes=False,
-    pfilter=None,
-    wcolumn=None,
-    hcolumn=None,
-    mdcolumn="M_MDEPTH",
-    ispolygons=False,
-    **kwargs,
-):  # pylint: disable=redefined-builtin
-    """Export XYZ (Points/Polygons) to file.
-
-    Args:
-        pfile (str): Name of file
-        fformat (str): File format xyz/poi/pol / rms_attr /rms_wellpicks
-        attributes (bool or list): List of extra columns to export (some formats)
-            or True for all attributes present
-        pfilter (dict): Filter on e.g. top name(s) with keys TopName
-            or ZoneName as {'TopName': ['Top1', 'Top2']}
-        wcolumn (str): Name of well column (rms_wellpicks format only)
-        hcolumn (str): Name of horizons column (rms_wellpicks format only)
-        mdcolumn (str): Name of MD column (rms_wellpicks format only)
-
-    Returns:
-        Number of points exported
-
-    Note that the rms_wellpicks will try to output to:
-
-    * HorizonName, WellName, MD  if a MD (mdcolumn) is present,
-    * HorizonName, WellName, X, Y, Z  otherwise
-
-    Raises:
-        KeyError if pfilter is set and key(s) are invalid
-
-    """
-    filter_deprecated = kwargs.get("filter", None)
-    if filter_deprecated is not None and pfilter is None:
-        pfilter = filter_deprecated
-
-    pfile = xtgeo._XTGeoFile(pfile)
-    pfile.check_folder(raiseerror=OSError)
-
-    ncount = 0
-    if xyz.dataframe is None:
-        logger.warning("Nothing to export!")
-        return ncount
-
-    if fformat is None or fformat in ["xyz", "poi", "pol"]:
-        # NB! reuse export_rms_attr function, but no attributes
-        # are possible
-        ncount = export_rms_attr(
-            xyz, pfile.name, attributes=False, pfilter=pfilter, ispolygons=ispolygons
-        )
-
-    elif fformat == "rms_attr":
-        ncount = export_rms_attr(
-            xyz,
-            pfile.name,
-            attributes=attributes,
-            pfilter=pfilter,
-            ispolygons=ispolygons,
-        )
-    elif fformat == "rms_wellpicks":
-        ncount = export_rms_wpicks(xyz, pfile.name, hcolumn, wcolumn, mdcolumn=mdcolumn)
-
-    if ncount is None:
-        ncount = 0
-
-    if ncount == 0:
-        logger.warning("Nothing to export!")
-
-    return ncount
-
-
-def export_rms_attr(self, pfile, attributes=True, pfilter=None, ispolygons=False):
-    """Export til RMS attribute, also called RMS extended set.
-
-    If attributes is None, then it will be a simple XYZ file.
-
-    Attributes can be a bool or a list. If True, then use all attributes.
-
-    Filter is on the form {TopName: ['Name1', 'Name2']}
-
-    Returns:
-        The number of values exported. If value is 0; then no file
-        is made.
-    """
-
-    df = self.dataframe.copy()
-
-    if not df.index.any():
-        logger.warning("Nothing to export")
-        return 0
-
-    columns = [self._xname, self._yname, self.zname]
-    df.fillna(value=999.0, inplace=True)
-
-    mode = "w"
-
-    transl = {"int": "Discrete", "float": "Float", "str": "String"}
-
-    logger.info("Attributes is %s", attributes)
-
-    # apply pfilter if any
-    if pfilter:
-        for key, val in pfilter.items():
-            if key in df.columns:
-                df = df.loc[df[key].isin(val)]
-            else:
-                raise KeyError(
-                    f"The requested pfilter key {key} was not found in dataframe. "
-                    f"Valid keys are {df.columns}"
-                )
-
-    if ispolygons:
-        if not attributes and self._pname in df.columns:
-            # need to convert the dataframe
-            df = _convert_idbased_xyz(self, df)
-    elif attributes is True:
-        attributes = list(self._attrs.keys())
-        logger.info("Use all attributes: %s", attributes)
-
-        for column in (self._xname, self._yname, self._zname):
-            try:
-                attributes.remove(column)
-            except ValueError:
-                continue
-    if isinstance(attributes, list):
-        mode = "a"
-        columns += attributes
-        with open(pfile, "w") as fout:
-            for col in attributes:
-                if col in df.columns:
-                    fout.write(transl[self._attrs[col]] + " " + col + "\n")
-                    if self._attrs[col] == "int":
-                        df[col].replace(xtgeo.UNDEF_INT, "UNDEF", inplace=True)
-                    elif self._attrs[col] == "float":
-                        df[col].replace(xtgeo.UNDEF, "UNDEF", inplace=True)
-
-    with open(pfile, mode) as fc:
-        df.to_csv(fc, sep=" ", header=None, columns=columns, index=False)
-
-    return len(df.index)
-
-
-def _convert_idbased_xyz(self, df):
-    """Conversion of format from ID column to 999 flag."""
-
-    # If polygons, there is a 4th column with POLY_ID. This needs
-    # to replaced by adding 999 line instead (for polygons)
-    # prior to XYZ export or when interactions in CXTGEO
-
-    idgroups = df.groupby(self._pname)
-
-    newdf = pd.DataFrame(
-        columns=[self._xname, self._yname, self._zname], dtype="float64"
-    )
-    udef = pd.DataFrame(
-        [[999.0, 999.0, 999.0]], columns=[self._xname, self._yname, self._zname]
-    )
-
-    for _id, gr in idgroups:
-        dfx = gr.drop(self._pname, axis=1)
-        newdf = pd.concat([newdf, dfx, udef], ignore_index=True)
-
-    return newdf
-
-
-def export_rms_wpicks(self, pfile, hcolumn, wcolumn, mdcolumn="M_MDEPTH"):
-    """Export til RMS wellpicks
-
-    If a MD column (mdcolumn) exists, it will use the MD
-
-    Args:
-        pfile (str): File to export to
-        hcolumn (str): Name of horizon/zone column in the point set
-        wcolumn (str): Name of well column in the point set
-        mdcolumn (str): Name of measured depht column (if any)
-    Returns:
-        The number of values exported. If value is 0; then no file
-        is made.
-
-    """
-
-    df = self.dataframe.copy()
-
-    if not df.index.any():
-        logger.warning("Nothing to export")
-        return 0
-
-    columns = []
-
-    if hcolumn in df.columns:
-        columns.append(hcolumn)
-    else:
-        raise ValueError(f"Column for horizons/zones <{hcolumn}> not present")
-
-    if wcolumn in df.columns:
-        columns.append(wcolumn)
-    else:
-        raise ValueError(f"Column for wells <{wcolumn}> not present")
-
-    if mdcolumn in df.columns:
-        columns.append(mdcolumn)
-    else:
-        columns += [self._xname, self._yname, self._zname]
-
-    if not df.index.any():
-        logger.warning("Nothing to export")
-        return 0
-
-    with open(pfile, "w") as fc:
-        df.to_csv(fc, sep=" ", header=None, columns=columns, index=False)
-
-    return len(df.index)
-
-
-def _from_list_like(plist, zname, attrs, is_polygons) -> pd.DataFrame:
-    """Import Points or Polygons from a list-like input.
-
-    The following 'list-like' inputs are possible:
-
-    * List of tuples [(x1, y1, z1, <id1>), (x2, y2, z2, <id2>), ...].
-    * List of lists  [[x1, y1, z1, <id1>], [x2, y2, z2, <id2>], ...].
-    * List of numpy arrays  [nparr1, nparr2, ...] where nparr1 is first row.
-    * A numpy array with shape [nrow, ncol], where ncol >= 3
-    * An existing pandas dataframe
-
-    Points scenaria:
-    * 3 columns, X Y Z
-    * 4 or more columns: rest columns are attributes
-
-    Polygons scenaria:
-    * 3 columns, X Y Z. Here P column is assigned 0 afterwards
-    * 4 or more columns:
-        - if totnum = lenattrs + 3 then POLY_ID is missing and will be made
-        - if totnum = lenattrs + 4 then assume that 4'th column is POLY_ID
-
-    It is currently not much error checking that lists/tuples are consistent, e.g.
-    if there always is either 3 or 4 elements per tuple, or that 4 number is
-    an integer.
-
-    Args:
-        plist (str): List of tuples, each tuple is length 3 or 4
-        zname (str): Name of third column
-        attrs (dict): Attributes, for Points
-        is_polygons (bool): Flag for Points or Polygons
-
-    Returns:
-        A valid datafram
-
-    Raises:
-        ValueError: If something is wrong with input
-
-    .. versionadded:: 2.16
-    """
-
-    dfr = None
-    if isinstance(plist, list):
-        plist = np.array(plist)
-
-    if isinstance(plist, np.ndarray):
-        logger.info("Process numpy to points")
-        if len(plist) == 0:
-            return pd.DataFrame([], columns=["X_UTME", "Y_UTMN", zname])
-
-        if plist.ndim != 2:
-            raise ValueError("Input numpy array must two-dimensional")
-        totnum = plist.shape[1]
-        lenattrs = len(attrs) if attrs is not None else 0
-        attr_first_col = 3
-        if totnum == 3 + lenattrs:
-            dfr = pd.DataFrame(plist[:, :3], columns=["X_UTME", "Y_UTMN", zname])
-            dfr = dfr.astype(float)
-            if is_polygons:
-                # pname column is missing but assign 0 as ID
-                dfr["POLY_ID"] = 0
-
-        elif totnum == 4 + lenattrs and is_polygons:
-            dfr = pd.DataFrame(
-                plist[:, :4],
-                columns=["X_UTME", "Y_UTMN", zname, "POLY_ID"],
-            )
-            attr_first_col = 4
-        else:
-            raise ValueError(
-                f"Wrong length detected of row: {totnum}. "
-                "Are attributes set correct?"
-            )
-        dfr.dropna()
-        dfr = dfr.astype(np.float64)
-        if is_polygons:
-            dfr["POLY_ID"] = dfr["POLY_ID"].astype(np.int32)
-
-        if lenattrs > 0:
-            for enum, (key, dtype) in enumerate(attrs.items()):
-                dfr[key] = plist[:, attr_first_col + enum]
-                dfr = dfr.astype({key: dtype})
-
-    else:
-        raise TypeError("Not possible to make XYZ from given input")
-
-    return dfr
+# -*- coding: utf-8 -*-
+"""Private import and export routines for XYZ stuff."""
+
+from collections import OrderedDict
+
+import numpy as np
+import pandas as pd
+
+import xtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def import_xyz(pfile, zname="Z_TVDSS"):
+    """Simple X Y Z file. All points as Pandas framework."""
+    return {
+        "zname": zname,
+        "xname": "X_UTME",
+        "yname": "Y_UTMN",
+        "values": pd.read_csv(
+            pfile.file,
+            delim_whitespace=True,
+            skiprows=0,
+            header=None,
+            names=["X_UTME", "Y_UTMN", zname],
+            dtype=np.float64,
+            na_values=999.00,
+        ),
+    }
+
+
+def import_zmap(pfile, zname="Z_TVDSS"):
+    """The zmap ascii polygon format; not sure about all details."""
+    # ...seems that I just
+    # take in the columns starting from @<blank> line as is.
+    # Potential here to improve...
+
+    #
+    # !
+    # ! File exported from RMS.
+    # !
+    # ! Project:
+    # ! Date:          2017-11-07T17:22:30
+    # !
+    # ! Polygons/points Z-MAP file generated for ''.
+    # ! Coordinate system is ''.
+    # !
+    # !------------------------------------------------------------------
+    # @FREE POINT        , DATA, 80, 1
+    # X (EASTING)        , 1, 1,  1,      1, 20,,    1.0E+30,,,   4, 0
+    # Y (NORTHING)       , 2, 2,  1,     21, 40,,    1.0E+30,,,   4, 0
+    # Z VALUE            , 3, 3,  1,     41, 60,,    1.0E+30,,,   4, 0
+    # SEG I.D.           , 4, 35, 1,     61, 70,,    1.0E+30,,,   0, 0
+    # @
+    #    457357.781250      6782685.500000      1744.463379         0
+    #    457359.343750      6782676.000000      1744.482056         0
+    #    457370.906250      6782606.000000      1744.619507         0
+    #    457370.468750      6782568.500000      1745.868286         0
+
+    xname = "X_UTME"
+    yname = "Y_UTMN"
+    pname = "POLY_ID"
+
+    dtype = {
+        xname: np.float64,
+        yname: np.float64,
+        zname: np.float64,
+        pname: np.int32,
+    }
+
+    df = pd.read_csv(
+        pfile.file,
+        delim_whitespace=True,
+        skiprows=16,
+        header=None,
+        names=[xname, yname, zname, pname],
+        dtype=dtype,
+        na_values=1.0e30,
+    )
+
+    return {"xname": xname, "yname": yname, "zname": zname, "values": df}
+
+
+def import_rms_attr(pfile, zname="Z_TVDSS"):
+    """The RMS ascii file Points format with attributes.
+
+    It appears that the the RMS attributes format is supported for Points only,
+    hence Polygons is not admitted.
+
+    Example::
+
+       Discrete  FaultBlock
+       String    FaultTag
+       Float     VerticalSep
+       519427.941  6733887.914  1968.988     6  UNDEF  UNDEF
+       519446.363  6732037.910  1806.782    19  UNDEF  UNDEF
+       519446.379  6732137.910  1795.707    19  UNDEF  UNDEF
+
+    Returns a kwargs list with the following items:
+        xname
+        yname
+        zname
+        values as a valid dataframe
+        attributes
+
+    Important notes from RMS manual and reverse engineering:
+
+    * For discrete numbers use 'Discrete' or 'Integer', not 'Int'
+    * For Discrete/Integer/Float both UNDEF and -999 will mark as undefined
+    * For Discrete/Integer, numbers less than -999 seems to accepted by RMS
+    * For String, use UNDEF only as undefined
+    """
+
+    kwargs = {}
+    _xn = kwargs["xname"] = "X_UTME"
+    _yn = kwargs["yname"] = "Y_UTMN"
+    _zn = kwargs["zname"] = zname
+
+    dtypes = {_xn: np.float64, _yn: np.float64, _zn: np.float64}
+
+    names = list(dtypes.keys())
+    _attrs = OrderedDict()
+
+    # parse header
+    skiprows = 0
+    with open(pfile.file, "r") as rmsfile:
+        for iline in range(20):
+            fields = rmsfile.readline().split()
+            if len(fields) != 2:
+                skiprows = iline
+                break
+
+            dty, cname = fields
+            dtyx = None
+
+            # note that Pandas treats dtype str as object, cf:
+            # https://stackoverflow.com/questions/34881079
+            if dty == "Discrete":
+                dtyx = "int"
+            elif dty == "String":
+                dtyx = "str"
+            elif dty == "Float":
+                dtyx = "float"
+            elif dty == "Int":
+                dtyx = "int"
+            else:
+                dtyx = "str"
+            names.append(cname)
+            _attrs[cname] = dtyx
+
+    dfr = pd.read_csv(
+        pfile.file,
+        delim_whitespace=True,
+        skiprows=skiprows,
+        header=None,
+        names=names,
+        dtype=dtypes,
+    )
+    for col in dfr.columns[3:]:
+        if col in _attrs:
+            if _attrs[col] == "float":
+                dfr[col].replace("UNDEF", xtgeo.UNDEF, inplace=True)
+            elif _attrs[col] == "int":
+                dfr[col].replace("UNDEF", xtgeo.UNDEF_INT, inplace=True)
+            # cast to numerical if possible
+        dfr[col] = pd.to_numeric(dfr[col], errors="ignore")
+
+    kwargs["values"] = dfr
+    kwargs["attributes"] = _attrs
+
+    return kwargs
+
+
+def to_file(
+    xyz,
+    pfile,
+    fformat="xyz",
+    attributes=False,
+    pfilter=None,
+    wcolumn=None,
+    hcolumn=None,
+    mdcolumn="M_MDEPTH",
+    ispolygons=False,
+    **kwargs,
+):  # pylint: disable=redefined-builtin
+    """Export XYZ (Points/Polygons) to file.
+
+    Args:
+        pfile (str): Name of file
+        fformat (str): File format xyz/poi/pol / rms_attr /rms_wellpicks
+        attributes (bool or list): List of extra columns to export (some formats)
+            or True for all attributes present
+        pfilter (dict): Filter on e.g. top name(s) with keys TopName
+            or ZoneName as {'TopName': ['Top1', 'Top2']}
+        wcolumn (str): Name of well column (rms_wellpicks format only)
+        hcolumn (str): Name of horizons column (rms_wellpicks format only)
+        mdcolumn (str): Name of MD column (rms_wellpicks format only)
+
+    Returns:
+        Number of points exported
+
+    Note that the rms_wellpicks will try to output to:
+
+    * HorizonName, WellName, MD  if a MD (mdcolumn) is present,
+    * HorizonName, WellName, X, Y, Z  otherwise
+
+    Raises:
+        KeyError if pfilter is set and key(s) are invalid
+
+    """
+    filter_deprecated = kwargs.get("filter", None)
+    if filter_deprecated is not None and pfilter is None:
+        pfilter = filter_deprecated
+
+    pfile = xtgeo._XTGeoFile(pfile)
+    pfile.check_folder(raiseerror=OSError)
+
+    ncount = 0
+    if xyz.dataframe is None:
+        logger.warning("Nothing to export!")
+        return ncount
+
+    if fformat is None or fformat in ["xyz", "poi", "pol"]:
+        # NB! reuse export_rms_attr function, but no attributes
+        # are possible
+        ncount = export_rms_attr(
+            xyz, pfile.name, attributes=False, pfilter=pfilter, ispolygons=ispolygons
+        )
+
+    elif fformat == "rms_attr":
+        ncount = export_rms_attr(
+            xyz,
+            pfile.name,
+            attributes=attributes,
+            pfilter=pfilter,
+            ispolygons=ispolygons,
+        )
+    elif fformat == "rms_wellpicks":
+        ncount = export_rms_wpicks(xyz, pfile.name, hcolumn, wcolumn, mdcolumn=mdcolumn)
+
+    if ncount is None:
+        ncount = 0
+
+    if ncount == 0:
+        logger.warning("Nothing to export!")
+
+    return ncount
+
+
+def export_rms_attr(self, pfile, attributes=True, pfilter=None, ispolygons=False):
+    """Export til RMS attribute, also called RMS extended set.
+
+    If attributes is None, then it will be a simple XYZ file.
+
+    Attributes can be a bool or a list. If True, then use all attributes.
+
+    Filter is on the form {TopName: ['Name1', 'Name2']}
+
+    Returns:
+        The number of values exported. If value is 0; then no file
+        is made.
+    """
+
+    df = self.dataframe.copy()
+
+    if not df.index.any():
+        logger.warning("Nothing to export")
+        return 0
+
+    columns = [self._xname, self._yname, self.zname]
+    df.fillna(value=999.0, inplace=True)
+
+    mode = "w"
+
+    transl = {"int": "Discrete", "float": "Float", "str": "String"}
+
+    logger.info("Attributes is %s", attributes)
+
+    # apply pfilter if any
+    if pfilter:
+        for key, val in pfilter.items():
+            if key in df.columns:
+                df = df.loc[df[key].isin(val)]
+            else:
+                raise KeyError(
+                    f"The requested pfilter key {key} was not found in dataframe. "
+                    f"Valid keys are {df.columns}"
+                )
+
+    if ispolygons:
+        if not attributes and self._pname in df.columns:
+            # need to convert the dataframe
+            df = _convert_idbased_xyz(self, df)
+    elif attributes is True:
+        attributes = list(self._attrs.keys())
+        logger.info("Use all attributes: %s", attributes)
+
+        for column in (self._xname, self._yname, self._zname):
+            try:
+                attributes.remove(column)
+            except ValueError:
+                continue
+    if isinstance(attributes, list):
+        mode = "a"
+        columns += attributes
+        with open(pfile, "w") as fout:
+            for col in attributes:
+                if col in df.columns:
+                    fout.write(transl[self._attrs[col]] + " " + col + "\n")
+                    if self._attrs[col] == "int":
+                        df[col].replace(xtgeo.UNDEF_INT, "UNDEF", inplace=True)
+                    elif self._attrs[col] == "float":
+                        df[col].replace(xtgeo.UNDEF, "UNDEF", inplace=True)
+
+    with open(pfile, mode) as fc:
+        df.to_csv(fc, sep=" ", header=None, columns=columns, index=False)
+
+    return len(df.index)
+
+
+def _convert_idbased_xyz(self, df):
+    """Conversion of format from ID column to 999 flag."""
+
+    # If polygons, there is a 4th column with POLY_ID. This needs
+    # to replaced by adding 999 line instead (for polygons)
+    # prior to XYZ export or when interactions in CXTGEO
+
+    idgroups = df.groupby(self._pname)
+
+    newdf = pd.DataFrame(
+        columns=[self._xname, self._yname, self._zname], dtype="float64"
+    )
+    udef = pd.DataFrame(
+        [[999.0, 999.0, 999.0]], columns=[self._xname, self._yname, self._zname]
+    )
+
+    for _id, gr in idgroups:
+        dfx = gr.drop(self._pname, axis=1)
+        newdf = pd.concat([newdf, dfx, udef], ignore_index=True)
+
+    return newdf
+
+
+def export_rms_wpicks(self, pfile, hcolumn, wcolumn, mdcolumn="M_MDEPTH"):
+    """Export til RMS wellpicks
+
+    If a MD column (mdcolumn) exists, it will use the MD
+
+    Args:
+        pfile (str): File to export to
+        hcolumn (str): Name of horizon/zone column in the point set
+        wcolumn (str): Name of well column in the point set
+        mdcolumn (str): Name of measured depht column (if any)
+    Returns:
+        The number of values exported. If value is 0; then no file
+        is made.
+
+    """
+
+    df = self.dataframe.copy()
+
+    if not df.index.any():
+        logger.warning("Nothing to export")
+        return 0
+
+    columns = []
+
+    if hcolumn in df.columns:
+        columns.append(hcolumn)
+    else:
+        raise ValueError(f"Column for horizons/zones <{hcolumn}> not present")
+
+    if wcolumn in df.columns:
+        columns.append(wcolumn)
+    else:
+        raise ValueError(f"Column for wells <{wcolumn}> not present")
+
+    if mdcolumn in df.columns:
+        columns.append(mdcolumn)
+    else:
+        columns += [self._xname, self._yname, self._zname]
+
+    if not df.index.any():
+        logger.warning("Nothing to export")
+        return 0
+
+    with open(pfile, "w") as fc:
+        df.to_csv(fc, sep=" ", header=None, columns=columns, index=False)
+
+    return len(df.index)
+
+
+def _from_list_like(plist, zname, attrs, is_polygons) -> pd.DataFrame:
+    """Import Points or Polygons from a list-like input.
+
+    The following 'list-like' inputs are possible:
+
+    * List of tuples [(x1, y1, z1, <id1>), (x2, y2, z2, <id2>), ...].
+    * List of lists  [[x1, y1, z1, <id1>], [x2, y2, z2, <id2>], ...].
+    * List of numpy arrays  [nparr1, nparr2, ...] where nparr1 is first row.
+    * A numpy array with shape [nrow, ncol], where ncol >= 3
+    * An existing pandas dataframe
+
+    Points scenaria:
+    * 3 columns, X Y Z
+    * 4 or more columns: rest columns are attributes
+
+    Polygons scenaria:
+    * 3 columns, X Y Z. Here P column is assigned 0 afterwards
+    * 4 or more columns:
+        - if totnum = lenattrs + 3 then POLY_ID is missing and will be made
+        - if totnum = lenattrs + 4 then assume that 4'th column is POLY_ID
+
+    It is currently not much error checking that lists/tuples are consistent, e.g.
+    if there always is either 3 or 4 elements per tuple, or that 4 number is
+    an integer.
+
+    Args:
+        plist (str): List of tuples, each tuple is length 3 or 4
+        zname (str): Name of third column
+        attrs (dict): Attributes, for Points
+        is_polygons (bool): Flag for Points or Polygons
+
+    Returns:
+        A valid datafram
+
+    Raises:
+        ValueError: If something is wrong with input
+
+    .. versionadded:: 2.16
+    """
+
+    dfr = None
+    if isinstance(plist, list):
+        plist = np.array(plist)
+
+    if isinstance(plist, np.ndarray):
+        logger.info("Process numpy to points")
+        if len(plist) == 0:
+            return pd.DataFrame([], columns=["X_UTME", "Y_UTMN", zname])
+
+        if plist.ndim != 2:
+            raise ValueError("Input numpy array must two-dimensional")
+        totnum = plist.shape[1]
+        lenattrs = len(attrs) if attrs is not None else 0
+        attr_first_col = 3
+        if totnum == 3 + lenattrs:
+            dfr = pd.DataFrame(plist[:, :3], columns=["X_UTME", "Y_UTMN", zname])
+            dfr = dfr.astype(float)
+            if is_polygons:
+                # pname column is missing but assign 0 as ID
+                dfr["POLY_ID"] = 0
+
+        elif totnum == 4 + lenattrs and is_polygons:
+            dfr = pd.DataFrame(
+                plist[:, :4],
+                columns=["X_UTME", "Y_UTMN", zname, "POLY_ID"],
+            )
+            attr_first_col = 4
+        else:
+            raise ValueError(
+                f"Wrong length detected of row: {totnum}. "
+                "Are attributes set correct?"
+            )
+        dfr.dropna()
+        dfr = dfr.astype(np.float64)
+        if is_polygons:
+            dfr["POLY_ID"] = dfr["POLY_ID"].astype(np.int32)
+
+        if lenattrs > 0:
+            for enum, (key, dtype) in enumerate(attrs.items()):
+                dfr[key] = plist[:, attr_first_col + enum]
+                dfr = dfr.astype({key: dtype})
+
+    else:
+        raise TypeError("Not possible to make XYZ from given input")
+
+    return dfr
```

## xtgeo/xyz/_xyz_lowlevel.py

 * *Ordering differences only*

```diff
@@ -1,47 +1,47 @@
-# -*- coding: utf-8 -*-
-"""Private low level routines (SWIG vs C)"""
-
-
-import numpy as np
-from xtgeo.common import XTGeoDialog
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-def convert_np_carr_int(xyz, np_array):  # pragma: no cover
-    """Convert numpy 1D array to C array, assuming int type."""
-
-    # The numpy is always a double (float64), so need to convert first
-    # xyz is the general object
-
-    carr = _cxtgeo.new_intarray(xyz.nrow)
-
-    np_array = np_array.astype(np.int32)
-
-    _cxtgeo.swig_numpy_to_carr_i1d(np_array, carr)
-
-    return carr
-
-
-def convert_np_carr_double(xyz, np_array):  # pragma: no cover
-    """Convert numpy 1D array to C array, assuming double type."""
-
-    carr = _cxtgeo.new_doublearray(xyz.nrow)
-
-    _cxtgeo.swig_numpy_to_carr_1d(np_array, carr)
-
-    return carr
-
-
-def convert_carr_double_np(xyz, carray, nlen=None):  # pragma: no cover
-    """Convert a C array to numpy, assuming double type."""
-
-    if nlen is None:
-        nlen = len(xyz._df.index)
-
-    nparray = _cxtgeo.swig_carr_to_numpy_1d(nlen, carray)
-
-    return nparray
+# -*- coding: utf-8 -*-
+"""Private low level routines (SWIG vs C)"""
+
+
+import numpy as np
+from xtgeo.common import XTGeoDialog
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+def convert_np_carr_int(xyz, np_array):  # pragma: no cover
+    """Convert numpy 1D array to C array, assuming int type."""
+
+    # The numpy is always a double (float64), so need to convert first
+    # xyz is the general object
+
+    carr = _cxtgeo.new_intarray(xyz.nrow)
+
+    np_array = np_array.astype(np.int32)
+
+    _cxtgeo.swig_numpy_to_carr_i1d(np_array, carr)
+
+    return carr
+
+
+def convert_np_carr_double(xyz, np_array):  # pragma: no cover
+    """Convert numpy 1D array to C array, assuming double type."""
+
+    carr = _cxtgeo.new_doublearray(xyz.nrow)
+
+    _cxtgeo.swig_numpy_to_carr_1d(np_array, carr)
+
+    return carr
+
+
+def convert_carr_double_np(xyz, carray, nlen=None):  # pragma: no cover
+    """Convert a C array to numpy, assuming double type."""
+
+    if nlen is None:
+        nlen = len(xyz._df.index)
+
+    nparray = _cxtgeo.swig_carr_to_numpy_1d(nlen, carray)
+
+    return nparray
```

## xtgeo/xyz/_xyz_oper.py

 * *Ordering differences only*

```diff
@@ -1,502 +1,502 @@
-# coding: utf-8
-"""Various operations on XYZ data"""
-
-
-import numpy as np
-import pandas as pd
-import shapely.geometry as sg
-from scipy.interpolate import UnivariateSpline, interp1d
-
-import xtgeo
-import xtgeo.cxtgeo._cxtgeo as _cxtgeo
-from xtgeo.common import XTGeoDialog
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-
-# pylint: disable=protected-access
-
-
-def operation_polygons(self, poly, value, opname="add", inside=True, where=True):
-    """
-    Operations re restricted to closed polygons, for points or polyline points.
-
-    If value is not float but 'poly', then the avg of each polygon Z value will
-    be used instead.
-
-    'Inside' several polygons will become a union, while 'outside' polygons
-    will be the intersection.
-
-    The "where" filter is reserved for future use.
-    """
-    logger.warning("Where is not imeplented: %s", where)
-
-    oper = {"set": 1, "add": 2, "sub": 3, "mul": 4, "div": 5, "eli": 11}
-
-    insidevalue = 0
-    if inside:
-        insidevalue = 1
-
-    logger.info("Operations of points inside polygon(s)...")
-    if not isinstance(poly, xtgeo.xyz.Polygons):
-        raise ValueError("The poly input is not a Polygons instance")
-
-    idgroups = poly.dataframe.groupby(poly.pname)
-
-    xcor = self.dataframe[self.xname].values
-    ycor = self.dataframe[self.yname].values
-    zcor = self.dataframe[self.zname].values
-
-    usepoly = False
-    if isinstance(value, str) and value == "poly":
-        usepoly = True
-
-    for id_, grp in idgroups:
-        pxcor = grp[poly.xname].values
-        pycor = grp[poly.yname].values
-        pvalue = value
-        if usepoly:
-            pvalue = grp[poly.zname].values.mean()
-        else:
-            pvalue = value
-
-        logger.info("C function for polygon %s...", id_)
-
-        ies = _cxtgeo.pol_do_points_inside(
-            xcor, ycor, zcor, pxcor, pycor, pvalue, oper[opname], insidevalue
-        )
-        logger.info("C function for polygon %s... done", id_)
-
-        if ies != 0:
-            raise RuntimeError(f"Something went wrong, code {ies}")
-
-    zcor[zcor > xtgeo.UNDEF_LIMIT] = np.nan
-    self.dataframe[self.zname] = zcor
-    # removing rows where Z column is undefined
-    self.dataframe.dropna(how="any", subset=[self.zname], inplace=True)
-    self.dataframe.reset_index(inplace=True, drop=True)
-    logger.info("Operations of points inside polygon(s)... done")
-
-
-def rescale_polygons(self, distance=10, addlen=False, kind="simple", mode2d=False):
-    """Rescale (resample) a polygons segment
-    Default settings will make it backwards compatible with 2.0
-    New options were added in 2.1:
-    * addlen
-    * kind
-    * mode2d
-    """
-
-    if kind in ("slinear", "cubic"):
-        _rescale_v2(self, distance, addlen, kind=kind, mode2d=mode2d)
-
-    else:
-        _rescale_v1(self, distance, addlen, mode2d=mode2d)
-
-
-def _rescale_v1(self, distance, addlen, mode2d):
-    # version 1, simple approach, will rescale in 2D since Shapely use 2D lengths
-    if not mode2d:
-        raise KeyError("Cannot combine 'simple' with mode2d False")
-
-    idgroups = self.dataframe.groupby(self.pname)
-
-    dfrlist = []
-    for idx, grp in idgroups:
-        if len(grp.index) < 2:
-            logger.warning("Cannot rescale polygons with less than two points. Skip")
-            continue
-
-        pxcor = grp[self.xname].values
-        pycor = grp[self.yname].values
-        pzcor = grp[self.zname].values
-        spoly = sg.LineString(np.stack([pxcor, pycor, pzcor], axis=1))
-
-        new_spoly = _redistribute_vertices(spoly, distance)
-        dfr = pd.DataFrame(
-            np.array(new_spoly.coords), columns=[self.xname, self.yname, self.zname]
-        )
-
-        dfr[self.pname] = idx
-        dfrlist.append(dfr)
-
-    dfr = pd.concat(dfrlist)
-    self.dataframe = dfr.reset_index(drop=True)
-
-    if addlen:
-        self.hlen()
-        self.tlen()
-
-
-def _redistribute_vertices(geom, distance):
-    """Local function to interpolate in a polyline using Shapely"""
-    if geom.geom_type == "LineString":
-        num_vert = int(round(geom.length / distance))
-        if num_vert == 0:
-            num_vert = 1
-        return sg.LineString(
-            [
-                geom.interpolate(float(n) / num_vert, normalized=True)
-                for n in range(num_vert + 1)
-            ]
-        )
-
-    if geom.geom_type == "MultiLineString":
-        parts = [_redistribute_vertices(part, distance) for part in geom]
-        return type(geom)([p for p in parts if not p.is_empty])
-
-    raise ValueError(f"Unhandled geometry {geom.geom_type}")
-
-
-def _handle_vertical_input(self, inframe):
-    """Treat the special vertical case.
-
-    A special case occurs for e.g. a 100% vertical well. Here the trick is distort
-    first and last row with a relative small number so that numerical problems are
-    avoided.
-    """
-
-    result = inframe.copy().reset_index()
-
-    # e.g. tvd.mean is 2000, then tolerance will be 0.002, and edit will be 2
-    tolerance = self.dataframe[self.zname].mean() * 0.000001
-    edit = tolerance * 1000
-    pseudo = self.copy()
-
-    if inframe[self.dhname].max() < tolerance:
-        result.at[0, self.xname] -= edit
-        result.at[result.index[-1], self.xname] += edit
-        pseudo.dataframe = result
-        pseudo.hlen()
-        pseudo.tlen()
-        result = pseudo.dataframe
-
-    return result
-
-
-def _rescale_v2(self, distance, addlen, kind="slinear", mode2d=True):
-    # Rescaling to constant increment is perhaps impossible, but this is
-    # perhaps quite close
-
-    self.hlen()
-    self.tlen()
-
-    idgroups = self.dataframe.groupby(self.pname)
-
-    dfrlist = []
-    for idx, grp in idgroups:
-        grp = _handle_vertical_input(self, grp)
-
-        # avoid duplicates when *_DELTALEN are 0.0 (makes scipy interp1d() fail
-        # when scipy >= 1.9)
-        grp = grp.drop(grp[(grp[self.dhname] == 0.0) | (grp[self.dtname] == 0.0)].index)
-
-        if len(grp.index) < 2:
-            logger.warning("Cannot rescale polygons with less than two points. Skip")
-            continue
-
-        points = [grp[self.xname], grp[self.yname], grp[self.zname]]
-
-        leng = grp[self.hname].iloc[-1]
-        gname = self.hname
-
-        if not mode2d:
-            leng = grp[self.tname].iloc[-1]
-            gname = self.tname
-
-        # to avoid numerical trouble of pure vertical sections
-        leng = leng - 0.001 * leng
-
-        nstep = int(leng / distance)
-        alpha = np.linspace(0, leng, num=nstep, endpoint=True)
-
-        if kind in ("slinear"):
-            points = np.array(points).T
-            interpolator = interp1d(
-                grp[gname],
-                points,
-                kind="slinear",
-                axis=0,
-                assume_sorted=True,
-                bounds_error=False,
-                fill_value="extrapolate",
-            )
-            ip = interpolator(alpha)
-        elif kind == "cubic":
-            splines = [UnivariateSpline(grp[gname], crd) for crd in points]
-
-            ip = np.vstack(list(spl(alpha) for spl in splines)).T
-        else:
-            raise ValueError(f"Invalid kind chosen: {kind}")
-
-        dfr = pd.DataFrame(np.array(ip), columns=[self.xname, self.yname, self.zname])
-
-        dfr[self.pname] = idx
-        dfrlist.append(dfr)
-
-    dfr = pd.concat(dfrlist)
-    self.dataframe = dfr.reset_index(drop=True)
-
-    if addlen:
-        self.tlen()
-        self.hlen()
-    else:
-        self.delete_columns([self.hname, self.dhname, self.tname, self.dtname])
-
-
-def get_fence(
-    self, distance=20, atleast=5, nextend=2, name=None, asnumpy=True, polyid=None
-):
-    """Get a fence suitable for plotting xsections, either as a numpy or as a
-    new Polygons instance.
-
-    The atleast parameter will win over the distance, meaning that if total length
-    horizontally is 50, and distance is set to 20, the actual length will be 50/5=10
-    In such cases, nextend will be modified automatically also to fulfill the original
-    intention of nextend*distance (approx).
-
-    The routine is still not perfect for "close to very vertical polygon"
-    but assumed to be sufficient for all practical cases
-
-    """
-    if atleast < 3:
-        raise ValueError("The atleast key must be 3 or greater")
-
-    orig_extend = nextend * distance
-    orig_distance = distance
-
-    fence = self.copy()
-
-    fence.hlen()
-
-    if len(fence.dataframe) < 2:
-        xtg.warn(f"Too few points in polygons for fence, return False (name: {name})")
-        return False
-
-    fence.filter_byid(polyid)
-
-    hxlen = fence.get_shapely_objects()[0].length
-
-    # perhaps a way to treat very vertical polys from e.g. wells:
-    if hxlen < 0.1 * orig_distance:
-        hxlen = 0.1 * orig_distance
-
-    if hxlen / (atleast + 1) < orig_distance:
-        distance = hxlen / (atleast + 1)
-
-    fence_keep = fence.copy()
-    fence.rescale(distance, kind="slinear", mode2d=True)
-
-    if len(fence.dataframe) < 2:
-        fence = fence_keep
-
-    fence.hlen()
-    updated_distance = fence.dataframe[fence.dhname].median()
-
-    if updated_distance < 0.5 * distance:
-        updated_distance = 0.5 * distance
-
-    newnextend = int(round(orig_extend / updated_distance))
-    fence.extend(updated_distance, nsamples=newnextend)
-
-    df = fence.dataframe
-    df0 = df.drop(df.index[1:])  # keep always first which has per def H_DELTALEN=0
-    df2 = df[df.H_DELTALEN > updated_distance * 0.01]  # skip very close points
-    fence.dataframe = pd.concat([df0, df2], axis=0, ignore_index=True)
-
-    # duplicates may still exist; skip those
-    fence.dataframe.drop_duplicates(
-        subset=[fence.xname, fence.yname], keep="first", inplace=True
-    )
-    fence.dataframe.reset_index(inplace=True, drop=True)
-
-    if name:
-        fence.name = name
-
-    if asnumpy is True:
-        rval = np.concatenate(
-            (
-                fence.dataframe[fence.xname].values,
-                fence.dataframe[fence.yname].values,
-                fence.dataframe[fence.zname].values,
-                fence.dataframe[fence.hname].values,
-                fence.dataframe[fence.dhname].values,
-            ),
-            axis=0,
-        )
-        return np.reshape(rval, (fence.nrow, 5), order="F")
-
-    return fence
-
-
-def snap_surface(self, surf, activeonly=True):
-    """Snap (or transfer) operation.
-
-    Points that falls outside the surface will be UNDEF, and they will be removed
-    if activeonly. Otherwise, the old values will be kept.
-    """
-
-    if not isinstance(surf, xtgeo.RegularSurface):
-        raise ValueError("Input object of wrong data type, must be RegularSurface")
-
-    zval = self.dataframe[self.zname].values.copy()
-
-    ier = _cxtgeo.surf_get_zv_from_xyv(
-        self.dataframe[self.xname].values,
-        self.dataframe[self.yname].values,
-        zval,
-        surf.ncol,
-        surf.nrow,
-        surf.xori,
-        surf.yori,
-        surf.xinc,
-        surf.yinc,
-        surf.yflip,
-        surf.rotation,
-        surf.get_values1d(),
-        0,
-    )
-
-    if ier != 0:
-        raise RuntimeError(f"Error code from C routine surf_get_zv_from_xyv is {ier}")
-    if activeonly:
-        self.dataframe[self.zname] = zval
-        self.dataframe = self.dataframe[self.dataframe[self.zname] < xtgeo.UNDEF_LIMIT]
-        self.dataframe.reset_index(inplace=True, drop=True)
-    else:
-        out = np.where(
-            zval < xtgeo.UNDEF_LIMIT, zval, self.dataframe[self.zname].values
-        )
-        self.dataframe[self.zname] = out
-
-
-def hlen(self, hname="H_CUMLEN", dhname="H_DELTALEN", atindex=0):
-    """Get the horizontal distance (cumulative and delta) between points in polygons."""
-
-    _generic_length(self, gname=hname, dgname=dhname, atindex=atindex, mode2d=True)
-
-
-def tlen(self, tname="T_CUMLEN", dtname="T_DELTALEN", atindex=0):
-    """Get the true 3D distance (cumulative and delta) between points in polygons."""
-
-    _generic_length(self, gname=tname, dgname=dtname, atindex=atindex, mode2d=False)
-
-
-def _generic_length(
-    self, gname="G_CUMLEN", dgname="G_DELTALEN", atindex=0, mode2d=True
-):
-    """Get the true or horizontal distance (cum/delta) between points in polygons.
-
-    The properties gname and ghname will be updated.
-
-    Note that Dxx at first location will be set equal to that of location 1
-    """
-
-    # Potential todo: Add an option that dH never gets 0.0 to avoid numerical trouble
-    # for e.g. rescale?
-
-    if not isinstance(self, xtgeo.Polygons):
-        raise ValueError("Input object of wrong data type, must be Polygons")
-
-    # delete existing self.hname and self.dhname columns
-    self.delete_columns([gname, dgname])
-
-    idgroups = self.dataframe.groupby(self.pname)
-
-    gdist = np.array([])
-    dgdist = np.array([])
-    for _id, grp in idgroups:
-        ier, tlenv, dtlenv, hlenv, dhlenv = _cxtgeo.pol_geometrics(
-            grp[self.xname].values.astype(np.float64),
-            grp[self.yname].values.astype(np.float64),
-            grp[self.zname].values.astype(np.float64),
-            len(grp),
-            len(grp),
-            len(grp),
-            len(grp),
-        )
-        if ier != 0:
-            raise RuntimeError(f"Error code from _cxtgeo.pol_geometrics is {ier}")
-
-        if mode2d:
-            dhlenv[0] = dhlenv[1]
-            if atindex > 0:
-                cumval = hlenv[atindex]
-                hlenv -= cumval
-
-            gdist = np.append(gdist, hlenv)
-            dgdist = np.append(dgdist, dhlenv)
-
-        else:
-            dtlenv[0] = dtlenv[1]
-            if atindex > 0:
-                cumval = tlenv[atindex]
-                tlenv -= cumval
-
-            gdist = np.append(gdist, tlenv)
-            dgdist = np.append(dgdist, dtlenv)
-
-    self.dataframe[gname] = gdist
-    self.dataframe[dgname] = dgdist
-
-    if mode2d:
-        self.hname = gname
-        self.dhname = dgname
-    else:
-        self.tname = gname
-        self.dtname = dgname
-
-
-def extend(self, distance, nsamples, addhlen=True):
-    """Extend polygon by distance, nsamples times.
-
-    It is default to recompute HLEN from nsamples.
-    """
-
-    if not isinstance(self, xtgeo.Polygons):
-        raise ValueError("Input object of wrong data type, must be Polygons")
-
-    for _ in range(nsamples):
-        # beginning of poly
-        row0 = self.dataframe.iloc[0]
-        row1 = self.dataframe.iloc[1]
-
-        rown = row0.copy()
-
-        # setting row0[2] as row1[2] is intentional, as this shall be a 2D lenght!
-        ier, newx, newy, _ = _cxtgeo.x_vector_linint2(
-            row1[0], row1[1], row1[2], row0[0], row0[1], row1[2], distance, 12
-        )
-
-        if ier != 0:
-            raise RuntimeError(f"Error code from _cxtgeo.x_vector_linint2 is {ier}")
-
-        rown[self.xname] = newx
-        rown[self.yname] = newy
-
-        df_to_add = rown.to_frame().T
-
-        self.dataframe = pd.concat([df_to_add, self.dataframe]).reset_index(drop=True)
-
-        # end of poly
-        row0 = self.dataframe.iloc[-2]
-        row1 = self.dataframe.iloc[-1]
-
-        rown = row1.copy()
-
-        # setting row1[2] as row0[2] is intentional, as this shall be a 2D lenght!
-        ier, newx, newy, _ = _cxtgeo.x_vector_linint2(
-            row0[0], row0[1], row0[2], row1[0], row1[1], row0[2], distance, 11
-        )
-
-        rown[self.xname] = newx
-        rown[self.yname] = newy
-
-        df_to_add = rown.to_frame().T
-
-        self.dataframe = pd.concat([self.dataframe, df_to_add]).reset_index(drop=True)
-
-    if addhlen:
-        self.hlen(atindex=nsamples)
+# coding: utf-8
+"""Various operations on XYZ data"""
+
+
+import numpy as np
+import pandas as pd
+import shapely.geometry as sg
+from scipy.interpolate import UnivariateSpline, interp1d
+
+import xtgeo
+import xtgeo.cxtgeo._cxtgeo as _cxtgeo
+from xtgeo.common import XTGeoDialog
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+
+# pylint: disable=protected-access
+
+
+def operation_polygons(self, poly, value, opname="add", inside=True, where=True):
+    """
+    Operations re restricted to closed polygons, for points or polyline points.
+
+    If value is not float but 'poly', then the avg of each polygon Z value will
+    be used instead.
+
+    'Inside' several polygons will become a union, while 'outside' polygons
+    will be the intersection.
+
+    The "where" filter is reserved for future use.
+    """
+    logger.warning("Where is not imeplented: %s", where)
+
+    oper = {"set": 1, "add": 2, "sub": 3, "mul": 4, "div": 5, "eli": 11}
+
+    insidevalue = 0
+    if inside:
+        insidevalue = 1
+
+    logger.info("Operations of points inside polygon(s)...")
+    if not isinstance(poly, xtgeo.xyz.Polygons):
+        raise ValueError("The poly input is not a Polygons instance")
+
+    idgroups = poly.dataframe.groupby(poly.pname)
+
+    xcor = self.dataframe[self.xname].values
+    ycor = self.dataframe[self.yname].values
+    zcor = self.dataframe[self.zname].values
+
+    usepoly = False
+    if isinstance(value, str) and value == "poly":
+        usepoly = True
+
+    for id_, grp in idgroups:
+        pxcor = grp[poly.xname].values
+        pycor = grp[poly.yname].values
+        pvalue = value
+        if usepoly:
+            pvalue = grp[poly.zname].values.mean()
+        else:
+            pvalue = value
+
+        logger.info("C function for polygon %s...", id_)
+
+        ies = _cxtgeo.pol_do_points_inside(
+            xcor, ycor, zcor, pxcor, pycor, pvalue, oper[opname], insidevalue
+        )
+        logger.info("C function for polygon %s... done", id_)
+
+        if ies != 0:
+            raise RuntimeError(f"Something went wrong, code {ies}")
+
+    zcor[zcor > xtgeo.UNDEF_LIMIT] = np.nan
+    self.dataframe[self.zname] = zcor
+    # removing rows where Z column is undefined
+    self.dataframe.dropna(how="any", subset=[self.zname], inplace=True)
+    self.dataframe.reset_index(inplace=True, drop=True)
+    logger.info("Operations of points inside polygon(s)... done")
+
+
+def rescale_polygons(self, distance=10, addlen=False, kind="simple", mode2d=False):
+    """Rescale (resample) a polygons segment
+    Default settings will make it backwards compatible with 2.0
+    New options were added in 2.1:
+    * addlen
+    * kind
+    * mode2d
+    """
+
+    if kind in ("slinear", "cubic"):
+        _rescale_v2(self, distance, addlen, kind=kind, mode2d=mode2d)
+
+    else:
+        _rescale_v1(self, distance, addlen, mode2d=mode2d)
+
+
+def _rescale_v1(self, distance, addlen, mode2d):
+    # version 1, simple approach, will rescale in 2D since Shapely use 2D lengths
+    if not mode2d:
+        raise KeyError("Cannot combine 'simple' with mode2d False")
+
+    idgroups = self.dataframe.groupby(self.pname)
+
+    dfrlist = []
+    for idx, grp in idgroups:
+        if len(grp.index) < 2:
+            logger.warning("Cannot rescale polygons with less than two points. Skip")
+            continue
+
+        pxcor = grp[self.xname].values
+        pycor = grp[self.yname].values
+        pzcor = grp[self.zname].values
+        spoly = sg.LineString(np.stack([pxcor, pycor, pzcor], axis=1))
+
+        new_spoly = _redistribute_vertices(spoly, distance)
+        dfr = pd.DataFrame(
+            np.array(new_spoly.coords), columns=[self.xname, self.yname, self.zname]
+        )
+
+        dfr[self.pname] = idx
+        dfrlist.append(dfr)
+
+    dfr = pd.concat(dfrlist)
+    self.dataframe = dfr.reset_index(drop=True)
+
+    if addlen:
+        self.hlen()
+        self.tlen()
+
+
+def _redistribute_vertices(geom, distance):
+    """Local function to interpolate in a polyline using Shapely"""
+    if geom.geom_type == "LineString":
+        num_vert = int(round(geom.length / distance))
+        if num_vert == 0:
+            num_vert = 1
+        return sg.LineString(
+            [
+                geom.interpolate(float(n) / num_vert, normalized=True)
+                for n in range(num_vert + 1)
+            ]
+        )
+
+    if geom.geom_type == "MultiLineString":
+        parts = [_redistribute_vertices(part, distance) for part in geom]
+        return type(geom)([p for p in parts if not p.is_empty])
+
+    raise ValueError(f"Unhandled geometry {geom.geom_type}")
+
+
+def _handle_vertical_input(self, inframe):
+    """Treat the special vertical case.
+
+    A special case occurs for e.g. a 100% vertical well. Here the trick is distort
+    first and last row with a relative small number so that numerical problems are
+    avoided.
+    """
+
+    result = inframe.copy().reset_index()
+
+    # e.g. tvd.mean is 2000, then tolerance will be 0.002, and edit will be 2
+    tolerance = self.dataframe[self.zname].mean() * 0.000001
+    edit = tolerance * 1000
+    pseudo = self.copy()
+
+    if inframe[self.dhname].max() < tolerance:
+        result.at[0, self.xname] -= edit
+        result.at[result.index[-1], self.xname] += edit
+        pseudo.dataframe = result
+        pseudo.hlen()
+        pseudo.tlen()
+        result = pseudo.dataframe
+
+    return result
+
+
+def _rescale_v2(self, distance, addlen, kind="slinear", mode2d=True):
+    # Rescaling to constant increment is perhaps impossible, but this is
+    # perhaps quite close
+
+    self.hlen()
+    self.tlen()
+
+    idgroups = self.dataframe.groupby(self.pname)
+
+    dfrlist = []
+    for idx, grp in idgroups:
+        grp = _handle_vertical_input(self, grp)
+
+        # avoid duplicates when *_DELTALEN are 0.0 (makes scipy interp1d() fail
+        # when scipy >= 1.9)
+        grp = grp.drop(grp[(grp[self.dhname] == 0.0) | (grp[self.dtname] == 0.0)].index)
+
+        if len(grp.index) < 2:
+            logger.warning("Cannot rescale polygons with less than two points. Skip")
+            continue
+
+        points = [grp[self.xname], grp[self.yname], grp[self.zname]]
+
+        leng = grp[self.hname].iloc[-1]
+        gname = self.hname
+
+        if not mode2d:
+            leng = grp[self.tname].iloc[-1]
+            gname = self.tname
+
+        # to avoid numerical trouble of pure vertical sections
+        leng = leng - 0.001 * leng
+
+        nstep = int(leng / distance)
+        alpha = np.linspace(0, leng, num=nstep, endpoint=True)
+
+        if kind in ("slinear"):
+            points = np.array(points).T
+            interpolator = interp1d(
+                grp[gname],
+                points,
+                kind="slinear",
+                axis=0,
+                assume_sorted=True,
+                bounds_error=False,
+                fill_value="extrapolate",
+            )
+            ip = interpolator(alpha)
+        elif kind == "cubic":
+            splines = [UnivariateSpline(grp[gname], crd) for crd in points]
+
+            ip = np.vstack(list(spl(alpha) for spl in splines)).T
+        else:
+            raise ValueError(f"Invalid kind chosen: {kind}")
+
+        dfr = pd.DataFrame(np.array(ip), columns=[self.xname, self.yname, self.zname])
+
+        dfr[self.pname] = idx
+        dfrlist.append(dfr)
+
+    dfr = pd.concat(dfrlist)
+    self.dataframe = dfr.reset_index(drop=True)
+
+    if addlen:
+        self.tlen()
+        self.hlen()
+    else:
+        self.delete_columns([self.hname, self.dhname, self.tname, self.dtname])
+
+
+def get_fence(
+    self, distance=20, atleast=5, nextend=2, name=None, asnumpy=True, polyid=None
+):
+    """Get a fence suitable for plotting xsections, either as a numpy or as a
+    new Polygons instance.
+
+    The atleast parameter will win over the distance, meaning that if total length
+    horizontally is 50, and distance is set to 20, the actual length will be 50/5=10
+    In such cases, nextend will be modified automatically also to fulfill the original
+    intention of nextend*distance (approx).
+
+    The routine is still not perfect for "close to very vertical polygon"
+    but assumed to be sufficient for all practical cases
+
+    """
+    if atleast < 3:
+        raise ValueError("The atleast key must be 3 or greater")
+
+    orig_extend = nextend * distance
+    orig_distance = distance
+
+    fence = self.copy()
+
+    fence.hlen()
+
+    if len(fence.dataframe) < 2:
+        xtg.warn(f"Too few points in polygons for fence, return False (name: {name})")
+        return False
+
+    fence.filter_byid(polyid)
+
+    hxlen = fence.get_shapely_objects()[0].length
+
+    # perhaps a way to treat very vertical polys from e.g. wells:
+    if hxlen < 0.1 * orig_distance:
+        hxlen = 0.1 * orig_distance
+
+    if hxlen / (atleast + 1) < orig_distance:
+        distance = hxlen / (atleast + 1)
+
+    fence_keep = fence.copy()
+    fence.rescale(distance, kind="slinear", mode2d=True)
+
+    if len(fence.dataframe) < 2:
+        fence = fence_keep
+
+    fence.hlen()
+    updated_distance = fence.dataframe[fence.dhname].median()
+
+    if updated_distance < 0.5 * distance:
+        updated_distance = 0.5 * distance
+
+    newnextend = int(round(orig_extend / updated_distance))
+    fence.extend(updated_distance, nsamples=newnextend)
+
+    df = fence.dataframe
+    df0 = df.drop(df.index[1:])  # keep always first which has per def H_DELTALEN=0
+    df2 = df[df.H_DELTALEN > updated_distance * 0.01]  # skip very close points
+    fence.dataframe = pd.concat([df0, df2], axis=0, ignore_index=True)
+
+    # duplicates may still exist; skip those
+    fence.dataframe.drop_duplicates(
+        subset=[fence.xname, fence.yname], keep="first", inplace=True
+    )
+    fence.dataframe.reset_index(inplace=True, drop=True)
+
+    if name:
+        fence.name = name
+
+    if asnumpy is True:
+        rval = np.concatenate(
+            (
+                fence.dataframe[fence.xname].values,
+                fence.dataframe[fence.yname].values,
+                fence.dataframe[fence.zname].values,
+                fence.dataframe[fence.hname].values,
+                fence.dataframe[fence.dhname].values,
+            ),
+            axis=0,
+        )
+        return np.reshape(rval, (fence.nrow, 5), order="F")
+
+    return fence
+
+
+def snap_surface(self, surf, activeonly=True):
+    """Snap (or transfer) operation.
+
+    Points that falls outside the surface will be UNDEF, and they will be removed
+    if activeonly. Otherwise, the old values will be kept.
+    """
+
+    if not isinstance(surf, xtgeo.RegularSurface):
+        raise ValueError("Input object of wrong data type, must be RegularSurface")
+
+    zval = self.dataframe[self.zname].values.copy()
+
+    ier = _cxtgeo.surf_get_zv_from_xyv(
+        self.dataframe[self.xname].values,
+        self.dataframe[self.yname].values,
+        zval,
+        surf.ncol,
+        surf.nrow,
+        surf.xori,
+        surf.yori,
+        surf.xinc,
+        surf.yinc,
+        surf.yflip,
+        surf.rotation,
+        surf.get_values1d(),
+        0,
+    )
+
+    if ier != 0:
+        raise RuntimeError(f"Error code from C routine surf_get_zv_from_xyv is {ier}")
+    if activeonly:
+        self.dataframe[self.zname] = zval
+        self.dataframe = self.dataframe[self.dataframe[self.zname] < xtgeo.UNDEF_LIMIT]
+        self.dataframe.reset_index(inplace=True, drop=True)
+    else:
+        out = np.where(
+            zval < xtgeo.UNDEF_LIMIT, zval, self.dataframe[self.zname].values
+        )
+        self.dataframe[self.zname] = out
+
+
+def hlen(self, hname="H_CUMLEN", dhname="H_DELTALEN", atindex=0):
+    """Get the horizontal distance (cumulative and delta) between points in polygons."""
+
+    _generic_length(self, gname=hname, dgname=dhname, atindex=atindex, mode2d=True)
+
+
+def tlen(self, tname="T_CUMLEN", dtname="T_DELTALEN", atindex=0):
+    """Get the true 3D distance (cumulative and delta) between points in polygons."""
+
+    _generic_length(self, gname=tname, dgname=dtname, atindex=atindex, mode2d=False)
+
+
+def _generic_length(
+    self, gname="G_CUMLEN", dgname="G_DELTALEN", atindex=0, mode2d=True
+):
+    """Get the true or horizontal distance (cum/delta) between points in polygons.
+
+    The properties gname and ghname will be updated.
+
+    Note that Dxx at first location will be set equal to that of location 1
+    """
+
+    # Potential todo: Add an option that dH never gets 0.0 to avoid numerical trouble
+    # for e.g. rescale?
+
+    if not isinstance(self, xtgeo.Polygons):
+        raise ValueError("Input object of wrong data type, must be Polygons")
+
+    # delete existing self.hname and self.dhname columns
+    self.delete_columns([gname, dgname])
+
+    idgroups = self.dataframe.groupby(self.pname)
+
+    gdist = np.array([])
+    dgdist = np.array([])
+    for _id, grp in idgroups:
+        ier, tlenv, dtlenv, hlenv, dhlenv = _cxtgeo.pol_geometrics(
+            grp[self.xname].values.astype(np.float64),
+            grp[self.yname].values.astype(np.float64),
+            grp[self.zname].values.astype(np.float64),
+            len(grp),
+            len(grp),
+            len(grp),
+            len(grp),
+        )
+        if ier != 0:
+            raise RuntimeError(f"Error code from _cxtgeo.pol_geometrics is {ier}")
+
+        if mode2d:
+            dhlenv[0] = dhlenv[1]
+            if atindex > 0:
+                cumval = hlenv[atindex]
+                hlenv -= cumval
+
+            gdist = np.append(gdist, hlenv)
+            dgdist = np.append(dgdist, dhlenv)
+
+        else:
+            dtlenv[0] = dtlenv[1]
+            if atindex > 0:
+                cumval = tlenv[atindex]
+                tlenv -= cumval
+
+            gdist = np.append(gdist, tlenv)
+            dgdist = np.append(dgdist, dtlenv)
+
+    self.dataframe[gname] = gdist
+    self.dataframe[dgname] = dgdist
+
+    if mode2d:
+        self.hname = gname
+        self.dhname = dgname
+    else:
+        self.tname = gname
+        self.dtname = dgname
+
+
+def extend(self, distance, nsamples, addhlen=True):
+    """Extend polygon by distance, nsamples times.
+
+    It is default to recompute HLEN from nsamples.
+    """
+
+    if not isinstance(self, xtgeo.Polygons):
+        raise ValueError("Input object of wrong data type, must be Polygons")
+
+    for _ in range(nsamples):
+        # beginning of poly
+        row0 = self.dataframe.iloc[0]
+        row1 = self.dataframe.iloc[1]
+
+        rown = row0.copy()
+
+        # setting row0[2] as row1[2] is intentional, as this shall be a 2D lenght!
+        ier, newx, newy, _ = _cxtgeo.x_vector_linint2(
+            row1[0], row1[1], row1[2], row0[0], row0[1], row1[2], distance, 12
+        )
+
+        if ier != 0:
+            raise RuntimeError(f"Error code from _cxtgeo.x_vector_linint2 is {ier}")
+
+        rown[self.xname] = newx
+        rown[self.yname] = newy
+
+        df_to_add = rown.to_frame().T
+
+        self.dataframe = pd.concat([df_to_add, self.dataframe]).reset_index(drop=True)
+
+        # end of poly
+        row0 = self.dataframe.iloc[-2]
+        row1 = self.dataframe.iloc[-1]
+
+        rown = row1.copy()
+
+        # setting row1[2] as row0[2] is intentional, as this shall be a 2D lenght!
+        ier, newx, newy, _ = _cxtgeo.x_vector_linint2(
+            row0[0], row0[1], row0[2], row1[0], row1[1], row0[2], distance, 11
+        )
+
+        rown[self.xname] = newx
+        rown[self.yname] = newy
+
+        df_to_add = rown.to_frame().T
+
+        self.dataframe = pd.concat([self.dataframe, df_to_add]).reset_index(drop=True)
+
+    if addhlen:
+        self.hlen(atindex=nsamples)
```

## xtgeo/xyz/_xyz_roxapi.py

 * *Ordering differences only*

```diff
@@ -1,557 +1,557 @@
-# coding: utf-8
-"""Roxar API functions for XTGeo Points/Polygons"""
-import os
-import tempfile
-
-import numpy as np
-import pandas as pd
-
-import xtgeo
-from xtgeo.common import XTGeoDialog, _XTGeoFile
-from xtgeo.roxutils import RoxUtils
-from xtgeo.xyz import _xyz_io
-
-xtg = XTGeoDialog()
-
-logger = xtg.functionlogger(__name__)
-
-# pylint: disable=protected-access
-VALID_STYPES = ["horizons", "zones", "clipboard", "general2d_data", "faults"]
-VALID_STYPES_EXPORT = VALID_STYPES + ["horizon_picks"]
-
-
-def _check_stypes_names_category(roxutils, stype, name, category, export=False):
-    """General check of some input values."""
-    stype = stype.lower()
-
-    valid_stypes = VALID_STYPES_EXPORT if export else VALID_STYPES
-
-    if stype not in valid_stypes:
-        raise ValueError(
-            f"Invalid stype value! For key <stype> the value {stype} is not supported, "
-            f"legal stype values are: {valid_stypes}"
-        )
-
-    if not name:
-        raise ValueError("The name is missing or empty.")
-
-    logger.info("The stype is: %s", stype)
-    if stype in ("horizons", "zones") and (name is None or not category):
-        raise ValueError(
-            "Need to spesify both name and category for horizons and zones"
-        )
-
-    # note: check of clipboard va Roxar API 1.2 is now removed as usage of
-    # such old API versions is obsolute.
-    if stype == "general2d_data" and not roxutils.version_required("1.6"):
-        raise NotImplementedError(
-            "API Support for general2d_data is missing in this RMS version"
-            f"(current API version is {roxutils.roxversion} - required is 1.6"
-        )
-
-
-def import_xyz_roxapi(
-    project, name, category, stype, realisation, attributes, is_polygons
-):  # pragma: no cover
-    """Import a Points or Polygons item via ROXAR API spec.
-
-    'Import' means transfer of data from Roxar API memory space to XTGeo memory space.
-
-    ~ a part of a classmethod, and it will return the following kwargs to __init__()::
-
-        xname
-        yname
-        zname
-        pname # optional for Polygons
-        name
-        dataframe
-        values=None  # since dataframe is set
-        filesrc
-    """
-
-    rox = RoxUtils(project, readonly=True)
-
-    _check_stypes_names_category(rox, stype, name, category)
-
-    if attributes and not rox.version_required("1.6"):
-        result = _roxapi_import_xyz_viafile(
-            rox, name, category, stype, realisation, is_polygons
-        )
-    else:
-        result = _roxapi_import_xyz(
-            rox, name, category, stype, realisation, is_polygons, attributes
-        )
-
-    rox.safe_close()
-    return result
-
-
-def _roxapi_import_xyz_viafile(
-    rox, name, category, stype, realisation, is_polygons
-):  # pragma: no cover
-    """Read XYZ from file due to a missing feature in Roxar API wrt attributes.
-
-    However, attributes will be present in Roxar API from RMS version 12, and this
-    routine should be replaced!
-    """
-
-    try:
-        import roxar  # pylint: disable=import-outside-toplevel
-    except ImportError as err:
-        raise ImportError(
-            "roxar not available, this functionality is not available"
-        ) from err
-
-    if not _check_category_etc(rox.project, name, category, stype, realisation):
-        raise RuntimeError(
-            f"It appears that name and or category is not present: name={name}, "
-            f"category/folder={category}, stype={stype}"
-        )
-
-    rox_xyz = _get_roxxyz(
-        rox,
-        name,
-        category,
-        stype,
-        mode="get",
-        is_polygons=is_polygons,
-    )
-
-    kwargs = {}
-    try:
-        with tempfile.TemporaryDirectory() as tmpdir:
-            logger.info("Made a tmp folder: %s", tmpdir)
-            tfile = os.path.join(tmpdir, "generic.rmsattr")
-            rox_xyz.save(tfile, roxar.FileFormat.RMS_POINTS)
-            pfile = _XTGeoFile(tfile)
-            kwargs = _xyz_io.import_rms_attr(pfile)
-
-    except KeyError as kwe:
-        logger.error(kwe)
-
-    return kwargs
-
-
-def _roxapi_import_xyz(
-    rox, name, category, stype, realisation, is_polygons, attributes
-):  # pragma: no cover
-    """From RMS to XTGeo"""
-    kwargs = {}
-
-    if not _check_category_etc(rox.project, name, category, stype, realisation):
-        raise RuntimeError(
-            f"It appears that name and or category is not present: name={name}, "
-            f"category/folder={category}, stype={stype}"
-        )
-
-    kwargs["xname"] = "X_UTME"
-    kwargs["yname"] = "Y_UTMN"
-    kwargs["zname"] = "Z_TVDSS"
-
-    if is_polygons:
-        kwargs["pname"] = "POLY_ID"
-
-    roxxyz = _get_roxxyz(
-        rox,
-        name,
-        category,
-        stype,
-        mode="get",
-        is_polygons=is_polygons,
-    )
-
-    values = _get_roxvalues(roxxyz, realisation=realisation)
-
-    dfr = _roxapi_xyz_to_dataframe(values, is_polygons=is_polygons)
-
-    # handling attributes for points, from Roxar API version 1.6
-    if attributes and not is_polygons:
-        attr_names = roxxyz.get_attributes_names(realisation=realisation)
-        logger.info("XYZ attribute names are: %s", attr_names)
-        attr_dict = _get_rox_attrvalues(roxxyz, attr_names, realisation=realisation)
-        dfr, datatypes = _add_attributes_to_dataframe(dfr, attr_dict)
-        kwargs["attributes"] = datatypes
-
-    kwargs["values"] = dfr
-    return kwargs
-
-
-def _roxapi_xyz_to_dataframe(roxxyz, is_polygons=False):  # pragma: no cover
-    """Transforming some XYZ from ROXAPI to a Pandas dataframe."""
-
-    # In ROXAPI, polygons/polylines are a list of numpies, while
-    # points is just a numpy array. Hence a polyg* may be identified
-    # by being a list after import
-
-    logger.info("Points/polygons/polylines from roxapi to xtgeo...")
-    cnames = ["X_UTME", "Y_UTMN", "Z_TVDSS"]
-
-    if is_polygons and isinstance(roxxyz, list):
-        # polylines/-gons
-        dfs = []
-        for idx, poly in enumerate(roxxyz):
-            dataset = pd.DataFrame.from_records(poly, columns=cnames)
-            dataset["POLY_ID"] = idx
-            dfs.append(dataset)
-
-        dfr = pd.concat(dfs)
-
-    elif not is_polygons and isinstance(roxxyz, np.ndarray):
-        # points
-        dfr = pd.DataFrame.from_records(roxxyz, columns=cnames)
-
-    else:
-        raise RuntimeError(f"Unknown error in getting data from Roxar: {type(roxxyz)}")
-
-    dfr.reset_index(drop=True, inplace=True)
-    return dfr
-
-
-def _add_attributes_to_dataframe(dfr, attributes: dict):  # pragma: no cover
-    """Add attributes to dataframe (points only) for Roxar API ver 1.6+"""
-
-    logger.info("Attributes adding to dataframe...")
-    newdfr = dfr.copy()
-
-    datatypes = {}
-    for name, values in attributes.items():
-        dtype = str(values.dtype)
-        if "int" in dtype:
-            datatypes[name] = "int"
-            values = np.ma.filled(values, fill_value=xtgeo.UNDEF_INT)
-        elif "float" in dtype:
-            datatypes[name] = "float"
-            values = np.ma.filled(values, fill_value=xtgeo.UNDEF)
-        else:
-            datatypes[name] = "str"
-            values = np.ma.filled(values, fill_value="UNDEF")
-
-        newdfr[name] = values
-
-    return newdfr, datatypes
-
-
-def export_xyz_roxapi(
-    self, project, name, category, stype, pfilter, realisation, attributes
-):  # pragma: no cover
-    """Export (store) a XYZ item from XTGeo to RMS via ROXAR API spec."""
-    rox = RoxUtils(project, readonly=False)
-
-    _check_stypes_names_category(rox, stype, name, category, export=True)
-
-    if stype == "horizon_picks":
-        _roxapi_export_xyz_hpicks(
-            self, rox, name, category, stype, realisation, attributes
-        )
-
-    if attributes and not rox.version_required("1.6"):
-        _roxapi_export_xyz_viafile(
-            self, rox, name, category, stype, pfilter, realisation, attributes
-        )
-    else:
-        _roxapi_export_xyz(
-            self, rox, name, category, stype, pfilter, realisation, attributes
-        )
-
-    if rox._roxexternal:
-        rox.project.save()
-
-    rox.safe_close()
-
-
-def _roxapi_export_xyz_hpicks(
-    self, rox, name, category, stype, realisation, attributes
-):  # pragma: no cover
-    """
-    Export/store as RMS horizon picks; this is only valid if points belong to wells
-    """
-    # need to think on design!
-    raise NotImplementedError
-
-
-def _roxapi_export_xyz_viafile(
-    self, rox, name, category, stype, pfilter, realisation, attributes
-):  # pragma: no cover
-    """Set points/polys within RMS with attributes, using file workaround"""
-
-    logger.warning("Realisation %s not in use", realisation)
-
-    try:
-        import roxar  # pylint: disable=import-outside-toplevel
-    except ImportError as err:
-        raise ImportError(
-            "roxar not available, this functionality is not available"
-        ) from err
-
-    proj = rox.project
-
-    if not _check_category_etc(proj, name, category, stype, realisation, mode="set"):
-        raise RuntimeError("Cannot access correct category or name in RMS")
-
-    roxxyz = _get_roxitem(self, proj, name, category, stype, mode="set")
-
-    # make a temporary folder and work within the with.. block
-    with tempfile.TemporaryDirectory() as tmpdir:
-        logger.info("Made a tmp folder: %s", tmpdir)
-        ncount = self.to_file(
-            os.path.join(tmpdir, "generic.rmsattr"),
-            fformat="rms_attr",
-            pfilter=pfilter,
-            attributes=attributes,
-        )
-
-        if ncount:
-            roxxyz.load(
-                os.path.join(tmpdir, "generic.rmsattr"), roxar.FileFormat.RMS_POINTS
-            )
-
-
-def _roxapi_export_xyz(
-    self, rox, name, category, stype, pfilter, realisation, attributes
-):  # pragma: no cover
-    logger.warning("Realisation %s not in use", realisation)
-
-    proj = rox.project
-    if not _check_category_etc(proj, name, category, stype, realisation, mode="set"):
-        raise RuntimeError("Cannot access correct category or name in RMS")
-
-    roxxyz = _get_roxitem(self, proj, name, category, stype, mode="set")
-
-    # pylint: disable=len-as-condition
-    if self.dataframe is None or len(self.dataframe.index) == 0:
-        return
-
-    dfrcopy = self.dataframe.copy()
-    # apply pfilter if any
-    if pfilter:
-        for key, val in pfilter.items():
-            if key in dfrcopy.columns:
-                dfrcopy = dfrcopy.loc[dfrcopy[key].isin(val)]
-            else:
-                raise KeyError(
-                    f"The requested pfilter key {key} was not found in dataframe. "
-                    f"Valid keys are {dfrcopy.columns}"
-                )
-
-    if isinstance(self, xtgeo.Polygons):
-        arrxyz = []
-        polys = dfrcopy.groupby(self.pname)
-        for _id, grp in polys:
-            arr = np.stack([grp[self.xname], grp[self.yname], grp[self.zname]], axis=1)
-            arrxyz.append(arr)
-    else:
-        xyz = dfrcopy
-        arrxyz = np.stack([xyz[self.xname], xyz[self.yname], xyz[self.zname]], axis=1)
-
-    if (
-        isinstance(arrxyz, np.ndarray)
-        and arrxyz.size == 0
-        or isinstance(arrxyz, list)
-        and len(arrxyz) == 0
-    ):
-        return
-
-    roxxyz.set_values(arrxyz)
-
-    if attributes and isinstance(self, xtgeo.Points) and len(self.dataframe) >= 1:
-        dfr = _cast_dataframe_attrs_to_numeric(dfrcopy)
-        for name in dfr.columns[3:]:
-            values = dfr[name].values
-            if "float" in str(values.dtype):
-                values = np.ma.masked_greater(values, xtgeo.UNDEF_LIMIT)
-            elif "int" in str(values.dtype):
-                values = np.ma.masked_greater(values, xtgeo.UNDEF_INT_LIMIT)
-            else:
-                # masking has no meaning for strings?
-                values = values.astype(str)
-                values = np.char.replace(values, "UNDEF", "")
-
-            logger.info("Store Point attribute %s to Roxar API", name)
-            roxxyz.set_attribute_values(name, values)
-
-
-def _cast_dataframe_attrs_to_numeric(dfr):
-    """Cast the attribute dataframe columns to numerical datatypes if possible.
-
-    In some case, attribute columns get dtype 'object' while they clearly
-    represents a numerical property (int or float). Here the pandas to_numerics()
-    function is applied per attribute column, and will do a conversion if possible;
-    otherwise the 'object' dtype will be preserved.
-    """
-    if len(dfr) < 1:
-        return dfr
-
-    newdfr = dfr.copy()
-    for name in dfr.columns[3:]:
-        newdfr[name] = pd.to_numeric(dfr[name], errors="ignore")
-    return newdfr
-
-
-def _check_category_etc(
-    proj, name, category, stype, realisation, mode="get"
-):  # pylint: disable=too-many-branches  # pragma: no cover
-    """Helper to check if valid placeholder' whithin RMS."""
-
-    logger.warning("Realisation %s not in use", realisation)
-
-    stypedict = {"horizons": proj.horizons, "zones": proj.zones, "faults": proj.faults}
-
-    if stype in stypedict.keys():
-        if name not in stypedict[stype]:
-            logger.error("Cannot access name in stype=%s: %s", stype, name)
-            return False
-        if category not in stypedict[stype].representations:
-            logger.error("Cannot access category in stype=%s: %s", stype, category)
-            return False
-
-    elif stype in ("clipboard", "general2d_data") and mode == "get":
-        folders = None
-        if category:
-            if isinstance(category, list):
-                folders = category
-            elif isinstance(category, str) and "|" in category:
-                folders = category.split("|")
-            elif isinstance(category, str) and "/" in category:
-                folders = category.split("/")
-            elif isinstance(category, str):
-                folders = []
-                folders.append(category)
-            else:
-                raise RuntimeError(
-                    f"Cannot parse category: {category}, see documentation!"
-                )
-            try:
-                roxxyz = getattr(proj, stype).folders[folders]
-            except KeyError as keyerr:
-                logger.error(
-                    "Cannot access clipboards folder (not existing?): %s", keyerr
-                )
-                return False
-        else:
-            roxxyz = proj.clipboard
-
-        if name not in roxxyz.keys():
-            raise ValueError(f"Name {name} is not within Clipboard...")
-
-    elif stype in ("clipboard", "general2d_data") and mode == "set":
-        logger.info("No need to check clipboard while setting data")
-    else:
-        raise ValueError(f"Invalid stype: {stype}")
-
-    return True
-
-
-def _get_roxitem(self, proj, name, category, stype, mode="set"):  # pragma: no cover
-    # pylint: disable=too-many-branches
-    if stype == "horizons":
-        roxxyz = proj.horizons[name][category]
-    elif stype == "zones":
-        roxxyz = proj.zones[name][category]
-    elif stype == "faults":
-        roxxyz = proj.faults[name][category]
-    elif stype in ["clipboard", "general2d_data"]:
-        folders = None
-        roxxyz = getattr(proj, stype)
-        if category:
-            if isinstance(category, list):
-                folders = category
-            elif isinstance(category, str) and "|" in category:
-                folders = category.split("|")
-            elif isinstance(category, str) and "/" in category:
-                folders = category.split("/")
-            elif isinstance(category, str):
-                folders = []
-                folders.append(category)
-            else:
-                raise RuntimeError(
-                    f"Cannot parse category: {category}, see documentation!"
-                )
-
-            if mode == "get":
-                roxxyz = roxxyz.folders[folders]
-
-        if mode == "get":
-            roxxyz = roxxyz[name]
-
-        elif mode == "set":
-            # clipboard folders will be created if not present, and overwritten else
-            if isinstance(self, xtgeo.Polygons):
-                roxxyz = roxxyz.create_polylines(name, folders)
-            else:
-                roxxyz = roxxyz.create_points(name, folders)
-
-    else:
-        raise ValueError(f"Unsupported stype: {stype}")
-
-    return roxxyz
-
-
-def _get_roxxyz(
-    rox, name, category, stype, mode="set", is_polygons=False
-):  # pragma: no cover
-    # pylint: disable=too-many-branches
-    """Get the correct rox_xyz which is some pointer to a RoxarAPI structure."""
-    if stype == "horizons":
-        rox_xyz = rox.project.horizons[name][category]
-    elif stype == "zones":
-        rox_xyz = rox.project.zones[name][category]
-    elif stype == "faults":
-        rox_xyz = rox.project.faults[name][category]
-    elif stype in ["clipboard", "general2d_data"]:
-        folders = None
-        rox_xyz = getattr(rox.project, stype)
-        if category:
-            if isinstance(category, list):
-                folders = category
-                folders.append(category)
-            elif isinstance(category, str) and "|" in category:
-                folders = category.split("|")
-            elif isinstance(category, str) and "/" in category:
-                folders = category.split("/")
-            elif isinstance(category, str):
-                folders = []
-                folders.append(category)
-            else:
-                raise RuntimeError(
-                    f"Cannot parse category: {category}, see documentation!"
-                )
-
-            if mode == "get":
-                rox_xyz = rox_xyz.folders[folders]
-
-        if mode == "get":
-            rox_xyz = rox_xyz[name]
-
-        elif mode == "set":
-            # clipboard folders will be created if not present, and overwritten else
-            if is_polygons:
-                rox_xyz = rox_xyz.create_polylines(name, folders)
-            else:
-                rox_xyz = rox_xyz.create_points(name, folders)
-
-    else:
-        raise TypeError(f"Unsupported stype: {stype}")  # shall never get this far...
-
-    return rox_xyz
-
-
-def _get_roxvalues(rox_xyz, realisation=0):  # pragma: no cover
-    """Return primary values from the Roxar API, numpy (Points) or list (Polygons)."""
-    try:
-        roxitem = rox_xyz.get_values(realisation)
-        logger.info(roxitem)
-    except KeyError as kwe:
-        logger.error(kwe)
-
-    return roxitem
-
-
-def _get_rox_attrvalues(rox_xyz, attrnames, realisation=0) -> dict:  # pragma: no cover
-    """Return attribute values from the Roxar API, numpy (Points) or list (Polygons)."""
-
-    roxitems = {}
-    for attrname in attrnames:
-        values = rox_xyz.get_attribute_values(attrname, realisation=realisation)
-        roxitems[attrname] = values
-
-    return roxitems
+# coding: utf-8
+"""Roxar API functions for XTGeo Points/Polygons"""
+import os
+import tempfile
+
+import numpy as np
+import pandas as pd
+
+import xtgeo
+from xtgeo.common import XTGeoDialog, _XTGeoFile
+from xtgeo.roxutils import RoxUtils
+from xtgeo.xyz import _xyz_io
+
+xtg = XTGeoDialog()
+
+logger = xtg.functionlogger(__name__)
+
+# pylint: disable=protected-access
+VALID_STYPES = ["horizons", "zones", "clipboard", "general2d_data", "faults"]
+VALID_STYPES_EXPORT = VALID_STYPES + ["horizon_picks"]
+
+
+def _check_stypes_names_category(roxutils, stype, name, category, export=False):
+    """General check of some input values."""
+    stype = stype.lower()
+
+    valid_stypes = VALID_STYPES_EXPORT if export else VALID_STYPES
+
+    if stype not in valid_stypes:
+        raise ValueError(
+            f"Invalid stype value! For key <stype> the value {stype} is not supported, "
+            f"legal stype values are: {valid_stypes}"
+        )
+
+    if not name:
+        raise ValueError("The name is missing or empty.")
+
+    logger.info("The stype is: %s", stype)
+    if stype in ("horizons", "zones") and (name is None or not category):
+        raise ValueError(
+            "Need to spesify both name and category for horizons and zones"
+        )
+
+    # note: check of clipboard va Roxar API 1.2 is now removed as usage of
+    # such old API versions is obsolute.
+    if stype == "general2d_data" and not roxutils.version_required("1.6"):
+        raise NotImplementedError(
+            "API Support for general2d_data is missing in this RMS version"
+            f"(current API version is {roxutils.roxversion} - required is 1.6"
+        )
+
+
+def import_xyz_roxapi(
+    project, name, category, stype, realisation, attributes, is_polygons
+):  # pragma: no cover
+    """Import a Points or Polygons item via ROXAR API spec.
+
+    'Import' means transfer of data from Roxar API memory space to XTGeo memory space.
+
+    ~ a part of a classmethod, and it will return the following kwargs to __init__()::
+
+        xname
+        yname
+        zname
+        pname # optional for Polygons
+        name
+        dataframe
+        values=None  # since dataframe is set
+        filesrc
+    """
+
+    rox = RoxUtils(project, readonly=True)
+
+    _check_stypes_names_category(rox, stype, name, category)
+
+    if attributes and not rox.version_required("1.6"):
+        result = _roxapi_import_xyz_viafile(
+            rox, name, category, stype, realisation, is_polygons
+        )
+    else:
+        result = _roxapi_import_xyz(
+            rox, name, category, stype, realisation, is_polygons, attributes
+        )
+
+    rox.safe_close()
+    return result
+
+
+def _roxapi_import_xyz_viafile(
+    rox, name, category, stype, realisation, is_polygons
+):  # pragma: no cover
+    """Read XYZ from file due to a missing feature in Roxar API wrt attributes.
+
+    However, attributes will be present in Roxar API from RMS version 12, and this
+    routine should be replaced!
+    """
+
+    try:
+        import roxar  # pylint: disable=import-outside-toplevel
+    except ImportError as err:
+        raise ImportError(
+            "roxar not available, this functionality is not available"
+        ) from err
+
+    if not _check_category_etc(rox.project, name, category, stype, realisation):
+        raise RuntimeError(
+            f"It appears that name and or category is not present: name={name}, "
+            f"category/folder={category}, stype={stype}"
+        )
+
+    rox_xyz = _get_roxxyz(
+        rox,
+        name,
+        category,
+        stype,
+        mode="get",
+        is_polygons=is_polygons,
+    )
+
+    kwargs = {}
+    try:
+        with tempfile.TemporaryDirectory() as tmpdir:
+            logger.info("Made a tmp folder: %s", tmpdir)
+            tfile = os.path.join(tmpdir, "generic.rmsattr")
+            rox_xyz.save(tfile, roxar.FileFormat.RMS_POINTS)
+            pfile = _XTGeoFile(tfile)
+            kwargs = _xyz_io.import_rms_attr(pfile)
+
+    except KeyError as kwe:
+        logger.error(kwe)
+
+    return kwargs
+
+
+def _roxapi_import_xyz(
+    rox, name, category, stype, realisation, is_polygons, attributes
+):  # pragma: no cover
+    """From RMS to XTGeo"""
+    kwargs = {}
+
+    if not _check_category_etc(rox.project, name, category, stype, realisation):
+        raise RuntimeError(
+            f"It appears that name and or category is not present: name={name}, "
+            f"category/folder={category}, stype={stype}"
+        )
+
+    kwargs["xname"] = "X_UTME"
+    kwargs["yname"] = "Y_UTMN"
+    kwargs["zname"] = "Z_TVDSS"
+
+    if is_polygons:
+        kwargs["pname"] = "POLY_ID"
+
+    roxxyz = _get_roxxyz(
+        rox,
+        name,
+        category,
+        stype,
+        mode="get",
+        is_polygons=is_polygons,
+    )
+
+    values = _get_roxvalues(roxxyz, realisation=realisation)
+
+    dfr = _roxapi_xyz_to_dataframe(values, is_polygons=is_polygons)
+
+    # handling attributes for points, from Roxar API version 1.6
+    if attributes and not is_polygons:
+        attr_names = roxxyz.get_attributes_names(realisation=realisation)
+        logger.info("XYZ attribute names are: %s", attr_names)
+        attr_dict = _get_rox_attrvalues(roxxyz, attr_names, realisation=realisation)
+        dfr, datatypes = _add_attributes_to_dataframe(dfr, attr_dict)
+        kwargs["attributes"] = datatypes
+
+    kwargs["values"] = dfr
+    return kwargs
+
+
+def _roxapi_xyz_to_dataframe(roxxyz, is_polygons=False):  # pragma: no cover
+    """Transforming some XYZ from ROXAPI to a Pandas dataframe."""
+
+    # In ROXAPI, polygons/polylines are a list of numpies, while
+    # points is just a numpy array. Hence a polyg* may be identified
+    # by being a list after import
+
+    logger.info("Points/polygons/polylines from roxapi to xtgeo...")
+    cnames = ["X_UTME", "Y_UTMN", "Z_TVDSS"]
+
+    if is_polygons and isinstance(roxxyz, list):
+        # polylines/-gons
+        dfs = []
+        for idx, poly in enumerate(roxxyz):
+            dataset = pd.DataFrame.from_records(poly, columns=cnames)
+            dataset["POLY_ID"] = idx
+            dfs.append(dataset)
+
+        dfr = pd.concat(dfs)
+
+    elif not is_polygons and isinstance(roxxyz, np.ndarray):
+        # points
+        dfr = pd.DataFrame.from_records(roxxyz, columns=cnames)
+
+    else:
+        raise RuntimeError(f"Unknown error in getting data from Roxar: {type(roxxyz)}")
+
+    dfr.reset_index(drop=True, inplace=True)
+    return dfr
+
+
+def _add_attributes_to_dataframe(dfr, attributes: dict):  # pragma: no cover
+    """Add attributes to dataframe (points only) for Roxar API ver 1.6+"""
+
+    logger.info("Attributes adding to dataframe...")
+    newdfr = dfr.copy()
+
+    datatypes = {}
+    for name, values in attributes.items():
+        dtype = str(values.dtype)
+        if "int" in dtype:
+            datatypes[name] = "int"
+            values = np.ma.filled(values, fill_value=xtgeo.UNDEF_INT)
+        elif "float" in dtype:
+            datatypes[name] = "float"
+            values = np.ma.filled(values, fill_value=xtgeo.UNDEF)
+        else:
+            datatypes[name] = "str"
+            values = np.ma.filled(values, fill_value="UNDEF")
+
+        newdfr[name] = values
+
+    return newdfr, datatypes
+
+
+def export_xyz_roxapi(
+    self, project, name, category, stype, pfilter, realisation, attributes
+):  # pragma: no cover
+    """Export (store) a XYZ item from XTGeo to RMS via ROXAR API spec."""
+    rox = RoxUtils(project, readonly=False)
+
+    _check_stypes_names_category(rox, stype, name, category, export=True)
+
+    if stype == "horizon_picks":
+        _roxapi_export_xyz_hpicks(
+            self, rox, name, category, stype, realisation, attributes
+        )
+
+    if attributes and not rox.version_required("1.6"):
+        _roxapi_export_xyz_viafile(
+            self, rox, name, category, stype, pfilter, realisation, attributes
+        )
+    else:
+        _roxapi_export_xyz(
+            self, rox, name, category, stype, pfilter, realisation, attributes
+        )
+
+    if rox._roxexternal:
+        rox.project.save()
+
+    rox.safe_close()
+
+
+def _roxapi_export_xyz_hpicks(
+    self, rox, name, category, stype, realisation, attributes
+):  # pragma: no cover
+    """
+    Export/store as RMS horizon picks; this is only valid if points belong to wells
+    """
+    # need to think on design!
+    raise NotImplementedError
+
+
+def _roxapi_export_xyz_viafile(
+    self, rox, name, category, stype, pfilter, realisation, attributes
+):  # pragma: no cover
+    """Set points/polys within RMS with attributes, using file workaround"""
+
+    logger.warning("Realisation %s not in use", realisation)
+
+    try:
+        import roxar  # pylint: disable=import-outside-toplevel
+    except ImportError as err:
+        raise ImportError(
+            "roxar not available, this functionality is not available"
+        ) from err
+
+    proj = rox.project
+
+    if not _check_category_etc(proj, name, category, stype, realisation, mode="set"):
+        raise RuntimeError("Cannot access correct category or name in RMS")
+
+    roxxyz = _get_roxitem(self, proj, name, category, stype, mode="set")
+
+    # make a temporary folder and work within the with.. block
+    with tempfile.TemporaryDirectory() as tmpdir:
+        logger.info("Made a tmp folder: %s", tmpdir)
+        ncount = self.to_file(
+            os.path.join(tmpdir, "generic.rmsattr"),
+            fformat="rms_attr",
+            pfilter=pfilter,
+            attributes=attributes,
+        )
+
+        if ncount:
+            roxxyz.load(
+                os.path.join(tmpdir, "generic.rmsattr"), roxar.FileFormat.RMS_POINTS
+            )
+
+
+def _roxapi_export_xyz(
+    self, rox, name, category, stype, pfilter, realisation, attributes
+):  # pragma: no cover
+    logger.warning("Realisation %s not in use", realisation)
+
+    proj = rox.project
+    if not _check_category_etc(proj, name, category, stype, realisation, mode="set"):
+        raise RuntimeError("Cannot access correct category or name in RMS")
+
+    roxxyz = _get_roxitem(self, proj, name, category, stype, mode="set")
+
+    # pylint: disable=len-as-condition
+    if self.dataframe is None or len(self.dataframe.index) == 0:
+        return
+
+    dfrcopy = self.dataframe.copy()
+    # apply pfilter if any
+    if pfilter:
+        for key, val in pfilter.items():
+            if key in dfrcopy.columns:
+                dfrcopy = dfrcopy.loc[dfrcopy[key].isin(val)]
+            else:
+                raise KeyError(
+                    f"The requested pfilter key {key} was not found in dataframe. "
+                    f"Valid keys are {dfrcopy.columns}"
+                )
+
+    if isinstance(self, xtgeo.Polygons):
+        arrxyz = []
+        polys = dfrcopy.groupby(self.pname)
+        for _id, grp in polys:
+            arr = np.stack([grp[self.xname], grp[self.yname], grp[self.zname]], axis=1)
+            arrxyz.append(arr)
+    else:
+        xyz = dfrcopy
+        arrxyz = np.stack([xyz[self.xname], xyz[self.yname], xyz[self.zname]], axis=1)
+
+    if (
+        isinstance(arrxyz, np.ndarray)
+        and arrxyz.size == 0
+        or isinstance(arrxyz, list)
+        and len(arrxyz) == 0
+    ):
+        return
+
+    roxxyz.set_values(arrxyz)
+
+    if attributes and isinstance(self, xtgeo.Points) and len(self.dataframe) >= 1:
+        dfr = _cast_dataframe_attrs_to_numeric(dfrcopy)
+        for name in dfr.columns[3:]:
+            values = dfr[name].values
+            if "float" in str(values.dtype):
+                values = np.ma.masked_greater(values, xtgeo.UNDEF_LIMIT)
+            elif "int" in str(values.dtype):
+                values = np.ma.masked_greater(values, xtgeo.UNDEF_INT_LIMIT)
+            else:
+                # masking has no meaning for strings?
+                values = values.astype(str)
+                values = np.char.replace(values, "UNDEF", "")
+
+            logger.info("Store Point attribute %s to Roxar API", name)
+            roxxyz.set_attribute_values(name, values)
+
+
+def _cast_dataframe_attrs_to_numeric(dfr):
+    """Cast the attribute dataframe columns to numerical datatypes if possible.
+
+    In some case, attribute columns get dtype 'object' while they clearly
+    represents a numerical property (int or float). Here the pandas to_numerics()
+    function is applied per attribute column, and will do a conversion if possible;
+    otherwise the 'object' dtype will be preserved.
+    """
+    if len(dfr) < 1:
+        return dfr
+
+    newdfr = dfr.copy()
+    for name in dfr.columns[3:]:
+        newdfr[name] = pd.to_numeric(dfr[name], errors="ignore")
+    return newdfr
+
+
+def _check_category_etc(
+    proj, name, category, stype, realisation, mode="get"
+):  # pylint: disable=too-many-branches  # pragma: no cover
+    """Helper to check if valid placeholder' whithin RMS."""
+
+    logger.warning("Realisation %s not in use", realisation)
+
+    stypedict = {"horizons": proj.horizons, "zones": proj.zones, "faults": proj.faults}
+
+    if stype in stypedict.keys():
+        if name not in stypedict[stype]:
+            logger.error("Cannot access name in stype=%s: %s", stype, name)
+            return False
+        if category not in stypedict[stype].representations:
+            logger.error("Cannot access category in stype=%s: %s", stype, category)
+            return False
+
+    elif stype in ("clipboard", "general2d_data") and mode == "get":
+        folders = None
+        if category:
+            if isinstance(category, list):
+                folders = category
+            elif isinstance(category, str) and "|" in category:
+                folders = category.split("|")
+            elif isinstance(category, str) and "/" in category:
+                folders = category.split("/")
+            elif isinstance(category, str):
+                folders = []
+                folders.append(category)
+            else:
+                raise RuntimeError(
+                    f"Cannot parse category: {category}, see documentation!"
+                )
+            try:
+                roxxyz = getattr(proj, stype).folders[folders]
+            except KeyError as keyerr:
+                logger.error(
+                    "Cannot access clipboards folder (not existing?): %s", keyerr
+                )
+                return False
+        else:
+            roxxyz = proj.clipboard
+
+        if name not in roxxyz.keys():
+            raise ValueError(f"Name {name} is not within Clipboard...")
+
+    elif stype in ("clipboard", "general2d_data") and mode == "set":
+        logger.info("No need to check clipboard while setting data")
+    else:
+        raise ValueError(f"Invalid stype: {stype}")
+
+    return True
+
+
+def _get_roxitem(self, proj, name, category, stype, mode="set"):  # pragma: no cover
+    # pylint: disable=too-many-branches
+    if stype == "horizons":
+        roxxyz = proj.horizons[name][category]
+    elif stype == "zones":
+        roxxyz = proj.zones[name][category]
+    elif stype == "faults":
+        roxxyz = proj.faults[name][category]
+    elif stype in ["clipboard", "general2d_data"]:
+        folders = None
+        roxxyz = getattr(proj, stype)
+        if category:
+            if isinstance(category, list):
+                folders = category
+            elif isinstance(category, str) and "|" in category:
+                folders = category.split("|")
+            elif isinstance(category, str) and "/" in category:
+                folders = category.split("/")
+            elif isinstance(category, str):
+                folders = []
+                folders.append(category)
+            else:
+                raise RuntimeError(
+                    f"Cannot parse category: {category}, see documentation!"
+                )
+
+            if mode == "get":
+                roxxyz = roxxyz.folders[folders]
+
+        if mode == "get":
+            roxxyz = roxxyz[name]
+
+        elif mode == "set":
+            # clipboard folders will be created if not present, and overwritten else
+            if isinstance(self, xtgeo.Polygons):
+                roxxyz = roxxyz.create_polylines(name, folders)
+            else:
+                roxxyz = roxxyz.create_points(name, folders)
+
+    else:
+        raise ValueError(f"Unsupported stype: {stype}")
+
+    return roxxyz
+
+
+def _get_roxxyz(
+    rox, name, category, stype, mode="set", is_polygons=False
+):  # pragma: no cover
+    # pylint: disable=too-many-branches
+    """Get the correct rox_xyz which is some pointer to a RoxarAPI structure."""
+    if stype == "horizons":
+        rox_xyz = rox.project.horizons[name][category]
+    elif stype == "zones":
+        rox_xyz = rox.project.zones[name][category]
+    elif stype == "faults":
+        rox_xyz = rox.project.faults[name][category]
+    elif stype in ["clipboard", "general2d_data"]:
+        folders = None
+        rox_xyz = getattr(rox.project, stype)
+        if category:
+            if isinstance(category, list):
+                folders = category
+                folders.append(category)
+            elif isinstance(category, str) and "|" in category:
+                folders = category.split("|")
+            elif isinstance(category, str) and "/" in category:
+                folders = category.split("/")
+            elif isinstance(category, str):
+                folders = []
+                folders.append(category)
+            else:
+                raise RuntimeError(
+                    f"Cannot parse category: {category}, see documentation!"
+                )
+
+            if mode == "get":
+                rox_xyz = rox_xyz.folders[folders]
+
+        if mode == "get":
+            rox_xyz = rox_xyz[name]
+
+        elif mode == "set":
+            # clipboard folders will be created if not present, and overwritten else
+            if is_polygons:
+                rox_xyz = rox_xyz.create_polylines(name, folders)
+            else:
+                rox_xyz = rox_xyz.create_points(name, folders)
+
+    else:
+        raise TypeError(f"Unsupported stype: {stype}")  # shall never get this far...
+
+    return rox_xyz
+
+
+def _get_roxvalues(rox_xyz, realisation=0):  # pragma: no cover
+    """Return primary values from the Roxar API, numpy (Points) or list (Polygons)."""
+    try:
+        roxitem = rox_xyz.get_values(realisation)
+        logger.info(roxitem)
+    except KeyError as kwe:
+        logger.error(kwe)
+
+    return roxitem
+
+
+def _get_rox_attrvalues(rox_xyz, attrnames, realisation=0) -> dict:  # pragma: no cover
+    """Return attribute values from the Roxar API, numpy (Points) or list (Polygons)."""
+
+    roxitems = {}
+    for attrname in attrnames:
+        values = rox_xyz.get_attribute_values(attrname, realisation=realisation)
+        roxitems[attrname] = values
+
+    return roxitems
```

## xtgeo/xyz/points.py

 * *Ordering differences only*

```diff
@@ -1,988 +1,988 @@
-# -*- coding: utf-8 -*-
-"""The XTGeo xyz.points module, which contains the Points class."""
-import functools
-import io
-import pathlib
-import warnings
-from collections import OrderedDict
-from copy import deepcopy
-from typing import Any, Dict, List, Optional, Union
-
-import deprecation
-import numpy as np
-import pandas as pd
-
-import xtgeo
-from xtgeo.common import XTGeoDialog, inherit_docstring
-from xtgeo.xyz import _xyz_io, _xyz_roxapi
-
-from . import _xyz_oper
-from ._xyz import XYZ
-
-xtg = XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def _data_reader_factory(file_format):
-    if file_format == "xyz":
-        return _xyz_io.import_xyz
-    if file_format == "zmap_ascii":
-        return _xyz_io.import_zmap
-    if file_format == "rms_attr":
-        return _xyz_io.import_rms_attr
-    raise ValueError(f"Unknown file format {file_format}")
-
-
-def _file_importer(
-    pfile: Union[str, pathlib.Path, io.BytesIO],
-    fformat: Optional[str] = None,
-):
-    """General function for points_from_file and (deprecated) method from_file."""
-    xtgeo_file = xtgeo._XTGeoFile(pfile)
-    if fformat is None or fformat == "guess":
-        fformat = xtgeo_file.detect_fformat()
-    else:
-        fformat = xtgeo_file.generic_format_by_proposal(fformat)  # default
-    kwargs = _data_reader_factory(fformat)(xtgeo_file)
-    kwargs["values"].dropna(inplace=True)
-    kwargs["filesrc"] = xtgeo_file.name
-    return kwargs
-
-
-def _surface_importer(surf, zname="Z_TVDSS"):
-    """General function for _read_surface() and (deprecated) method from_surface()."""
-    val = surf.values
-    xc, yc = surf.get_xy_values()
-
-    coord = []
-    for vv in [xc, yc, val]:
-        vv = np.ma.filled(vv.flatten(order="C"), fill_value=np.nan)
-        vv = vv[~np.isnan(vv)]
-        coord.append(vv)
-
-    return {
-        "values": pd.DataFrame(
-            {
-                "X_UTME": coord[0],
-                "Y_UTMN": coord[1],
-                zname: coord[2],
-            }
-        ),
-        "zname": zname,
-    }
-
-
-def _roxar_importer(
-    project,
-    name: str,
-    category: str,
-    stype: str = "horizons",
-    realisation: int = 0,
-    attributes: bool = False,
-):
-    return _xyz_roxapi.import_xyz_roxapi(
-        project, name, category, stype, realisation, attributes, False
-    )
-
-
-def _wells_importer(
-    wells: List[xtgeo.Well],
-    tops: bool = True,
-    incl_limit: Optional[float] = None,
-    top_prefix: str = "Top",
-    zonelist: Optional[list] = None,
-    use_undef: bool = False,
-) -> Dict:
-    """General function importing from wells"""
-    dflist = []
-    for well in wells:
-        wp = well.get_zonation_points(
-            tops=tops,
-            incl_limit=incl_limit,
-            top_prefix=top_prefix,
-            zonelist=zonelist,
-            use_undef=use_undef,
-        )
-
-        if wp is not None:
-            dflist.append(wp)
-
-    dfr = pd.concat(dflist, ignore_index=True)
-
-    attrs = {}
-    for col in dfr.columns:
-        if col == "Zone":
-            attrs[col] = "int"
-        elif col == "ZoneName":
-            attrs[col] = "str"
-        elif col == "WellName":
-            attrs[col] = "str"
-        else:
-            attrs[col] = "float"
-
-    return {"values": dfr, "attributes": attrs}
-
-
-def _wells_dfrac_importer(
-    wells: List[xtgeo.Well],
-    dlogname: str,
-    dcodes: List[int],
-    incl_limit: float = 90,
-    count_limit: int = 3,
-    zonelist: list = None,
-    zonelogname: str = None,
-) -> Dict:
-    """General function, get fraction of discrete code(s) e.g. facies per zone."""
-
-    dflist = []
-    for well in wells:
-        wpf = well.get_fraction_per_zone(
-            dlogname,
-            dcodes,
-            zonelist=zonelist,
-            incl_limit=incl_limit,
-            count_limit=count_limit,
-            zonelogname=zonelogname,
-        )
-
-        if wpf is not None:
-            dflist.append(wpf)
-
-    dfr = pd.concat(dflist, ignore_index=True)
-
-    attrs = {}
-    for col in dfr.columns[3:]:
-        if col.lower() == "zone":
-            attrs[col] = "int"
-        elif col.lower() == "zonename":
-            attrs[col] = "str"
-        elif col.lower() == "wellname":
-            attrs[col] = "str"
-        else:
-            attrs[col] = "float"
-
-    return {
-        "values": dfr,
-        "attributes": attrs,
-        "zname": "DFRAC",
-    }
-
-
-def points_from_file(pfile: Union[str, pathlib.Path], fformat: Optional[str] = "guess"):
-    """Make an instance of a Points object directly from file import.
-
-    Supported formats are:
-
-        * 'xyz' or 'poi' or 'pol': Simple XYZ format
-        * 'zmap': ZMAP line format as exported from RMS (e.g. fault lines)
-        * 'rms_attr': RMS points formats with attributes (extra columns)
-        * 'guess': Try to choose file format based on extension
-
-
-    Args:
-        pfile: Name of file or pathlib object.
-        fformat: File format, xyz/pol/... Default is `guess` where file
-            extension or file signature is parsed to guess the correct format.
-
-    Example::
-
-        import xtgeo
-        mypoints = xtgeo.points_from_file('somefile.xyz')
-    """
-    return Points(**_file_importer(pfile, fformat=fformat))
-
-
-def points_from_roxar(
-    project,
-    name: str,
-    category: str,
-    stype: str = "horizons",
-    realisation: int = 0,
-    attributes: bool = False,
-):
-    """Load a Points instance from Roxar RMS project.
-
-    The import from the RMS project can be done either within the project
-    or outside the project.
-
-    Note also that horizon/zone/faults name and category must exists
-    in advance, otherwise an Exception will be raised.
-
-    Args:
-        project: Name of project (as folder) if outside RMS, or just use the
-            magic `project` word if within RMS.
-        name: Name of points item
-        category: For horizons/zones/faults: for example 'DL_depth'
-            or use a folder notation on clipboard/general2d_data.
-        stype: RMS folder type, 'horizons' (default), 'zones', 'clipboard',
-            'general2d_data'
-        realisation: Realisation number, default is 0
-        attributes: If True, attributes will be preserved (from RMS 11)
-
-    Example::
-
-        # inside RMS:
-        import xtgeo
-        mypoints = xtgeo.points_from_roxar(project, 'TopEtive', 'DP_seismic')
-
-    .. versionadded:: 2.19 general2d_data support is added
-    """
-
-    return Points(
-        **_roxar_importer(
-            project,
-            name,
-            category,
-            stype,
-            realisation,
-            attributes,
-        )
-    )
-
-
-def points_from_surface(
-    regular_surface,
-    zname: str = "Z_TVDSS",
-):
-    """This makes an instance of a Points directly from a RegularSurface object.
-
-    Each surface node will be stored as a X Y Z point.
-
-    Args:
-        regular_surface: XTGeo RegularSurface() instance
-        zname: Name of third column
-
-    .. versionadded:: 2.16
-       Replaces the from_surface() method.
-    """
-
-    return Points(**_surface_importer(regular_surface, zname=zname))
-
-
-def points_from_wells(
-    wells: List[xtgeo.Well],
-    tops: bool = True,
-    incl_limit: Optional[float] = None,
-    top_prefix: str = "Top",
-    zonelist: Optional[list] = None,
-    use_undef: bool = False,
-):
-    """Get tops or zone points data from a list of wells.
-
-    Args:
-        wells: List of XTGeo well objects.
-            If a list of well files, the routine will try to load well based on file
-            signature and/or extension, but only default settings are applied. Hence
-            this is less flexible and more fragile.
-        tops: Get the tops if True (default), otherwise zone.
-        incl_limit: Inclination limit for zones (thickness points)
-        top_prefix: Prefix used for Tops.
-        zonelist: Which zone numbers to apply, None means all.
-        use_undef: If True, then transition from UNDEF is also used.
-
-    Returns:
-        None if empty data, otherwise a Points() instance.
-
-    Example::
-
-            wells = [xtgeo.Well("w1.w"), xtgeo.Well("w2.w")]
-            points = xtgeo.points_from_wells(wells)
-
-    Note:
-        The deprecated method :py:meth:`~Points.from_wells` returns the number of
-        wells that contribute with points. This is now implemented through the
-        function `get_nwells()`. Hence the following code::
-
-            nwells_applied = poi.from_wells(...)  # deprecated method
-            # vs
-            poi = xtgeo.points_from_wells(...)
-            nwells_applied = poi.get_nwells()
-
-    .. versionadded:: 2.16 Replaces :meth:`~Points.from_wells`
-
-    """
-    return Points(
-        **_wells_importer(wells, tops, incl_limit, top_prefix, zonelist, use_undef)
-    )
-
-
-def points_from_wells_dfrac(
-    wells: List[xtgeo.Well],
-    dlogname: str,
-    dcodes: List[int],
-    incl_limit: float = 90,
-    count_limit: int = 3,
-    zonelist: Optional[list] = None,
-    zonelogname: Optional[str] = None,
-):
-    """Get fraction of discrete code(s) e.g. facies per zone.
-
-    Args:
-        wells: List of XTGeo well objects.
-            If a list of file names, the routine will try to load well based on file
-            signature and/or extension, but only default settings are applied. Hence
-            this is less flexible and more fragile.
-        dlogname: Name of discrete log (e.g. Facies)
-        dcodes: Code(s) to get fraction for, e.g. [3]
-        incl_limit: Inclination limit for zones (thickness points)
-        count_limit: Min. no of counts per segment for valid result
-        zonelist: Which zone numbers to apply, default None means all.
-        zonelogname: If None, the zonelogname property in the well object will be
-            applied. This option is particualr useful if one uses wells directly from
-            files.
-
-    Returns:
-        None if empty data, otherwise a Points() instance.
-
-    Example::
-
-            wells = [xtgeo.Well("w1.w"), xtgeo.Well("w2.w")]
-            points = xtgeo.points_from_wells_dfrac(
-                    wells, dlogname="Facies", dcodes=[4], zonelogname="ZONELOG"
-                )
-
-    Note:
-        The deprecated method :py:meth:`~Points.dfrac_from_wells` returns the number of
-        wells that contribute with points. This is now implemented through the
-        method `get_nwells()`. Hence the following code::
-
-            nwells_applied = poi.dfrac_from_wells(...)  # deprecated method
-            # vs
-            poi = xtgeo.points_from_wells_dfrac(...)
-            nwells_applied = poi.get_nwells()
-
-    .. versionadded:: 2.16 Replaces :meth:`~Points.dfrac_from_wells`
-    """
-    return Points(
-        **_wells_dfrac_importer(
-            wells, dlogname, dcodes, incl_limit, count_limit, zonelist, zonelogname
-        )
-    )
-
-
-def _allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of Points and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    # Introduced post xtgeo version 2.15
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        # Checking if we are doing an initialization from file or surface and raise a
-        # deprecation warning if we are.
-        if len(args) == 1:
-            if isinstance(args[0], (str, pathlib.Path)):
-                warnings.warn(
-                    "Initializing directly from file name is deprecated and will be "
-                    "removed in xtgeo version 4.0. Use: "
-                    "poi = xtgeo.points_from_file('some_file.xx') instead!",
-                    DeprecationWarning,
-                )
-                fformat = kwargs.get("fformat", "guess")
-                return func(cls, **_file_importer(args[0], fformat))
-
-            elif isinstance(args[0], xtgeo.RegularSurface):
-                warnings.warn(
-                    "Initializing directly from RegularSurface is deprecated "
-                    "and will be removed in xtgeo version 4.0. Use: "
-                    "poi = xtgeo.points_from_surface(regsurf) instead",
-                    DeprecationWarning,
-                )
-                zname = kwargs.get("zname", "Z_TVDSS")
-                return func(cls, **_surface_importer(args[0], zname=zname))
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-class Points(XYZ):  # pylint: disable=too-many-public-methods, function-redefined
-    """Class for a Points data in XTGeo.
-
-    The Points class is a subclass of the :py:class:`~xtgeo.xyz._xyz.XYZ` abstract
-    class, and the point set itself is a `pandas <http://pandas.pydata.org>`_
-    dataframe object.
-
-    For points, 3 float columns (X Y Z) are mandatory. In addition it is possible to
-    have addiotional points attribute columns, and such attributes may be integer,
-    strings or floats.
-
-    The instance can be made either from file (then as classmethod), from another
-    object or by a spesification, e.g. from file or a surface::
-
-        xp1 = xtgeo.points_from_file('somefilename', fformat='xyz')
-        # or
-        regsurf = xtgeo.surface_from_file("somefile.gri")
-        xp2 = xtgeo.points_from_surface(regsurf)
-
-    You can also initialise points from list of tuples/lists in Python, where
-    each tuple is a (X, Y, Z) coordinate::
-
-        plist = [(234, 556, 12), (235, 559, 14), (255, 577, 12)]
-        mypoints = Points(values=plist)
-
-    The tuples can also contain point attributes which needs spesification via
-    an attributes dictionary::
-
-        plist = [
-            (234, 556, 12, "Well1", 22),
-            (235, 559, 14, "Well2", 44),
-            (255, 577, 12, "Well3", 55)]
-        attrs = {"WellName": "str", "ID", "int"}
-        mypoints = Points(values=plist, attributes=attrs)
-
-    And points can be initialised from a 2D numpy array or an existing dataframe::
-
-        >>> mypoints1 = Points(values=[(1,1,1), (2,2,2), (3,3,3)])
-        >>> mypoints2 = Points(
-        ...     values=pd.DataFrame(
-        ...          [[1, 2, 3], [1, 2, 3], [1, 2, 3]],
-        ...          columns=["X_UTME", "Y_UTMN", "Z_TVDSS"]
-        ...     )
-        ... )
-
-
-    Similar as for lists, attributes are alse possible for numpy and dataframes.
-
-    Default column names in the dataframe:
-
-    * X_UTME: UTM X coordinate  as self._xname
-    * Y_UTMN: UTM Y coordinate  as self._yname
-    * Z_TVDSS: Z coordinate, often depth below TVD SS, but may also be
-      something else! Use zname attribute to change name.
-
-    Note:
-        Attributes may have undefined entries. Pandas version 0.21 (which is applied
-        for RMS version up to 12.0.x) do not support NaN values for Integers. The
-        solution is store undefined values as large numbers, xtgeo.UNDEF_INT
-        (2000000000) for integers and xtgeo.UNDEF (10e32) for float values.
-        This will change from xtgeo version 3.x where Pandas version 1 and
-        above will be required, which in turn support will pandas.NA
-        entries.
-
-    Args:
-        values: Provide input values on various forms (list-like or dataframe).
-        xname: Name of first (X) mandatory column, default is X_UTME.
-        yname: Name of second (Y) mandatory column, default is Y_UTMN.
-        zname: Name of third (Z) mandatory column, default is Z_TVDSS.
-        attributes: A dictionary for attribute columns as 'name: type', e.g.
-            {"WellName": "str", "IX": "int"}. This is applied when values are input
-            and is to name and type the extra attribute columns in a point set.
-    """
-
-    @_allow_deprecated_init
-    def __init__(
-        self,
-        values: Union[list, np.ndarray, pd.DataFrame] = None,
-        xname: str = "X_UTME",
-        yname: str = "Y_UTMN",
-        zname: str = "Z_TVDSS",
-        attributes: Optional[dict] = None,
-        filesrc: str = None,
-    ):
-        """Initialisation of Points()."""
-        super().__init__(xname, yname, zname)
-        if values is None:
-            values = []
-        self._reset(
-            values=values,
-            xname=xname,
-            yname=yname,
-            zname=zname,
-            attributes=attributes,
-            filesrc=filesrc,
-        )
-
-    def _reset(
-        self,
-        values: Union[list, np.ndarray, pd.DataFrame] = None,
-        xname: str = "X_UTME",
-        yname: str = "Y_UTMN",
-        zname: str = "Z_TVDSS",
-        attributes: Optional[dict] = None,
-        filesrc: str = None,
-    ):  # pylint: disable=arguments-differ
-        """Used in deprecated methods."""
-        super()._reset(xname, yname, zname)
-
-        self._attrs = attributes if attributes is not None else dict()
-        self._filesrc = filesrc
-
-        if not isinstance(values, pd.DataFrame):
-            self._df = _xyz_io._from_list_like(values, self._zname, attributes, False)
-        else:
-            self._df: pd.DataFrame = values
-            self._dataframe_consistency_check()
-
-    def _dataframe_consistency_check(self):
-        if self.xname not in self.dataframe:
-            raise ValueError(
-                f"xname={self.xname} is not a column "
-                f"of dataframe {self.dataframe.columns}"
-            )
-        if self.yname not in self.dataframe:
-            raise ValueError(
-                f"yname={self.yname} is not a column "
-                f"of dataframe {self.dataframe.columns}"
-            )
-        if self.zname not in self.dataframe:
-            raise ValueError(
-                f"zname={self.zname} is not a column "
-                f"of dataframe {self.dataframe.columns}"
-            )
-        for attr in self._attrs:
-            if attr not in self.dataframe:
-                raise ValueError(
-                    f"Attribute {attr} is not a column "
-                    f"of dataframe {self.dataframe.columns}"
-                )
-
-    def __repr__(self):
-        # should be able to newobject = eval(repr(thisobject))
-        myrp = f"{self.__class__.__name__} (filesrc={self._filesrc!r}, ID={id(self)})"
-        return myrp
-
-    def __str__(self):
-        """User friendly print."""
-        return self.describe(flush=False)
-
-    def __eq__(self, value):
-        """Magic method for ==."""
-        return self.dataframe[self.zname] == value
-
-    def __gt__(self, value):
-        return self.dataframe[self.zname] > value
-
-    def __ge__(self, value):
-        return self.dataframe[self.zname] >= value
-
-    def __lt__(self, value):
-        return self.dataframe[self.zname] < value
-
-    def __le__(self, value):
-        return self.dataframe[self.zname] <= value
-
-    # ----------------------------------------------------------------------------------
-    # Methods
-    # ----------------------------------------------------------------------------------
-
-    @property
-    def dataframe(self) -> pd.DataFrame:
-        """Returns or set the Pandas dataframe object."""
-        return self._df
-
-    @dataframe.setter
-    def dataframe(self, df):
-        self._df = df.apply(deepcopy)
-
-    def _random(self, nrandom=10):
-        """Generate nrandom random points within the range 0..1
-
-        Args:
-            nrandom (int): Number of random points (default 10)
-
-        .. versionadded:: 2.3
-        """
-
-        # currently a non-published method
-
-        self._df = pd.DataFrame(
-            np.random.rand(nrandom, 3), columns=[self._xname, self._yname, self._zname]
-        )
-
-    @inherit_docstring(inherit_from=XYZ.from_file)
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.points_from_file() instead",
-    )
-    def from_file(self, pfile, fformat="xyz"):
-        self._reset(**_file_importer(pfile, fformat))
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.points_from_roxar() instead.",
-    )
-    def from_roxar(
-        self,
-        project: Union[str, Any],
-        name: str,
-        category: str,
-        stype: str = "horizons",
-        realisation: int = 0,
-        attributes: bool = False,
-    ):  # pragma: no cover
-        """Load a points/polygons item from a Roxar RMS project (deprecated).
-
-        The import from the RMS project can be done either within the project
-        or outside the project.
-
-        Note that the preferred shortform for (use polygons as example)::
-
-          import xtgeo
-          mypoly = xtgeo.xyz.Polygons()
-          mypoly.from_roxar(project, 'TopAare', 'DepthPolys')
-
-        is now::
-
-          import xtgeo
-          mysurf = xtgeo.polygons_from_roxar(project, 'TopAare', 'DepthPolys')
-
-        Note also that horizon/zone/faults name and category must exists
-        in advance, otherwise an Exception will be raised.
-
-        Args:
-            project (str or special): Name of project (as folder) if
-                outside RMS, og just use the magic project word if within RMS.
-            name (str): Name of polygons item
-            category (str): For horizons/zones/faults: for example 'DL_depth'
-                or use a folder notation on clipboard/general2d_data.
-
-            stype (str): RMS folder type, 'horizons' (default) or 'zones' etc!
-            realisation (int): Realisation number, default is 0
-            attributes (bool): If True, attributes will be preserved (from RMS 11)
-
-        Returns:
-            Object instance updated
-
-        Raises:
-            ValueError: Various types of invalid inputs.
-
-        .. deprecated:: 2.16
-           Use xtgeo.points_from_roxar() or xtgeo.polygons_from_roxar()
-        """
-        self._reset(
-            **_roxar_importer(
-                project,
-                name,
-                category,
-                stype,
-                realisation,
-                attributes,
-            )
-        )
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use "
-        "xtgeo.Points("
-        "values=dfr[[east, nort, tvdsml]], xname=east, yname=north, zname=tvdmsl"
-        ") instead",
-    )
-    def from_dataframe(self, dfr, east="X", north="Y", tvdmsl="Z", attributes=None):
-        """Import points/polygons from existing Pandas dataframe.
-
-        Args:
-            dfr (dataframe): Pandas dataframe.
-            east (str): Name of easting column in input dataframe.
-            north (str): Name of northing column in input dataframe.
-            tvdmsl (str): Name of depth column in input dataframe.
-            attributes (dict): Additional metadata columns, on form {"IX": "I", ...};
-                "IX" here is the name of the target column, and "I" is the name in the
-                input dataframe.
-
-        .. versionadded:: 2.13
-        .. deprecated:: 2.16 Use points constructor directly instead
-        """
-        if not all(item in dfr.columns for item in (east, north, tvdmsl)):
-            raise ValueError("One or more column names are not correct")
-
-        if attributes and not all(item in dfr.columns for item in attributes.values()):
-            raise ValueError("One or more attribute column names are not correct")
-
-        input = OrderedDict()
-        input["X_UTME"] = dfr[east]
-        input["Y_UTMN"] = dfr[north]
-        input["Z_TVDSS"] = dfr[tvdmsl]
-
-        if attributes:
-            for target, source in attributes.items():
-                input[target] = dfr[source]
-
-        df = pd.DataFrame(input)
-        df.dropna(inplace=True)
-        self._reset(values=df, filesrc="DataFrame input")
-
-    def to_file(
-        self,
-        pfile,
-        fformat="xyz",
-        attributes=True,
-        pfilter=None,
-        wcolumn=None,
-        hcolumn=None,
-        mdcolumn="M_MDEPTH",
-        **kwargs,
-    ):  # pylint: disable=redefined-builtin
-        """Export Points to file.
-
-        Args:
-            pfile (str): Name of file
-            fformat (str): File format xyz/poi/pol or rms_attr
-            attributes (bool or list): List of extra columns to export (some formats)
-                or True for all attributes present
-            pfilter (dict): Filter on e.g. top name(s) with keys TopName
-                or ZoneName as {'TopName': ['Top1', 'Top2']}.
-            wcolumn (str): Name of well column (rms_wellpicks format only)
-            hcolumn (str): Name of horizons column (rms_wellpicks format only)
-            mdcolumn (str): Name of MD column (rms_wellpicks format only)
-
-        Returns:
-            Number of points exported
-
-        Note that the rms_wellpicks will try to output to:
-
-        * HorizonName, WellName, MD  if a MD (mdcolumn) is present,
-        * HorizonName, WellName, X, Y, Z  otherwise
-
-        Note:
-            For backward compatibility, the key ``filter`` can be applied instead of
-            ``pfilter``.
-
-        Raises:
-            KeyError if pfilter is set and key(s) are invalid
-
-        """
-        return _xyz_io.to_file(
-            self,
-            pfile,
-            fformat=fformat,
-            attributes=attributes,
-            pfilter=pfilter,
-            wcolumn=wcolumn,
-            hcolumn=hcolumn,
-            mdcolumn=mdcolumn,
-            **kwargs,
-        )
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.points_from_wells() instead.",
-    )
-    def from_wells(
-        self,
-        wells,
-        tops=True,
-        incl_limit=None,
-        top_prefix="Top",
-        zonelist=None,
-        use_undef=False,
-    ):
-        """Get tops or zone points data from a list of wells.
-
-        Args:
-            wells (list): List of XTGeo well objects
-            tops (bool): Get the tops if True (default), otherwise zone
-            incl_limit (float): Inclination limit for zones (thickness points)
-            top_prefix (str): Prefix used for Tops
-            zonelist (list-like): Which zone numbers to apply.
-            use_undef (bool): If True, then transition from UNDEF within zonelog
-                is also used.
-
-        Returns:
-            None if well list is empty; otherwise the number of wells.
-
-        Raises:
-            Todo
-
-        .. deprecated:: 2.16
-           Use classmethod :py:func:`points_from_wells()` instead
-        """
-        self._reset(
-            **_wells_importer(wells, tops, incl_limit, top_prefix, zonelist, use_undef)
-        )
-        return self.dataframe["WellName"].nunique()
-
-    @inherit_docstring(inherit_from=XYZ.from_list)
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use direct Points() initialisation instead",
-    )
-    def from_list(self, plist):
-        self._reset(_xyz_io._from_list_like(plist, "Z_TVDSS", None, False))
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.points_from_wells_dfrac() instead.",
-    )
-    def dfrac_from_wells(
-        self,
-        wells,
-        dlogname,
-        dcodes,
-        incl_limit=90,
-        count_limit=3,
-        zonelist=None,
-        zonelogname=None,
-    ):
-        """Get fraction of discrete code(s) (e.g. facies) per zone.
-
-        Args:
-            wells (list): List of XTGeo well objects
-            dlogname (str): Name of discrete log (e.g. Facies)
-            dcodes (list of int): Code(s) to get fraction for, e.g. [3]
-            incl_limit (float): Inclination limit for zones (thickness points)
-            count_limit (int): Min. no of counts per segment for valid result
-            zonelist (list of int): Whihc zones to compute for (default None
-                means that all zones will be evaluated)
-            zonelogname (str): Name of zonelog; if None than the
-                well.zonelogname property will be applied.
-
-        Returns:
-            None if well list is empty; otherwise the number of wells.
-
-        Raises:
-            Todo
-
-        .. deprecated:: 2.16
-           Use classmethod :py:func:`points_from_wells_dfrac()` instead.
-        """
-
-        self._reset(
-            **_wells_dfrac_importer(
-                wells, dlogname, dcodes, incl_limit, count_limit, zonelist, zonelogname
-            )
-        )
-
-    def to_roxar(
-        self,
-        project,
-        name,
-        category,
-        stype="horizons",
-        pfilter=None,
-        realisation=0,
-        attributes=False,
-    ):  # pragma: no cover
-        """Export (store) a Points item to a Roxar RMS project.
-
-        The export to the RMS project can be done either within the project
-        or outside the project.
-
-        Note also that horizon/zone name and category must exists in advance,
-        otherwise an Exception will be raised.
-
-        Note:
-            When project is file path (direct access, outside RMS) then
-            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-            will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str or special): Name of project (as folder) if
-                outside RMS, og just use the magic project word if within RMS.
-            name (str): Name of polygons item
-            category (str): For horizons/zones/faults: for example 'DL_depth'
-            pfilter (dict): Filter on e.g. top name(s) with keys TopName
-                or ZoneName as {'TopName': ['Top1', 'Top2']}
-            stype (str): RMS folder type, 'horizons' (default), 'zones'
-                or 'faults' or 'clipboard', general2d_data (in prep: well picks)
-            realisation (int): Realisation number, default is 0
-            attributes (bool): If True, attributes will be preserved (from RMS 11)
-
-
-        Returns:
-            Object instance updated
-
-        Raises:
-            ValueError: Various types of invalid inputs.
-            NotImplementedError: Not supported in this ROXAPI version
-
-        .. versionadded:: 2.19 general2d_data support is added
-        """
-
-        valid_stypes = [
-            "horizons",
-            "zones",
-            "faults",
-            "clipboard",
-            "general2d_data",
-            "horizon_picks",
-        ]
-
-        if stype.lower() not in valid_stypes:
-            raise ValueError(f"Invalid stype, only {valid_stypes} stypes is supported.")
-
-        _xyz_roxapi.export_xyz_roxapi(
-            self,
-            project,
-            name,
-            category,
-            stype,
-            pfilter,
-            realisation,
-            attributes,
-        )
-
-    def copy(self):
-        """Returns a deep copy of an instance."""
-        mycopy = self.__class__()
-        mycopy._df = self._df.apply(deepcopy)
-        mycopy._xname = self._xname
-        mycopy._yname = self._yname
-        mycopy._zname = self._zname
-
-        return mycopy
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.points_from_surface() instead",
-    )
-    def from_surface(self, surf, zname="Z_TVDSS"):
-        """Get points as X Y Value from a surface object nodes (deprecated).
-
-        Note that undefined surface nodes will not be included.
-
-        Args:
-            surf (RegularSurface): A XTGeo RegularSurface object instance.
-            zname (str): Name of value column (3'rd column)
-
-        Example::
-
-            topx = RegularSurface('topx.gri')
-            topx_aspoints = Points()
-            topx_aspoints.from_surface(topx)
-
-            # alternative shortform:
-            topx_aspoints = Points(topx)  # get an instance directly
-
-            topx_aspoints.to_file('mypoints.poi')  # export as XYZ file
-
-        .. deprecated:: 2.16 Use xtgeo.points_from_surface() instead.
-        """
-
-        self._reset(**_surface_importer(surf, zname=zname))
-
-    def snap_surface(self, surf, activeonly=True):
-        """Snap (transfer) the points Z values to a RegularSurface
-
-        Args:
-            surf (~xtgeo.surface.regular_surface.RegularSurface): Surface to snap to.
-            activeonly (bool): If True (default), the points outside the defined surface
-                will be removed. If False, these points will keep the original values.
-
-        Returns:
-            None (instance is updated inplace)
-
-        Raises:
-            ValueError: Input object of wrong data type, must be RegularSurface
-            RuntimeError: Error code from C routine surf_get_zv_from_xyv is ...
-
-        .. versionadded:: 2.1
-
-        """
-        _xyz_oper.snap_surface(self, surf, activeonly=activeonly)
-
-    @inherit_docstring(inherit_from=XYZ.get_boundary)
-    def get_boundary(self):
-        return super().get_boundary()
+# -*- coding: utf-8 -*-
+"""The XTGeo xyz.points module, which contains the Points class."""
+import functools
+import io
+import pathlib
+import warnings
+from collections import OrderedDict
+from copy import deepcopy
+from typing import Any, Dict, List, Optional, Union
+
+import deprecation
+import numpy as np
+import pandas as pd
+
+import xtgeo
+from xtgeo.common import XTGeoDialog, inherit_docstring
+from xtgeo.xyz import _xyz_io, _xyz_roxapi
+
+from . import _xyz_oper
+from ._xyz import XYZ
+
+xtg = XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def _data_reader_factory(file_format):
+    if file_format == "xyz":
+        return _xyz_io.import_xyz
+    if file_format == "zmap_ascii":
+        return _xyz_io.import_zmap
+    if file_format == "rms_attr":
+        return _xyz_io.import_rms_attr
+    raise ValueError(f"Unknown file format {file_format}")
+
+
+def _file_importer(
+    pfile: Union[str, pathlib.Path, io.BytesIO],
+    fformat: Optional[str] = None,
+):
+    """General function for points_from_file and (deprecated) method from_file."""
+    xtgeo_file = xtgeo._XTGeoFile(pfile)
+    if fformat is None or fformat == "guess":
+        fformat = xtgeo_file.detect_fformat()
+    else:
+        fformat = xtgeo_file.generic_format_by_proposal(fformat)  # default
+    kwargs = _data_reader_factory(fformat)(xtgeo_file)
+    kwargs["values"].dropna(inplace=True)
+    kwargs["filesrc"] = xtgeo_file.name
+    return kwargs
+
+
+def _surface_importer(surf, zname="Z_TVDSS"):
+    """General function for _read_surface() and (deprecated) method from_surface()."""
+    val = surf.values
+    xc, yc = surf.get_xy_values()
+
+    coord = []
+    for vv in [xc, yc, val]:
+        vv = np.ma.filled(vv.flatten(order="C"), fill_value=np.nan)
+        vv = vv[~np.isnan(vv)]
+        coord.append(vv)
+
+    return {
+        "values": pd.DataFrame(
+            {
+                "X_UTME": coord[0],
+                "Y_UTMN": coord[1],
+                zname: coord[2],
+            }
+        ),
+        "zname": zname,
+    }
+
+
+def _roxar_importer(
+    project,
+    name: str,
+    category: str,
+    stype: str = "horizons",
+    realisation: int = 0,
+    attributes: bool = False,
+):
+    return _xyz_roxapi.import_xyz_roxapi(
+        project, name, category, stype, realisation, attributes, False
+    )
+
+
+def _wells_importer(
+    wells: List[xtgeo.Well],
+    tops: bool = True,
+    incl_limit: Optional[float] = None,
+    top_prefix: str = "Top",
+    zonelist: Optional[list] = None,
+    use_undef: bool = False,
+) -> Dict:
+    """General function importing from wells"""
+    dflist = []
+    for well in wells:
+        wp = well.get_zonation_points(
+            tops=tops,
+            incl_limit=incl_limit,
+            top_prefix=top_prefix,
+            zonelist=zonelist,
+            use_undef=use_undef,
+        )
+
+        if wp is not None:
+            dflist.append(wp)
+
+    dfr = pd.concat(dflist, ignore_index=True)
+
+    attrs = {}
+    for col in dfr.columns:
+        if col == "Zone":
+            attrs[col] = "int"
+        elif col == "ZoneName":
+            attrs[col] = "str"
+        elif col == "WellName":
+            attrs[col] = "str"
+        else:
+            attrs[col] = "float"
+
+    return {"values": dfr, "attributes": attrs}
+
+
+def _wells_dfrac_importer(
+    wells: List[xtgeo.Well],
+    dlogname: str,
+    dcodes: List[int],
+    incl_limit: float = 90,
+    count_limit: int = 3,
+    zonelist: list = None,
+    zonelogname: str = None,
+) -> Dict:
+    """General function, get fraction of discrete code(s) e.g. facies per zone."""
+
+    dflist = []
+    for well in wells:
+        wpf = well.get_fraction_per_zone(
+            dlogname,
+            dcodes,
+            zonelist=zonelist,
+            incl_limit=incl_limit,
+            count_limit=count_limit,
+            zonelogname=zonelogname,
+        )
+
+        if wpf is not None:
+            dflist.append(wpf)
+
+    dfr = pd.concat(dflist, ignore_index=True)
+
+    attrs = {}
+    for col in dfr.columns[3:]:
+        if col.lower() == "zone":
+            attrs[col] = "int"
+        elif col.lower() == "zonename":
+            attrs[col] = "str"
+        elif col.lower() == "wellname":
+            attrs[col] = "str"
+        else:
+            attrs[col] = "float"
+
+    return {
+        "values": dfr,
+        "attributes": attrs,
+        "zname": "DFRAC",
+    }
+
+
+def points_from_file(pfile: Union[str, pathlib.Path], fformat: Optional[str] = "guess"):
+    """Make an instance of a Points object directly from file import.
+
+    Supported formats are:
+
+        * 'xyz' or 'poi' or 'pol': Simple XYZ format
+        * 'zmap': ZMAP line format as exported from RMS (e.g. fault lines)
+        * 'rms_attr': RMS points formats with attributes (extra columns)
+        * 'guess': Try to choose file format based on extension
+
+
+    Args:
+        pfile: Name of file or pathlib object.
+        fformat: File format, xyz/pol/... Default is `guess` where file
+            extension or file signature is parsed to guess the correct format.
+
+    Example::
+
+        import xtgeo
+        mypoints = xtgeo.points_from_file('somefile.xyz')
+    """
+    return Points(**_file_importer(pfile, fformat=fformat))
+
+
+def points_from_roxar(
+    project,
+    name: str,
+    category: str,
+    stype: str = "horizons",
+    realisation: int = 0,
+    attributes: bool = False,
+):
+    """Load a Points instance from Roxar RMS project.
+
+    The import from the RMS project can be done either within the project
+    or outside the project.
+
+    Note also that horizon/zone/faults name and category must exists
+    in advance, otherwise an Exception will be raised.
+
+    Args:
+        project: Name of project (as folder) if outside RMS, or just use the
+            magic `project` word if within RMS.
+        name: Name of points item
+        category: For horizons/zones/faults: for example 'DL_depth'
+            or use a folder notation on clipboard/general2d_data.
+        stype: RMS folder type, 'horizons' (default), 'zones', 'clipboard',
+            'general2d_data'
+        realisation: Realisation number, default is 0
+        attributes: If True, attributes will be preserved (from RMS 11)
+
+    Example::
+
+        # inside RMS:
+        import xtgeo
+        mypoints = xtgeo.points_from_roxar(project, 'TopEtive', 'DP_seismic')
+
+    .. versionadded:: 2.19 general2d_data support is added
+    """
+
+    return Points(
+        **_roxar_importer(
+            project,
+            name,
+            category,
+            stype,
+            realisation,
+            attributes,
+        )
+    )
+
+
+def points_from_surface(
+    regular_surface,
+    zname: str = "Z_TVDSS",
+):
+    """This makes an instance of a Points directly from a RegularSurface object.
+
+    Each surface node will be stored as a X Y Z point.
+
+    Args:
+        regular_surface: XTGeo RegularSurface() instance
+        zname: Name of third column
+
+    .. versionadded:: 2.16
+       Replaces the from_surface() method.
+    """
+
+    return Points(**_surface_importer(regular_surface, zname=zname))
+
+
+def points_from_wells(
+    wells: List[xtgeo.Well],
+    tops: bool = True,
+    incl_limit: Optional[float] = None,
+    top_prefix: str = "Top",
+    zonelist: Optional[list] = None,
+    use_undef: bool = False,
+):
+    """Get tops or zone points data from a list of wells.
+
+    Args:
+        wells: List of XTGeo well objects.
+            If a list of well files, the routine will try to load well based on file
+            signature and/or extension, but only default settings are applied. Hence
+            this is less flexible and more fragile.
+        tops: Get the tops if True (default), otherwise zone.
+        incl_limit: Inclination limit for zones (thickness points)
+        top_prefix: Prefix used for Tops.
+        zonelist: Which zone numbers to apply, None means all.
+        use_undef: If True, then transition from UNDEF is also used.
+
+    Returns:
+        None if empty data, otherwise a Points() instance.
+
+    Example::
+
+            wells = [xtgeo.Well("w1.w"), xtgeo.Well("w2.w")]
+            points = xtgeo.points_from_wells(wells)
+
+    Note:
+        The deprecated method :py:meth:`~Points.from_wells` returns the number of
+        wells that contribute with points. This is now implemented through the
+        function `get_nwells()`. Hence the following code::
+
+            nwells_applied = poi.from_wells(...)  # deprecated method
+            # vs
+            poi = xtgeo.points_from_wells(...)
+            nwells_applied = poi.get_nwells()
+
+    .. versionadded:: 2.16 Replaces :meth:`~Points.from_wells`
+
+    """
+    return Points(
+        **_wells_importer(wells, tops, incl_limit, top_prefix, zonelist, use_undef)
+    )
+
+
+def points_from_wells_dfrac(
+    wells: List[xtgeo.Well],
+    dlogname: str,
+    dcodes: List[int],
+    incl_limit: float = 90,
+    count_limit: int = 3,
+    zonelist: Optional[list] = None,
+    zonelogname: Optional[str] = None,
+):
+    """Get fraction of discrete code(s) e.g. facies per zone.
+
+    Args:
+        wells: List of XTGeo well objects.
+            If a list of file names, the routine will try to load well based on file
+            signature and/or extension, but only default settings are applied. Hence
+            this is less flexible and more fragile.
+        dlogname: Name of discrete log (e.g. Facies)
+        dcodes: Code(s) to get fraction for, e.g. [3]
+        incl_limit: Inclination limit for zones (thickness points)
+        count_limit: Min. no of counts per segment for valid result
+        zonelist: Which zone numbers to apply, default None means all.
+        zonelogname: If None, the zonelogname property in the well object will be
+            applied. This option is particualr useful if one uses wells directly from
+            files.
+
+    Returns:
+        None if empty data, otherwise a Points() instance.
+
+    Example::
+
+            wells = [xtgeo.Well("w1.w"), xtgeo.Well("w2.w")]
+            points = xtgeo.points_from_wells_dfrac(
+                    wells, dlogname="Facies", dcodes=[4], zonelogname="ZONELOG"
+                )
+
+    Note:
+        The deprecated method :py:meth:`~Points.dfrac_from_wells` returns the number of
+        wells that contribute with points. This is now implemented through the
+        method `get_nwells()`. Hence the following code::
+
+            nwells_applied = poi.dfrac_from_wells(...)  # deprecated method
+            # vs
+            poi = xtgeo.points_from_wells_dfrac(...)
+            nwells_applied = poi.get_nwells()
+
+    .. versionadded:: 2.16 Replaces :meth:`~Points.dfrac_from_wells`
+    """
+    return Points(
+        **_wells_dfrac_importer(
+            wells, dlogname, dcodes, incl_limit, count_limit, zonelist, zonelogname
+        )
+    )
+
+
+def _allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of Points and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    # Introduced post xtgeo version 2.15
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        # Checking if we are doing an initialization from file or surface and raise a
+        # deprecation warning if we are.
+        if len(args) == 1:
+            if isinstance(args[0], (str, pathlib.Path)):
+                warnings.warn(
+                    "Initializing directly from file name is deprecated and will be "
+                    "removed in xtgeo version 4.0. Use: "
+                    "poi = xtgeo.points_from_file('some_file.xx') instead!",
+                    DeprecationWarning,
+                )
+                fformat = kwargs.get("fformat", "guess")
+                return func(cls, **_file_importer(args[0], fformat))
+
+            elif isinstance(args[0], xtgeo.RegularSurface):
+                warnings.warn(
+                    "Initializing directly from RegularSurface is deprecated "
+                    "and will be removed in xtgeo version 4.0. Use: "
+                    "poi = xtgeo.points_from_surface(regsurf) instead",
+                    DeprecationWarning,
+                )
+                zname = kwargs.get("zname", "Z_TVDSS")
+                return func(cls, **_surface_importer(args[0], zname=zname))
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+class Points(XYZ):  # pylint: disable=too-many-public-methods, function-redefined
+    """Class for a Points data in XTGeo.
+
+    The Points class is a subclass of the :py:class:`~xtgeo.xyz._xyz.XYZ` abstract
+    class, and the point set itself is a `pandas <http://pandas.pydata.org>`_
+    dataframe object.
+
+    For points, 3 float columns (X Y Z) are mandatory. In addition it is possible to
+    have addiotional points attribute columns, and such attributes may be integer,
+    strings or floats.
+
+    The instance can be made either from file (then as classmethod), from another
+    object or by a spesification, e.g. from file or a surface::
+
+        xp1 = xtgeo.points_from_file('somefilename', fformat='xyz')
+        # or
+        regsurf = xtgeo.surface_from_file("somefile.gri")
+        xp2 = xtgeo.points_from_surface(regsurf)
+
+    You can also initialise points from list of tuples/lists in Python, where
+    each tuple is a (X, Y, Z) coordinate::
+
+        plist = [(234, 556, 12), (235, 559, 14), (255, 577, 12)]
+        mypoints = Points(values=plist)
+
+    The tuples can also contain point attributes which needs spesification via
+    an attributes dictionary::
+
+        plist = [
+            (234, 556, 12, "Well1", 22),
+            (235, 559, 14, "Well2", 44),
+            (255, 577, 12, "Well3", 55)]
+        attrs = {"WellName": "str", "ID", "int"}
+        mypoints = Points(values=plist, attributes=attrs)
+
+    And points can be initialised from a 2D numpy array or an existing dataframe::
+
+        >>> mypoints1 = Points(values=[(1,1,1), (2,2,2), (3,3,3)])
+        >>> mypoints2 = Points(
+        ...     values=pd.DataFrame(
+        ...          [[1, 2, 3], [1, 2, 3], [1, 2, 3]],
+        ...          columns=["X_UTME", "Y_UTMN", "Z_TVDSS"]
+        ...     )
+        ... )
+
+
+    Similar as for lists, attributes are alse possible for numpy and dataframes.
+
+    Default column names in the dataframe:
+
+    * X_UTME: UTM X coordinate  as self._xname
+    * Y_UTMN: UTM Y coordinate  as self._yname
+    * Z_TVDSS: Z coordinate, often depth below TVD SS, but may also be
+      something else! Use zname attribute to change name.
+
+    Note:
+        Attributes may have undefined entries. Pandas version 0.21 (which is applied
+        for RMS version up to 12.0.x) do not support NaN values for Integers. The
+        solution is store undefined values as large numbers, xtgeo.UNDEF_INT
+        (2000000000) for integers and xtgeo.UNDEF (10e32) for float values.
+        This will change from xtgeo version 3.x where Pandas version 1 and
+        above will be required, which in turn support will pandas.NA
+        entries.
+
+    Args:
+        values: Provide input values on various forms (list-like or dataframe).
+        xname: Name of first (X) mandatory column, default is X_UTME.
+        yname: Name of second (Y) mandatory column, default is Y_UTMN.
+        zname: Name of third (Z) mandatory column, default is Z_TVDSS.
+        attributes: A dictionary for attribute columns as 'name: type', e.g.
+            {"WellName": "str", "IX": "int"}. This is applied when values are input
+            and is to name and type the extra attribute columns in a point set.
+    """
+
+    @_allow_deprecated_init
+    def __init__(
+        self,
+        values: Union[list, np.ndarray, pd.DataFrame] = None,
+        xname: str = "X_UTME",
+        yname: str = "Y_UTMN",
+        zname: str = "Z_TVDSS",
+        attributes: Optional[dict] = None,
+        filesrc: str = None,
+    ):
+        """Initialisation of Points()."""
+        super().__init__(xname, yname, zname)
+        if values is None:
+            values = []
+        self._reset(
+            values=values,
+            xname=xname,
+            yname=yname,
+            zname=zname,
+            attributes=attributes,
+            filesrc=filesrc,
+        )
+
+    def _reset(
+        self,
+        values: Union[list, np.ndarray, pd.DataFrame] = None,
+        xname: str = "X_UTME",
+        yname: str = "Y_UTMN",
+        zname: str = "Z_TVDSS",
+        attributes: Optional[dict] = None,
+        filesrc: str = None,
+    ):  # pylint: disable=arguments-differ
+        """Used in deprecated methods."""
+        super()._reset(xname, yname, zname)
+
+        self._attrs = attributes if attributes is not None else dict()
+        self._filesrc = filesrc
+
+        if not isinstance(values, pd.DataFrame):
+            self._df = _xyz_io._from_list_like(values, self._zname, attributes, False)
+        else:
+            self._df: pd.DataFrame = values
+            self._dataframe_consistency_check()
+
+    def _dataframe_consistency_check(self):
+        if self.xname not in self.dataframe:
+            raise ValueError(
+                f"xname={self.xname} is not a column "
+                f"of dataframe {self.dataframe.columns}"
+            )
+        if self.yname not in self.dataframe:
+            raise ValueError(
+                f"yname={self.yname} is not a column "
+                f"of dataframe {self.dataframe.columns}"
+            )
+        if self.zname not in self.dataframe:
+            raise ValueError(
+                f"zname={self.zname} is not a column "
+                f"of dataframe {self.dataframe.columns}"
+            )
+        for attr in self._attrs:
+            if attr not in self.dataframe:
+                raise ValueError(
+                    f"Attribute {attr} is not a column "
+                    f"of dataframe {self.dataframe.columns}"
+                )
+
+    def __repr__(self):
+        # should be able to newobject = eval(repr(thisobject))
+        myrp = f"{self.__class__.__name__} (filesrc={self._filesrc!r}, ID={id(self)})"
+        return myrp
+
+    def __str__(self):
+        """User friendly print."""
+        return self.describe(flush=False)
+
+    def __eq__(self, value):
+        """Magic method for ==."""
+        return self.dataframe[self.zname] == value
+
+    def __gt__(self, value):
+        return self.dataframe[self.zname] > value
+
+    def __ge__(self, value):
+        return self.dataframe[self.zname] >= value
+
+    def __lt__(self, value):
+        return self.dataframe[self.zname] < value
+
+    def __le__(self, value):
+        return self.dataframe[self.zname] <= value
+
+    # ----------------------------------------------------------------------------------
+    # Methods
+    # ----------------------------------------------------------------------------------
+
+    @property
+    def dataframe(self) -> pd.DataFrame:
+        """Returns or set the Pandas dataframe object."""
+        return self._df
+
+    @dataframe.setter
+    def dataframe(self, df):
+        self._df = df.apply(deepcopy)
+
+    def _random(self, nrandom=10):
+        """Generate nrandom random points within the range 0..1
+
+        Args:
+            nrandom (int): Number of random points (default 10)
+
+        .. versionadded:: 2.3
+        """
+
+        # currently a non-published method
+
+        self._df = pd.DataFrame(
+            np.random.rand(nrandom, 3), columns=[self._xname, self._yname, self._zname]
+        )
+
+    @inherit_docstring(inherit_from=XYZ.from_file)
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.points_from_file() instead",
+    )
+    def from_file(self, pfile, fformat="xyz"):
+        self._reset(**_file_importer(pfile, fformat))
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.points_from_roxar() instead.",
+    )
+    def from_roxar(
+        self,
+        project: Union[str, Any],
+        name: str,
+        category: str,
+        stype: str = "horizons",
+        realisation: int = 0,
+        attributes: bool = False,
+    ):  # pragma: no cover
+        """Load a points/polygons item from a Roxar RMS project (deprecated).
+
+        The import from the RMS project can be done either within the project
+        or outside the project.
+
+        Note that the preferred shortform for (use polygons as example)::
+
+          import xtgeo
+          mypoly = xtgeo.xyz.Polygons()
+          mypoly.from_roxar(project, 'TopAare', 'DepthPolys')
+
+        is now::
+
+          import xtgeo
+          mysurf = xtgeo.polygons_from_roxar(project, 'TopAare', 'DepthPolys')
+
+        Note also that horizon/zone/faults name and category must exists
+        in advance, otherwise an Exception will be raised.
+
+        Args:
+            project (str or special): Name of project (as folder) if
+                outside RMS, og just use the magic project word if within RMS.
+            name (str): Name of polygons item
+            category (str): For horizons/zones/faults: for example 'DL_depth'
+                or use a folder notation on clipboard/general2d_data.
+
+            stype (str): RMS folder type, 'horizons' (default) or 'zones' etc!
+            realisation (int): Realisation number, default is 0
+            attributes (bool): If True, attributes will be preserved (from RMS 11)
+
+        Returns:
+            Object instance updated
+
+        Raises:
+            ValueError: Various types of invalid inputs.
+
+        .. deprecated:: 2.16
+           Use xtgeo.points_from_roxar() or xtgeo.polygons_from_roxar()
+        """
+        self._reset(
+            **_roxar_importer(
+                project,
+                name,
+                category,
+                stype,
+                realisation,
+                attributes,
+            )
+        )
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use "
+        "xtgeo.Points("
+        "values=dfr[[east, nort, tvdsml]], xname=east, yname=north, zname=tvdmsl"
+        ") instead",
+    )
+    def from_dataframe(self, dfr, east="X", north="Y", tvdmsl="Z", attributes=None):
+        """Import points/polygons from existing Pandas dataframe.
+
+        Args:
+            dfr (dataframe): Pandas dataframe.
+            east (str): Name of easting column in input dataframe.
+            north (str): Name of northing column in input dataframe.
+            tvdmsl (str): Name of depth column in input dataframe.
+            attributes (dict): Additional metadata columns, on form {"IX": "I", ...};
+                "IX" here is the name of the target column, and "I" is the name in the
+                input dataframe.
+
+        .. versionadded:: 2.13
+        .. deprecated:: 2.16 Use points constructor directly instead
+        """
+        if not all(item in dfr.columns for item in (east, north, tvdmsl)):
+            raise ValueError("One or more column names are not correct")
+
+        if attributes and not all(item in dfr.columns for item in attributes.values()):
+            raise ValueError("One or more attribute column names are not correct")
+
+        input = OrderedDict()
+        input["X_UTME"] = dfr[east]
+        input["Y_UTMN"] = dfr[north]
+        input["Z_TVDSS"] = dfr[tvdmsl]
+
+        if attributes:
+            for target, source in attributes.items():
+                input[target] = dfr[source]
+
+        df = pd.DataFrame(input)
+        df.dropna(inplace=True)
+        self._reset(values=df, filesrc="DataFrame input")
+
+    def to_file(
+        self,
+        pfile,
+        fformat="xyz",
+        attributes=True,
+        pfilter=None,
+        wcolumn=None,
+        hcolumn=None,
+        mdcolumn="M_MDEPTH",
+        **kwargs,
+    ):  # pylint: disable=redefined-builtin
+        """Export Points to file.
+
+        Args:
+            pfile (str): Name of file
+            fformat (str): File format xyz/poi/pol or rms_attr
+            attributes (bool or list): List of extra columns to export (some formats)
+                or True for all attributes present
+            pfilter (dict): Filter on e.g. top name(s) with keys TopName
+                or ZoneName as {'TopName': ['Top1', 'Top2']}.
+            wcolumn (str): Name of well column (rms_wellpicks format only)
+            hcolumn (str): Name of horizons column (rms_wellpicks format only)
+            mdcolumn (str): Name of MD column (rms_wellpicks format only)
+
+        Returns:
+            Number of points exported
+
+        Note that the rms_wellpicks will try to output to:
+
+        * HorizonName, WellName, MD  if a MD (mdcolumn) is present,
+        * HorizonName, WellName, X, Y, Z  otherwise
+
+        Note:
+            For backward compatibility, the key ``filter`` can be applied instead of
+            ``pfilter``.
+
+        Raises:
+            KeyError if pfilter is set and key(s) are invalid
+
+        """
+        return _xyz_io.to_file(
+            self,
+            pfile,
+            fformat=fformat,
+            attributes=attributes,
+            pfilter=pfilter,
+            wcolumn=wcolumn,
+            hcolumn=hcolumn,
+            mdcolumn=mdcolumn,
+            **kwargs,
+        )
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.points_from_wells() instead.",
+    )
+    def from_wells(
+        self,
+        wells,
+        tops=True,
+        incl_limit=None,
+        top_prefix="Top",
+        zonelist=None,
+        use_undef=False,
+    ):
+        """Get tops or zone points data from a list of wells.
+
+        Args:
+            wells (list): List of XTGeo well objects
+            tops (bool): Get the tops if True (default), otherwise zone
+            incl_limit (float): Inclination limit for zones (thickness points)
+            top_prefix (str): Prefix used for Tops
+            zonelist (list-like): Which zone numbers to apply.
+            use_undef (bool): If True, then transition from UNDEF within zonelog
+                is also used.
+
+        Returns:
+            None if well list is empty; otherwise the number of wells.
+
+        Raises:
+            Todo
+
+        .. deprecated:: 2.16
+           Use classmethod :py:func:`points_from_wells()` instead
+        """
+        self._reset(
+            **_wells_importer(wells, tops, incl_limit, top_prefix, zonelist, use_undef)
+        )
+        return self.dataframe["WellName"].nunique()
+
+    @inherit_docstring(inherit_from=XYZ.from_list)
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use direct Points() initialisation instead",
+    )
+    def from_list(self, plist):
+        self._reset(_xyz_io._from_list_like(plist, "Z_TVDSS", None, False))
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.points_from_wells_dfrac() instead.",
+    )
+    def dfrac_from_wells(
+        self,
+        wells,
+        dlogname,
+        dcodes,
+        incl_limit=90,
+        count_limit=3,
+        zonelist=None,
+        zonelogname=None,
+    ):
+        """Get fraction of discrete code(s) (e.g. facies) per zone.
+
+        Args:
+            wells (list): List of XTGeo well objects
+            dlogname (str): Name of discrete log (e.g. Facies)
+            dcodes (list of int): Code(s) to get fraction for, e.g. [3]
+            incl_limit (float): Inclination limit for zones (thickness points)
+            count_limit (int): Min. no of counts per segment for valid result
+            zonelist (list of int): Whihc zones to compute for (default None
+                means that all zones will be evaluated)
+            zonelogname (str): Name of zonelog; if None than the
+                well.zonelogname property will be applied.
+
+        Returns:
+            None if well list is empty; otherwise the number of wells.
+
+        Raises:
+            Todo
+
+        .. deprecated:: 2.16
+           Use classmethod :py:func:`points_from_wells_dfrac()` instead.
+        """
+
+        self._reset(
+            **_wells_dfrac_importer(
+                wells, dlogname, dcodes, incl_limit, count_limit, zonelist, zonelogname
+            )
+        )
+
+    def to_roxar(
+        self,
+        project,
+        name,
+        category,
+        stype="horizons",
+        pfilter=None,
+        realisation=0,
+        attributes=False,
+    ):  # pragma: no cover
+        """Export (store) a Points item to a Roxar RMS project.
+
+        The export to the RMS project can be done either within the project
+        or outside the project.
+
+        Note also that horizon/zone name and category must exists in advance,
+        otherwise an Exception will be raised.
+
+        Note:
+            When project is file path (direct access, outside RMS) then
+            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+            will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str or special): Name of project (as folder) if
+                outside RMS, og just use the magic project word if within RMS.
+            name (str): Name of polygons item
+            category (str): For horizons/zones/faults: for example 'DL_depth'
+            pfilter (dict): Filter on e.g. top name(s) with keys TopName
+                or ZoneName as {'TopName': ['Top1', 'Top2']}
+            stype (str): RMS folder type, 'horizons' (default), 'zones'
+                or 'faults' or 'clipboard', general2d_data (in prep: well picks)
+            realisation (int): Realisation number, default is 0
+            attributes (bool): If True, attributes will be preserved (from RMS 11)
+
+
+        Returns:
+            Object instance updated
+
+        Raises:
+            ValueError: Various types of invalid inputs.
+            NotImplementedError: Not supported in this ROXAPI version
+
+        .. versionadded:: 2.19 general2d_data support is added
+        """
+
+        valid_stypes = [
+            "horizons",
+            "zones",
+            "faults",
+            "clipboard",
+            "general2d_data",
+            "horizon_picks",
+        ]
+
+        if stype.lower() not in valid_stypes:
+            raise ValueError(f"Invalid stype, only {valid_stypes} stypes is supported.")
+
+        _xyz_roxapi.export_xyz_roxapi(
+            self,
+            project,
+            name,
+            category,
+            stype,
+            pfilter,
+            realisation,
+            attributes,
+        )
+
+    def copy(self):
+        """Returns a deep copy of an instance."""
+        mycopy = self.__class__()
+        mycopy._df = self._df.apply(deepcopy)
+        mycopy._xname = self._xname
+        mycopy._yname = self._yname
+        mycopy._zname = self._zname
+
+        return mycopy
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.points_from_surface() instead",
+    )
+    def from_surface(self, surf, zname="Z_TVDSS"):
+        """Get points as X Y Value from a surface object nodes (deprecated).
+
+        Note that undefined surface nodes will not be included.
+
+        Args:
+            surf (RegularSurface): A XTGeo RegularSurface object instance.
+            zname (str): Name of value column (3'rd column)
+
+        Example::
+
+            topx = RegularSurface('topx.gri')
+            topx_aspoints = Points()
+            topx_aspoints.from_surface(topx)
+
+            # alternative shortform:
+            topx_aspoints = Points(topx)  # get an instance directly
+
+            topx_aspoints.to_file('mypoints.poi')  # export as XYZ file
+
+        .. deprecated:: 2.16 Use xtgeo.points_from_surface() instead.
+        """
+
+        self._reset(**_surface_importer(surf, zname=zname))
+
+    def snap_surface(self, surf, activeonly=True):
+        """Snap (transfer) the points Z values to a RegularSurface
+
+        Args:
+            surf (~xtgeo.surface.regular_surface.RegularSurface): Surface to snap to.
+            activeonly (bool): If True (default), the points outside the defined surface
+                will be removed. If False, these points will keep the original values.
+
+        Returns:
+            None (instance is updated inplace)
+
+        Raises:
+            ValueError: Input object of wrong data type, must be RegularSurface
+            RuntimeError: Error code from C routine surf_get_zv_from_xyv is ...
+
+        .. versionadded:: 2.1
+
+        """
+        _xyz_oper.snap_surface(self, surf, activeonly=activeonly)
+
+    @inherit_docstring(inherit_from=XYZ.get_boundary)
+    def get_boundary(self):
+        return super().get_boundary()
```

## xtgeo/xyz/polygons.py

 * *Ordering differences only*

```diff
@@ -1,864 +1,864 @@
-"""XTGeo xyz.polygons module, which contains the Polygons class."""
-
-# For polygons, the order of the points sequence is important. In
-# addition, a Polygons dataframe _must_ have a INT column called 'POLY_ID'
-# which identifies each polygon piece.
-import functools
-import io
-import pathlib
-import warnings
-from copy import deepcopy
-from typing import Any, List, Optional, Union
-
-import deprecation
-import numpy as np
-import pandas as pd
-import shapely.geometry as sg
-
-import xtgeo
-from xtgeo.common import inherit_docstring
-from xtgeo.xyz import _xyz_io, _xyz_roxapi
-
-from . import _polygons_oper, _xyz_oper
-from ._xyz import XYZ
-from ._xyz_io import _convert_idbased_xyz
-
-xtg = xtgeo.common.XTGeoDialog()
-logger = xtg.functionlogger(__name__)
-
-
-def _data_reader_factory(file_format):
-    if file_format == "xyz":
-        return _xyz_io.import_xyz
-    if file_format == "zmap_ascii":
-        return _xyz_io.import_zmap
-    raise ValueError(f"Unknown file format {file_format}")
-
-
-def _file_importer(
-    pfile: Union[str, pathlib.Path, io.BytesIO],
-    fformat: Optional[str] = None,
-):
-    """General function for polygons_from_file and (deprecated) method from_file."""
-    pfile = xtgeo._XTGeoFile(pfile)
-    if fformat is None or fformat == "guess":
-        fformat = pfile.detect_fformat()
-    else:
-        fformat = pfile.generic_format_by_proposal(fformat)  # default
-    kwargs = _data_reader_factory(fformat)(pfile)
-
-    if "POLY_ID" not in kwargs["values"].columns:
-        kwargs["values"]["POLY_ID"] = (
-            kwargs["values"].isnull().all(axis=1).cumsum().dropna()
-        )
-        kwargs["values"].dropna(axis=0, inplace=True)
-        kwargs["values"].reset_index(inplace=True, drop=True)
-    kwargs["name"] = "poly"
-    return kwargs
-
-
-def _roxar_importer(
-    project: Union[str, Any],
-    name: str,
-    category: str,
-    stype: Optional[str] = "horizons",
-    realisation: Optional[int] = 0,
-):  # pragma: no cover
-    kwargs = _xyz_roxapi.import_xyz_roxapi(
-        project, name, category, stype, realisation, None, True
-    )
-
-    kwargs["name"] = "poly"
-    return kwargs
-
-
-def _wells_importer(
-    wells: List[xtgeo.Well],
-    zone: Optional[int] = None,
-    resample: Optional[int] = 1,
-):
-    """Get line segments from a list of wells and a single zone number.
-
-    A future extension is that zone could be a list of zone numbers and/or mechanisms
-    to retrieve well segments by other measures, e.g. >= depth.
-    """
-
-    dflist = []
-    maxid = 0
-    for well in wells:
-        wp = well.get_zone_interval(zone, resample=resample)
-        if wp is not None:
-            wp["WellName"] = well.name
-            # as well segments may have overlapping POLY_ID:
-            wp["POLY_ID"] += maxid
-            maxid = wp["POLY_ID"].max() + 1
-            dflist.append(wp)
-
-    if not dflist:
-        return {}
-    dfr = pd.concat(dflist, ignore_index=True)
-    dfr.reset_index(inplace=True, drop=True)
-    return {
-        "values": dfr,
-        "attributes": {"WellName": "str"},
-    }
-
-
-def polygons_from_file(
-    pfile: Union[str, pathlib.Path], fformat: Optional[str] = "guess"
-):
-    """Make an instance of a Polygons object directly from file import.
-
-    Supported formats are:
-
-        * 'xyz' or 'pol': Simple XYZ format
-        * 'zmap': ZMAP line format as exported from RMS (e.g. fault lines)
-        * 'guess': Try to choose file format based on extension
-
-    Args:
-        pfile (str): Name of file
-        fformat (str): See :meth:`Polygons.from_file`
-
-    Example::
-
-        import xtgeo
-        mypoly = xtgeo.polygons_from_file('somefile.xyz')
-    """
-    return Polygons(**_file_importer(pfile, fformat=fformat))
-
-
-def polygons_from_roxar(
-    project: Union[str, Any],
-    name: str,
-    category: str,
-    stype: Optional[str] = "horizons",
-    realisation: Optional[int] = 0,
-):  # pragma: no cover
-    """Load a Polygons instance from Roxar RMS project.
-
-    Note also that horizon/zone/faults name and category must exists
-    in advance, otherwise an Exception will be raised.
-
-    Args:
-        project: Name of project (as folder) if outside RMS, or just use the magic
-            `project` word if within RMS.
-        name: Name of polygons item
-        category: For horizons/zones/faults: for example 'DL_depth'
-            or use a folder notation on clipboard/general2d_data.
-        stype: RMS folder type, 'horizons' (default), 'zones', 'clipboard',
-            'faults', 'general2d_data'
-        realisation: Realisation number, default is 0
-
-    Example::
-
-        import xtgeo
-        mysurf = xtgeo.polygons_from_roxar(project, 'TopAare', 'DepthPolys')
-
-    .. versionadded:: 2.19 general2d_data support is added
-    """
-
-    return Polygons(
-        **_roxar_importer(
-            project,
-            name,
-            category,
-            stype,
-            realisation,
-        )
-    )
-
-
-def polygons_from_wells(
-    wells: List[xtgeo.Well],
-    zone: Optional[int] = 1,
-    resample: Optional[int] = 1,
-):
-    """Get polygons from wells and a single zone number.
-
-    Args:
-        wells: List of XTGeo well objects, a single XTGeo well or a list of well files.
-            If a list of well files, the routine will try to load well based on file
-            signature and/or extension, but only default settings are applied. Hence
-            this is less flexible and more fragile.
-        zone: The zone number to extract the linepiece from
-        resample: If given, resample every N'th sample to make
-            polylines smaller in terms of bits and bytes.
-            1 = No resampling, which means just use well sampling (which can be rather
-            dense; typically 15 cm).
-
-
-    Returns:
-        None if empty data, otherwise a Polygons() instance.
-
-    Example::
-
-        wells = ["w1.w", "w2.w"]
-        points = xtgeo.polygons_from_wells(wells, zone=2)
-
-    Note:
-        This method replaces the deprecated method :py:meth:`~Polygons.from_wells`.
-        The latter returns the number of wells that contribute with polygon segments.
-        This is now implemented through the function `get_nwells()`. Hence the
-        following code::
-
-            nwells_applied = poly.from_wells(...)  # deprecated method
-            # vs
-            poly = xtgeo.polygons_from_wells(...)
-            nwells_applied = poly.get_nwells()
-
-    .. versionadded: 2.16
-    """
-    return Polygons(**_wells_importer(wells, zone, resample))
-
-
-def _allow_deprecated_init(func):
-    # This decorator is here to maintain backwards compatibility in the construction
-    # of Polygons and should be deleted once the deprecation period has expired,
-    # the construction will then follow the new pattern.
-    # Introduced post xtgeo version 2.15
-    @functools.wraps(func)
-    def wrapper(cls, *args, **kwargs):
-        # Checking if we are doing an initialization from file or surface and raise a
-        # deprecation warning if we are.
-        if len(args) == 1 and isinstance(args[0], (str, pathlib.Path)):
-            warnings.warn(
-                "Initializing directly from file name is deprecated and will be "
-                "removed in xtgeo version 4.0. Use: "
-                "pol = xtgeo.polygons_from_file('some_file.xx') instead!",
-                DeprecationWarning,
-            )
-            fformat = kwargs.get("fformat", "guess")
-            return func(cls, **_file_importer(args[0], fformat))
-
-        return func(cls, *args, **kwargs)
-
-    return wrapper
-
-
-class Polygons(XYZ):  # pylint: disable=too-many-public-methods
-    """Class for a Polygons object (connected points) in the XTGeo framework.
-
-    The term Polygons is here used in a wider context, as it includes
-    polylines that do not connect into closed polygons. A Polygons
-    instance may contain several pieces of polylines/polygons, which are
-    identified by POLY_ID.
-
-    The polygons are stored in Python as a Pandas dataframe, which
-    allow for flexible manipulation and fast execution.
-
-    A Polygons instance will have 4 mandatory columns; here by default names:
-
-    * X_UTME - for X UTM coordinate (Easting)
-    * Y_UTMN - For Y UTM coordinate (Northing)
-    * Z_TVDSS - For depth or property from mean SeaLevel; Depth positive down
-    * POLY_ID - for polygon ID as there may be several polylines segments
-
-    Each Polygons instance can also a name (through the name attribute).
-    Default is 'poly'. E.g. if a well fence, it is logical to name the
-    instance to be the same as the well name.
-
-    Args:
-        values: Provide input values on various forms (list-like or dataframe).
-        xname: Name of first (X) mandatory column, default is X_UTME.
-        yname: Name of second (Y) mandatory column, default is Y_UTMN.
-        zname: Name of third (Z) mandatory column, default is Z_TVDSS.
-        pname: Name of forth (P) mandatory enumerating column, default is POLY_ID.
-        hname: Name of cumulative horizontal length, defaults to "H_CUMLEN" if
-            in dataframe otherwise None.
-        dhname: Name of delta horizontal length, defaults to "H_DELTALEN" if in
-            dataframe otherwise None.
-        tname: Name of cumulative total length, defaults to "T_CUMLEN" if in
-            dataframe otherwise None.
-        dtname: Name of delta total length, defaults to "T_DELTALEN" if in
-            dataframe otherwise None.
-        attributes: A dictionary for attribute columns as 'name: type', e.g.
-            {"WellName": "str", "IX": "int"}. This is applied when values are input
-            and is to name and type the extra attribute columns in a point set.
-    """
-
-    @_allow_deprecated_init
-    def __init__(
-        self,
-        values: Union[list, np.ndarray, pd.DataFrame] = None,
-        xname: str = "X_UTME",
-        yname: str = "Y_UTMN",
-        zname: str = "Z_TVDSS",
-        pname: str = "POLY_ID",
-        hname: str = "H_CUMLEN",
-        dhname: str = "H_DELTALEN",
-        tname: str = "T_CUMLEN",
-        dtname: str = "T_DELTALEN",
-        name: str = "poly",
-        attributes: Optional[dict] = None,
-        # from legacy initialization, remove in 4.0, undocumented by purpose:
-        fformat: str = "guess",
-    ):
-        super().__init__(xname, yname, zname)
-
-        if values is None:
-            values = []
-
-        logger.info("Legacy fformat key with value %s shall be removed in 4.0", fformat)
-
-        self._reset(
-            values=values,
-            xname=xname,
-            yname=yname,
-            zname=zname,
-            pname=pname,
-            hname=hname,
-            dhname=dhname,
-            tname=tname,
-            dtname=dtname,
-            name=name,
-            attributes=attributes,
-        )
-
-    def _reset(  # pylint: disable=arguments-renamed
-        self,
-        values: Union[list, np.ndarray, pd.DataFrame],
-        xname: str = "X_UTME",
-        yname: str = "Y_UTMN",
-        zname: str = "Z_TVDSS",
-        pname: str = "POLY_ID",
-        hname: str = "H_CUMLEN",
-        dhname: str = "H_DELTALEN",
-        tname: str = "T_CUMLEN",
-        dtname: str = "T_DELTALEN",
-        name: str = "poly",
-        attributes: Optional[dict] = None,
-    ):  # pylint: disable=arguments-differ
-        """Used in deprecated methods."""
-
-        super()._reset(xname, yname, zname)
-        # additonal state properties for Polygons
-        self._pname = pname
-
-        self._hname = hname
-        self._dhname = dhname
-        self._tname = tname
-        self._dtname = dtname
-        self._name = name
-
-        if not isinstance(values, pd.DataFrame):
-            self._df = _xyz_io._from_list_like(values, self._zname, attributes, True)
-        else:
-            self._df = values
-
-    @property
-    def name(self):
-        """Returns or sets the name of the instance."""
-        return self._name
-
-    @name.setter
-    def name(self, newname):
-        self._name = newname
-
-    @property
-    def pname(self):
-        return self._pname
-
-    @pname.setter
-    def pname(self, value):
-        super()._check_name(value)
-        self._pname = value
-
-    @property
-    def hname(self):
-        """Returns or set the name of the cumulative horizontal length.
-
-        If the column does not exist, None is returned. Default name is H_CUMLEN.
-
-        .. versionadded:: 2.1
-        """
-        return self._hname
-
-    @hname.setter
-    def hname(self, value):
-        super()._check_name(value)
-        self._hname = value
-
-    @property
-    def dhname(self):
-        """Returns or set the name of the delta horizontal length column if it exists.
-
-        If the column does not exist, None is returned. Default name is H_DELTALEN.
-
-        .. versionadded:: 2.1
-        """
-        return self._dhname
-
-    @dhname.setter
-    def dhname(self, value):
-        super()._check_name(value)
-        self._dhname = value
-
-    @property
-    def tname(self):
-        """Returns or set the name of the cumulative total length column if it exists.
-
-        .. versionadded:: 2.1
-        """
-        return self._tname
-
-    @tname.setter
-    def tname(self, value):
-        super()._check_name(value)
-        self._tname = value
-
-    @property
-    def dtname(self):
-        """Returns or set the name of the delta total length column if it exists.
-
-        .. versionadded:: 2.1
-        """
-        return self._dtname
-
-    @dtname.setter
-    def dtname(self, value):
-        super()._check_name(value)
-        self._dtname = value
-
-    @property
-    def dataframe(self) -> pd.DataFrame:
-        """Returns or set the Pandas dataframe object."""
-        return self._df
-
-    @dataframe.setter
-    def dataframe(self, df):
-        self._df = df.apply(deepcopy)
-        self._name_to_none_if_missing()
-
-    def _name_to_none_if_missing(self):
-        if self._dtname not in self._df.columns:
-            self._dtname = None
-        if self._dhname not in self._df.columns:
-            self._dhname = None
-        if self._tname not in self._df.columns:
-            self._tname = None
-        if self._hname not in self._df.columns:
-            self._hname = None
-
-    # ----------------------------------------------------------------------------------
-    # Class methods
-    # ----------------------------------------------------------------------------------
-
-    @classmethod
-    def boundary_from_points(
-        cls,
-        points,
-        alpha_factor: Optional[float] = 1.0,
-        alpha: Optional[float] = None,
-        convex: bool = False,
-    ):
-        """Instantiate polygons from detecting the boundary around points.
-
-        .. image:: images/boundary_polygons.png
-           :width: 600
-           :align: center
-
-        |
-
-        Args:
-            points: The XTGeo Points instance to estimate boundary/boundaries around.
-            alpha_factor: The alpha factor is a multiplier to alpha. Normally it will
-                be around 1, but can be increased to get a looser boundary. Dependent
-                on the points topology, it can also be decreased to some extent.
-            alpha: The alpha factor for determine the 'precision' in how to delineate
-                the polygon. A large value will produce a smoother polygon. The default
-                is to detect the value from the data, but note that this default may be
-                far from optimal for you needs. Usually use the ``alpha_factor`` to tune
-                the best value. The actual alpha applied in the concave hull algorithm
-                is alpha_factor multiplied with alpha.
-            convex: If True, then compute a maximum boundary (convex), and note that
-                alpha_factor and alpha are not applied in ths case. Default is False.
-
-        Returns:
-            A Polygons instance.
-
-        .. versionadded: 3.1.0
-        """
-
-        return cls(
-            _polygons_oper.boundary_from_points(points, alpha_factor, alpha, convex)
-        )
-
-    # ----------------------------------------------------------------------------------
-    # Instance methods
-    # ----------------------------------------------------------------------------------
-    @inherit_docstring(inherit_from=XYZ.protected_columns)
-    def protected_columns(self):
-        return super().protected_columns() + [self.pname]
-
-    @inherit_docstring(inherit_from=XYZ.from_file)
-    @deprecation.deprecated(
-        deprecated_in="2.21",  # should have been 2.16, but was forgotten until 2.21
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.polygons_from_file() instead",
-    )
-    def from_file(self, pfile, fformat="xyz"):
-        self._reset(**_file_importer(pfile, fformat))
-
-    def to_file(
-        self,
-        pfile,
-        fformat="xyz",
-    ):
-        """Export Polygons to file.
-
-        Args:
-            pfile (str): Name of file
-            fformat (str): File format xyz/poi/pol
-
-        Returns:
-            Number of polygon points exported
-        """
-
-        return _xyz_io.to_file(self, pfile, fformat=fformat, ispolygons=True)
-
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use xtgeo.polygons_from_wells(...) instead",
-    )
-    def from_wells(self, wells, zone, resample=1):
-        """Get line segments from a list of wells and a single zone number.
-
-        Args:
-            wells (list): List of XTGeo well objects
-            zone (int): Which zone to apply
-            resample (int): If given, resample every N'th sample to make
-                polylines smaller in terms of bits and bytes.
-                1 = No resampling which means well sampling (which can be rather
-                dense; typically 15 cm).
-
-        Returns:
-            None if well list is empty; otherwise the number of wells that
-            have one or more line segments to return
-
-        """
-        if not wells:
-            return None
-
-        self._reset(**_wells_importer(wells, zone, resample))
-
-        nwells = self.dataframe["WellName"].nunique()
-        # as the previous versions did not have the WellName column, this is dropped
-        # here for backward compatibility:
-        self.dataframe = self.dataframe.drop("WellName", axis=1)
-
-        if nwells == 0:
-            return None
-        else:
-            return nwells
-
-    def to_roxar(
-        self,
-        project,
-        name,
-        category,
-        stype="horizons",
-        realisation=0,
-    ):  # pragma: no cover
-        """Export (store) a Polygons item to a Roxar RMS project.
-
-        The export to the RMS project can be done either within the project
-        or outside the project.
-
-        Note also that horizon/zone name and category must exists in advance,
-        otherwise an Exception will be raised.
-
-        Note:
-            When project is file path (direct access, outside RMS) then
-            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
-            will not be saved until the user do an explicit project save action.
-
-        Args:
-            project (str or special): Name of project (as folder) if
-                outside RMS, og just use the magic project word if within RMS.
-            name (str): Name of polygons item
-            category (str): For horizons/zones/faults: for example 'DL_depth' and use
-                a folder notation for clipboard/general2d_data
-            stype (str): RMS folder type, 'horizons' (default), 'zones'
-                or 'faults' or 'clipboard'  (in prep: well picks)
-            realisation (int): Realisation number, default is 0
-
-
-        Returns:
-            Object instance updated
-
-        Raises:
-            ValueError: Various types of invalid inputs.
-            NotImplementedError: Not supported in this ROXAPI version
-
-        .. versionadded:: 2.19 general2d_data support is added
-        """
-
-        _xyz_roxapi.export_xyz_roxapi(
-            self,
-            project,
-            name,
-            category,
-            stype,
-            None,
-            realisation,
-            None,
-        )
-
-    def copy(self):
-        """Returns a deep copy of an instance"""
-        mycopy = self.__class__()
-        mycopy._df = self._df.apply(deepcopy)  # df.copy() is not fully deep!
-        mycopy._xname = self._xname
-        mycopy._yname = self._yname
-        mycopy._zname = self._zname
-        mycopy._pname = self._pname
-        mycopy._hname = self._hname
-        mycopy._dhname = self._dhname
-        mycopy._tname = self._tname
-        mycopy._dtname = self._dtname
-
-        return mycopy
-
-    @inherit_docstring(inherit_from=XYZ.from_list)
-    @deprecation.deprecated(
-        deprecated_in="2.16",
-        removed_in="4.0",
-        current_version=xtgeo.version,
-        details="Use direct Polygons() initialisation instead",
-    )
-    def from_list(self, plist):
-        kwargs = {}
-        kwargs["values"] = _xyz_io._from_list_like(plist, "Z_TVDSS", None, True)
-        self._reset(**kwargs)
-
-    def get_xyz_dataframe(self):
-        """Get a dataframe copy from the Polygons points with no ID column.
-
-        Convert from POLY_ID based to XYZ, where a new polygon is marked with a 999
-        value as flag.
-        """
-        return _convert_idbased_xyz(self, self.dataframe)
-
-    def get_shapely_objects(self):
-        """Returns a list of Shapely LineString objects, one per POLY_ID.
-
-        .. versionadded:: 2.1
-        """
-        spolys = []
-        idgroups = self.dataframe.groupby(self.pname)
-
-        for _idx, grp in idgroups:
-            pxcor = grp[self.xname].values
-            pycor = grp[self.yname].values
-            pzcor = grp[self.zname].values
-            spoly = sg.LineString(np.stack([pxcor, pycor, pzcor], axis=1))
-            spolys.append(spoly)
-
-        return spolys
-
-    @inherit_docstring(inherit_from=XYZ.get_boundary)
-    def get_boundary(self):
-        return super().get_boundary()
-
-    def simplify(
-        self, tolerance: Optional[float] = 0.1, preserve_topology: Optional[bool] = True
-    ) -> bool:
-        """Simply a polygon, i.e. remove unneccesary points.
-
-        This is based on `Shapely's simplify() method
-        <https://shapely.readthedocs.io/en/latest/manual.html#object.simplify>`_
-
-        Args:
-            tolerance: Cf. Shapely's documentation
-            preserve_topology: Default is True, if False a faster algorithm is applied
-
-        Returns:
-            True if simplification is achieved. The polygons instance is
-            updated in-place.
-
-        .. versionadded: 3.1
-
-
-        """
-
-        return _polygons_oper.simplify_polygons(self, tolerance, preserve_topology)
-
-    def filter_byid(self, polyid=None):
-        """Remove all line segments not in polyid.
-
-        The instance is updated in-place.
-
-        Args:
-            polyid (int or list of int): Which ID(s) to keep, None means use first.
-
-        Example::
-
-            mypoly.filter_byid(polyid=[2, 4])  # keep POLY_ID 2 and 4
-
-        .. versionadded:: 2.1
-        """
-        if polyid is None:
-            polyid = int(self.dataframe[self.pname].iloc[0])
-
-        if not isinstance(polyid, list):
-            polyid = [polyid]
-
-        dflist = []
-        for pid in polyid:
-            dflist.append(self.dataframe[self.dataframe[self.pname] == pid])
-
-        self.dataframe = pd.concat(dflist)
-
-    def tlen(self, tname="T_CUMLEN", dtname="T_DELTALEN", atindex=0):
-        """Compute and add or replace columns for cum. total 3D length and delta length.
-
-        The instance is updated in-place.
-
-        Args:
-            tname (str): Name of cumulative total length. Default is T_CUMLEN.
-            dtname (str): Name of delta length column. Default is T_DELTALEN.
-            atindex (int): Which index which shall be 0.0 for cumulative length.
-
-        .. versionadded:: 2.1
-        """
-        _xyz_oper.tlen(self, tname=tname, dtname=dtname, atindex=atindex)
-
-    def hlen(self, hname="H_CUMLEN", dhname="H_DELTALEN", atindex=0):
-        """Compute and add/replace columns for cum. horizontal length and delta length.
-
-        The instance is updated in-place.
-
-        Args:
-            hname (str): Name of cumulative horizontal length. Default is H_CUMLEN.
-            dhname (str): Name of delta length column. Default is H_DELTALEN.
-            atindex (int): Which index which shall be 0.0 for cumulative length.
-
-        .. versionadded:: 2.1
-        """
-        _xyz_oper.hlen(self, hname=hname, dhname=dhname, atindex=atindex)
-
-    def extend(self, distance, nsamples=1, mode2d=True):
-        """Extend polyline by `distance` at both ends, nsmaples times.
-
-        The instance is updated in-place.
-
-        Args:
-            distance (float): The horizontal distance (sampling) to extend
-            nsamples (int): Number of samples to extend.
-            mode2d (bool): XY extension (only True is supported)
-
-        .. versionadded:: 2.1
-        """
-        _xyz_oper.extend(self, distance, nsamples, mode2d)
-
-    def rescale(self, distance, addlen=False, kind="simple", mode2d=True):
-        """Rescale (resample) by using a new increment.
-
-        The increment (distance) may be a horizontal or a True 3D
-        distance dependent on mode2d.
-
-        The instance is updated in-place.
-
-        If the distance is larger than the total input poly-line length,
-        nothing is done. Note that the result distance may differ from the
-        requested distance caused to rounding to fit original length.
-
-        Hence actual distance is input distance +- 50%.
-
-        Args:
-            distance (float): New distance between points
-            addlen (str): If True, total and horizontal cum. and delta length
-                columns will be added.
-            kind (str): What kind of rescaling: slinear/cubic/simple
-            mode2d (bool): The distance may be a 2D (XY) ora 3D (XYZ) mode.
-
-        .. versionchanged:: 2.1 a new algorithm
-
-        """
-        _xyz_oper.rescale_polygons(
-            self, distance=distance, addlen=addlen, kind=kind, mode2d=mode2d
-        )
-
-    def get_fence(
-        self, distance=20, atleast=5, nextend=2, name=None, asnumpy=True, polyid=None
-    ):
-        """Extracts a fence with constant horizontal sampling.
-
-        Additonal H_CUMLEN and H_DELTALEN vectors will be added, suitable for
-        X sections.
-
-        Args:
-            distance (float): New horizontal distance between points
-            atleast (int): Minimum number of points. If the true length/atleast is
-                less than distance, than distance will be be reset to
-                length/atleast. Values below 3 are not permitted
-            nextend (int): Number of samples to extend at each end. Note that
-                in case of internal resetting of distance (due to 'atleast'), then
-                nextend internally will be modified in order to fulfill the
-                initial intention. Hence keep distance*nextend as target.
-            name (str): Name of polygon (if asnumpy=False)
-            asnumpy (bool): Return a [:, 5] numpy array with
-                columns X.., Y.., Z.., HLEN, dH
-            polyid (int): Which POLY_ID to use. Default (if None) is to use the
-                first found.
-
-        Returns:
-            A numpy array (if asnumpy=True) or a new Polygons() object
-
-        .. versionadded:: 2.1
-        """
-        logger.info("Getting fence within a Polygons instance...")
-        return _xyz_oper.get_fence(
-            self,
-            distance=distance,
-            atleast=atleast,
-            nextend=nextend,
-            name=name,
-            asnumpy=asnumpy,
-            polyid=polyid,
-        )
-
-    # ==================================================================================
-    # Plotting
-    # ==================================================================================
-
-    def quickplot(
-        self,
-        filename=None,
-        others=None,
-        title="QuickPlot for Polygons",
-        subtitle=None,
-        infotext=None,
-        linewidth=1.0,
-        color="r",
-    ):
-        """Simple plotting of polygons using matplotlib.
-
-        Args:
-            filename (str): Name of plot file; None will plot to screen.
-            others (list of Polygons): List of other polygon instances to plot
-            title (str): Title of plot
-            subtitle (str): Subtitle of plot
-            infotext (str): Additonal info on plot.
-            linewidth (float): Width of line.
-            color (str): Name of color (may use matplotib shortcuts, e.g. 'r' for 'red')
-        """
-        mymap = xtgeo.plot.Map()
-        mymap.canvas(title=title, subtitle=subtitle, infotext=infotext)
-
-        if others:
-            for other in others:
-                lwid = linewidth / 2.0
-                mymap.plot_polygons(
-                    other, idname=other.pname, linewidth=lwid, color="black"
-                )
-
-        mymap.plot_polygons(self, idname=self.pname, linewidth=linewidth, color=color)
-
-        if filename is None:
-            mymap.show()
-        else:
-            mymap.savefig(filename)
+"""XTGeo xyz.polygons module, which contains the Polygons class."""
+
+# For polygons, the order of the points sequence is important. In
+# addition, a Polygons dataframe _must_ have a INT column called 'POLY_ID'
+# which identifies each polygon piece.
+import functools
+import io
+import pathlib
+import warnings
+from copy import deepcopy
+from typing import Any, List, Optional, Union
+
+import deprecation
+import numpy as np
+import pandas as pd
+import shapely.geometry as sg
+
+import xtgeo
+from xtgeo.common import inherit_docstring
+from xtgeo.xyz import _xyz_io, _xyz_roxapi
+
+from . import _polygons_oper, _xyz_oper
+from ._xyz import XYZ
+from ._xyz_io import _convert_idbased_xyz
+
+xtg = xtgeo.common.XTGeoDialog()
+logger = xtg.functionlogger(__name__)
+
+
+def _data_reader_factory(file_format):
+    if file_format == "xyz":
+        return _xyz_io.import_xyz
+    if file_format == "zmap_ascii":
+        return _xyz_io.import_zmap
+    raise ValueError(f"Unknown file format {file_format}")
+
+
+def _file_importer(
+    pfile: Union[str, pathlib.Path, io.BytesIO],
+    fformat: Optional[str] = None,
+):
+    """General function for polygons_from_file and (deprecated) method from_file."""
+    pfile = xtgeo._XTGeoFile(pfile)
+    if fformat is None or fformat == "guess":
+        fformat = pfile.detect_fformat()
+    else:
+        fformat = pfile.generic_format_by_proposal(fformat)  # default
+    kwargs = _data_reader_factory(fformat)(pfile)
+
+    if "POLY_ID" not in kwargs["values"].columns:
+        kwargs["values"]["POLY_ID"] = (
+            kwargs["values"].isnull().all(axis=1).cumsum().dropna()
+        )
+        kwargs["values"].dropna(axis=0, inplace=True)
+        kwargs["values"].reset_index(inplace=True, drop=True)
+    kwargs["name"] = "poly"
+    return kwargs
+
+
+def _roxar_importer(
+    project: Union[str, Any],
+    name: str,
+    category: str,
+    stype: Optional[str] = "horizons",
+    realisation: Optional[int] = 0,
+):  # pragma: no cover
+    kwargs = _xyz_roxapi.import_xyz_roxapi(
+        project, name, category, stype, realisation, None, True
+    )
+
+    kwargs["name"] = "poly"
+    return kwargs
+
+
+def _wells_importer(
+    wells: List[xtgeo.Well],
+    zone: Optional[int] = None,
+    resample: Optional[int] = 1,
+):
+    """Get line segments from a list of wells and a single zone number.
+
+    A future extension is that zone could be a list of zone numbers and/or mechanisms
+    to retrieve well segments by other measures, e.g. >= depth.
+    """
+
+    dflist = []
+    maxid = 0
+    for well in wells:
+        wp = well.get_zone_interval(zone, resample=resample)
+        if wp is not None:
+            wp["WellName"] = well.name
+            # as well segments may have overlapping POLY_ID:
+            wp["POLY_ID"] += maxid
+            maxid = wp["POLY_ID"].max() + 1
+            dflist.append(wp)
+
+    if not dflist:
+        return {}
+    dfr = pd.concat(dflist, ignore_index=True)
+    dfr.reset_index(inplace=True, drop=True)
+    return {
+        "values": dfr,
+        "attributes": {"WellName": "str"},
+    }
+
+
+def polygons_from_file(
+    pfile: Union[str, pathlib.Path], fformat: Optional[str] = "guess"
+):
+    """Make an instance of a Polygons object directly from file import.
+
+    Supported formats are:
+
+        * 'xyz' or 'pol': Simple XYZ format
+        * 'zmap': ZMAP line format as exported from RMS (e.g. fault lines)
+        * 'guess': Try to choose file format based on extension
+
+    Args:
+        pfile (str): Name of file
+        fformat (str): See :meth:`Polygons.from_file`
+
+    Example::
+
+        import xtgeo
+        mypoly = xtgeo.polygons_from_file('somefile.xyz')
+    """
+    return Polygons(**_file_importer(pfile, fformat=fformat))
+
+
+def polygons_from_roxar(
+    project: Union[str, Any],
+    name: str,
+    category: str,
+    stype: Optional[str] = "horizons",
+    realisation: Optional[int] = 0,
+):  # pragma: no cover
+    """Load a Polygons instance from Roxar RMS project.
+
+    Note also that horizon/zone/faults name and category must exists
+    in advance, otherwise an Exception will be raised.
+
+    Args:
+        project: Name of project (as folder) if outside RMS, or just use the magic
+            `project` word if within RMS.
+        name: Name of polygons item
+        category: For horizons/zones/faults: for example 'DL_depth'
+            or use a folder notation on clipboard/general2d_data.
+        stype: RMS folder type, 'horizons' (default), 'zones', 'clipboard',
+            'faults', 'general2d_data'
+        realisation: Realisation number, default is 0
+
+    Example::
+
+        import xtgeo
+        mysurf = xtgeo.polygons_from_roxar(project, 'TopAare', 'DepthPolys')
+
+    .. versionadded:: 2.19 general2d_data support is added
+    """
+
+    return Polygons(
+        **_roxar_importer(
+            project,
+            name,
+            category,
+            stype,
+            realisation,
+        )
+    )
+
+
+def polygons_from_wells(
+    wells: List[xtgeo.Well],
+    zone: Optional[int] = 1,
+    resample: Optional[int] = 1,
+):
+    """Get polygons from wells and a single zone number.
+
+    Args:
+        wells: List of XTGeo well objects, a single XTGeo well or a list of well files.
+            If a list of well files, the routine will try to load well based on file
+            signature and/or extension, but only default settings are applied. Hence
+            this is less flexible and more fragile.
+        zone: The zone number to extract the linepiece from
+        resample: If given, resample every N'th sample to make
+            polylines smaller in terms of bits and bytes.
+            1 = No resampling, which means just use well sampling (which can be rather
+            dense; typically 15 cm).
+
+
+    Returns:
+        None if empty data, otherwise a Polygons() instance.
+
+    Example::
+
+        wells = ["w1.w", "w2.w"]
+        points = xtgeo.polygons_from_wells(wells, zone=2)
+
+    Note:
+        This method replaces the deprecated method :py:meth:`~Polygons.from_wells`.
+        The latter returns the number of wells that contribute with polygon segments.
+        This is now implemented through the function `get_nwells()`. Hence the
+        following code::
+
+            nwells_applied = poly.from_wells(...)  # deprecated method
+            # vs
+            poly = xtgeo.polygons_from_wells(...)
+            nwells_applied = poly.get_nwells()
+
+    .. versionadded: 2.16
+    """
+    return Polygons(**_wells_importer(wells, zone, resample))
+
+
+def _allow_deprecated_init(func):
+    # This decorator is here to maintain backwards compatibility in the construction
+    # of Polygons and should be deleted once the deprecation period has expired,
+    # the construction will then follow the new pattern.
+    # Introduced post xtgeo version 2.15
+    @functools.wraps(func)
+    def wrapper(cls, *args, **kwargs):
+        # Checking if we are doing an initialization from file or surface and raise a
+        # deprecation warning if we are.
+        if len(args) == 1 and isinstance(args[0], (str, pathlib.Path)):
+            warnings.warn(
+                "Initializing directly from file name is deprecated and will be "
+                "removed in xtgeo version 4.0. Use: "
+                "pol = xtgeo.polygons_from_file('some_file.xx') instead!",
+                DeprecationWarning,
+            )
+            fformat = kwargs.get("fformat", "guess")
+            return func(cls, **_file_importer(args[0], fformat))
+
+        return func(cls, *args, **kwargs)
+
+    return wrapper
+
+
+class Polygons(XYZ):  # pylint: disable=too-many-public-methods
+    """Class for a Polygons object (connected points) in the XTGeo framework.
+
+    The term Polygons is here used in a wider context, as it includes
+    polylines that do not connect into closed polygons. A Polygons
+    instance may contain several pieces of polylines/polygons, which are
+    identified by POLY_ID.
+
+    The polygons are stored in Python as a Pandas dataframe, which
+    allow for flexible manipulation and fast execution.
+
+    A Polygons instance will have 4 mandatory columns; here by default names:
+
+    * X_UTME - for X UTM coordinate (Easting)
+    * Y_UTMN - For Y UTM coordinate (Northing)
+    * Z_TVDSS - For depth or property from mean SeaLevel; Depth positive down
+    * POLY_ID - for polygon ID as there may be several polylines segments
+
+    Each Polygons instance can also a name (through the name attribute).
+    Default is 'poly'. E.g. if a well fence, it is logical to name the
+    instance to be the same as the well name.
+
+    Args:
+        values: Provide input values on various forms (list-like or dataframe).
+        xname: Name of first (X) mandatory column, default is X_UTME.
+        yname: Name of second (Y) mandatory column, default is Y_UTMN.
+        zname: Name of third (Z) mandatory column, default is Z_TVDSS.
+        pname: Name of forth (P) mandatory enumerating column, default is POLY_ID.
+        hname: Name of cumulative horizontal length, defaults to "H_CUMLEN" if
+            in dataframe otherwise None.
+        dhname: Name of delta horizontal length, defaults to "H_DELTALEN" if in
+            dataframe otherwise None.
+        tname: Name of cumulative total length, defaults to "T_CUMLEN" if in
+            dataframe otherwise None.
+        dtname: Name of delta total length, defaults to "T_DELTALEN" if in
+            dataframe otherwise None.
+        attributes: A dictionary for attribute columns as 'name: type', e.g.
+            {"WellName": "str", "IX": "int"}. This is applied when values are input
+            and is to name and type the extra attribute columns in a point set.
+    """
+
+    @_allow_deprecated_init
+    def __init__(
+        self,
+        values: Union[list, np.ndarray, pd.DataFrame] = None,
+        xname: str = "X_UTME",
+        yname: str = "Y_UTMN",
+        zname: str = "Z_TVDSS",
+        pname: str = "POLY_ID",
+        hname: str = "H_CUMLEN",
+        dhname: str = "H_DELTALEN",
+        tname: str = "T_CUMLEN",
+        dtname: str = "T_DELTALEN",
+        name: str = "poly",
+        attributes: Optional[dict] = None,
+        # from legacy initialization, remove in 4.0, undocumented by purpose:
+        fformat: str = "guess",
+    ):
+        super().__init__(xname, yname, zname)
+
+        if values is None:
+            values = []
+
+        logger.info("Legacy fformat key with value %s shall be removed in 4.0", fformat)
+
+        self._reset(
+            values=values,
+            xname=xname,
+            yname=yname,
+            zname=zname,
+            pname=pname,
+            hname=hname,
+            dhname=dhname,
+            tname=tname,
+            dtname=dtname,
+            name=name,
+            attributes=attributes,
+        )
+
+    def _reset(  # pylint: disable=arguments-renamed
+        self,
+        values: Union[list, np.ndarray, pd.DataFrame],
+        xname: str = "X_UTME",
+        yname: str = "Y_UTMN",
+        zname: str = "Z_TVDSS",
+        pname: str = "POLY_ID",
+        hname: str = "H_CUMLEN",
+        dhname: str = "H_DELTALEN",
+        tname: str = "T_CUMLEN",
+        dtname: str = "T_DELTALEN",
+        name: str = "poly",
+        attributes: Optional[dict] = None,
+    ):  # pylint: disable=arguments-differ
+        """Used in deprecated methods."""
+
+        super()._reset(xname, yname, zname)
+        # additonal state properties for Polygons
+        self._pname = pname
+
+        self._hname = hname
+        self._dhname = dhname
+        self._tname = tname
+        self._dtname = dtname
+        self._name = name
+
+        if not isinstance(values, pd.DataFrame):
+            self._df = _xyz_io._from_list_like(values, self._zname, attributes, True)
+        else:
+            self._df = values
+
+    @property
+    def name(self):
+        """Returns or sets the name of the instance."""
+        return self._name
+
+    @name.setter
+    def name(self, newname):
+        self._name = newname
+
+    @property
+    def pname(self):
+        return self._pname
+
+    @pname.setter
+    def pname(self, value):
+        super()._check_name(value)
+        self._pname = value
+
+    @property
+    def hname(self):
+        """Returns or set the name of the cumulative horizontal length.
+
+        If the column does not exist, None is returned. Default name is H_CUMLEN.
+
+        .. versionadded:: 2.1
+        """
+        return self._hname
+
+    @hname.setter
+    def hname(self, value):
+        super()._check_name(value)
+        self._hname = value
+
+    @property
+    def dhname(self):
+        """Returns or set the name of the delta horizontal length column if it exists.
+
+        If the column does not exist, None is returned. Default name is H_DELTALEN.
+
+        .. versionadded:: 2.1
+        """
+        return self._dhname
+
+    @dhname.setter
+    def dhname(self, value):
+        super()._check_name(value)
+        self._dhname = value
+
+    @property
+    def tname(self):
+        """Returns or set the name of the cumulative total length column if it exists.
+
+        .. versionadded:: 2.1
+        """
+        return self._tname
+
+    @tname.setter
+    def tname(self, value):
+        super()._check_name(value)
+        self._tname = value
+
+    @property
+    def dtname(self):
+        """Returns or set the name of the delta total length column if it exists.
+
+        .. versionadded:: 2.1
+        """
+        return self._dtname
+
+    @dtname.setter
+    def dtname(self, value):
+        super()._check_name(value)
+        self._dtname = value
+
+    @property
+    def dataframe(self) -> pd.DataFrame:
+        """Returns or set the Pandas dataframe object."""
+        return self._df
+
+    @dataframe.setter
+    def dataframe(self, df):
+        self._df = df.apply(deepcopy)
+        self._name_to_none_if_missing()
+
+    def _name_to_none_if_missing(self):
+        if self._dtname not in self._df.columns:
+            self._dtname = None
+        if self._dhname not in self._df.columns:
+            self._dhname = None
+        if self._tname not in self._df.columns:
+            self._tname = None
+        if self._hname not in self._df.columns:
+            self._hname = None
+
+    # ----------------------------------------------------------------------------------
+    # Class methods
+    # ----------------------------------------------------------------------------------
+
+    @classmethod
+    def boundary_from_points(
+        cls,
+        points,
+        alpha_factor: Optional[float] = 1.0,
+        alpha: Optional[float] = None,
+        convex: bool = False,
+    ):
+        """Instantiate polygons from detecting the boundary around points.
+
+        .. image:: images/boundary_polygons.png
+           :width: 600
+           :align: center
+
+        |
+
+        Args:
+            points: The XTGeo Points instance to estimate boundary/boundaries around.
+            alpha_factor: The alpha factor is a multiplier to alpha. Normally it will
+                be around 1, but can be increased to get a looser boundary. Dependent
+                on the points topology, it can also be decreased to some extent.
+            alpha: The alpha factor for determine the 'precision' in how to delineate
+                the polygon. A large value will produce a smoother polygon. The default
+                is to detect the value from the data, but note that this default may be
+                far from optimal for you needs. Usually use the ``alpha_factor`` to tune
+                the best value. The actual alpha applied in the concave hull algorithm
+                is alpha_factor multiplied with alpha.
+            convex: If True, then compute a maximum boundary (convex), and note that
+                alpha_factor and alpha are not applied in ths case. Default is False.
+
+        Returns:
+            A Polygons instance.
+
+        .. versionadded: 3.1.0
+        """
+
+        return cls(
+            _polygons_oper.boundary_from_points(points, alpha_factor, alpha, convex)
+        )
+
+    # ----------------------------------------------------------------------------------
+    # Instance methods
+    # ----------------------------------------------------------------------------------
+    @inherit_docstring(inherit_from=XYZ.protected_columns)
+    def protected_columns(self):
+        return super().protected_columns() + [self.pname]
+
+    @inherit_docstring(inherit_from=XYZ.from_file)
+    @deprecation.deprecated(
+        deprecated_in="2.21",  # should have been 2.16, but was forgotten until 2.21
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.polygons_from_file() instead",
+    )
+    def from_file(self, pfile, fformat="xyz"):
+        self._reset(**_file_importer(pfile, fformat))
+
+    def to_file(
+        self,
+        pfile,
+        fformat="xyz",
+    ):
+        """Export Polygons to file.
+
+        Args:
+            pfile (str): Name of file
+            fformat (str): File format xyz/poi/pol
+
+        Returns:
+            Number of polygon points exported
+        """
+
+        return _xyz_io.to_file(self, pfile, fformat=fformat, ispolygons=True)
+
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use xtgeo.polygons_from_wells(...) instead",
+    )
+    def from_wells(self, wells, zone, resample=1):
+        """Get line segments from a list of wells and a single zone number.
+
+        Args:
+            wells (list): List of XTGeo well objects
+            zone (int): Which zone to apply
+            resample (int): If given, resample every N'th sample to make
+                polylines smaller in terms of bits and bytes.
+                1 = No resampling which means well sampling (which can be rather
+                dense; typically 15 cm).
+
+        Returns:
+            None if well list is empty; otherwise the number of wells that
+            have one or more line segments to return
+
+        """
+        if not wells:
+            return None
+
+        self._reset(**_wells_importer(wells, zone, resample))
+
+        nwells = self.dataframe["WellName"].nunique()
+        # as the previous versions did not have the WellName column, this is dropped
+        # here for backward compatibility:
+        self.dataframe = self.dataframe.drop("WellName", axis=1)
+
+        if nwells == 0:
+            return None
+        else:
+            return nwells
+
+    def to_roxar(
+        self,
+        project,
+        name,
+        category,
+        stype="horizons",
+        realisation=0,
+    ):  # pragma: no cover
+        """Export (store) a Polygons item to a Roxar RMS project.
+
+        The export to the RMS project can be done either within the project
+        or outside the project.
+
+        Note also that horizon/zone name and category must exists in advance,
+        otherwise an Exception will be raised.
+
+        Note:
+            When project is file path (direct access, outside RMS) then
+            ``to_roxar()`` will implicitly do a project save. Otherwise, the project
+            will not be saved until the user do an explicit project save action.
+
+        Args:
+            project (str or special): Name of project (as folder) if
+                outside RMS, og just use the magic project word if within RMS.
+            name (str): Name of polygons item
+            category (str): For horizons/zones/faults: for example 'DL_depth' and use
+                a folder notation for clipboard/general2d_data
+            stype (str): RMS folder type, 'horizons' (default), 'zones'
+                or 'faults' or 'clipboard'  (in prep: well picks)
+            realisation (int): Realisation number, default is 0
+
+
+        Returns:
+            Object instance updated
+
+        Raises:
+            ValueError: Various types of invalid inputs.
+            NotImplementedError: Not supported in this ROXAPI version
+
+        .. versionadded:: 2.19 general2d_data support is added
+        """
+
+        _xyz_roxapi.export_xyz_roxapi(
+            self,
+            project,
+            name,
+            category,
+            stype,
+            None,
+            realisation,
+            None,
+        )
+
+    def copy(self):
+        """Returns a deep copy of an instance"""
+        mycopy = self.__class__()
+        mycopy._df = self._df.apply(deepcopy)  # df.copy() is not fully deep!
+        mycopy._xname = self._xname
+        mycopy._yname = self._yname
+        mycopy._zname = self._zname
+        mycopy._pname = self._pname
+        mycopy._hname = self._hname
+        mycopy._dhname = self._dhname
+        mycopy._tname = self._tname
+        mycopy._dtname = self._dtname
+
+        return mycopy
+
+    @inherit_docstring(inherit_from=XYZ.from_list)
+    @deprecation.deprecated(
+        deprecated_in="2.16",
+        removed_in="4.0",
+        current_version=xtgeo.version,
+        details="Use direct Polygons() initialisation instead",
+    )
+    def from_list(self, plist):
+        kwargs = {}
+        kwargs["values"] = _xyz_io._from_list_like(plist, "Z_TVDSS", None, True)
+        self._reset(**kwargs)
+
+    def get_xyz_dataframe(self):
+        """Get a dataframe copy from the Polygons points with no ID column.
+
+        Convert from POLY_ID based to XYZ, where a new polygon is marked with a 999
+        value as flag.
+        """
+        return _convert_idbased_xyz(self, self.dataframe)
+
+    def get_shapely_objects(self):
+        """Returns a list of Shapely LineString objects, one per POLY_ID.
+
+        .. versionadded:: 2.1
+        """
+        spolys = []
+        idgroups = self.dataframe.groupby(self.pname)
+
+        for _idx, grp in idgroups:
+            pxcor = grp[self.xname].values
+            pycor = grp[self.yname].values
+            pzcor = grp[self.zname].values
+            spoly = sg.LineString(np.stack([pxcor, pycor, pzcor], axis=1))
+            spolys.append(spoly)
+
+        return spolys
+
+    @inherit_docstring(inherit_from=XYZ.get_boundary)
+    def get_boundary(self):
+        return super().get_boundary()
+
+    def simplify(
+        self, tolerance: Optional[float] = 0.1, preserve_topology: Optional[bool] = True
+    ) -> bool:
+        """Simply a polygon, i.e. remove unneccesary points.
+
+        This is based on `Shapely's simplify() method
+        <https://shapely.readthedocs.io/en/latest/manual.html#object.simplify>`_
+
+        Args:
+            tolerance: Cf. Shapely's documentation
+            preserve_topology: Default is True, if False a faster algorithm is applied
+
+        Returns:
+            True if simplification is achieved. The polygons instance is
+            updated in-place.
+
+        .. versionadded: 3.1
+
+
+        """
+
+        return _polygons_oper.simplify_polygons(self, tolerance, preserve_topology)
+
+    def filter_byid(self, polyid=None):
+        """Remove all line segments not in polyid.
+
+        The instance is updated in-place.
+
+        Args:
+            polyid (int or list of int): Which ID(s) to keep, None means use first.
+
+        Example::
+
+            mypoly.filter_byid(polyid=[2, 4])  # keep POLY_ID 2 and 4
+
+        .. versionadded:: 2.1
+        """
+        if polyid is None:
+            polyid = int(self.dataframe[self.pname].iloc[0])
+
+        if not isinstance(polyid, list):
+            polyid = [polyid]
+
+        dflist = []
+        for pid in polyid:
+            dflist.append(self.dataframe[self.dataframe[self.pname] == pid])
+
+        self.dataframe = pd.concat(dflist)
+
+    def tlen(self, tname="T_CUMLEN", dtname="T_DELTALEN", atindex=0):
+        """Compute and add or replace columns for cum. total 3D length and delta length.
+
+        The instance is updated in-place.
+
+        Args:
+            tname (str): Name of cumulative total length. Default is T_CUMLEN.
+            dtname (str): Name of delta length column. Default is T_DELTALEN.
+            atindex (int): Which index which shall be 0.0 for cumulative length.
+
+        .. versionadded:: 2.1
+        """
+        _xyz_oper.tlen(self, tname=tname, dtname=dtname, atindex=atindex)
+
+    def hlen(self, hname="H_CUMLEN", dhname="H_DELTALEN", atindex=0):
+        """Compute and add/replace columns for cum. horizontal length and delta length.
+
+        The instance is updated in-place.
+
+        Args:
+            hname (str): Name of cumulative horizontal length. Default is H_CUMLEN.
+            dhname (str): Name of delta length column. Default is H_DELTALEN.
+            atindex (int): Which index which shall be 0.0 for cumulative length.
+
+        .. versionadded:: 2.1
+        """
+        _xyz_oper.hlen(self, hname=hname, dhname=dhname, atindex=atindex)
+
+    def extend(self, distance, nsamples=1, mode2d=True):
+        """Extend polyline by `distance` at both ends, nsmaples times.
+
+        The instance is updated in-place.
+
+        Args:
+            distance (float): The horizontal distance (sampling) to extend
+            nsamples (int): Number of samples to extend.
+            mode2d (bool): XY extension (only True is supported)
+
+        .. versionadded:: 2.1
+        """
+        _xyz_oper.extend(self, distance, nsamples, mode2d)
+
+    def rescale(self, distance, addlen=False, kind="simple", mode2d=True):
+        """Rescale (resample) by using a new increment.
+
+        The increment (distance) may be a horizontal or a True 3D
+        distance dependent on mode2d.
+
+        The instance is updated in-place.
+
+        If the distance is larger than the total input poly-line length,
+        nothing is done. Note that the result distance may differ from the
+        requested distance caused to rounding to fit original length.
+
+        Hence actual distance is input distance +- 50%.
+
+        Args:
+            distance (float): New distance between points
+            addlen (str): If True, total and horizontal cum. and delta length
+                columns will be added.
+            kind (str): What kind of rescaling: slinear/cubic/simple
+            mode2d (bool): The distance may be a 2D (XY) ora 3D (XYZ) mode.
+
+        .. versionchanged:: 2.1 a new algorithm
+
+        """
+        _xyz_oper.rescale_polygons(
+            self, distance=distance, addlen=addlen, kind=kind, mode2d=mode2d
+        )
+
+    def get_fence(
+        self, distance=20, atleast=5, nextend=2, name=None, asnumpy=True, polyid=None
+    ):
+        """Extracts a fence with constant horizontal sampling.
+
+        Additonal H_CUMLEN and H_DELTALEN vectors will be added, suitable for
+        X sections.
+
+        Args:
+            distance (float): New horizontal distance between points
+            atleast (int): Minimum number of points. If the true length/atleast is
+                less than distance, than distance will be be reset to
+                length/atleast. Values below 3 are not permitted
+            nextend (int): Number of samples to extend at each end. Note that
+                in case of internal resetting of distance (due to 'atleast'), then
+                nextend internally will be modified in order to fulfill the
+                initial intention. Hence keep distance*nextend as target.
+            name (str): Name of polygon (if asnumpy=False)
+            asnumpy (bool): Return a [:, 5] numpy array with
+                columns X.., Y.., Z.., HLEN, dH
+            polyid (int): Which POLY_ID to use. Default (if None) is to use the
+                first found.
+
+        Returns:
+            A numpy array (if asnumpy=True) or a new Polygons() object
+
+        .. versionadded:: 2.1
+        """
+        logger.info("Getting fence within a Polygons instance...")
+        return _xyz_oper.get_fence(
+            self,
+            distance=distance,
+            atleast=atleast,
+            nextend=nextend,
+            name=name,
+            asnumpy=asnumpy,
+            polyid=polyid,
+        )
+
+    # ==================================================================================
+    # Plotting
+    # ==================================================================================
+
+    def quickplot(
+        self,
+        filename=None,
+        others=None,
+        title="QuickPlot for Polygons",
+        subtitle=None,
+        infotext=None,
+        linewidth=1.0,
+        color="r",
+    ):
+        """Simple plotting of polygons using matplotlib.
+
+        Args:
+            filename (str): Name of plot file; None will plot to screen.
+            others (list of Polygons): List of other polygon instances to plot
+            title (str): Title of plot
+            subtitle (str): Subtitle of plot
+            infotext (str): Additonal info on plot.
+            linewidth (float): Width of line.
+            color (str): Name of color (may use matplotib shortcuts, e.g. 'r' for 'red')
+        """
+        mymap = xtgeo.plot.Map()
+        mymap.canvas(title=title, subtitle=subtitle, infotext=infotext)
+
+        if others:
+            for other in others:
+                lwid = linewidth / 2.0
+                mymap.plot_polygons(
+                    other, idname=other.pname, linewidth=lwid, color="black"
+                )
+
+        mymap.plot_polygons(self, idname=self.pname, linewidth=linewidth, color=color)
+
+        if filename is None:
+            mymap.show()
+        else:
+            mymap.savefig(filename)
```

## Comparing `xtgeo-3.1.1rc1.dist-info/LICENSE.md` & `xtgeo-3.1.2.dist-info/LICENSE.md`

 * *Ordering differences only*

 * *Files 10% similar despite different names*

```diff
@@ -1,165 +1,165 @@
-                   GNU LESSER GENERAL PUBLIC LICENSE
-                       Version 3, 29 June 2007
-
- Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
- Everyone is permitted to copy and distribute verbatim copies
- of this license document, but changing it is not allowed.
-
-
-  This version of the GNU Lesser General Public License incorporates
-the terms and conditions of version 3 of the GNU General Public
-License, supplemented by the additional permissions listed below.
-
-  0. Additional Definitions.
-
-  As used herein, "this License" refers to version 3 of the GNU Lesser
-General Public License, and the "GNU GPL" refers to version 3 of the GNU
-General Public License.
-
-  "The Library" refers to a covered work governed by this License,
-other than an Application or a Combined Work as defined below.
-
-  An "Application" is any work that makes use of an interface provided
-by the Library, but which is not otherwise based on the Library.
-Defining a subclass of a class defined by the Library is deemed a mode
-of using an interface provided by the Library.
-
-  A "Combined Work" is a work produced by combining or linking an
-Application with the Library.  The particular version of the Library
-with which the Combined Work was made is also called the "Linked
-Version".
-
-  The "Minimal Corresponding Source" for a Combined Work means the
-Corresponding Source for the Combined Work, excluding any source code
-for portions of the Combined Work that, considered in isolation, are
-based on the Application, and not on the Linked Version.
-
-  The "Corresponding Application Code" for a Combined Work means the
-object code and/or source code for the Application, including any data
-and utility programs needed for reproducing the Combined Work from the
-Application, but excluding the System Libraries of the Combined Work.
-
-  1. Exception to Section 3 of the GNU GPL.
-
-  You may convey a covered work under sections 3 and 4 of this License
-without being bound by section 3 of the GNU GPL.
-
-  2. Conveying Modified Versions.
-
-  If you modify a copy of the Library, and, in your modifications, a
-facility refers to a function or data to be supplied by an Application
-that uses the facility (other than as an argument passed when the
-facility is invoked), then you may convey a copy of the modified
-version:
-
-   a) under this License, provided that you make a good faith effort to
-   ensure that, in the event an Application does not supply the
-   function or data, the facility still operates, and performs
-   whatever part of its purpose remains meaningful, or
-
-   b) under the GNU GPL, with none of the additional permissions of
-   this License applicable to that copy.
-
-  3. Object Code Incorporating Material from Library Header Files.
-
-  The object code form of an Application may incorporate material from
-a header file that is part of the Library.  You may convey such object
-code under terms of your choice, provided that, if the incorporated
-material is not limited to numerical parameters, data structure
-layouts and accessors, or small macros, inline functions and templates
-(ten or fewer lines in length), you do both of the following:
-
-   a) Give prominent notice with each copy of the object code that the
-   Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the object code with a copy of the GNU GPL and this license
-   document.
-
-  4. Combined Works.
-
-  You may convey a Combined Work under terms of your choice that,
-taken together, effectively do not restrict modification of the
-portions of the Library contained in the Combined Work and reverse
-engineering for debugging such modifications, if you also do each of
-the following:
-
-   a) Give prominent notice with each copy of the Combined Work that
-   the Library is used in it and that the Library and its use are
-   covered by this License.
-
-   b) Accompany the Combined Work with a copy of the GNU GPL and this license
-   document.
-
-   c) For a Combined Work that displays copyright notices during
-   execution, include the copyright notice for the Library among
-   these notices, as well as a reference directing the user to the
-   copies of the GNU GPL and this license document.
-
-   d) Do one of the following:
-
-       0) Convey the Minimal Corresponding Source under the terms of this
-       License, and the Corresponding Application Code in a form
-       suitable for, and under terms that permit, the user to
-       recombine or relink the Application with a modified version of
-       the Linked Version to produce a modified Combined Work, in the
-       manner specified by section 6 of the GNU GPL for conveying
-       Corresponding Source.
-
-       1) Use a suitable shared library mechanism for linking with the
-       Library.  A suitable mechanism is one that (a) uses at run time
-       a copy of the Library already present on the user's computer
-       system, and (b) will operate properly with a modified version
-       of the Library that is interface-compatible with the Linked
-       Version.
-
-   e) Provide Installation Information, but only if you would otherwise
-   be required to provide such information under section 6 of the
-   GNU GPL, and only to the extent that such information is
-   necessary to install and execute a modified version of the
-   Combined Work produced by recombining or relinking the
-   Application with a modified version of the Linked Version. (If
-   you use option 4d0, the Installation Information must accompany
-   the Minimal Corresponding Source and Corresponding Application
-   Code. If you use option 4d1, you must provide the Installation
-   Information in the manner specified by section 6 of the GNU GPL
-   for conveying Corresponding Source.)
-
-  5. Combined Libraries.
-
-  You may place library facilities that are a work based on the
-Library side by side in a single library together with other library
-facilities that are not Applications and are not covered by this
-License, and convey such a combined library under terms of your
-choice, if you do both of the following:
-
-   a) Accompany the combined library with a copy of the same work based
-   on the Library, uncombined with any other library facilities,
-   conveyed under the terms of this License.
-
-   b) Give prominent notice with the combined library that part of it
-   is a work based on the Library, and explaining where to find the
-   accompanying uncombined form of the same work.
-
-  6. Revised Versions of the GNU Lesser General Public License.
-
-  The Free Software Foundation may publish revised and/or new versions
-of the GNU Lesser General Public License from time to time. Such new
-versions will be similar in spirit to the present version, but may
-differ in detail to address new problems or concerns.
-
-  Each version is given a distinguishing version number. If the
-Library as you received it specifies that a certain numbered version
-of the GNU Lesser General Public License "or any later version"
-applies to it, you have the option of following the terms and
-conditions either of that published version or of any later version
-published by the Free Software Foundation. If the Library as you
-received it does not specify a version number of the GNU Lesser
-General Public License, you may choose any version of the GNU Lesser
-General Public License ever published by the Free Software Foundation.
-
-  If the Library as you received it specifies that a proxy can decide
-whether future versions of the GNU Lesser General Public License shall
-apply, that proxy's public statement of acceptance of any version is
-permanent authorization for you to choose that version for the
-Library.
+                   GNU LESSER GENERAL PUBLIC LICENSE
+                       Version 3, 29 June 2007
+
+ Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
+ Everyone is permitted to copy and distribute verbatim copies
+ of this license document, but changing it is not allowed.
+
+
+  This version of the GNU Lesser General Public License incorporates
+the terms and conditions of version 3 of the GNU General Public
+License, supplemented by the additional permissions listed below.
+
+  0. Additional Definitions.
+
+  As used herein, "this License" refers to version 3 of the GNU Lesser
+General Public License, and the "GNU GPL" refers to version 3 of the GNU
+General Public License.
+
+  "The Library" refers to a covered work governed by this License,
+other than an Application or a Combined Work as defined below.
+
+  An "Application" is any work that makes use of an interface provided
+by the Library, but which is not otherwise based on the Library.
+Defining a subclass of a class defined by the Library is deemed a mode
+of using an interface provided by the Library.
+
+  A "Combined Work" is a work produced by combining or linking an
+Application with the Library.  The particular version of the Library
+with which the Combined Work was made is also called the "Linked
+Version".
+
+  The "Minimal Corresponding Source" for a Combined Work means the
+Corresponding Source for the Combined Work, excluding any source code
+for portions of the Combined Work that, considered in isolation, are
+based on the Application, and not on the Linked Version.
+
+  The "Corresponding Application Code" for a Combined Work means the
+object code and/or source code for the Application, including any data
+and utility programs needed for reproducing the Combined Work from the
+Application, but excluding the System Libraries of the Combined Work.
+
+  1. Exception to Section 3 of the GNU GPL.
+
+  You may convey a covered work under sections 3 and 4 of this License
+without being bound by section 3 of the GNU GPL.
+
+  2. Conveying Modified Versions.
+
+  If you modify a copy of the Library, and, in your modifications, a
+facility refers to a function or data to be supplied by an Application
+that uses the facility (other than as an argument passed when the
+facility is invoked), then you may convey a copy of the modified
+version:
+
+   a) under this License, provided that you make a good faith effort to
+   ensure that, in the event an Application does not supply the
+   function or data, the facility still operates, and performs
+   whatever part of its purpose remains meaningful, or
+
+   b) under the GNU GPL, with none of the additional permissions of
+   this License applicable to that copy.
+
+  3. Object Code Incorporating Material from Library Header Files.
+
+  The object code form of an Application may incorporate material from
+a header file that is part of the Library.  You may convey such object
+code under terms of your choice, provided that, if the incorporated
+material is not limited to numerical parameters, data structure
+layouts and accessors, or small macros, inline functions and templates
+(ten or fewer lines in length), you do both of the following:
+
+   a) Give prominent notice with each copy of the object code that the
+   Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the object code with a copy of the GNU GPL and this license
+   document.
+
+  4. Combined Works.
+
+  You may convey a Combined Work under terms of your choice that,
+taken together, effectively do not restrict modification of the
+portions of the Library contained in the Combined Work and reverse
+engineering for debugging such modifications, if you also do each of
+the following:
+
+   a) Give prominent notice with each copy of the Combined Work that
+   the Library is used in it and that the Library and its use are
+   covered by this License.
+
+   b) Accompany the Combined Work with a copy of the GNU GPL and this license
+   document.
+
+   c) For a Combined Work that displays copyright notices during
+   execution, include the copyright notice for the Library among
+   these notices, as well as a reference directing the user to the
+   copies of the GNU GPL and this license document.
+
+   d) Do one of the following:
+
+       0) Convey the Minimal Corresponding Source under the terms of this
+       License, and the Corresponding Application Code in a form
+       suitable for, and under terms that permit, the user to
+       recombine or relink the Application with a modified version of
+       the Linked Version to produce a modified Combined Work, in the
+       manner specified by section 6 of the GNU GPL for conveying
+       Corresponding Source.
+
+       1) Use a suitable shared library mechanism for linking with the
+       Library.  A suitable mechanism is one that (a) uses at run time
+       a copy of the Library already present on the user's computer
+       system, and (b) will operate properly with a modified version
+       of the Library that is interface-compatible with the Linked
+       Version.
+
+   e) Provide Installation Information, but only if you would otherwise
+   be required to provide such information under section 6 of the
+   GNU GPL, and only to the extent that such information is
+   necessary to install and execute a modified version of the
+   Combined Work produced by recombining or relinking the
+   Application with a modified version of the Linked Version. (If
+   you use option 4d0, the Installation Information must accompany
+   the Minimal Corresponding Source and Corresponding Application
+   Code. If you use option 4d1, you must provide the Installation
+   Information in the manner specified by section 6 of the GNU GPL
+   for conveying Corresponding Source.)
+
+  5. Combined Libraries.
+
+  You may place library facilities that are a work based on the
+Library side by side in a single library together with other library
+facilities that are not Applications and are not covered by this
+License, and convey such a combined library under terms of your
+choice, if you do both of the following:
+
+   a) Accompany the combined library with a copy of the same work based
+   on the Library, uncombined with any other library facilities,
+   conveyed under the terms of this License.
+
+   b) Give prominent notice with the combined library that part of it
+   is a work based on the Library, and explaining where to find the
+   accompanying uncombined form of the same work.
+
+  6. Revised Versions of the GNU Lesser General Public License.
+
+  The Free Software Foundation may publish revised and/or new versions
+of the GNU Lesser General Public License from time to time. Such new
+versions will be similar in spirit to the present version, but may
+differ in detail to address new problems or concerns.
+
+  Each version is given a distinguishing version number. If the
+Library as you received it specifies that a certain numbered version
+of the GNU Lesser General Public License "or any later version"
+applies to it, you have the option of following the terms and
+conditions either of that published version or of any later version
+published by the Free Software Foundation. If the Library as you
+received it does not specify a version number of the GNU Lesser
+General Public License, you may choose any version of the GNU Lesser
+General Public License ever published by the Free Software Foundation.
+
+  If the Library as you received it specifies that a proxy can decide
+whether future versions of the GNU Lesser General Public License shall
+apply, that proxy's public statement of acceptance of any version is
+permanent authorization for you to choose that version for the
+Library.
```

## Comparing `xtgeo-3.1.1rc1.dist-info/METADATA` & `xtgeo-3.1.2.dist-info/METADATA`

 * *Files 21% similar despite different names*

```diff
@@ -1,136 +1,136 @@
-Metadata-Version: 2.1
-Name: xtgeo
-Version: 3.1.1rc1
-Summary: XTGeo is a Python library for 3D grids, surfaces, wells, etc
-Home-page: https://github.com/equinor/xtgeo
-Author: Equinor R&T
-License: LGPL-3.0
-Project-URL: Documentation, https://xtgeo.readthedocs.io/
-Project-URL: Issue Tracker, https://github.com/equinor/xtgeo/issues
-Keywords: xtgeo
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Intended Audience :: Developers
-Classifier: Intended Audience :: Science/Research
-Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)
-Classifier: Operating System :: POSIX :: Linux
-Classifier: Natural Language :: English
-Classifier: Programming Language :: Python
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Topic :: Scientific/Engineering
-Classifier: Topic :: Scientific/Engineering :: Physics
-Classifier: Topic :: Software Development :: Libraries
-Classifier: Topic :: Utilities
-Description-Content-Type: text/markdown
-License-File: LICENSE.md
-Requires-Dist: deprecation
-Requires-Dist: numpy (>=1.19)
-Requires-Dist: shapely (>=1.6.2)
-Requires-Dist: matplotlib (>=3.3)
-Requires-Dist: scipy (>=1.5)
-Requires-Dist: segyio (>1.8.0)
-Requires-Dist: pandas (>=1.1)
-Requires-Dist: h5py (>=3)
-Requires-Dist: hdf5plugin (>=2.3)
-Requires-Dist: tables (>=3.5.1)
-Requires-Dist: roffio (>=0.0.2)
-Requires-Dist: ecl-data-io (>=2.1)
-Requires-Dist: typing-extensions
-Provides-Extra: docs
-Requires-Dist: setuptools (>=43) ; extra == 'docs'
-Requires-Dist: cmake (>=3.13.3) ; extra == 'docs'
-Requires-Dist: scikit-build ; extra == 'docs'
-Requires-Dist: ninja ; extra == 'docs'
-Requires-Dist: setuptools-scm ; extra == 'docs'
-Requires-Dist: pydocstyle ; extra == 'docs'
-Requires-Dist: Sphinx (<4.0) ; extra == 'docs'
-Requires-Dist: sphinx-rtd-theme ; extra == 'docs'
-Requires-Dist: sphinx-toolbox ; extra == 'docs'
-Requires-Dist: autoclasstoc ; extra == 'docs'
-Requires-Dist: myst-parser ; extra == 'docs'
-Provides-Extra: tests
-Requires-Dist: pytest ; extra == 'tests'
-Requires-Dist: hypothesis ; extra == 'tests'
-Requires-Dist: pytest-benchmark ; extra == 'tests'
-Requires-Dist: pytest-mock ; extra == 'tests'
-Requires-Dist: pytest-snapshot ; extra == 'tests'
-
-![XTGeo](https://github.com/equinor/xtgeo/blob/main/docs/images/xtgeo-logo-wide.png)
-![builds](https://github.com/equinor/xtgeo/workflows/builds/badge.svg)
-![linting](https://github.com/equinor/xtgeo/workflows/linting/badge.svg)
-[![Codacy Badge](https://api.codacy.com/project/badge/Grade/c209aeed6a2a40b08ea859aeadf31cb0)](https://www.codacy.com/app/jcrivenaes/xtgeo?utm_source=github.com&utm_medium=referral&utm_content=equinor/xtgeo&utm_campaign=Badge_Grade)
-[![codecov](https://codecov.io/gh/equinor/xtgeo/branch/main/graph/badge.svg)](https://codecov.io/gh/equinor/xtgeo)
-[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)
-[![PyPI version](https://badge.fury.io/py/xtgeo.svg)](https://badge.fury.io/py/xtgeo)
-[![Documentation Status](https://readthedocs.org/projects/xtgeo/badge/?version=latest)](https://xtgeo.readthedocs.io/en/latest/?badge=latest)
-![PyPI - Python Version](https://img.shields.io/pypi/pyversions/xtgeo.svg)
-![PyPI - License](https://img.shields.io/pypi/l/xtgeo.svg)
-
-## Introduction
-
-XTGeo is a LGPL licensed Python library with C backend to support
-manipulation of (oil industry) subsurface reservoir modelling. Typical
-users are geoscientist and reservoir engineers working with
-reservoir modelling, in relation with RMS. XTGeo is developed in Equinor.
-
-Detailed documentation for [XTGeo at Read _the_ Docs](https://xtgeo.readthedocs.io)
-
-## Feature summary
-
--   Python 3.8+ support
--   Focus on high speed, using numpy and pandas with C backend
--   Regular surfaces, i.e. 2D maps with regular sampling and rotation
--   3D grids (corner-point), supporting several formats such as
-    RMS and Eclipse
--   Support of seismic cubes, using
-    [segyio](https://github.com/equinor/segyio) as backend for SEGY format
--   Support of well data, line and polygons (still somewhat immature)
--   Operations between the data types listed above; e.g. slice a surface
-    with a seismic cube
--   Optional integration with ROXAR API python for several data types
-    (see note later)
--   Linux is main development platform, but Windows and MacOS (64 bit) are supported
-    and PYPI wheels for all three platforms are provided.
-
-## Installation
-
-For Linux, Windows and MacOS 64bit, PYPI installation is enabled:
-
-```
-pip install xtgeo
-```
-
-For detailed installation instructions (implies C compiling), see
-the documentation.
-
-## Getting started
-
-```python
-from xtgeo.surface import RegularSurface
-
-# create an instance of a surface, read from file
-mysurf = RegularSurface("myfile.gri")  # Irap binary as default
-
-print("Mean is {}".format(mysurf.values.mean()))
-
-# change date so all values less than 2000 becomes 2000
-# The values attribute gives the Numpy array
-
-mysurface.values[mysurface.values < 2000] = 2000
-
-# export the modified surface:
-mysurface.to_file("newfile.gri")
-```
-
-## Note on RMS Roxar API integration
-
-The following applies to the part of the XTGeo API that is
-connected to Roxar API (RMS):
-
-> RMS is neither an open source software nor a free software and
-> any use of it needs a software license agreement in place.
-
-
-See HISTORY.md
+Metadata-Version: 2.1
+Name: xtgeo
+Version: 3.1.2
+Summary: XTGeo is a Python library for 3D grids, surfaces, wells, etc
+Home-page: https://github.com/equinor/xtgeo
+Author: Equinor R&T
+License: LGPL-3.0
+Project-URL: Documentation, https://xtgeo.readthedocs.io/
+Project-URL: Issue Tracker, https://github.com/equinor/xtgeo/issues
+Keywords: xtgeo
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Intended Audience :: Developers
+Classifier: Intended Audience :: Science/Research
+Classifier: License :: OSI Approved :: GNU Lesser General Public License v3 or later (LGPLv3+)
+Classifier: Operating System :: POSIX :: Linux
+Classifier: Natural Language :: English
+Classifier: Programming Language :: Python
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Topic :: Scientific/Engineering
+Classifier: Topic :: Scientific/Engineering :: Physics
+Classifier: Topic :: Software Development :: Libraries
+Classifier: Topic :: Utilities
+Description-Content-Type: text/markdown
+License-File: LICENSE.md
+Requires-Dist: deprecation
+Requires-Dist: numpy (>=1.19)
+Requires-Dist: shapely (>=1.6.2)
+Requires-Dist: matplotlib (>=3.3)
+Requires-Dist: scipy (>=1.5)
+Requires-Dist: segyio (>1.8.0)
+Requires-Dist: pandas (>=1.1)
+Requires-Dist: h5py (>=3)
+Requires-Dist: hdf5plugin (>=2.3)
+Requires-Dist: tables (>=3.5.1)
+Requires-Dist: roffio (>=0.0.2)
+Requires-Dist: ecl-data-io (>=2.1)
+Requires-Dist: typing-extensions
+Provides-Extra: docs
+Requires-Dist: setuptools (>=43) ; extra == 'docs'
+Requires-Dist: cmake (>=3.13.3) ; extra == 'docs'
+Requires-Dist: scikit-build ; extra == 'docs'
+Requires-Dist: ninja ; extra == 'docs'
+Requires-Dist: setuptools-scm ; extra == 'docs'
+Requires-Dist: pydocstyle ; extra == 'docs'
+Requires-Dist: Sphinx (<4.0) ; extra == 'docs'
+Requires-Dist: sphinx-rtd-theme ; extra == 'docs'
+Requires-Dist: sphinx-toolbox ; extra == 'docs'
+Requires-Dist: autoclasstoc ; extra == 'docs'
+Requires-Dist: myst-parser ; extra == 'docs'
+Provides-Extra: tests
+Requires-Dist: pytest ; extra == 'tests'
+Requires-Dist: hypothesis ; extra == 'tests'
+Requires-Dist: pytest-benchmark ; extra == 'tests'
+Requires-Dist: pytest-mock ; extra == 'tests'
+Requires-Dist: pytest-snapshot ; extra == 'tests'
+
+![XTGeo](https://github.com/equinor/xtgeo/blob/main/docs/images/xtgeo-logo-wide.png)
+![builds](https://github.com/equinor/xtgeo/workflows/builds/badge.svg)
+![linting](https://github.com/equinor/xtgeo/workflows/linting/badge.svg)
+[![Codacy Badge](https://api.codacy.com/project/badge/Grade/c209aeed6a2a40b08ea859aeadf31cb0)](https://www.codacy.com/app/jcrivenaes/xtgeo?utm_source=github.com&utm_medium=referral&utm_content=equinor/xtgeo&utm_campaign=Badge_Grade)
+[![codecov](https://codecov.io/gh/equinor/xtgeo/branch/main/graph/badge.svg)](https://codecov.io/gh/equinor/xtgeo)
+[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/python/black)
+[![PyPI version](https://badge.fury.io/py/xtgeo.svg)](https://badge.fury.io/py/xtgeo)
+[![Documentation Status](https://readthedocs.org/projects/xtgeo/badge/?version=latest)](https://xtgeo.readthedocs.io/en/latest/?badge=latest)
+![PyPI - Python Version](https://img.shields.io/pypi/pyversions/xtgeo.svg)
+![PyPI - License](https://img.shields.io/pypi/l/xtgeo.svg)
+
+## Introduction
+
+XTGeo is a LGPL licensed Python library with C backend to support
+manipulation of (oil industry) subsurface reservoir modelling. Typical
+users are geoscientist and reservoir engineers working with
+reservoir modelling, in relation with RMS. XTGeo is developed in Equinor.
+
+Detailed documentation for [XTGeo at Read _the_ Docs](https://xtgeo.readthedocs.io)
+
+## Feature summary
+
+-   Python 3.8+ support
+-   Focus on high speed, using numpy and pandas with C backend
+-   Regular surfaces, i.e. 2D maps with regular sampling and rotation
+-   3D grids (corner-point), supporting several formats such as
+    RMS and Eclipse
+-   Support of seismic cubes, using
+    [segyio](https://github.com/equinor/segyio) as backend for SEGY format
+-   Support of well data, line and polygons (still somewhat immature)
+-   Operations between the data types listed above; e.g. slice a surface
+    with a seismic cube
+-   Optional integration with ROXAR API python for several data types
+    (see note later)
+-   Linux is main development platform, but Windows and MacOS (64 bit) are supported
+    and PYPI wheels for all three platforms are provided.
+
+## Installation
+
+For Linux, Windows and MacOS 64bit, PYPI installation is enabled:
+
+```
+pip install xtgeo
+```
+
+For detailed installation instructions (implies C compiling), see
+the documentation.
+
+## Getting started
+
+```python
+from xtgeo.surface import RegularSurface
+
+# create an instance of a surface, read from file
+mysurf = RegularSurface("myfile.gri")  # Irap binary as default
+
+print("Mean is {}".format(mysurf.values.mean()))
+
+# change date so all values less than 2000 becomes 2000
+# The values attribute gives the Numpy array
+
+mysurface.values[mysurface.values < 2000] = 2000
+
+# export the modified surface:
+mysurface.to_file("newfile.gri")
+```
+
+## Note on RMS Roxar API integration
+
+The following applies to the part of the XTGeo API that is
+connected to Roxar API (RMS):
+
+> RMS is neither an open source software nor a free software and
+> any use of it needs a software license agreement in place.
+
+
+See HISTORY.md
```

## Comparing `xtgeo-3.1.1rc1.dist-info/RECORD` & `xtgeo-3.1.2.dist-info/RECORD`

 * *Files 24% similar despite different names*

```diff
@@ -1,108 +1,108 @@
-xtgeo/__init__.py,sha256=zHwCoY4xedSJjR1nIhDA1hxfAz-YQZbH1SDGgzLwhiw,5955
-xtgeo/_theversion.py,sha256=VY36SOQuXshI6OD8RKb5Nu-N2GNsrS5lUX3Q0tuTIIU,167
-xtgeo/common/__init__.py,sha256=H9UZy4xKN4-fY8YfF0iO4XgTYErZ2wTacNKbdLSiaCk,385
-xtgeo/common/calc.py,sha256=7zdcC8sr1rZYPrm8-xlmgVnuQ1LvH7Gm75g-DOM8gI0,9518
-xtgeo/common/constants.py,sha256=Rj670yAazC4cLJUaGRf-xtyR9O3h_n-D6Srq0VAAiMQ,557
-xtgeo/common/exceptions.py,sha256=OgU1Dfs5-pFma5M_7cQgNC6Weai7S9Mgayz2s71Cing,1110
-xtgeo/common/sys.py,sha256=bim1GXifg0iwIF4FHG9mqnmmFSPCYUfQfiFIbr2yxfc,24110
-xtgeo/common/xtgeo_dialog.py,sha256=UsgM5vy-awxNqHRwACxxqzq6z2XzP3fBpOF1Z7flkCk,17736
-xtgeo/cube/__init__.py,sha256=DnPILNdBjYD6DzS3lVdLyepE_Sp7AwXsjk8EFrDZ7sQ,153
-xtgeo/cube/_cube_export.py,sha256=UUIw2WDhJGjBKYtZCVpH8ym9dR94w_hYfhcSAYY_CcU,6790
-xtgeo/cube/_cube_import.py,sha256=3zRNd_R9751jZptJW9MF-cGbtmxuEMbF-zYTXS87bh4,20060
-xtgeo/cube/_cube_roxapi.py,sha256=DnNegRccvbtoPMB74x3cBLsgcOmmOOp8O3QL-QowTK0,5108
-xtgeo/cube/_cube_utils.py,sha256=sZ5DHeajfm8i4gs7bscVWjOdGfaxm6BAMD5mulG5uUM,7731
-xtgeo/cube/cube1.py,sha256=XT9jsswQwW1Ma6dXpACejgWa6TQusco-82GDoArexD8,36529
-xtgeo/cxtgeo/__init__.py,sha256=frcCV1k9oG9oKj3dpUqdJg1PxRT2RSN_XKdLCPjaYaY,2
-xtgeo/cxtgeo/_cxtgeo.cp39-win_amd64.pyd,sha256=RO1IbIG8jmDxspmgZ7xmPlOaXmJHTMnX578lOBzGkIU,605696
-xtgeo/grid3d/__init__.py,sha256=qmcG3ufcPl0EaKP1YX1XtiGH2djXucq5GVwclkNBOnY,356
-xtgeo/grid3d/_ecl_grid.py,sha256=PH2Io19Q6zI9pw3TYhqEFnYbky6RCvCwJWJikgNI_Q0,26874
-xtgeo/grid3d/_ecl_inte_head.py,sha256=-aXj6yQQb2IYSKx7KGx68irsiJuK2JZv1B-bp9t8UMs,4310
-xtgeo/grid3d/_ecl_logi_head.py,sha256=g2Gw1Lk9ycWVox_dEt4jHVaIjJhq2thi4wH8um8nmAw,2733
-xtgeo/grid3d/_ecl_output_file.py,sha256=b8DK8iZX97rVqjyqH7tbu2Hcll6gZ-l6xDmR8oyYcp4,1987
-xtgeo/grid3d/_egrid.py,sha256=823hbnj6P7DiPi6_Yr3BeuGi-Ps16R0Q3JuOIZ33VD8,34754
-xtgeo/grid3d/_find_gridprop_in_eclrun.py,sha256=XJFoRJn6nQLq5-eZXt91CcQyFUim6qz7lBM4h4s2Gdw,22108
-xtgeo/grid3d/_grdecl_format.py,sha256=oh5hyRszwDEs5jvKJeCKk4KJZIPtSFoovVX-OUzuWM4,8496
-xtgeo/grid3d/_grdecl_grid.py,sha256=36aFWPdMr9wlCnD5V7DOfp_I8mYg2ZSNOJvQul615B4,13075
-xtgeo/grid3d/_grid3d.py,sha256=lN4CEdxevurcLZINrZJKkMuOCeEeSIE1-TlnKQWvJtQ,1227
-xtgeo/grid3d/_grid3d_fence.py,sha256=nB4M3h-d2Aeaf7xl_mgUq24aQgxsBTUPcc6LRGqXI4g,5127
-xtgeo/grid3d/_grid3d_utils.py,sha256=HdvOrK7dCf4bXXSiw53GcX3168ujfLf_pjcanFuf9D8,5859
-xtgeo/grid3d/_grid_etc1.py,sha256=C9NGNnRUEtwTF1J4iplgW60eqbZQiqQP5yBwQZIzRGc,38736
-xtgeo/grid3d/_grid_export.py,sha256=j7maRBhnP3j7TNOIZiBop6wFg2ulJvYpaRR1335fqx4,6502
-xtgeo/grid3d/_grid_hybrid.py,sha256=jiUFSaNO3pMeODrRQMQv-wFNcJGS6DPoMz55nl7j1j4,1444
-xtgeo/grid3d/_grid_import.py,sha256=XWYIKdYvHZfzW35aVKfjA1hhsVPdv_giFWdqZHLVLSI,1926
-xtgeo/grid3d/_grid_import_ecl.py,sha256=x1-hU9I-_tup6rJ1TOt_vOklLlR4K6ItfAwytAZ60Mk,3191
-xtgeo/grid3d/_grid_import_roff.py,sha256=nangmUFZU1y1q4Lv5MH4vu9g8OJ-gBuzR4FQz1w7cG4,3550
-xtgeo/grid3d/_grid_import_xtgcpgeom.py,sha256=CnYtpBYhHNOzuuK0YeNCyUXpZes5u-08IVEg8vK2rwo,8955
-xtgeo/grid3d/_grid_refine.py,sha256=DzaWq06SqUDxpX5Lto4_38m0FdK7JdNCLWtx8KF-z8g,3611
-xtgeo/grid3d/_grid_roxapi.py,sha256=tOR9a4rBfyKWF7gA2fvwKD6qUwenraPB7pxClezR1q0,15576
-xtgeo/grid3d/_grid_wellzone.py,sha256=Vi1NAMAtim3bz8ISiRjSRqngikL0d-COwD4eHBStFEI,5298
-xtgeo/grid3d/_gridprop_export.py,sha256=ZU4PvoWA4J_7_OeDZu2pSpI_NTiNKSn2t_UdeAdNKNE,3957
-xtgeo/grid3d/_gridprop_import_eclrun.py,sha256=wQZK7xywGOeaECowpExugElp_RIFhdNOrZlA_87d8XM,4848
-xtgeo/grid3d/_gridprop_import_grdecl.py,sha256=bsEACqOOLMY7Gsnr_xKfaiq6fE8RJTW9MyR-jKscjdE,3518
-xtgeo/grid3d/_gridprop_import_roff.py,sha256=f9cVCR-cguaw3143Xg7qN3mmLQq4M5htxIbznUtEtUs,1566
-xtgeo/grid3d/_gridprop_import_xtgcpprop.py,sha256=2TQjnWTMmld4tLjZHpaB-DaJohiSWInfx4tBd85GOIs,3139
-xtgeo/grid3d/_gridprop_lowlevel.py,sha256=rgCz40_W_LppmY4wYU8cOfHHQ0_78NsXUI5eeAL82U4,4877
-xtgeo/grid3d/_gridprop_op1.py,sha256=XSFyTdEuTMKRp2KkC--BbYUKP3sSgYF59AdOoi3ov8g,4939
-xtgeo/grid3d/_gridprop_roxapi.py,sha256=mYOcc3dqLl1r28ZdvJZxXiSFJKq9rCgHCajVDLrBjxQ,6742
-xtgeo/grid3d/_gridprop_value_init.py,sha256=gXjTaQiusuqBlKJmumhcYWs5v2Gyx2ufKd7NYSPzFLE,3369
-xtgeo/grid3d/_gridprops_import_eclrun.py,sha256=pCnhxNEo1Vu8LQ3aokZdnNGeVZLuBqB2WTirIHEDed4,10671
-xtgeo/grid3d/_gridprops_import_roff.py,sha256=8gpSq6iDkXolWh4U2ZZMFRXMQOSlt-T748UQjQcweIY,2009
-xtgeo/grid3d/_roff_grid.py,sha256=GWl7_zK4Iy0C6NPBkM33gLibaCz44E3NILPuxTV9fgQ,16965
-xtgeo/grid3d/_roff_parameter.py,sha256=PYdjg7nk2S08LR7H5C1JZ2L-NjtXdun_GBBJ4-IjohA,10778
-xtgeo/grid3d/grid.py,sha256=vS2Sk2mp06evQcw8pS5GEvUX4NiAAP6WTkr6w_bGBcE,95006
-xtgeo/grid3d/grid_properties.py,sha256=HFMX8cNRuAAlCV2BOTaj7hVEl5ZwzuRceeviZY-X9Lo,29435
-xtgeo/grid3d/grid_property.py,sha256=YhLGO0ON1QP4GVnTbzXhP7T8aoy-EBzQc9EkMZ345x4,50918
-xtgeo/metadata/__init__.py,sha256=-U-T6WJHQCRkThBl4CQEMekprQVGxyiZ5rEPGiM1QBw,191
-xtgeo/metadata/metadata.py,sha256=0CYn3Jdy_DkT0Bpeyb4RhTaYY2cHtKFuAjJIV7lS4UQ,13133
-xtgeo/plot/__init__.py,sha256=q6cIj3UajfxbVz5HatEPd65YhOFllNust3K7MQS4UJg,277
-xtgeo/plot/_colortables.py,sha256=Sznm9cP1kMVhDCaBjS6_Mq0uVmuW8_JIgPm_c9pW8LI,12395
-xtgeo/plot/baseplot.py,sha256=sZsV9TAClabpalOhDiYUIa7qkf3n7cvRvuHt94IlRls,8556
-xtgeo/plot/grid3d_slice.py,sha256=wLvuenLdkBCkYDUlw5pOHgcmZptZB3phbV9DeAxXs2A,6968
-xtgeo/plot/xsection.py,sha256=lD3yBkIdBEPfW68mDY1H4_5fWcl1PO_VN8G23HNE5EI,38946
-xtgeo/plot/xtmap.py,sha256=sRR9Jb-gk6idtB3k0NLq_NKKjBBDFU5-5kJ_YdVRt3M,7926
-xtgeo/roxutils/__init__.py,sha256=ami1fp8Gb5McU2RfGIJIHUPx3EdK-Pj7LtlX4Wq1VOA,121
-xtgeo/roxutils/_roxutils_etc.py,sha256=tSYkule8_JSl4X5nkBYe9zSCldBjbhYcJIyF3deK0pU,4185
-xtgeo/roxutils/roxutils.py,sha256=G4ynoRgDzFoeg7VV8IRxqPGhaAjuic5Y8zFyD7w3ueU,7297
-xtgeo/surface/__init__.py,sha256=JNfsPaRUekRWhqcNYdlhfg_qn9WZevYhHiUvfnyATtY,132
-xtgeo/surface/_regsurf_boundary.py,sha256=wM5oj5jaiNxEzhmga1qgw5Urpn3YOpp5C8i87kmLEm4,856
-xtgeo/surface/_regsurf_cube.py,sha256=to-o0FYjQVVxXnhTCSLNXGiPI1jnlEjLVGJye0y0xsM,5288
-xtgeo/surface/_regsurf_cube_window.py,sha256=WY9wuV_HalfdDs9bgV7Ib7hiMbhp_ZE2LUxtCqY4NKE,10289
-xtgeo/surface/_regsurf_cube_window_v2.py,sha256=ORh7rbL-CR19QKxH_PueDJy5_GmJvl0zVg7h6I3BmyA,6673
-xtgeo/surface/_regsurf_export.py,sha256=b-Jnr185LL89p1TaGRb2GKV0FquSHZDURZOqLb7l4Gk,12876
-xtgeo/surface/_regsurf_grid3d.py,sha256=DgEyrT5jfmZt4Or_z7JD7xhe-bGfAv5kD5UHg6jey28,5137
-xtgeo/surface/_regsurf_gridding.py,sha256=TRFsrNTTj5-OD0yJGmHZijt6KivaM_bQH85iXZLzOj4,10368
-xtgeo/surface/_regsurf_import.py,sha256=qMMwm6fh6IEZGi0DiqlJn7hzQql3aYNQaXpqo2OwoN8,15699
-xtgeo/surface/_regsurf_lowlevel.py,sha256=m17aCW2Suqch7QaAmI1i8Qzhec5R-uXjoV8l9LGexrs,3610
-xtgeo/surface/_regsurf_oper.py,sha256=xKn_O1JgACD-UKmKxmJQFpYHwxvqBV5_Zx-UnW54yyo,14313
-xtgeo/surface/_regsurf_roxapi.py,sha256=TQ-6fs3v9OtiMFUr_yM4RZSKwKA5ZHszaPtm5dB5hLw,8174
-xtgeo/surface/_regsurf_utils.py,sha256=7Epc-7wRkSAVx-KxDR5ekcCvjALYcnmRJ3PdM9aC1wc,1408
-xtgeo/surface/_surfs_import.py,sha256=kYe47GDnZGYd778psdOCPpp7HmtJXo1MydRt7RiCJRo,1337
-xtgeo/surface/_zmap_parser.py,sha256=BzGJAatp6p3ASk3t43yHfJv-iXYhl_buCAfJ_2BgCJY,4354
-xtgeo/surface/regular_surface.py,sha256=nG87NBz7J_-P7QB4V9hSbChQNIhZoE2-pLBV9wTfeaU,115479
-xtgeo/surface/surfaces.py,sha256=fo8TA-iZyqeLtDle3YQXjJC7dedhXUenPpdgNmCi2d0,7842
-xtgeo/well/__init__.py,sha256=N3BEiRyxoOIOov7t0oAfeM2MygS_pnDdMT2oPzZHEvI,208
-xtgeo/well/_blockedwell_roxapi.py,sha256=KG6O_cUiSGNogqmqnJHtj9BDaxCuynXi01-1WAgSsIE,6760
-xtgeo/well/_blockedwells_roxapi.py,sha256=wxoEls0_cGekvRyC2IhqFAQPraOBLjP2Ejnz6rf6bJ4,2029
-xtgeo/well/_well_io.py,sha256=Q5XRgbc0RuHE_xPnf0YtzT_kXmRbVQDny6zHv2MVa9w,10201
-xtgeo/well/_well_oper.py,sha256=FZPRXy-p2Iocjge_uYyUr38_9w4rokmNifinVhSemMA,18723
-xtgeo/well/_well_roxapi.py,sha256=pV2A2gGTL0cpSckjO8LqMuGFK5GmreLE-zu0YfqBGgw,9086
-xtgeo/well/_wellmarkers.py,sha256=S348g3iqX6g2V7g7KDXT-ARZPs9jOolyTaYbN9ARf04,14345
-xtgeo/well/_wells_utils.py,sha256=FDIRdWlxQZLbl4iRz69brFJTkqHll7nHaAZtLdSmtxs,4983
-xtgeo/well/blocked_well.py,sha256=-fr8mQGbCE5y16odfbNUiOn57gHkwp3bNxkdfyaKBl8,7787
-xtgeo/well/blocked_wells.py,sha256=fdF4pzZQAUjMjQ8o3cyHXwImBTsGxDPg1Zy4UTzKsXA,5980
-xtgeo/well/well1.py,sha256=eZ9691hq7_S9tkSf23_GPtQMDdQRPzm7AzphKmZDUhs,59650
-xtgeo/well/wells.py,sha256=ALJAbXS3ClxOhq1uyPQJk0X3nbZmIOPQXNSuxq0piso,9592
-xtgeo/xyz/__init__.py,sha256=cJTKgST1ejl0Zo-56D0wk2oP8K66uWo2JQbkPzk6B3c,207
-xtgeo/xyz/_polygons_oper.py,sha256=J3ErkoqhKAmXa32YMz-mN1JSkjJOP585SFcj33uRks4,9561
-xtgeo/xyz/_xyz.py,sha256=CwgXbYGjJ003TyVhOJZU5kFwaJh72iJNaC8nZeMTsW0,13727
-xtgeo/xyz/_xyz_io.py,sha256=YWM1qsywlIEyp-gVCiCu6J9MU1j1XWUg7fw5zBe3BwU,15033
-xtgeo/xyz/_xyz_lowlevel.py,sha256=nOdmENFDbkpyPyBgfxlKRbpewbhMvByjMBjihiKWgko,1172
-xtgeo/xyz/_xyz_oper.py,sha256=qqBa2zHzwaga2ABcTKPwcGqMrFC1lJQcXEwm2e8a20U,15881
-xtgeo/xyz/_xyz_roxapi.py,sha256=6MH_RbvdzYgllG8N-zdeyMfMYEVjS1jKYTvgVvuJb5I,19050
-xtgeo/xyz/points.py,sha256=H3eZMf4f4l6SaoO9UnbXyoJvb8VlmLAxyjW1lhPGJQY,33599
-xtgeo/xyz/polygons.py,sha256=xaTNJRYrvOGitOgqhAVDWFtns-thod41vbdCyieMQSM,29860
-xtgeo-3.1.1rc1.dist-info/LICENSE.md,sha256=fTqV5eBpeAZO0_jit8j4Ref9ikBSlHJ8xwj5TLg7gFk,7817
-xtgeo-3.1.1rc1.dist-info/METADATA,sha256=pZHo1rGUZqRnvJQUcr_TIeHLKn_vOQKBi7qacLvJsKk,5636
-xtgeo-3.1.1rc1.dist-info/WHEEL,sha256=YoO0-DmSs1D87Tlb4zCCTxGtIWnrG_uruXu2RsilZzY,94
-xtgeo-3.1.1rc1.dist-info/top_level.txt,sha256=Pt9j2InNWpJFISrQTTbqXRV2blCf1HlGu27Zhprqc78,6
-xtgeo-3.1.1rc1.dist-info/RECORD,,
+xtgeo-3.1.2.dist-info/METADATA,sha256=omwfuEIV8mtpswNYxp_itBTP8Zv24aRwmRlOHnp3Qbw,5497
+xtgeo-3.1.2.dist-info/LICENSE.md,sha256=46mU2C5kSwOnkqkw9XQAJlhBL2JAf1_uCD8lVcXyMRg,7652
+xtgeo-3.1.2.dist-info/top_level.txt,sha256=Pt9j2InNWpJFISrQTTbqXRV2blCf1HlGu27Zhprqc78,6
+xtgeo-3.1.2.dist-info/WHEEL,sha256=0JueSH4SDu5P2EQaaFfkqBxHzdkqRWcoI979r26_tAU,142
+xtgeo-3.1.2.dist-info/RECORD,,
+xtgeo/_theversion.py,sha256=r7hKxKtouMvEpfJdb_ELcTh7-jQTuCTfA2lJQMCJR2U,160
+xtgeo/__init__.py,sha256=-KPLg1alFwRZPHGISJCQcT4tLdC76N2YQHIyd6neZNQ,5746
+xtgeo/xyz/polygons.py,sha256=VsqA1hu9SCPhNL2-GYkZw9XK-HQoc1U9skVrDoIpPrY,28996
+xtgeo/xyz/_polygons_oper.py,sha256=KbY5NksP2ZNieQyFdWt8cuLk5vXmodCu1UvAu9rn6UA,9279
+xtgeo/xyz/_xyz.py,sha256=UWsJzsObFUUVQ-4ceATwfeCjrFVNtu5mfdZarfsbdZ0,13334
+xtgeo/xyz/_xyz_oper.py,sha256=qZFioCwGSbSVi0P_BUqmOvqB2Xcxpn6Z0rUBStSnSZ4,15379
+xtgeo/xyz/__init__.py,sha256=2Szuy8iWYbXyVzPXQwwf5XP6ub4MEVb6Bz0QqWEbFa4,199
+xtgeo/xyz/_xyz_io.py,sha256=2wkwyBkmCn-FqUgjYioLBvATMus-U9KE3wVZS-QIdfM,14552
+xtgeo/xyz/_xyz_roxapi.py,sha256=6gNmMBFsSgMl-LMZmriX3hUb4b-BNAZIMA52mZKSDfo,18493
+xtgeo/xyz/_xyz_lowlevel.py,sha256=LfyICg7AUrW5Wt-fxwI8Ln6tPhrbtvSn0aWKfUrAJ9E,1125
+xtgeo/xyz/points.py,sha256=bd5M0ng3XFYv0JsGVbgJkcytFuvR3H6TqHP3UBMP5AQ,32611
+xtgeo/cube/cube1.py,sha256=397SYVKo_SYMFVtAiVm7bs18eFToUFxGn8GrRBYLzd4,35461
+xtgeo/cube/__init__.py,sha256=Nc4NCJruRAdhQ5mnEPK5fX-i5XCZiGb809_4HxYuAgg,149
+xtgeo/cube/_cube_roxapi.py,sha256=6FzFL89HiU0oJhZtsAG6vntBAPMnGFEOUYEtN44xa8w,4921
+xtgeo/cube/_cube_utils.py,sha256=Kzg9WeZneUaL_60lJE6b4Fz95xzoWATY_AiMJoClF64,7442
+xtgeo/cube/_cube_import.py,sha256=SBnTN_tbzW2eHg9Ciwqiy6u56hlyxBYCBR8HZxyY9Lg,19440
+xtgeo/cube/_cube_export.py,sha256=a34qQag49pa5NcQT2v2T3OBZNnTxU-FN-nGepOZO7Xg,6571
+xtgeo/common/constants.py,sha256=884aclAbhoZe_ilsu1oJ5Ohm1nz257d6Bb0On_43ups,534
+xtgeo/common/exceptions.py,sha256=DzyWIGOOrQPzcyrUpncHFV9N2Rbgm3BMEOQwyyycmTE,1071
+xtgeo/common/__init__.py,sha256=k9IYjoFc11whcDBYjhPIGX8ONKJSGcPJcA6hguYapVQ,372
+xtgeo/common/calc.py,sha256=H_HKTORauABM4UipWyynM0PUVGAPGuBeN57rarrpxZ8,9157
+xtgeo/common/xtgeo_dialog.py,sha256=3tXoMoi3JWNivZdxvQj-L61Agif2OCdfkET8fH3Ca7I,17129
+xtgeo/common/sys.py,sha256=spvqFW2ORkmJs-AhQFmKLKF4o357-YKsk6fNpnkLkgU,23421
+xtgeo/roxutils/_roxutils_etc.py,sha256=wg-ZaeEZ7qY3fMF550EnbpuTP454ykdcXteiz1tYbak,4052
+xtgeo/roxutils/__init__.py,sha256=EqSzIqo3NzP3fPoK9kk5l9UzYIMAO2kK4uz2mC03Xdg,115
+xtgeo/roxutils/roxutils.py,sha256=W5jq4UZFqGjK9i_t44h6XhRUvaTKo5WPKhSJ4r2FLFI,7058
+xtgeo/plot/baseplot.py,sha256=5xhgiB9iofJf8XTB3et-7wKN3mOHItPgLLQDJX6_O-4,8300
+xtgeo/plot/__init__.py,sha256=zPAM-8DzLNFbwlO29QsffoJ1xzIhEbYMqYXzvqzIhns,267
+xtgeo/plot/grid3d_slice.py,sha256=QgljeDY7cJyFoQDvy5ioxFs6UjwuOB1ctOjM-1tBkHE,6761
+xtgeo/plot/xsection.py,sha256=GPYU35ohEvd67EANQ5P-CWGy9Ebaw3CCU2fmRxd6cIU,37742
+xtgeo/plot/_colortables.py,sha256=Bo-5ab__wg-tVJ0P9ekj1v-1ItOzZnXzQoh3PcGQi4o,12040
+xtgeo/plot/xtmap.py,sha256=GIb4UafCVrhJgvAs72kst5_EZ0FUyU3YWRL8EliBuBE,7676
+xtgeo/cxtgeo/_cxtgeo.cpython-39-x86_64-linux-gnu.so,sha256=pPC7NR_h3yqtCsmSDVwY9eVIcXyKunLpe7lIRM1Y8aQ,648840
+xtgeo/cxtgeo/__init__.py,sha256=AbpHGcgLb-kRsJGnwFEktk7uzpZOCcBY74-YBdrKVGs,1
+xtgeo/metadata/metadata.py,sha256=CZ_MrMcK3G6kQ7J3VEnhhiBetgoESoorEvTpPCJS0KY,12689
+xtgeo/metadata/__init__.py,sha256=oqGZhGswZTMWqsy_1fOYuphltoBDpo-z1trF_fhKkUU,185
+xtgeo/surface/_regsurf_oper.py,sha256=PIHGIheThviBF4fPEHxHNktYf1Msatzf3lh-bRjXnd4,13772
+xtgeo/surface/_regsurf_import.py,sha256=Pot1QcLZJJbcy7g2Bn1t7_oJoVXI1gA9e9haDqdo004,15166
+xtgeo/surface/_zmap_parser.py,sha256=B9dqD_7UygHYJ_ZXVWVShbYm8dvg4S8fTXC5f139rP0,4212
+xtgeo/surface/_regsurf_utils.py,sha256=gCyD-nTiZevi9nreZPhheKqoVVFEKPDfjpoBSjmiTFM,1353
+xtgeo/surface/__init__.py,sha256=LZSMubu4lWrttHFtaA9LJhvfcGvgv89y8PpgmR4dX-0,126
+xtgeo/surface/_regsurf_gridding.py,sha256=eKs5CA9ZnnFNAX-npbiFFyoMA2akE0fKEWHLgmQFZ8Y,10025
+xtgeo/surface/_regsurf_cube_window.py,sha256=56OPnvmufuaV8KyApIMf9JT0B5Mwat9RThJiRlQcF7A,9910
+xtgeo/surface/_regsurf_cube_window_v2.py,sha256=gJWU7scTpPIEdzE_JKQ6tXOqJbDIKnTTe4zC_sgOLnk,6366
+xtgeo/surface/_regsurf_lowlevel.py,sha256=Cl_4sijI-ez-9U5UokxbvH7YedHw8TNxHsjB2FVLhSc,3488
+xtgeo/surface/_surfs_import.py,sha256=hGnx8aGx95oyCvvHBIN9msb6xwug_f0UKnOsoYexcec,1290
+xtgeo/surface/surfaces.py,sha256=jtUuHAkPY1prWSNHqBqX-Y5poIfIf_-jiS6Kqpaxf-w,7609
+xtgeo/surface/regular_surface.py,sha256=eklFHeYvg2BLyHpL06b-gndY431y0lKXi8pvSVQ1dDo,112381
+xtgeo/surface/_regsurf_export.py,sha256=JCuBXUbJXD3sYE1rhgcuX_fj19yNDzISMUlfgEeYIUE,12391
+xtgeo/surface/_regsurf_cube.py,sha256=CJAFdFI55GNqIk4Vv8PSNa_U3iPu8lEUmb0Q9BKENWc,5067
+xtgeo/surface/_regsurf_roxapi.py,sha256=Zq6xRvhG5XU-7ppAhumXbDJ7JG5hdF1NAXBF7Bf6gSA,7940
+xtgeo/surface/_regsurf_boundary.py,sha256=3J9u12hA9oTlHEQ4eBDnWJpYGKpsJKQ--3tIwz_GB_I,828
+xtgeo/surface/_regsurf_grid3d.py,sha256=E5iwdq1G9fQ-AnnAdTicpKCFUQAllfWo2UdmM-x-qh8,4950
+xtgeo/well/_well_oper.py,sha256=yILHBLD3aMZAWsIyEGdi1-LdtO9U_CJ3wIr__e4kaAU,18145
+xtgeo/well/_wells_utils.py,sha256=jpzM47sj3iJQusAEC4cDXnYVyV06seBK9-iN47G_WUw,4836
+xtgeo/well/well1.py,sha256=Q1dXIDxrOWYHP1f8VI2TZTnGlXngcJlxSKIpVbSkJB4,57921
+xtgeo/well/_well_roxapi.py,sha256=0cbpjpmXBixqaSovmGl4Ug8vXP4f9V4KPBLpIAYxQt0,8792
+xtgeo/well/blocked_well.py,sha256=Qnh7aFI1uCch_VO_DFaYjB6aXX_s5v2xrNMzlazs3TA,7549
+xtgeo/well/__init__.py,sha256=e6ZaMen3qMXrCMSaKgCTP_JPlVTZgKs50zUz1A3VWkc,198
+xtgeo/well/blocked_wells.py,sha256=pvCLJd-ezjl5cGQK7sKNqP2FNMpDX2ZF50bmCq-2pqs,5792
+xtgeo/well/_wellmarkers.py,sha256=a2NkX1f2hf8MaFibjeTDf8UOxMZdbhGXxfL9Lf4ZCug,13868
+xtgeo/well/_well_io.py,sha256=0CJ6ko40tr7KwBsLWF3kEXCZO_Hjaf2S98FXXEgG5xw,9871
+xtgeo/well/wells.py,sha256=dE8-mIRQPN-G46wZL7RBLCRZuQbUtc6cKMWo_V_53FI,9294
+xtgeo/well/_blockedwells_roxapi.py,sha256=2-iAdBzhiRotH2NZ9ZHtJSdY_uvRrbcZMFKssIWkj7Q,1960
+xtgeo/well/_blockedwell_roxapi.py,sha256=qc_wxu8qA7ZQn3IZ38zQ7VhfuJfhOEiWA_-Vw5FJvvE,6555
+xtgeo/grid3d/_gridprop_op1.py,sha256=FKqB6nZU6-AOYWLFnJFOUIAXQnvAg26h7OZn-dXmpIU,4779
+xtgeo/grid3d/_ecl_output_file.py,sha256=n-XrsKM0jPAtIvgJkTgQj08dnGdFVTENLJVSjEPVXsU,1909
+xtgeo/grid3d/_grid3d.py,sha256=dYIBAgNEXFDmrcb1dJrXmzehCPvZ0nRj8r1zP7ZtfqE,1182
+xtgeo/grid3d/_gridprops_import_roff.py,sha256=hA7CVRw0bTrTzNumIeRCUZvuxuGLOW2YK-L-3832axk,1944
+xtgeo/grid3d/_gridprop_lowlevel.py,sha256=mjlO9XpJjFjGOY-0HU4_GjX4gI4bboHUMRcw4QErFiY,4717
+xtgeo/grid3d/_gridprop_import_grdecl.py,sha256=OdJ6XT82_jY5cwa0p7ji1xJB3iNDc64mnwwPW0_pddQ,3412
+xtgeo/grid3d/_egrid.py,sha256=m1uu1-imwXdG9GlZad4a7PPj0g2GO99tvGP8EsI4sCg,33747
+xtgeo/grid3d/_grid_import_ecl.py,sha256=FfeOwk-tyXm8whp1cjaTzr5SgNuZQpJvpVysoori7d0,3096
+xtgeo/grid3d/_ecl_grid.py,sha256=Kv7MvyxjIW6nh0m4hT0qlBA3OFob7eFybLlMhyZb5oA,26086
+xtgeo/grid3d/_find_gridprop_in_eclrun.py,sha256=I8wH9YtpYHWgDH0DouGk_TY4HTNRYT-gPwM5XZ3oF3A,21476
+xtgeo/grid3d/_ecl_logi_head.py,sha256=BGjxoYhurCs7Ue3DUAS1_0d-SJ1OFJZQRudZUXlg-Do,2663
+xtgeo/grid3d/_grid3d_fence.py,sha256=CChvmhLS2B3-DL4wJnAEO5ULyJLCfdVHV_nuqGCYhNE,4969
+xtgeo/grid3d/__init__.py,sha256=aqeehdBVvJgNjjJaXNCSfmZZ40DnYbmkyzHNAXBEnfM,341
+xtgeo/grid3d/_grid_refine.py,sha256=ZqMLuHY9Rxyo7ZofupjD6qQ2FYdGyIAfCqx9eD1t-8Q,3491
+xtgeo/grid3d/_gridprop_import_xtgcpprop.py,sha256=_gVuYPM2pAkMUcojEyB0sbah4ahbo1KkME4tINUZ884,3032
+xtgeo/grid3d/_gridprop_import_roff.py,sha256=2Lq1Zrv9gA9iijFAEElLsG164kK1OYNMAakZuSfdJC8,1514
+xtgeo/grid3d/_grid_import_roff.py,sha256=h0h54LUMyD7uNiKq_cZI9x1zgJ5ExVU25Cek2LZvw7s,3433
+xtgeo/grid3d/_roff_parameter.py,sha256=WdF4tYCkowRnzwmXyehJ6y8UwBOXIpFhYoeCzx3E9CU,10489
+xtgeo/grid3d/_grid3d_utils.py,sha256=ZhnkKaQsJeA83cYwCNVfqg6mYv_IkNurqhcbZ7fUeWU,5650
+xtgeo/grid3d/_roff_grid.py,sha256=DTdosdidWlrm8DdePCf7ufsfVPVoswxtJQx3pr2vRWs,16521
+xtgeo/grid3d/_grid_import.py,sha256=phBK1AEV7dLLuDdlLmoyNyC96R-4Rwb1t8yGuwoqX4w,1869
+xtgeo/grid3d/_grid_hybrid.py,sha256=JbKuo4dXiYVLM9G2qtUkqW9kIRlu-H5gl78VCBTdl38,1383
+xtgeo/grid3d/_gridprop_export.py,sha256=7MAcRMlpWrCd7hpyQTum7sMAaPbejcvyehVIWpV9mzU,3818
+xtgeo/grid3d/_grdecl_format.py,sha256=1jcbB6idXQr1JeD3BGTIMSJzPOzasQRsarCEyGojYrA,8239
+xtgeo/grid3d/grid_properties.py,sha256=05PtXHFI2tzL9Zl0aa6F9ktsOQYUMiw__lQyzKpVjaA,28624
+xtgeo/grid3d/_grid_wellzone.py,sha256=Z7IW4O9tOlChWGonl4NOJ35iKJ5134BKDMYV_aaM-X4,5144
+xtgeo/grid3d/_gridprop_roxapi.py,sha256=3SjVx7gFjgJP8sgKbV50dW98UxF-tF3ERTlERQ6FK04,6515
+xtgeo/grid3d/_grid_export.py,sha256=7DIDdtVIFlhuawf2aFBhfqzWVfL3_MltnU39O_mcs28,6299
+xtgeo/grid3d/grid.py,sha256=nj3Cr2HYB52r5ZbM3URVlFCKrBmVF9oCcav-gJRyHvc,92430
+xtgeo/grid3d/_gridprop_import_eclrun.py,sha256=3x0BkE6VfXRWzgnTZMGDDz57NVgiuuFbfcG7h30OsSs,4704
+xtgeo/grid3d/_gridprop_value_init.py,sha256=JyN-H8i529kQ-jbXAS_r3zA18co6TbT045FP3bHxLfU,3268
+xtgeo/grid3d/_grdecl_grid.py,sha256=WdZ-99Tq3Sjaz8YmRlmoZv9qHdNrZqSxF1hzRwrhoo8,12695
+xtgeo/grid3d/_grid_etc1.py,sha256=8UGuEwnUcC6luXSvbA0H679SDgkAp4kQ6JVIfa3V0pQ,37296
+xtgeo/grid3d/_grid_import_xtgcpgeom.py,sha256=X-jy_GrQQ8wWYwXSTZkWC5D6I8CkBEIXdTA6Z-O_-pk,8665
+xtgeo/grid3d/grid_property.py,sha256=8HjXU06JQUqEo2RSG5kpj2vTqCwfC_uScWqqeAANhTo,49493
+xtgeo/grid3d/_gridprops_import_eclrun.py,sha256=BRh2_Bfe7WKtVLZqbhb-zVQ0JmDxZOZTbs_0CGMHNx0,10365
+xtgeo/grid3d/_ecl_inte_head.py,sha256=QxO0zis880zcTyCBFVemd52bDszlc9SHKnwNUpq3ewQ,4170
+xtgeo/grid3d/_grid_roxapi.py,sha256=sfzrwLal5o8-5ct3U2V97OiH207oB0p6hIw78FIOIq0,15062
```

