# Comparing `tmp/fastdatasets-0.9.7.post0-py3-none-any.whl.zip` & `tmp/fastdatasets-0.9.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,54 @@
-Zip file size: 42841 bytes, number of entries: 37
--rw-rw-rw-  2.0 fat      116 b- defN 23-May-02 04:08 fastdatasets/__init__.py
--rw-rw-rw-  2.0 fat     2316 b- defN 23-May-07 14:24 fastdatasets/setup.py
--rw-rw-rw-  2.0 fat       76 b- defN 23-May-02 04:08 fastdatasets/common/__init__.py
--rw-rw-rw-  2.0 fat    14096 b- defN 23-May-02 04:08 fastdatasets/common/iterable_dataset.py
--rw-rw-rw-  2.0 fat    10767 b- defN 23-May-02 04:08 fastdatasets/common/random_dataset.py
--rw-rw-rw-  2.0 fat     4251 b- defN 23-May-07 14:23 fastdatasets/common/writer.py
--rw-rw-rw-  2.0 fat     1135 b- defN 23-May-02 04:08 fastdatasets/common/writer_object.py
--rw-rw-rw-  2.0 fat      209 b- defN 23-May-02 04:08 fastdatasets/leveldb/__init__.py
--rw-rw-rw-  2.0 fat     6031 b- defN 23-May-02 04:08 fastdatasets/leveldb/dataset.py
--rw-rw-rw-  2.0 fat     4489 b- defN 23-May-02 04:08 fastdatasets/leveldb/writer.py
--rw-rw-rw-  2.0 fat     7148 b- defN 23-May-02 04:08 fastdatasets/leveldb/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     4716 b- defN 23-May-02 04:08 fastdatasets/leveldb/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat      215 b- defN 23-May-02 04:08 fastdatasets/lmdb/__init__.py
--rw-rw-rw-  2.0 fat     6778 b- defN 23-May-02 04:08 fastdatasets/lmdb/dataset.py
--rw-rw-rw-  2.0 fat     4696 b- defN 23-May-02 04:08 fastdatasets/lmdb/writer.py
--rw-rw-rw-  2.0 fat     7765 b- defN 23-May-02 04:08 fastdatasets/lmdb/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     5796 b- defN 23-May-02 04:08 fastdatasets/lmdb/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat      209 b- defN 23-May-02 04:08 fastdatasets/memory/__init__.py
--rw-rw-rw-  2.0 fat     4704 b- defN 23-May-02 04:08 fastdatasets/memory/dataset.py
--rw-rw-rw-  2.0 fat     3741 b- defN 23-May-02 04:08 fastdatasets/memory/writer.py
--rw-rw-rw-  2.0 fat     6474 b- defN 23-May-02 04:08 fastdatasets/memory/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     4304 b- defN 23-May-02 04:08 fastdatasets/memory/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat      209 b- defN 23-May-02 04:08 fastdatasets/record/__init__.py
--rw-rw-rw-  2.0 fat     6007 b- defN 23-May-02 04:08 fastdatasets/record/dataset.py
--rw-rw-rw-  2.0 fat     3800 b- defN 23-May-02 04:08 fastdatasets/record/writer.py
--rw-rw-rw-  2.0 fat     7358 b- defN 23-May-02 04:08 fastdatasets/record/iterable_dataset/__init__.py
--rw-rw-rw-  2.0 fat     6788 b- defN 23-May-02 04:08 fastdatasets/record/random_dataset/__init__.py
--rw-rw-rw-  2.0 fat     5576 b- defN 23-May-02 04:08 fastdatasets/torch_dataset/__init__.py
--rw-rw-rw-  2.0 fat      980 b- defN 23-May-02 04:08 fastdatasets/utils/MEMORY.py
--rw-rw-rw-  2.0 fat       51 b- defN 23-May-02 04:08 fastdatasets/utils/__init__.py
--rw-rw-rw-  2.0 fat    12220 b- defN 23-May-02 04:08 fastdatasets/utils/numpyadapter.py
--rw-rw-rw-  2.0 fat     5457 b- defN 23-May-02 04:08 fastdatasets/utils/parallel.py
--rw-rw-rw-  2.0 fat      346 b- defN 23-May-02 04:08 fastdatasets/utils/py_features.py
--rw-rw-rw-  2.0 fat    12115 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       13 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     3357 b- defN 23-May-07 14:26 fastdatasets-0.9.7.post0.dist-info/RECORD
-37 files, 164401 bytes uncompressed, 37381 bytes compressed:  77.3%
+Zip file size: 57958 bytes, number of entries: 52
+-rw-rw-rw-  2.0 fat      116 b- defN 23-May-04 00:32 fastdatasets/__init__.py
+-rw-rw-rw-  2.0 fat     2311 b- defN 23-Jul-03 04:35 fastdatasets/setup.py
+-rw-rw-rw-  2.0 fat      243 b- defN 23-Jul-03 05:45 fastdatasets/arrow/__init__.py
+-rw-rw-rw-  2.0 fat     6558 b- defN 23-Jul-03 04:35 fastdatasets/arrow/dataset.py
+-rw-rw-rw-  2.0 fat      240 b- defN 23-Jul-03 04:35 fastdatasets/arrow/default.py
+-rw-rw-rw-  2.0 fat     2996 b- defN 23-Jul-03 04:35 fastdatasets/arrow/writer.py
+-rw-rw-rw-  2.0 fat     9904 b- defN 23-Jul-03 05:46 fastdatasets/arrow/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5996 b- defN 23-Jul-03 05:47 fastdatasets/arrow/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat       76 b- defN 23-May-04 00:32 fastdatasets/common/__init__.py
+-rw-rw-rw-  2.0 fat    14096 b- defN 23-May-04 00:32 fastdatasets/common/iterable_dataset.py
+-rw-rw-rw-  2.0 fat    10767 b- defN 23-May-04 00:32 fastdatasets/common/random_dataset.py
+-rw-rw-rw-  2.0 fat     4251 b- defN 23-May-08 00:28 fastdatasets/common/writer.py
+-rw-rw-rw-  2.0 fat     1135 b- defN 23-May-04 00:32 fastdatasets/common/writer_object.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-May-04 00:32 fastdatasets/leveldb/__init__.py
+-rw-rw-rw-  2.0 fat     5687 b- defN 23-Jul-03 04:35 fastdatasets/leveldb/dataset.py
+-rw-rw-rw-  2.0 fat      206 b- defN 23-Jul-03 04:35 fastdatasets/leveldb/default.py
+-rw-rw-rw-  2.0 fat     4489 b- defN 23-May-04 00:32 fastdatasets/leveldb/writer.py
+-rw-rw-rw-  2.0 fat     7803 b- defN 23-Jul-03 04:35 fastdatasets/leveldb/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     4615 b- defN 23-Jul-03 04:35 fastdatasets/leveldb/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      215 b- defN 23-May-04 00:32 fastdatasets/lmdb/__init__.py
+-rw-rw-rw-  2.0 fat     6145 b- defN 23-Jul-03 04:35 fastdatasets/lmdb/dataset.py
+-rw-rw-rw-  2.0 fat      602 b- defN 23-Jul-03 04:35 fastdatasets/lmdb/default.py
+-rw-rw-rw-  2.0 fat     4696 b- defN 23-May-04 00:32 fastdatasets/lmdb/writer.py
+-rw-rw-rw-  2.0 fat     8233 b- defN 23-Jul-03 04:35 fastdatasets/lmdb/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5541 b- defN 23-Jul-03 04:35 fastdatasets/lmdb/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-May-04 00:32 fastdatasets/memory/__init__.py
+-rw-rw-rw-  2.0 fat     5608 b- defN 23-Jul-03 04:35 fastdatasets/memory/dataset.py
+-rw-rw-rw-  2.0 fat     3741 b- defN 23-May-04 00:32 fastdatasets/memory/writer.py
+-rw-rw-rw-  2.0 fat     6895 b- defN 23-Jul-03 04:35 fastdatasets/memory/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     4351 b- defN 23-Jul-03 04:35 fastdatasets/memory/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      243 b- defN 23-Jul-03 05:45 fastdatasets/parquet/__init__.py
+-rw-rw-rw-  2.0 fat     6578 b- defN 23-Jul-03 04:35 fastdatasets/parquet/dataset.py
+-rw-rw-rw-  2.0 fat      348 b- defN 23-Jul-03 04:35 fastdatasets/parquet/default.py
+-rw-rw-rw-  2.0 fat     3019 b- defN 23-Jul-03 04:35 fastdatasets/parquet/writer.py
+-rw-rw-rw-  2.0 fat     9619 b- defN 23-Jul-03 05:47 fastdatasets/parquet/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5516 b- defN 23-Jul-03 05:49 fastdatasets/parquet/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      209 b- defN 23-May-04 00:32 fastdatasets/record/__init__.py
+-rw-rw-rw-  2.0 fat     6391 b- defN 23-Jul-03 05:19 fastdatasets/record/dataset.py
+-rw-rw-rw-  2.0 fat      188 b- defN 23-Jul-03 05:42 fastdatasets/record/default.py
+-rw-rw-rw-  2.0 fat     3800 b- defN 23-May-04 00:32 fastdatasets/record/writer.py
+-rw-rw-rw-  2.0 fat     7768 b- defN 23-Jul-03 04:35 fastdatasets/record/iterable_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     6806 b- defN 23-Jul-03 04:35 fastdatasets/record/random_dataset/__init__.py
+-rw-rw-rw-  2.0 fat     5576 b- defN 23-May-04 00:32 fastdatasets/torch_dataset/__init__.py
+-rw-rw-rw-  2.0 fat      986 b- defN 23-Jul-03 04:35 fastdatasets/utils/MEMORY.py
+-rw-rw-rw-  2.0 fat       51 b- defN 23-May-04 00:32 fastdatasets/utils/__init__.py
+-rw-rw-rw-  2.0 fat    12220 b- defN 23-May-04 00:32 fastdatasets/utils/numpyadapter.py
+-rw-rw-rw-  2.0 fat     5457 b- defN 23-May-04 00:32 fastdatasets/utils/parallel.py
+-rw-rw-rw-  2.0 fat      346 b- defN 23-May-04 00:32 fastdatasets/utils/py_features.py
+-rw-rw-rw-  2.0 fat    14385 b- defN 23-Jul-03 05:50 fastdatasets-0.9.9.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-Jul-03 05:50 fastdatasets-0.9.9.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       13 b- defN 23-Jul-03 05:50 fastdatasets-0.9.9.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4698 b- defN 23-Jul-03 05:50 fastdatasets-0.9.9.dist-info/RECORD
+52 files, 222243 bytes uncompressed, 50372 bytes compressed:  77.3%
```

## zipnote {}

```diff
@@ -1,13 +1,31 @@
 Filename: fastdatasets/__init__.py
 Comment: 
 
 Filename: fastdatasets/setup.py
 Comment: 
 
+Filename: fastdatasets/arrow/__init__.py
+Comment: 
+
+Filename: fastdatasets/arrow/dataset.py
+Comment: 
+
+Filename: fastdatasets/arrow/default.py
+Comment: 
+
+Filename: fastdatasets/arrow/writer.py
+Comment: 
+
+Filename: fastdatasets/arrow/iterable_dataset/__init__.py
+Comment: 
+
+Filename: fastdatasets/arrow/random_dataset/__init__.py
+Comment: 
+
 Filename: fastdatasets/common/__init__.py
 Comment: 
 
 Filename: fastdatasets/common/iterable_dataset.py
 Comment: 
 
 Filename: fastdatasets/common/random_dataset.py
@@ -21,14 +39,17 @@
 
 Filename: fastdatasets/leveldb/__init__.py
 Comment: 
 
 Filename: fastdatasets/leveldb/dataset.py
 Comment: 
 
+Filename: fastdatasets/leveldb/default.py
+Comment: 
+
 Filename: fastdatasets/leveldb/writer.py
 Comment: 
 
 Filename: fastdatasets/leveldb/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/leveldb/random_dataset/__init__.py
@@ -36,14 +57,17 @@
 
 Filename: fastdatasets/lmdb/__init__.py
 Comment: 
 
 Filename: fastdatasets/lmdb/dataset.py
 Comment: 
 
+Filename: fastdatasets/lmdb/default.py
+Comment: 
+
 Filename: fastdatasets/lmdb/writer.py
 Comment: 
 
 Filename: fastdatasets/lmdb/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/lmdb/random_dataset/__init__.py
@@ -60,20 +84,41 @@
 
 Filename: fastdatasets/memory/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/memory/random_dataset/__init__.py
 Comment: 
 
+Filename: fastdatasets/parquet/__init__.py
+Comment: 
+
+Filename: fastdatasets/parquet/dataset.py
+Comment: 
+
+Filename: fastdatasets/parquet/default.py
+Comment: 
+
+Filename: fastdatasets/parquet/writer.py
+Comment: 
+
+Filename: fastdatasets/parquet/iterable_dataset/__init__.py
+Comment: 
+
+Filename: fastdatasets/parquet/random_dataset/__init__.py
+Comment: 
+
 Filename: fastdatasets/record/__init__.py
 Comment: 
 
 Filename: fastdatasets/record/dataset.py
 Comment: 
 
+Filename: fastdatasets/record/default.py
+Comment: 
+
 Filename: fastdatasets/record/writer.py
 Comment: 
 
 Filename: fastdatasets/record/iterable_dataset/__init__.py
 Comment: 
 
 Filename: fastdatasets/record/random_dataset/__init__.py
@@ -93,20 +138,20 @@
 
 Filename: fastdatasets/utils/parallel.py
 Comment: 
 
 Filename: fastdatasets/utils/py_features.py
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/METADATA
+Filename: fastdatasets-0.9.9.dist-info/METADATA
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/WHEEL
+Filename: fastdatasets-0.9.9.dist-info/WHEEL
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/top_level.txt
+Filename: fastdatasets-0.9.9.dist-info/top_level.txt
 Comment: 
 
-Filename: fastdatasets-0.9.7.post0.dist-info/RECORD
+Filename: fastdatasets-0.9.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fastdatasets/setup.py

```diff
@@ -16,24 +16,24 @@
 if __name__ == '__main__':
     package_list = setuptools.find_packages('./fastdatasets', exclude=['tests.*'])
     pprint.pprint(package_list)
     package_list = ['fastdatasets'] +  [ 'fastdatasets.' + p for p in package_list]
 
     setuptools.setup(
         name=package_name,
-        version="0.9.7@post0",
+        version="0.9.9",
         author="ssbuild",
         author_email="9727464@qq.com",
         description=title,
         long_description_content_type='text/markdown',
         long_description=project_description_str,
         url="https://github.com/ssbuild/fastdatasets",
         packages=package_list,
         python_requires='>=3, <4',  # python的依赖关系
-        install_requires=['tfrecords >= 0.2.6 , < 0.3','data_serialize >= 0.2.1','numpy'],
+        install_requires=['tfrecords >= 0.2.10 , < 0.3','data_serialize >= 0.2.1','numpy'],
         classifiers=[
             'Development Status :: 5 - Production/Stable',
             'Intended Audience :: Developers',
             'Intended Audience :: Education',
             'Intended Audience :: Science/Research',
             'License :: OSI Approved :: Apache Software License',
             'Programming Language :: C++',
```

## fastdatasets/leveldb/dataset.py

```diff
@@ -1,141 +1,123 @@
 # @Time    : 2022/9/20 21:55
 # @Author  : tk
 # @FileName: dataset.py
-
+import copy
 import typing
 from tfrecords.python.io import gfile
 from tfrecords import LEVELDB as DB
 from typing import Union,List,AnyStr
 
 from .iterable_dataset import SingleLeveldbIterableDataset,MultiLeveldbIterableDataset
 from .random_dataset import SingleLeveldbRandomDataset,MultiLeveldbRandomDataset
+from .default import global_default_options
 
 
 __all__ = [
-           #  "SingleLeveldbIterableDataset",
-           # "MultiLeveldbIterableDataset",
-           # "SingleLeveldbRandomDataset",
-           # "MultiLeveldbRandomDataset",
-           "DB",
-           "load_dataset",
-           "gfile",
-           ]
-
-_DefaultOptions = DB.LeveldbOptions(create_if_missing=False, error_if_exists=False)
-
-def LeveldbIterableDatasetLoader(data_path: Union[List[Union[AnyStr, typing.Iterator]], AnyStr, typing.Iterator],
-                                 buffer_size: typing.Optional[int] = 128,
-                                 cycle_length=1,
-                                 block_length=1,
-                                 options=_DefaultOptions,
-                                 ):
-    if isinstance(data_path, list):
-        if len(data_path) == 1:
-            cls = SingleLeveldbIterableDataset(data_path[0], buffer_size, block_length, options,
+    "DB",
+    "load_dataset",
+    "gfile",
+]
 
-                                               )
-        else:
-            cls = MultiLeveldbIterableDataset(data_path, buffer_size, cycle_length, block_length, options)
-    elif isinstance(data_path, str):
-        cls = SingleLeveldbIterableDataset(data_path, buffer_size, block_length, options,
-
-                                           )
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
-
-def LeveldbRandomDatasetLoader(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
-                               data_key_prefix_list=('input',),
-                               num_key='total_num',
-                               options=_DefaultOptions,
-                               ):
-    if isinstance(data_path, list):
-        if len(data_path) == 1:
-            cls = SingleLeveldbRandomDataset(data_path[0], data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,
 
-                                             )
-        else:
-            cls = MultiLeveldbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,
-                                            )
-    elif isinstance(data_path, str):
-        cls = SingleLeveldbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,
-                                         )
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
 
 class load_dataset:
 
     @staticmethod
-    def IterableDataset(data_path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=_DefaultOptions,
-                     ):
-        return LeveldbIterableDatasetLoader(data_path,
-                                            buffer_size,
-                                            cycle_length,
-                                            block_length,
-                                            options=options, )
-
-    @staticmethod
-    def SingleIterableDataset( data_path_or_iterator: typing.Union[typing.AnyStr,typing.Iterator],
-                 buffer_size: typing.Optional[int] = 64,
-                 block_length=1,
-                 options=_DefaultOptions,
-                 ):
+    def IterableDataset(path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
+                        buffer_size: typing.Optional[int] = 128,
+                        batch_size: typing.Optional[int] = None,
+                        cycle_length=1,
+                        block_length=1,
+                        options=copy.deepcopy(global_default_options)):
+        if isinstance(path, list) and len(path) == 1:
+            path = path[0]
+
+        if isinstance(path, list):
+            cls = MultiLeveldbIterableDataset(path,
+                                              buffer_size=buffer_size,
+                                              batch_size=batch_size,
+                                              cycle_length=cycle_length,
+                                              block_length=block_length,
+                                              options=options)
+        elif isinstance(path, str):
+            cls = SingleLeveldbIterableDataset(path,
+                                               buffer_size=buffer_size,
+                                               block_length=block_length,
+                                               options=options)
+        else:
+            raise Exception('data_path must be list or single string')
+        return cls
 
-            return SingleLeveldbIterableDataset(data_path_or_iterator, buffer_size, block_length, options,
-                                                )
+    @staticmethod
+    def SingleIterableDataset(data_path_or_iterator: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 64,
+                              batch_size: typing.Optional[int] = None,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options)):
+
+        return SingleLeveldbIterableDataset(data_path_or_iterator,
+                                            buffer_size = buffer_size,
+                                            batch_size=batch_size,
+                                            block_length = block_length,
+                                            options = options)
 
     @staticmethod
     def MultiIterableDataset(data_path_or_iterator: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = _DefaultOptions,
-                 ):
-
-            return MultiLeveldbIterableDataset(data_path_or_iterator, buffer_size, cycle_length, block_length, options,
-                                               )
+                             buffer_size: typing.Optional[int]=64,
+                             batch_size: typing.Optional[int] = None,
+                             cycle_length=None,
+                             block_length=1,
+                             options=copy.deepcopy(global_default_options)):
+
+        return MultiLeveldbIterableDataset(data_path_or_iterator,
+                                           buffer_size=buffer_size,
+                                           batch_size=batch_size,
+                                           cycle_length=cycle_length,
+                                           block_length=block_length,
+                                           options = options)
 
     @staticmethod
-    def RandomDataset(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
+    def RandomDataset(path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                       data_key_prefix_list=('input',),
                       num_key='total_num',
-                      options = _DefaultOptions,
-                      ):
+                      options=copy.deepcopy(global_default_options)):
 
-        return LeveldbRandomDatasetLoader(data_path,
-                                          data_key_prefix_list,
-                                          num_key, options,
-                                          )
+        if isinstance(path, list) and len(path) == 1:
+            path = path[0]
+
+        if isinstance(path, list):
+            cls = MultiLeveldbRandomDataset(path,
+                                            data_key_prefix_list=data_key_prefix_list,
+                                            num_key=num_key,
+                                            options=options)
+        elif isinstance(path, str):
+            cls = SingleLeveldbRandomDataset(path,
+                                             data_key_prefix_list=data_key_prefix_list,
+                                             num_key=num_key,
+                                             options=options)
+        else:
+            raise Exception('data_path must be list or single string')
+        return cls
 
     @staticmethod
     def SingleRandomDataset(data_path: typing.Union[typing.AnyStr,typing.Sized],
                             data_key_prefix_list=('input',),
                             num_key='total_num',
-                 options=_DefaultOptions
-                 ):
+                            options=copy.deepcopy(global_default_options)):
         return SingleLeveldbRandomDataset(data_path,
-                                          data_key_prefix_list,
-                                          num_key,
+                                          data_key_prefix_list=data_key_prefix_list,
+                                          num_key = num_key,
                                           options=options,
                                           )
 
     @staticmethod
     def MutiRandomDataset(data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
-                        data_key_prefix_list=('input',),
-                        num_key='total_num',
-                        options =_DefaultOptions,
-                        ):
-        return MultiLeveldbRandomDataset(data_path,
-                                         data_key_prefix_list,
-                                         num_key,
-                                         options,
-                                         )
-
-
+                          data_key_prefix_list=('input',),
+                          num_key='total_num',
+                          options=copy.deepcopy(global_default_options)):
 
+        return MultiLeveldbRandomDataset(data_path,
+                                         data_key_prefix_list = data_key_prefix_list,
+                                         num_key=num_key,
+                                         options = options)
```

## fastdatasets/leveldb/iterable_dataset/__init__.py

```diff
@@ -2,43 +2,50 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2022/9/8 15:49
 
 import os
 import warnings
 import typing
 from collections.abc import Iterator
-import tfrecords
 from tfrecords import LEVELDB
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
+from ..default import global_default_options
 import copy
 
-__all__ = ["SingleLeveldbIterableDataset", "MultiLeveldbIterableDataset", "tfrecords", "warnings"]
+__all__ = [
+    "SingleLeveldbIterableDataset",
+    "MultiLeveldbIterableDataset"
+]
 
 class SingleLeveldbIterableDataset(IterableDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
+                 batch_size: typing.Optional[int] = None,
                  block_length=1,
-                 options=LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ):
 
 
         assert block_length > 0
 
+        self.batch_size = batch_size if batch_size is not None else 1
+        assert self.batch_size > 0
         self.with_share_memory = with_share_memory
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
         self.buffer_size = buffer_size
+        assert self.buffer_size  >0
 
         self.buffer = []
         self.iterator_ = None
         self.iterator_obj = None
         self.reset()
 
     def __del__(self):
@@ -54,16 +61,16 @@
             self.iterator_.close()
             self.iterator_ = None
             self.iterator_obj = None
 
     def __reopen__(self):
         self.block_id = -1
         self.close()
-        if os.path.exists(self.data_path):
-            self.iterator_ = LEVELDB.Leveldb(self.data_path, options=self.options)
+        if os.path.exists(self.path):
+            self.iterator_ = LEVELDB.Leveldb(self.path, options=self.options)
             self.iterator_obj = self.iterator_.get_iterater()
         else:
             self.iterator_ = None
 
         self.repeat_done_num += 1
         return True
 
@@ -80,27 +87,31 @@
         self.block_id += 1
         return it
 
     def __next_ex__(self):
         iterator : LEVELDB.LeveldbIterater = self.iterator_obj
         if iterator is None:
             raise StopIteration
-        if self.buffer_size > 1:
-            if len(self.buffer) == 0:
-                try:
-                    for _ in range(self.buffer_size):
-                        self.buffer.append(next(iterator))
-                except StopIteration:
-                    pass
-            if len(self.buffer) == 0:
-                raise StopIteration
+
+        if len(self.buffer) < self.batch_size:
+            try:
+                for _ in range(max(self.buffer_size,self.batch_size-len(self.buffer) + 1)):
+                    self.buffer.append(next(iterator))
+            except StopIteration:
+                pass
+            except Exception as e:
+                warnings.warn('data corrupted in {} , err {}'.format(self.path, str(e)))
+                pass
+        if len(self.buffer) == 0:
+            raise StopIteration
+
+        if self.batch_size == 1:
             return self.buffer.pop(0)
-        else:
-            result = next(iterator)
-        return result
+
+        return [self.buffer.pop(0) for i in range(min(len(self.buffer), self.batch_size))]
 
 class MultiLeveldbIterableDataset(IterableDatasetBase):
     """Parse (generic) TFTables dataset into `IterableDataset` object,
     which contain `np.ndarrays`s. By default (when `sequence_description`
     is None), it treats the TFTables as containing `tf.Example`.
     Otherwise, it assumes it is a `tf.SequenceExample`.
 
@@ -115,39 +126,41 @@
     block_length: default 1
     options: TFTableOptions
     """
 
     def __init__(self,
                  data_path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
+                 batch_size: typing.Optional[int] = None,
                  cycle_length=None,
                  block_length=1,
                  options =LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
                  with_share_memory=False
                  ) -> None:
         super(MultiLeveldbIterableDataset, self).__init__()
 
         assert block_length > 0
 
         if cycle_length is None:
             cycle_length = cpu_count()
 
+        self.batch_size = batch_size
         self.with_share_memory = with_share_memory
         self.options = options
         self.cycle_length = min(cycle_length,len(data_path))
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.buffer_size = buffer_size
 
         if self.buffer_size is None:
             self.buffer_size = 1
         self.reset()
 
     def reset(self):
-        self.iterators_ = [{"valid": False,"file": self.data_path[i]} for i in range(len(self.data_path))]
+        self.iterators_ = [{"valid": False,"file": self.path[i]} for i in range(len(self.path))]
         self.cicle_iterators_ = []
         self.fresh_iter_ids = False
         self.cur_id = 0
         self.__reopen__()
 
     def close(self):
         for iter_obj in self.iterators_:
@@ -161,20 +174,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleLeveldbIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             self.with_share_memory
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -206,15 +214,20 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         batch_size=self.batch_size,
+                                                         block_length = self.block_length,
+                                                         options = self.options,
+                                                         with_share_memory = self.with_share_memory)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

## fastdatasets/leveldb/random_dataset/__init__.py

```diff
@@ -1,31 +1,33 @@
 # @Time    : 2022/9/18 10:49
 # @Author  : tk
 # @FileName: __init__.py.py
 import logging
 import typing
 import os
 from typing import List
-import tfrecords
 from tfrecords import LEVELDB
 from .. import RandomDatasetBase
+from ..default import global_default_options
 import copy
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleLeveldbRandomDataset", "MultiLeveldbRandomDataset", "tfrecords", "logging"]
+__all__ = [
+    "SingleLeveldbRandomDataset",
+    "MultiLeveldbRandomDataset"
+]
 
 class SingleLeveldbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Sized],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options=LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
-                 
+                 options=copy.deepcopy(global_default_options),
                  ):
         super(SingleLeveldbRandomDataset, self).__init__()
 
         self.data_key_prefix_list = data_key_prefix_list
         self.data_path = data_path
         self.options = options
 
@@ -81,16 +83,15 @@
 
 
 class MultiLeveldbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options = LEVELDB.LeveldbOptions(create_if_missing=False, error_if_exists=False),
-                 
+                 options=copy.deepcopy(global_default_options),
                  ) -> None:
         super(MultiLeveldbRandomDataset, self).__init__()
 
         self.options = options
         self.data_path = data_path
         self.data_key_prefix_list = data_key_prefix_list
         self.num_key = num_key
@@ -132,10 +133,10 @@
         for i,it_obj in enumerate(self.iterators_):
             tmp_obj = it_obj['inst']
             if item < cur_len + len(tmp_obj):
                 obj = tmp_obj
                 break
             cur_len += len(tmp_obj)
         if obj is None:
-            raise tfrecords.OutOfRangeError
+            raise OverflowError
         real_index =  item - cur_len
         return obj[real_index]
```

## fastdatasets/lmdb/dataset.py

```diff
@@ -1,150 +1,137 @@
 # @Time    : 2022/9/20 21:55
 # @Author  : tk
 # @FileName: dataset.py
-
+import copy
 import typing
 from tfrecords.python.io import gfile
 from tfrecords import LMDB as DB
 from typing import Union,List,AnyStr
 
 from .iterable_dataset import SingleLmdbIterableDataset,MultiLmdbIterableDataset
 from .random_dataset import SingleLmdbRandomDataset,MultiLmdbRandomDataset
+from .default import global_default_options
 
 __all__ = [
-           # "SingleLmdbIterableDataset",
-           # "MultiLmdbIterableDataset",
-           # "SingleLmdbRandomDataset",
-           # "MultiLmdbRandomDataset",
-            "DB",
-           "load_dataset",
-           "gfile",
-           ]
-
-_DefaultOptions = DB.LmdbOptions(env_open_flag = DB.LmdbFlag.MDB_RDONLY,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = DB.LmdbFlag.MDB_RDONLY,
-                dbi_flag = 0,
-                put_flag = 0)
-
-def LmdbIterableDatasetLoader(data_path: Union[List[Union[AnyStr, typing.Iterator]], AnyStr, typing.Iterator],
-                                 buffer_size: typing.Optional[int] = 128,
-                                 cycle_length=1,
-                                 block_length=1,
-                                 options=_DefaultOptions,
-                                 map_size=0,
-                                 ):
-    if isinstance(data_path, list):
-        if len(data_path) == 1:
-            cls = SingleLmdbIterableDataset(data_path[0], buffer_size, block_length, options=options,map_size=map_size
-                                               )
-        else:
-            cls = MultiLmdbIterableDataset(data_path, buffer_size, cycle_length, block_length, options=options,map_size=map_size
-                                           )
-    elif isinstance(data_path, str) :
-        cls = SingleLmdbIterableDataset(data_path, buffer_size, block_length, options=options,map_size=map_size
-                                           )
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
-
-def LmdbRandomDatasetLoader(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
-                               data_key_prefix_list=('input',),
-                               num_key='total_num',
-                               options=_DefaultOptions,
-                            map_size=0,
-                               ):
-    if isinstance(data_path, list):
-        if len(data_path) == 1:
-            cls = SingleLmdbRandomDataset(data_path[0], data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,map_size=map_size
-                                             )
-        else:
-            cls = MultiLmdbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,map_size=map_size
-                                            )
-    elif isinstance(data_path, str) :
-        cls = SingleLmdbRandomDataset(data_path, data_key_prefix_list=data_key_prefix_list, num_key=num_key, options=options,map_size=map_size
-                                         )
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
+    "DB",
+    "load_dataset",
+    "gfile",
+]
+
+
+
+
 
 class load_dataset:
 
     @staticmethod
-    def IterableDataset(data_path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=_DefaultOptions,
-                     map_size=0,
-                     ):
-        return LmdbIterableDatasetLoader(data_path,
-                                            buffer_size,
-                                            cycle_length,
-                                            block_length,
-                                            options=options, 
-                                            map_size=map_size
-                                         )
-
-    @staticmethod
-    def SingleIterableDataset( data_path: typing.Union[typing.AnyStr,typing.Iterator],
-                 buffer_size: typing.Optional[int] = 64,
-                 block_length=1,
-                 options=_DefaultOptions,
-                 map_size=0
-                 ):
-
-            return SingleLmdbIterableDataset(data_path, buffer_size, block_length, options,map_size=map_size,
-                                                )
-
-    @staticmethod
-    def MultiIterableDataset(data_path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = _DefaultOptions,
-                 map_size=0,
-                 ):
+    def IterableDataset(path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
+                        buffer_size: typing.Optional[int] = 128,
+                        batch_size: typing.Optional[int] = None,
+                        cycle_length=1,
+                        block_length=1,
+                        options=copy.deepcopy(global_default_options),
+                        map_size=0):
+        if isinstance(path, list) and len(path) == 1:
+            path = path[0]
+
+        if isinstance(path, list):
+            cls = MultiLmdbIterableDataset(path,
+                                           buffer_size=buffer_size,
+                                           batch_size=batch_size,
+                                           cycle_length=cycle_length,
+                                           block_length=block_length,
+                                           options=options,
+                                           map_size=map_size)
+        elif isinstance(path, str):
+            cls = SingleLmdbIterableDataset(path,
+                                            buffer_size=buffer_size,
+                                            batch_size=batch_size,
+                                            block_length=block_length,
+                                            options=options,
+                                            map_size=map_size)
+        else:
+            raise Exception('path must be list or single string')
+        return cls
 
-            return MultiLmdbIterableDataset(data_path, buffer_size, cycle_length, block_length, options=options,map_size=map_size,
-                                               )
+    @staticmethod
+    def SingleIterableDataset(path: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 64,
+                              batch_size: typing.Optional[int] = None,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options),
+                              map_size=0):
+
+        return SingleLmdbIterableDataset(path,
+                                         buffer_size=buffer_size,
+                                         batch_size=batch_size,
+                                         block_length=block_length,
+                                         options=options,
+                                         map_size=map_size)
+
+    @staticmethod
+    def MultiIterableDataset(path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
+                             buffer_size: typing.Optional[int]=64,
+                             batch_size: typing.Optional[int] = None,
+                             cycle_length=None,
+                             block_length=1,
+                             options=copy.deepcopy(global_default_options),
+                             map_size=0):
+
+        return MultiLmdbIterableDataset(path,
+                                        buffer_size=buffer_size,
+                                        batch_size=batch_size,
+                                        cycle_length=cycle_length,
+                                        block_length=block_length,
+                                        options=options,
+                                        map_size=map_size)
 
     @staticmethod
-    def RandomDataset(data_path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
+    def RandomDataset(path: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                       data_key_prefix_list=('input',),
                       num_key='total_num',
-                      options=_DefaultOptions,
+                      options=copy.deepcopy(global_default_options),
                       map_size=0,
                       ):
 
-        return LmdbRandomDatasetLoader(data_path,
-                                          data_key_prefix_list,
-                                          num_key, options=options,map_size=map_size
-                                          )
+        if isinstance(path, list) and len(path) == 1:
+            path = path[0]
 
-    @staticmethod
-    def SingleRandomDataset(data_path: typing.Union[typing.AnyStr,typing.Sized],
-                            data_key_prefix_list=('input',),
-                            num_key='total_num',
-                            options=_DefaultOptions,
-                            map_size=0,
-                 ):
-        return SingleLmdbRandomDataset(data_path,
-                                          data_key_prefix_list,
-                                          num_key,
+        if isinstance(path, list):
+            cls = MultiLmdbRandomDataset(path,
+                                         data_key_prefix_list=data_key_prefix_list,
+                                         num_key=num_key,
+                                         options=options,
+                                         map_size=map_size)
+        elif isinstance(path, str):
+            cls = SingleLmdbRandomDataset(path,
+                                          data_key_prefix_list=data_key_prefix_list,
+                                          num_key=num_key,
                                           options=options,
-                                          map_size=map_size
-                                          )
+                                          map_size=map_size)
+        else:
+            raise Exception('path must be list or single string')
+        return cls
 
     @staticmethod
-    def MutiRandomDataset(data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
-                        data_key_prefix_list=('input',),
-                        num_key='total_num',
-                        options = _DefaultOptions,
-                        map_size=0,
-                        ):
-        return MultiLmdbRandomDataset(data_path,
-                                    data_key_prefix_list,
-                                    num_key,
-                                    options=options,
-                                    map_size=map_size)
+    def SingleRandomDataset(path: typing.Union[typing.AnyStr,typing.Sized],
+                            data_key_prefix_list=('input',),
+                            num_key='total_num',
+                            options=copy.deepcopy(global_default_options),
+                            map_size=0):
+        return SingleLmdbRandomDataset(path,
+                                       data_key_prefix_list=data_key_prefix_list,
+                                       num_key=num_key,
+                                       options=options, map_size=map_size)
+
+    @staticmethod
+    def MutiRandomDataset(path: List[typing.Union[typing.AnyStr,typing.Sized]],
+                          data_key_prefix_list=('input',),
+                          num_key='total_num',
+                          options=copy.deepcopy(global_default_options),
+                          map_size=0):
+
+        return MultiLmdbRandomDataset(path,
+                                      data_key_prefix_list=data_key_prefix_list,
+                                      num_key=num_key,
+                                      options=options, map_size=map_size)
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## fastdatasets/lmdb/iterable_dataset/__init__.py

```diff
@@ -1,56 +1,58 @@
 """Load tfrecord files into torch datasets."""
 # -*- coding: utf-8 -*-
 # @Time    : 2022/9/8 15:49
 
 import os
 import warnings
 import typing
-import tfrecords
 from tfrecords import LMDB
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
+import copy
+from ..default import global_default_options2 as global_default_options
 
-__all__ = ["SingleLmdbIterableDataset", "MultiLmdbIterableDataset", "tfrecords", "warnings"]
+__all__ = [
+    "SingleLmdbIterableDataset",
+    "MultiLmdbIterableDataset",
+    "warnings"
+]
 
 
 
-DefaultOptions = LMDB.LmdbOptions(env_open_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                dbi_flag = 0,
-                put_flag = 0)
-
-   
 
 class SingleLmdbIterableDataset(IterableDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
+                 batch_size: typing.Optional[int] = None,
                  block_length=1,
-                 options=DefaultOptions,
+                 options=copy.deepcopy(global_default_options),
                  map_size=0,
                  max_readers: int = 128,
                  max_dbs: int = 0
                  ):
 
 
         assert block_length > 0
 
+        self.batch_size = batch_size if batch_size is not None else 1
+        assert self.batch_size > 0
         self.map_size = map_size
         self.max_readers = max_readers
         self.max_dbs = max_dbs
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
         self.buffer_size = buffer_size
+        assert self.buffer_size > 0
 
         self.buffer = []
         self.iterator_ = None
         self.iterator_obj = None
         self.reset()
 
     def __del__(self):
@@ -66,16 +68,16 @@
             self.iterator_.close()
             self.iterator_ = None
             self.iterator_obj = None
 
     def __reopen__(self):
         self.block_id = -1
         self.close()
-        if os.path.exists(self.data_path):
-            self.iterator_ = LMDB.Lmdb(self.data_path,
+        if os.path.exists(self.path):
+            self.iterator_ = LMDB.Lmdb(self.path,
                                        options=self.options,
                                        map_size=self.map_size,
                                        max_readers=self.max_readers,
                                        max_dbs=self.max_dbs)
 
             self.iterator_obj = self.iterator_.get_iterater(reverse=False)
         else:
@@ -97,27 +99,26 @@
         self.block_id += 1
         return it
 
     def __next_ex__(self):
         iterator : LMDB.LmdbIterater = self.iterator_obj
         if iterator is None:
             raise StopIteration
-        if self.buffer_size > 1:
-            if len(self.buffer) == 0:
-                try:
-                    for _ in range(self.buffer_size):
-                        self.buffer.append(next(iterator))
-                except StopIteration:
-                    pass
-            if len(self.buffer) == 0:
-                raise StopIteration
-            return self.buffer.pop(0)
-        else:
-            result = next(iterator)
-        return result
+        if len(self.buffer) == 0:
+            try:
+                for _ in range(max(self.buffer_size,self.batch_size-len(self.buffer) + 1)):
+                    self.buffer.append(next(iterator))
+            except StopIteration:
+                pass
+            except Exception as e:
+                warnings.warn('data corrupted in {} , err {}'.format(self.path, str(e)))
+                pass
+        if len(self.buffer) == 0:
+            raise StopIteration
+        return self.buffer.pop(0)
 
 class MultiLmdbIterableDataset(IterableDatasetBase):
     """Parse (generic) TFTables dataset into `IterableDataset` object,
     which contain `np.ndarrays`s. By default (when `sequence_description`
     is None), it treats the TFTables as containing `tf.Example`.
     Otherwise, it assumes it is a `tf.SequenceExample`.
 
@@ -132,43 +133,45 @@
     block_length: default 1
     options: TFTableOptions
     """
 
     def __init__(self,
                  data_path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
+                 batch_size: typing.Optional[int] = None,
                  cycle_length=None,
                  block_length=1,
-                 options =DefaultOptions,
+                 options = copy.deepcopy(global_default_options),
                  map_size =0,
                  max_readers: int = 128,
                  max_dbs: int = 0
                  ) -> None:
         super(MultiLmdbIterableDataset, self).__init__()
 
         assert block_length > 0
 
         if cycle_length is None:
             cycle_length = cpu_count()
 
+        self.batch_size = batch_size
         self.map_size = map_size
         self.max_readers = max_readers
         self.max_dbs = max_dbs
         self.options = options
         self.cycle_length = min(cycle_length,len(data_path))
         self.block_length = block_length
-        self.data_path = data_path
+        self.path = data_path
         self.buffer_size = buffer_size
 
         if self.buffer_size is None:
             self.buffer_size = 1
         self.reset()
 
     def reset(self):
-        self.iterators_ = [{"valid": False,"file": self.data_path[i]} for i in range(len(self.data_path))]
+        self.iterators_ = [{"valid": False,"file": self.path[i]} for i in range(len(self.path))]
         self.cicle_iterators_ = []
         self.fresh_iter_ids = False
         self.cur_id = 0
         self.__reopen__()
 
     def close(self):
         for iter_obj in self.iterators_:
@@ -182,22 +185,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleLmdbIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             self.map_size,
-                             self.max_readers,
-                             self.max_dbs,
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -229,15 +225,22 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         batch_size=self.batch_size,
+                                                         block_length = self.block_length,
+                                                         options = self.options,
+                                                         map_size = self.map_size,
+                                                         max_readers = self.max_readers,
+                                                         max_dbs = self.max_dbs,)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## fastdatasets/lmdb/random_dataset/__init__.py

```diff
@@ -1,34 +1,35 @@
 # @Time    : 2022/9/18 10:49
 # @Author  : tk
 # @FileName: __init__.py.py
+import copy
 import logging
 import typing
 import os
 from typing import List
-import tfrecords
 from tfrecords import LMDB
 from .. import RandomDatasetBase
+from ..default import global_default_options
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleLmdbRandomDataset", "MultiLmdbRandomDataset", "tfrecords", "logging"]
+__all__ = [
+    "SingleLmdbRandomDataset",
+    "MultiLmdbRandomDataset"
+]
+
+
 
-DefaultOptions = LMDB.LmdbOptions( env_open_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                env_open_mode = 0o664, # 8进制表示
-                txn_flag = LMDB.LmdbFlag.MDB_RDONLY,
-                dbi_flag = 0,
-                put_flag = 0)
 class SingleLmdbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: typing.Union[typing.AnyStr,typing.Sized],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options=DefaultOptions,
+                 options=copy.deepcopy(global_default_options),
                  map_size=0,
                  max_readers: int = 128,
                  max_dbs: int = 0
                  ):
         super(SingleLmdbRandomDataset, self).__init__()
 
         self.data_key_prefix_list = data_key_prefix_list
@@ -96,15 +97,15 @@
 
 
 class MultiLmdbRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path: List[typing.Union[typing.AnyStr,typing.Sized]],
                  data_key_prefix_list=('input',),
                  num_key='total_num',
-                 options = DefaultOptions,
+                 options = copy.deepcopy(global_default_options),
                  map_size=0,
                  max_readers: int = 128,
                  max_dbs: int = 0,
                  ) -> None:
         super(MultiLmdbRandomDataset, self).__init__()
 
         self.options = options
@@ -129,21 +130,20 @@
                 iter_obj["instance"].close()
                 iter_obj["valid"] = False
                 iter_obj["instance"] = None
 
     def __reopen__(self):
         for it_obj in self.iterators_:
             it_obj['inst'] = SingleLmdbRandomDataset(it_obj["file"],
-                                                        data_key_prefix_list=self.data_key_prefix_list,
-                                                        num_key=self.num_key,
-                                                        options=self.options,
-                                                        map_size=self.map_size,
-                                                        max_readers = self.max_readers,
-                                                        max_dbs = self.max_dbs
-                                                     )
+                                                     data_key_prefix_list=self.data_key_prefix_list,
+                                                     num_key=self.num_key,
+                                                     options=self.options,
+                                                     map_size=self.map_size,
+                                                     max_readers = self.max_readers,
+                                                     max_dbs = self.max_dbs)
 
     def __len__(self):
         total_len = 0
         for it_obj in self.iterators_:
             total_len += len(it_obj['inst'])
         return total_len
 
@@ -156,10 +156,10 @@
         for i,it_obj in enumerate(self.iterators_):
             tmp_obj = it_obj['inst']
             if item < cur_len + len(tmp_obj):
                 obj = tmp_obj
                 break
             cur_len += len(tmp_obj)
         if obj is None:
-            raise tfrecords.OutOfRangeError
+            raise OverflowError
         real_index =  item - cur_len
         return obj[real_index]
```

### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

## fastdatasets/memory/dataset.py

```diff
@@ -6,111 +6,118 @@
 from tfrecords.python.io import gfile
 from collections.abc import Iterator,Sized
 from typing import Union,List,AnyStr
 from .iterable_dataset import SingleMemoryIterableDataset,MultiMemoryIterableDataset
 from .random_dataset import SingleMemoryRandomDataset,MultiMemoryRandomDataset
 
 __all__ = [
-   #  "SingleMemoryIterableDataset",
-   # "MultiMemoryIterableDataset",
-   # "SingleMemoryRandomDataset",
-   # "MultiMemoryRandomDataset",
    "load_dataset",
    "gfile"
 ]
 
-def MemoryIterableDatasetLoader(data_list: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=None,
-                     ):
-    if isinstance(data_list, list):
-        if len(data_list) == 1:
-            cls = SingleMemoryIterableDataset(data_list[0], buffer_size, block_length, options)
-        else:
-            cls = MultiMemoryIterableDataset(data_list, buffer_size, cycle_length, block_length, options)
-    elif isinstance(data_list, Iterator):
-        cls = SingleMemoryIterableDataset(data_list, buffer_size, block_length, options)
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
-
-def MemoryRandomDatasetLoader(data_list: typing.Union[typing.List,typing.AnyStr,typing.Sized],
-                            index_path=None,
-                            use_index_cache=True,
-                            options=None,
-                            ):
-
-    if isinstance(data_list, list):
-        if len(data_list) > 0 and isinstance(data_list[0], list):
-            cls = MultiMemoryRandomDataset(data_list, index_path=index_path, use_index_cache=use_index_cache,
-                                           options=options)
-        else:
-            cls = SingleMemoryRandomDataset(data_list, index_path=index_path, use_index_cache=use_index_cache,
-                                        options=options)
-
-    elif isinstance(data_list, Sized):
-        cls = SingleMemoryRandomDataset(data_list, index_path=index_path, use_index_cache=use_index_cache, options=options)
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
 
 class load_dataset:
 
     @staticmethod
     def IterableDataset(data_list: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=None,
-                     ):
-        return MemoryIterableDatasetLoader(data_list,
-                     buffer_size,
-                     cycle_length,
-                     block_length,
-                     options=options)
-
-    @staticmethod
-    def SingleIterableDataset( data_list: typing.Union[typing.AnyStr,typing.Iterator],
-                                buffer_size: typing.Optional[int] = 64,
-                                block_length=1,
-                                options=None,
-                 ):
+                        buffer_size: typing.Optional[int] = 128,
+                        batch_size: typing.Optional[int] = None,
+                        cycle_length=1,
+                        block_length=1,
+                        options=None):
+
+        if isinstance(data_list, list) and len(data_list) == 1:
+            data_list = data_list[0]
+
+        if isinstance(data_list, list):
+            cls = MultiMemoryIterableDataset(data_list,
+                                             buffer_size=buffer_size,
+                                             batch_size=batch_size,
+                                             cycle_length=cycle_length,
+                                             block_length=block_length,
+                                             options=options)
+        elif isinstance(data_list, Iterator):
+            cls = SingleMemoryIterableDataset(data_list,
+                                              buffer_size=buffer_size,
+                                              block_length=block_length,
+                                              options=options)
+        else:
+            raise Exception('data_path must be list or single string')
+        return cls
 
-            return SingleMemoryIterableDataset(data_list, buffer_size,block_length,options)
+    @staticmethod
+    def SingleIterableDataset(data_list: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 64,
+                              batch_size: typing.Optional[int] = None,
+                              block_length=1,
+                              options=None):
+        return SingleMemoryIterableDataset(data_list,
+                                           buffer_size=buffer_size,
+                                           batch_size=batch_size,
+                                           block_length=block_length,
+                                           options=options)
 
     @staticmethod
     def MultiIterableDataset(data_list: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = None,
-                 ):
-
-            return MultiMemoryIterableDataset(data_list, buffer_size,cycle_length,block_length,options)
+                             buffer_size: typing.Optional[int]=64,
+                             batch_size: typing.Optional[int] = None,
+                             cycle_length=None,
+                             block_length=1,
+                             options = None):
+
+        return MultiMemoryIterableDataset(data_list,
+                                          buffer_size=buffer_size,
+                                          batch_size=batch_size,
+                                          cycle_length=cycle_length,
+                                          block_length=block_length,
+                                          options=options)
 
     @staticmethod
     def RandomDataset(data_list: typing.Union[typing.List, typing.AnyStr, typing.Sized],
                       index_path=None,
                       use_index_cache=True,
                       options=None):
 
-        return MemoryRandomDatasetLoader(data_list,index_path,use_index_cache,options,)
+        if isinstance(data_list, list):
+            if len(data_list) > 0 and isinstance(data_list[0], list):
+                cls = MultiMemoryRandomDataset(data_list,
+                                               index_path=index_path,
+                                               use_index_cache=use_index_cache,
+                                               options=options)
+            else:
+                cls = SingleMemoryRandomDataset(data_list,
+                                                index_path=index_path,
+                                                use_index_cache=use_index_cache,
+                                                options=options)
+
+        elif isinstance(data_list, Sized):
+            cls = SingleMemoryRandomDataset(data_list,
+                                            index_path=index_path,
+                                            use_index_cache=use_index_cache,
+                                            options=options)
+        else:
+            raise Exception('data_path must be list or single string')
+        return cls
 
     @staticmethod
     def SingleRandomDataset(data_list: typing.Union[typing.AnyStr,typing.Sized],
-                 index_path: str = None,
-                 use_index_cache=True,
-                 options=None):
-        return SingleMemoryRandomDataset(data_list, index_path, use_index_cache, options=options)
+                            index_path: str = None,
+                            use_index_cache=True,
+                            options=None):
+        return SingleMemoryRandomDataset(data_list,
+                                         index_path=index_path,
+                                         use_index_cache=use_index_cache,
+                                         options=options)
 
     @staticmethod
     def MutiRandomDataset(data_list: List[typing.Union[typing.AnyStr,typing.Sized]],
-                 index_path = None,
-                 use_index_cache=True,
-                 options = None):
-        return MultiMemoryRandomDataset(data_list, index_path, use_index_cache, options)
+                          index_path = None,
+                          use_index_cache=True,
+                          options = None):
+        return MultiMemoryRandomDataset(data_list,
+                                        index_path=index_path,
+                                        use_index_cache=use_index_cache,
+                                        options=options)
```

## fastdatasets/memory/iterable_dataset/__init__.py

```diff
@@ -5,29 +5,33 @@
 import warnings
 import typing
 from collections.abc import Iterator
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
 import copy
 
-__all__ = ["SingleMemoryIterableDataset",'MultiMemoryIterableDataset',"warnings"]
+__all__ = [
+    "SingleMemoryIterableDataset",
+    "MultiMemoryIterableDataset"
+]
 
 class SingleMemoryIterableDataset(IterableDatasetBase):
     def __init__(self,
                  data_iterator: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
+                 batch_size: typing.Optional[int] = None,
                  block_length=1,
                  options:any = None,
-                 
                  ):
 
         assert isinstance(data_iterator,Iterator)
         assert block_length > 0
 
-
+        self.batch_size = batch_size if batch_size is not None else 1
+        assert self.batch_size > 0
         self.block_length = block_length
         self.data_iterator = data_iterator
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
@@ -73,15 +77,15 @@
     def __next_ex__(self):
         iterator = self.iterator_
         if iterator is None:
             raise StopIteration
         if self.buffer_size > 1:
             if len(self.buffer) == 0:
                 try:
-                    for _ in range(self.buffer_size):
+                    for _ in range(max(self.buffer_size,self.batch_size-len(self.buffer) + 1)):
                         self.buffer.append(next(iterator))
                 except StopIteration:
                     pass
             if len(self.buffer) == 0:
                 raise StopIteration
             return self.buffer.pop(0)
         else:
@@ -105,27 +109,27 @@
     block_length: default 1
     options: TFMemoryOptions
     """
 
     def __init__(self,
                  data_iterator: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
+                 batch_size: typing.Optional[int] = None,
                  cycle_length=None,
                  block_length=1,
                  options:any = None,
-                 
                  ) -> None:
         super(MultiMemoryIterableDataset, self).__init__()
 
         assert block_length > 0
 
         if cycle_length is None:
             cycle_length = cpu_count()
 
-        
+        self.batch_size=batch_size
         self.options = options
         self.cycle_length = min(cycle_length,len(data_iterator))
         self.block_length = block_length
         self.data_iterator = data_iterator
         self.buffer_size = buffer_size
 
         if self.buffer_size is None:
@@ -151,19 +155,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleMemoryIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -195,15 +195,19 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         batch_size=self.batch_size,
+                                                         block_length=self.block_length,
+                                                         options=self.options,)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

## fastdatasets/memory/random_dataset/__init__.py

```diff
@@ -9,17 +9,19 @@
 import pickle
 from collections.abc import Sized
 import copy
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleMemoryRandomDataset",
-           "MultiMemoryRandomDataset",
-           "logging"]
+__all__ = [
+   "SingleMemoryRandomDataset",
+   "MultiMemoryRandomDataset",
+   "logging"
+]
 
 
 class SingleMemoryRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_list: typing.Union[typing.AnyStr,typing.Sized],
                  index_path: str = None,
                  use_index_cache=True,
@@ -113,15 +115,16 @@
             if iter_obj["valid"] and "instance" in iter_obj and iter_obj["instance"]:
                 iter_obj["instance"].close()
                 iter_obj["valid"] = False
                 iter_obj["instance"] = None
 
     def __reopen__(self):
         for it_obj in self.iterators_:
-            it_obj['inst'] = SingleMemoryRandomDataset(it_obj["file"], index_path=self.index_path,
+            it_obj['inst'] = SingleMemoryRandomDataset(it_obj["file"],
+                                                       index_path=self.index_path,
                                                        use_index_cache=self.use_index_cache,
                                                        options=self.options,
                                                        with_share_memory=self.with_share_memory)
 
     def __len__(self):
         total_len = 0
         for it_obj in self.iterators_:
```

## fastdatasets/record/dataset.py

```diff
@@ -1,131 +1,135 @@
 # @Time    : 2022/9/20 21:55
 # @Author  : tk
 # @FileName: dataset.py
-
+import copy
 import typing
 from tfrecords.python.io import gfile
 from tfrecords import RECORD
 from collections.abc import Iterator,Sized
 from typing import Union,List,AnyStr
 from .iterable_dataset import SingleRecordIterableDataset,MultiRecordIterableDataset
 from .random_dataset import SingleRecordRandomDataset,MultiRecordRandomDataset
-
+from .default import global_default_options
 
 __all__ = [
-           #  "SingleRecordIterableDataset",
-           # "MultiRecordIterableDataset",
-           # "SingleRecordRandomDataset",
-           # "MultiRecordRandomDataset",
-           "RECORD",
-           "load_dataset",
-           "gfile",
-           ]
-
-def RecordIterableDatasetLoader(path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                     with_share_memory=False):
-    if isinstance(path, list):
-        if len(path) == 1:
-            cls = SingleRecordIterableDataset(path[0], buffer_size, block_length, options,
-                with_share_memory=with_share_memory
-            )
-        else:
-            cls = MultiRecordIterableDataset(path, buffer_size, cycle_length, block_length, options)
-    elif isinstance(path, str):
-        cls = SingleRecordIterableDataset(path, buffer_size, block_length, options,
-            with_share_memory=with_share_memory
-        )
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
+    "RECORD",
+    "load_dataset",
+    "gfile",
+]
+
 
-def RecordRandomDatasetLoader(path: typing.Union[typing.List,typing.AnyStr],
-                            index_path=None,
-                            use_index_cache=True,
-                            options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                            with_share_memory=False):
-    if isinstance(path, list):
-        if len(path) == 1:
-            cls = SingleRecordRandomDataset(path[0], index_path=index_path, use_index_cache=use_index_cache, options=options,
-                                            with_share_memory=with_share_memory
-                                            )
-        else:
-            cls = MultiRecordRandomDataset(path, index_path=index_path, use_index_cache=use_index_cache, options=options,
-                                           with_share_memory=with_share_memory)
-    elif isinstance(path, str):
-        cls = SingleRecordRandomDataset(path, index_path=index_path, use_index_cache=use_index_cache, options=options,
-                                        with_share_memory=with_share_memory)
-    else:
-        raise Exception('data_path must be list or single string')
-    return cls
 
 class load_dataset:
 
     @staticmethod
     def IterableDataset(path: Union[List[Union[AnyStr,typing.Iterator]],AnyStr,typing.Iterator],
-                     buffer_size: typing.Optional[int] = 128,
-                     cycle_length=1,
-                     block_length=1,
-                     options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                     with_share_memory=False):
-        return RecordIterableDatasetLoader(path,
-                     buffer_size,
-                     cycle_length,
-                     block_length,
-                     options=options,with_share_memory=with_share_memory)
-
-    @staticmethod
-    def SingleIterableDataset( path: typing.Union[typing.AnyStr,typing.Iterator],
-                 buffer_size: typing.Optional[int] = 64,
-                 block_length=1,
-                 options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
+                        buffer_size: typing.Optional[int] = 128,
+                        batch_size: typing.Optional[int] = None,
+                        cycle_length=1,
+                        block_length=1,
+                        options=copy.deepcopy(global_default_options),
+                        with_share_memory=False):
+        if isinstance(path, list) and len(path) == 1:
+            path = path[0]
+
+        if isinstance(path, list):
+            cls = MultiRecordIterableDataset(path,
+                                             buffer_size=buffer_size,
+                                             batch_size=batch_size,
+                                             block_length=block_length,
+                                             cycle_length=cycle_length,
+                                             options=options,
+                                             with_share_memory=with_share_memory)
+        elif isinstance(path, str):
+            cls = SingleRecordIterableDataset(path,
+                                              buffer_size=buffer_size,
+                                              batch_size=batch_size,
+                                              block_length=block_length,
+                                              options=options,
+                                              with_share_memory=with_share_memory)
+        else:
+            raise Exception('data_path must be list or single string')
+        return cls
 
-            return SingleRecordIterableDataset(path, buffer_size,block_length,options,
-                                               with_share_memory=with_share_memory)
+    @staticmethod
+    def SingleIterableDataset(path: typing.Union[typing.AnyStr,typing.Iterator],
+                              buffer_size: typing.Optional[int] = 128,
+                              batch_size: typing.Optional[int] = None,
+                              block_length=1,
+                              options=copy.deepcopy(global_default_options),
+                              with_share_memory=False):
+
+        return SingleRecordIterableDataset(path,
+                                           buffer_size=buffer_size,
+                                           batch_size=batch_size,
+                                           block_length=block_length,
+                                           options=options,
+                                           with_share_memory=with_share_memory)
 
     @staticmethod
     def MultiIterableDataset(path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
-                 buffer_size: typing.Optional[int]=64,
-                 cycle_length=None,
-                 block_length=1,
-                 options = RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-
-            return MultiRecordIterableDataset(path, buffer_size,cycle_length,block_length,options,
-                                              with_share_memory=with_share_memory)
+                             buffer_size: typing.Optional[int]=64,
+                             batch_size: typing.Optional[int] = None,
+                             cycle_length=None,
+                             block_length=1,
+                             options=copy.deepcopy(global_default_options),
+                             with_share_memory=False):
+
+        return MultiRecordIterableDataset(path,
+                                          buffer_size=buffer_size,
+                                          batch_size=batch_size,
+                                          cycle_length=cycle_length,
+                                          block_length=block_length,
+                                          options=options,
+                                          with_share_memory=with_share_memory)
 
     @staticmethod
     def RandomDataset(path: typing.Union[typing.List, typing.AnyStr],
                       index_path=None,
                       use_index_cache=True,
-                      options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
+                      options=copy.deepcopy(global_default_options),
                       with_share_memory=False):
 
-        return RecordRandomDatasetLoader(path,index_path,use_index_cache,options,
-                                         with_share_memory=with_share_memory)
+        if isinstance(path, list) and len(path) == 1:
+            path = path[0]
+
+        if isinstance(path, list):
+            cls = MultiRecordRandomDataset(path,
+                                           index_path=index_path,
+                                           use_index_cache=use_index_cache,
+                                           options=options, with_share_memory=with_share_memory)
+        elif isinstance(path, str):
+            cls = SingleRecordRandomDataset(path,
+                                            index_path=index_path,
+                                            use_index_cache=use_index_cache,
+                                            options=options, with_share_memory=with_share_memory)
+        else:
+            raise Exception('data_path must be list or single string')
+        return cls
 
     @staticmethod
     def SingleRandomDataset(path: typing.Union[typing.AnyStr],
-                 index_path: str = None,
-                 use_index_cache=True,
-                 options=RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-        return SingleRecordRandomDataset(path, index_path, use_index_cache, options=options,
+                            index_path: str = None,
+                            use_index_cache=True,
+                            options=copy.deepcopy(global_default_options),
+                            with_share_memory=False):
+        return SingleRecordRandomDataset(path,
+                                         index_path=index_path,
+                                         use_index_cache=use_index_cache,
+                                         options=options,
                                          with_share_memory=with_share_memory)
 
     @staticmethod
     def MutiRandomDataset(path: List[typing.Union[typing.AnyStr]],
-                 index_path = None,
-                 use_index_cache=True,
-                 options = RECORD.TFRecordOptions(RECORD.TFRecordCompressionType.NONE),
-                 with_share_memory=False):
-        return MultiRecordRandomDataset(path, index_path, use_index_cache, options,
+                          index_path = None,
+                          use_index_cache=True,
+                          options=copy.deepcopy(global_default_options),
+                          with_share_memory=False):
+        return MultiRecordRandomDataset(path,
+                                        index_path=index_path,
+                                        use_index_cache=use_index_cache,
+                                        options=options,
                                         with_share_memory=with_share_memory)
 
 
 
-
```

## fastdatasets/record/iterable_dataset/__init__.py

```diff
@@ -6,38 +6,45 @@
 import warnings
 import typing
 # from collections.abc import Iterator
 import tfrecords
 from multiprocessing import cpu_count
 from .. import IterableDatasetBase
 import copy
+from ..default import global_default_options
 
-__all__ = ["SingleRecordIterableDataset",'MultiRecordIterableDataset',"tfrecords","warnings"]
+__all__ = [
+    "SingleRecordIterableDataset",
+    "MultiRecordIterableDataset",
+]
 
 class SingleRecordIterableDataset(IterableDatasetBase):
     def __init__(self,
                  path: typing.Union[typing.AnyStr,typing.Iterator],
                  buffer_size: typing.Optional[int] = 64,
+                 batch_size: typing.Optional[int] = None,
                  block_length=1,
-                 options=tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ):
 
         assert block_length > 0
         self.with_share_memory = with_share_memory
 
-
+        self.batch_size = batch_size if batch_size is not None else 1
+        assert self.batch_size > 0
         self.block_length = block_length
         self.path = path
         self.options  = options
 
         self.block_id = -1
         if buffer_size is None:
             buffer_size = 1
         self.buffer_size = buffer_size
+        assert self.buffer_size > 0
 
         self.buffer = []
         self.iterator_ = None
         self.reset()
 
     def __del__(self):
        self.close()
@@ -54,15 +61,16 @@
 
 
     def __reopen__(self):
         self.block_id = -1
         self.close()
 
         if os.path.exists(self.path):
-            self.iterator_ = tfrecords.tf_record_iterator(self.path, options=self.options,
+            self.iterator_ = tfrecords.tf_record_iterator(self.path,
+                                                          options=self.options,
                                                           with_share_memory=self.with_share_memory)
         else:
             self.iterator_ = None
 
 
         self.repeat_done_num += 1
         return True
@@ -80,32 +88,32 @@
         self.block_id += 1
         return iter
 
     def __next_ex__(self):
         iterator = self.iterator_
         if iterator is None:
             raise StopIteration
-        if self.buffer_size > 1:
-            if len(self.buffer) == 0:
-                try:
-                    for _ in range(self.buffer_size):
-                        self.buffer.append(next(iterator))
-                except StopIteration:
-                    pass
-                except tfrecords.DataLossError:
-                    warnings.warn('data corrupted in {} Is this even a TFRecord file?'.format(self.path))
-                    pass
-                    # warnings.warn("Number of elements in the iterator is less than the "
-                    #               f"queue size (N={self.buffer_size}).")
-            if len(self.buffer) == 0:
-                raise StopIteration
+
+        if len(self.buffer) < self.batch_size:
+            try:
+                for _ in range(max(self.buffer_size,self.batch_size-len(self.buffer) + 1)):
+                    self.buffer.append(next(iterator))
+            except StopIteration:
+                pass
+            except tfrecords.DataLossError:
+                warnings.warn('data corrupted in {} Is this even a TFRecord file?'.format(self.path))
+                pass
+
+        if len(self.buffer) == 0:
+            raise StopIteration
+
+        if self.batch_size == 1:
             return self.buffer.pop(0)
-        else:
-            iterator = next(iterator)
-        return iterator
+
+        return [self.buffer.pop(0) for i in range(min(len(self.buffer), self.batch_size))]
 
 class MultiRecordIterableDataset(IterableDatasetBase):
     """Parse (generic) TFRecords dataset into `IterableDataset` object,
     which contain `np.ndarrays`s. By default (when `sequence_description`
     is None), it treats the TFRecords as containing `tf.Example`.
     Otherwise, it assumes it is a `tf.SequenceExample`.
 
@@ -120,17 +128,18 @@
     block_length: default 1
     options: TFRecordOptions
     """
 
     def __init__(self,
                  path: typing.List[typing.Union[typing.AnyStr,typing.Iterator]],
                  buffer_size: typing.Optional[int]=64,
+                 batch_size: typing.Optional[int] = 1,
                  cycle_length=None,
                  block_length=1,
-                 options = tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ) -> None:
         super(MultiRecordIterableDataset, self).__init__()
 
         assert block_length > 0
 
         if cycle_length is None:
@@ -138,14 +147,15 @@
 
         self.with_share_memory = with_share_memory
         self.options = options
         self.cycle_length = min(cycle_length,len(path))
         self.block_length = block_length
         self.path = path
         self.buffer_size = buffer_size
+        self.batch_size = batch_size
 
         if self.buffer_size is None:
             self.buffer_size = 1
         self.reset()
 
     def reset(self):
         self.iterators_ = [{"valid": False,"file": self.path[i]} for i in range(len(self.path))]
@@ -166,20 +176,15 @@
         for it_obj in iterators_:
             if len(self.cicle_iterators_) >= self.cycle_length:
                 break
             self.iterators_.remove(it_obj)
             self.cicle_iterators_.append(
                 {
                     "class": SingleRecordIterableDataset,
-                    "args": (it_obj["file"],
-                             self.buffer_size,
-                             self.block_length,
-                             self.options,
-                             self.with_share_memory
-                             ),
+                    "file": it_obj["file"],
                     "instance": None
                 }
             )
 
 
     def get_iterator(self):
         if len(self.cicle_iterators_) == 0 or self.fresh_iter_ids:
@@ -211,15 +216,20 @@
 
     def __next_ex(self):
         iter_obj = self.get_iterator()
         if iter_obj is None:
             raise StopIteration
         try:
             if iter_obj['instance'] is None:
-                iter_obj['instance'] = iter_obj['class'](*iter_obj['args'])
+                iter_obj['instance'] = iter_obj['class'](path=iter_obj["file"],
+                                                         buffer_size=self.buffer_size,
+                                                         batch_size=self.batch_size,
+                                                         block_length= self.block_length,
+                                                         options= self.options,
+                                                         with_share_memory=self.with_share_memory)
             iter = iter_obj['instance']
             it = next(iter)
             if iter.reach_block():
                 self.cur_id += 1
                 self.cur_id = self.cur_id % len(self.cicle_iterators_) if len(self.cicle_iterators_) else 0
             return it
         except StopIteration:
```

## fastdatasets/record/random_dataset/__init__.py

```diff
@@ -6,33 +6,35 @@
 import os
 from typing import List
 import tfrecords
 from .. import RandomDatasetBase
 import pickle
 # from collections.abc import Sized
 import copy
+from ..default import global_default_options
 
 logging.basicConfig(level=logging.INFO)
 
 
-__all__ = ["SingleRecordRandomDataset","MultiRecordRandomDataset", "tfrecords", "logging"]
+__all__ = [
+    "SingleRecordRandomDataset",
+    "MultiRecordRandomDataset"
+]
 
 
 class SingleRecordRandomDataset(RandomDatasetBase):
     def __init__(self,
                  path: typing.Union[typing.AnyStr,typing.Sized],
                  index_path: str = None,
                  use_index_cache=True,
-                 options=tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ):
         super(SingleRecordRandomDataset, self).__init__()
 
-
-
         self.with_share_memory = with_share_memory
 
         if index_path is None:
             index_path = os.path.join(os.path.dirname(path), '.' + os.path.basename(path)+ '.INDEX')
         else:
             index_path = os.path.join(index_path, '.' + os.path.basename(path)+ '.INDEX')
 
@@ -139,15 +141,15 @@
 
 
 class MultiRecordRandomDataset(RandomDatasetBase):
     def __init__(self,
                  data_path_data_list: List[typing.Union[typing.AnyStr,typing.Sized]],
                  index_path = None,
                  use_index_cache=True,
-                 options = tfrecords.TFRecordOptions(tfrecords.TFRecordCompressionType.NONE),
+                 options=copy.deepcopy(global_default_options),
                  with_share_memory=False
                  ) -> None:
         super(MultiRecordRandomDataset, self).__init__()
 
         self.with_share_memory = with_share_memory
         self.options = options
         self.data_path_data_list = data_path_data_list
@@ -167,15 +169,16 @@
             if iter_obj["valid"] and "instance" in iter_obj and iter_obj["instance"]:
                 iter_obj["instance"].close()
                 iter_obj["valid"] = False
                 iter_obj["instance"] = None
 
     def __reopen__(self):
         for it_obj in self.iterators_:
-            it_obj['inst'] = SingleRecordRandomDataset(it_obj["file"], index_path=self.index_path,
+            it_obj['inst'] = SingleRecordRandomDataset(it_obj["file"],
+                                                       index_path=self.index_path,
                                                        use_index_cache=self.use_index_cache,
                                                        options=self.options,
                                                        with_share_memory=self.with_share_memory)
 
     def __len__(self):
         total_len = 0
         for it_obj in self.iterators_:
@@ -191,10 +194,10 @@
         for i,it_obj in enumerate(self.iterators_):
             tmp_obj = it_obj['inst']
             if item < cur_len + len(tmp_obj):
                 obj = tmp_obj
                 break
             cur_len += len(tmp_obj)
         if obj is None:
-            raise tfrecords.OutOfRangeError
+            raise OverflowError
         real_index =  item - cur_len
         return obj[real_index]
```

## fastdatasets/utils/MEMORY.py

```diff
@@ -1,14 +1,15 @@
 # -*- coding: utf-8 -*-
 # @Time    : 2022/11/11 9:04
 import typing
 from .py_features import Final
 
 __all__ = [
-    'MemoryOptions','MemoryWriter'
+    'MemoryOptions',
+    'MemoryWriter'
 ]
 
 
 
 
 
 class MemoryOptions:
```

## Comparing `fastdatasets-0.9.7.post0.dist-info/METADATA` & `fastdatasets-0.9.9.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 Metadata-Version: 2.1
 Name: fastdatasets
-Version: 0.9.7-post0
+Version: 0.9.9
 Summary: fastdatasets: datasets for tfrecords
 Home-page: https://github.com/ssbuild/fastdatasets
 Author: ssbuild
 Author-email: 9727464@qq.com
 License: Apache 2.0
 Keywords: fastdatasets,fastdatasets,tfrecords,dataset,datasets
+Platform: UNKNOWN
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: Intended Audience :: Education
 Classifier: Intended Audience :: Science/Research
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Programming Language :: C++
 Classifier: Programming Language :: Python :: 3
@@ -24,22 +25,23 @@
 Classifier: Topic :: Scientific/Engineering :: Mathematics
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Software Development
 Classifier: Topic :: Software Development :: Libraries
 Classifier: Topic :: Software Development :: Libraries :: Python Modules
 Requires-Python: >=3, <4
 Description-Content-Type: text/markdown
-Requires-Dist: tfrecords (<0.3,>=0.2.6)
+Requires-Dist: tfrecords (<0.3,>=0.2.10)
 Requires-Dist: data-serialize (>=0.2.1)
 Requires-Dist: numpy
 
 
 ## The update statement 
 
 ```text
+2023-07-02: support arrow parquet
 2023-04-28: fix lmdb mutiprocess
 2023-02-13: add TopDataset with iterable_dataset and patch
 2022-12-07: modify a bug for randomdataset for batch reminder
 2022-11-07: add numpy writer and parser,add memory writer and parser
 2022-10-29: add kv dataset 
 2022-10-19: update and modify for __all__ module
 ```
@@ -57,15 +59,15 @@
 
 ```python
 import data_serialize
 from fastdatasets.record import load_dataset, gfile,TFRecordOptions, TFRecordCompressionType, TFRecordWriter
 
 # Example Features结构兼容tensorflow.dataset
 def test_write_featrue():
-    options = TFRecordOptions(compression_type=TFRecordCompressionType.NONE)
+    options = 'GZIP'
 
     def test_write(filename, N=3, context='aaa'):
         with TFRecordWriter(filename, options=options) as file_writer:
             for _ in range(N):
                 val1 = data_serialize.Int64List(value=[1, 2, 3] * 20)
                 val2 = data_serialize.FloatList(value=[1, 2, 3] * 20)
                 val3 = data_serialize.BytesList(value=[b'The china', b'boy'])
@@ -82,15 +84,15 @@
     test_write('d:/example.tfrecords0', 3, 'file0')
     test_write('d:/example.tfrecords1', 10, 'file1')
     test_write('d:/example.tfrecords2', 12, 'file2')
 
 
 # 写任意字符串
 def test_write_string():
-    options = TFRecordOptions(compression_type=TFRecordCompressionType.NONE)
+    options = 'GZIP'
 
     def test_write(filename, N=3, context='aaa'):
         with TFRecordWriter(filename, options=options) as file_writer:
             for _ in range(N):
                 # x, y = np.random.random(), np.random.random()
                 file_writer.write(context + '____' + str(_))
 
@@ -317,15 +319,15 @@
         values.append(train_node)
         if (i+1) % 10000 == 0:
             f.put_batch(keys,values)
             keys.clear()
             values.clear()
     if len(keys):
         f.put_batch(keys, values)
-        
+
     f.get_writer.put('total_num',str(n))
     f.close()
 
 
 
 def test_random(db_path):
     options = DB.LeveldbOptions(create_if_missing=False, error_if_exists=False)
@@ -405,7 +407,111 @@
     for i in tqdm(range(len(dataset)), total=len(dataset)):
         d = dataset[i]
         print(d)
 
 test_write(db_path)
 test_random(db_path)
 ```
+
+
+
+### 7. arrow dataset 
+
+
+```python
+from fastdatasets.arrow.writer import PythonWriter
+from fastdatasets.arrow.dataset import load_dataset,arrow
+
+
+path_file = 'd:/tmp/data.arrow'
+
+
+with_stream = False
+def test_write():
+    fs = PythonWriter(path_file,
+                        schema={'id': 'int32', 'text': 'str', 'text2': 'str'},
+                        with_stream=with_stream,
+                        options=None)
+    for i in range(3):
+        data = {
+            "id": list(range(i * 10,(i+ 1) * 10)),
+            'text': ['asdasdasdas' + str(i) for i in range(10)],
+            'text2': ['asdasdasdas3asdadas' + str(i) for i in range(10)]
+        }
+        # fs.write_batch(data.keys(),data.values())
+        fs.write_table(data.keys(),data.values())
+
+
+    fs.close()
+
+def test_random():
+    dataset = load_dataset.RandomDataset(path_file,with_share_memory=not with_stream)
+    print('total', len(dataset))
+    for i in range(len(dataset)):
+        print(dataset[i])
+
+
+
+def test_read_iter():
+    dataset = load_dataset.IterableDataset(path_file,with_share_memory=not with_stream,batch_size=4)
+    for d in dataset:
+        print(d)
+
+
+test_write()
+
+test_random()
+
+# test_read_iter()
+
+```
+
+### 8. parquet dataset 
+
+```python
+from fastdatasets.parquet.writer import PythonWriter
+from fastdatasets.parquet.dataset import load_dataset
+from tfrecords.python.io.arrow import ParquetReader,arrow
+
+
+path_file = 'd:/tmp/data.parquet'
+
+
+
+def test_write():
+    fs = PythonWriter(path_file,
+                        schema={'id': 'int32','text': 'str','text2': 'str'},
+                        parquet_options=dict(write_batch_size = 10))
+    for i in range(3):
+        data = {
+            "id": list(range(i * 10,(i+ 1) * 10)),
+            'text': ['asdasdasdas' + str(i) for i in range(10)],
+            'text2': ['asdasdasdas3asdadas' + str(i) for i in range(10)]
+        }
+        # fs.write_batch(data.keys(),data.values())
+        fs.write_table(data.keys(),data.values())
+
+
+    fs.close()
+
+def test_random():
+    dataset = load_dataset.RandomDataset(path_file)
+    print('total', len(dataset))
+    for i in range(len(dataset)):
+        print(dataset[i])
+
+
+
+def test_read_iter():
+    dataset = load_dataset.IterableDataset(path_file,batch_size=4)
+    for d in dataset:
+        print(d)
+
+
+test_write()
+
+test_random()
+
+# test_read_iter()
+
+```
+
```

## Comparing `fastdatasets-0.9.7.post0.dist-info/RECORD` & `fastdatasets-0.9.9.dist-info/RECORD`

 * *Files 25% similar despite different names*

```diff
@@ -1,37 +1,52 @@
 fastdatasets/__init__.py,sha256=IWbktO9_fb7lbkdmAGQtoCOcTJqMjDjOdYzqd_eol_I,116
-fastdatasets/setup.py,sha256=Lld9EYqZo12jZaf-wxcC24sQ2-wFKySvtWHNb8Hvhys,2316
+fastdatasets/setup.py,sha256=ce-2HQElkk2u9nCWHYspYjXuhXGfCdzyO4FA1t4x4A4,2311
+fastdatasets/arrow/__init__.py,sha256=Ra_nq4L4KJBPUDHvxD8T6RE1n9sTiVX-Z6gL9N553PI,243
+fastdatasets/arrow/dataset.py,sha256=nrMbYSwkI9cNF7dgnSg50uX8SQU9PFgVUkLSflyM8ns,6558
+fastdatasets/arrow/default.py,sha256=TJCrx0bsMaZIx26j5k7Ln6WR0NK8kA1lTytyKBHgHDo,240
+fastdatasets/arrow/writer.py,sha256=xfEjtBR-EbywK5a7mZ5jCz4txVxfWy5t75DBKV--sYA,2996
+fastdatasets/arrow/iterable_dataset/__init__.py,sha256=iz4t45t7FnqjHLTOTuZRp5i8JwGpGMeUl7EUHNf53_M,9904
+fastdatasets/arrow/random_dataset/__init__.py,sha256=z6TRSUkdejdajeaX9CBUMXJEffKkVR9Qp2T0Td_HCBI,5996
 fastdatasets/common/__init__.py,sha256=sv6vMerE_nO2Y6W-Fav0GqpVrorbFHt1UN_UjzMW-Lw,76
 fastdatasets/common/iterable_dataset.py,sha256=3c9CqZZGUjZCQd-sxKcyb7_bmjIQ0leZQOFdZijZvSE,14096
 fastdatasets/common/random_dataset.py,sha256=Z7yuPgo9G2mi7lFrfSNYw7OGFOwLZSe83KwhkCeRhd0,10767
 fastdatasets/common/writer.py,sha256=klLfxiMzJ2Ne00lFYqztBwrC8myER18TwKmHGW3Xrug,4251
 fastdatasets/common/writer_object.py,sha256=LZZ7j67e83sAn1rjqGOQHbBSWNq2JPvnpHJ8lodWMKs,1135
 fastdatasets/leveldb/__init__.py,sha256=6SYGU-XQEv88zoG69UiSpLydsLyBAWFz8nYu5oLQq1I,209
-fastdatasets/leveldb/dataset.py,sha256=NFJSACfRBmuR5CKVoxA7_Jg5OtW7gwl5tNNyYlc44e8,6031
+fastdatasets/leveldb/dataset.py,sha256=qXSj1wYl7wHvxjzZn6PUzH-DEEj87OU4RVN5n49yYG0,5687
+fastdatasets/leveldb/default.py,sha256=6ZP4RVLhJlzxDd8ixBcX-zFCO25j1D4G8UTy-ysXd30,206
 fastdatasets/leveldb/writer.py,sha256=ho1G57JGpXQE7D3FChHR69PilXqULC2gTRfIww3Lx6E,4489
-fastdatasets/leveldb/iterable_dataset/__init__.py,sha256=eFpT0GTxmtuPOQaPLVyDyKAi6HLbiNHJaQ00TuDi9TI,7148
-fastdatasets/leveldb/random_dataset/__init__.py,sha256=0gQTDFxc__uMdzKOXDNfiQgcEXgzp-aEAi89dj4t3H4,4716
+fastdatasets/leveldb/iterable_dataset/__init__.py,sha256=SsDEUcfmH4tK4iyK8zXZ7XwWvIUDW8veQANW_mqahyQ,7803
+fastdatasets/leveldb/random_dataset/__init__.py,sha256=vDxZm4gPpM0oiBty_9ra5w3USkLIgcshfVy4P2Dcpas,4615
 fastdatasets/lmdb/__init__.py,sha256=L9E4x2kIuVDsOgXXylwQRsURWGtZNw41t-vIxImNwuM,215
-fastdatasets/lmdb/dataset.py,sha256=H2GgEKmMkBf9FTKtvawVPDX_cxFBEwGh1BviUqGdW0Q,6778
+fastdatasets/lmdb/dataset.py,sha256=gRs3YFQOw_gSc65OWwzhu6IwTcMIxUf5NGHPoLvgKWg,6145
+fastdatasets/lmdb/default.py,sha256=_98NgnaaHI3vhTZ59drIoEXqqo2tfcFsN8l3Ce8yoLs,602
 fastdatasets/lmdb/writer.py,sha256=hFKVyWNM-zlrmnAYi0cjBWIx-atr4XevfDn8i1Ds2rY,4696
-fastdatasets/lmdb/iterable_dataset/__init__.py,sha256=eNFFSJ7bubgga3BjM__fpcXjRj644A-55nt7315rwRE,7765
-fastdatasets/lmdb/random_dataset/__init__.py,sha256=HYRTZqia4uQcjE_OGWBx-xaSPDNAr2lSvIjd386vHxg,5796
+fastdatasets/lmdb/iterable_dataset/__init__.py,sha256=OTnPucsAwLiAa7tUxFcO8BjQlMg3cetNE13cfOD7y8M,8233
+fastdatasets/lmdb/random_dataset/__init__.py,sha256=j4hCSMe2RD2dRDKZB5iMzRvzJKYRtvOafFBHctVjhLA,5541
 fastdatasets/memory/__init__.py,sha256=CYhm3FSbi0vO3UoZwpFtu3b1r4nAqGQrSOYTTF-HYa0,209
-fastdatasets/memory/dataset.py,sha256=_0cUbYO5nx2LQYmC6728Lj1nNBCO9yXYMJemIAbvjrE,4704
+fastdatasets/memory/dataset.py,sha256=vah0ZJ44rEoA-mzVHfdM_4tn0ivaLkNdZFTsAYVCXVQ,5608
 fastdatasets/memory/writer.py,sha256=BHytyWyoy95ICY7HxpQamyPA70Eybw8mosfiRxLz_4c,3741
-fastdatasets/memory/iterable_dataset/__init__.py,sha256=D6J0wiumaGNS3a-869I7Ceir5VdxNYojZW9sJUkxrv4,6474
-fastdatasets/memory/random_dataset/__init__.py,sha256=NOtCwaZYuKbZBZpU2GIoBqQsiCW9ziwi4a3Zu4mhOj8,4304
+fastdatasets/memory/iterable_dataset/__init__.py,sha256=jrCjppDTRsG768cN_J8gMuywXqC6Ux1e3sRj0qP9ZeE,6895
+fastdatasets/memory/random_dataset/__init__.py,sha256=FOcShvVmBPyvj_xWvdV0jusE_XgCm9OjSP-OmeiuMmk,4351
+fastdatasets/parquet/__init__.py,sha256=Ra_nq4L4KJBPUDHvxD8T6RE1n9sTiVX-Z6gL9N553PI,243
+fastdatasets/parquet/dataset.py,sha256=liTC_yqwf2EVdKDE-nje-iZvsZt0kR8hi4xq09vPMGE,6578
+fastdatasets/parquet/default.py,sha256=XpypOdIBLXF-RxAJ5kcnfN8f6K-W5mAwv-NE1A_CbWg,348
+fastdatasets/parquet/writer.py,sha256=Y7vmty9bf2Ny9BokWmGSqlimhenbX4JZ5YlhN8LRbB8,3019
+fastdatasets/parquet/iterable_dataset/__init__.py,sha256=i5YYXyojaUUxw6gOf5mOLBv_66MAJnMbF4uzPObyzz4,9619
+fastdatasets/parquet/random_dataset/__init__.py,sha256=gCOD3t5Tydtcs35Y1jlIlm5_FeiPKOn0KrXkU1iuiMI,5516
 fastdatasets/record/__init__.py,sha256=CYhm3FSbi0vO3UoZwpFtu3b1r4nAqGQrSOYTTF-HYa0,209
-fastdatasets/record/dataset.py,sha256=LpT-C4N5hnLLl8zkF5JSYMYwS0HQExyBJnmFEE5126A,6007
+fastdatasets/record/dataset.py,sha256=MUBTTo6Vf-qORmCFn6vKo2_CBzN28Ud-aBXTWYU4WRI,6391
+fastdatasets/record/default.py,sha256=CwqHHzGbcmFRQ1S8kmwWfphQVw9YSrphni0qj1KLZIk,188
 fastdatasets/record/writer.py,sha256=O9XxXzh49SZuMEESbuR9EUiTvsWK4on_9smud-uGn24,3800
-fastdatasets/record/iterable_dataset/__init__.py,sha256=2T7quMNkxRH9X0PRJqV4dM1Fmin5T3EHN7HiIVTL97M,7358
-fastdatasets/record/random_dataset/__init__.py,sha256=GGTJ5kIoYrina58Vp7RV9fq9lzYwmn9DYVoRI2Bjjmc,6788
+fastdatasets/record/iterable_dataset/__init__.py,sha256=_ueVa2ATUJvLJ55oONQRYVgfFgCk6ZdPQ7x37K2qNXI,7768
+fastdatasets/record/random_dataset/__init__.py,sha256=uRZR7UozQfZIRdMNl8zYaTzs821D94UW7iKTu51O7sc,6806
 fastdatasets/torch_dataset/__init__.py,sha256=yOgyipcBb3Eciz9RASpVl1Su_y_64DvFr9Za-oCZffs,5576
-fastdatasets/utils/MEMORY.py,sha256=Nbc6_x8BVj7yG8xs4mHVVk-K_dd3tIHXA3bHdyCcZpQ,980
+fastdatasets/utils/MEMORY.py,sha256=vG4QrlMVQneWftedlIsqnSwGlT7umZj_1xrEPttT_ss,986
 fastdatasets/utils/__init__.py,sha256=y8NZKf2Vim0cuGnafXjCDS2OUSDFfmkwcmE2CdS7mgU,51
 fastdatasets/utils/numpyadapter.py,sha256=jE5Oha5DKNhdgOJlBzL7LHhR8G1skj-9UfdeLDDJZTM,12220
 fastdatasets/utils/parallel.py,sha256=7pxBQ7w26H5wo1gHEMJwFwrkl57PDUA7rF3y8GD5kjM,5457
 fastdatasets/utils/py_features.py,sha256=_yPn5zwI6SH5RLC8hTKSMp1el951_5qMDrVmTuP2xmk,346
-fastdatasets-0.9.7.post0.dist-info/METADATA,sha256=RKUj_8TxUDYNiELlxSTb98cgpSTXRGmhI0RJKCF4fgs,12115
-fastdatasets-0.9.7.post0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-fastdatasets-0.9.7.post0.dist-info/top_level.txt,sha256=cSNnK2OJMFJZoleds15VI1qc8MXMYTpgmVdwQ3jItzo,13
-fastdatasets-0.9.7.post0.dist-info/RECORD,,
+fastdatasets-0.9.9.dist-info/METADATA,sha256=PWnbZLKkuxMioJGxhkyXzYbfJ4vagbhK9IvRtb-QWtg,14385
+fastdatasets-0.9.9.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+fastdatasets-0.9.9.dist-info/top_level.txt,sha256=cSNnK2OJMFJZoleds15VI1qc8MXMYTpgmVdwQ3jItzo,13
+fastdatasets-0.9.9.dist-info/RECORD,,
```

