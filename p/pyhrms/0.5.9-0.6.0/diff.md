# Comparing `tmp/pyhrms-0.5.9.zip` & `tmp/pyhrms-0.6.0.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 73970 bytes, number of entries: 15
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jun-30 06:47 pyhrms-0.5.9/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms/
-drwxrwxrwx  2.0 fat        0 b- stor 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms.egg-info/
--rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.5.9/License.txt
--rw-rw-rw-  2.0 fat    23461 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/PKG-INFO
--rw-rw-rw-  2.0 fat    22549 b- defN 23-Jun-30 06:44 pyhrms-0.5.9/README.rst
--rw-rw-rw-  2.0 fat       42 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/setup.cfg
--rw-rw-rw-  2.0 fat      899 b- defN 23-Jun-30 06:46 pyhrms-0.5.9/setup.py
--rw-rw-rw-  2.0 fat   217265 b- defN 23-Jun-29 23:04 pyhrms-0.5.9/pyhrms/pyhrms.py
--rw-rw-rw-  2.0 fat     1311 b- defN 23-Jun-29 20:20 pyhrms-0.5.9/pyhrms/__init__.py
--rw-rw-rw-  2.0 fat        1 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms.egg-info/dependency_links.txt
--rw-rw-rw-  2.0 fat    23461 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms.egg-info/PKG-INFO
--rw-rw-rw-  2.0 fat      157 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms.egg-info/requires.txt
--rw-rw-rw-  2.0 fat      216 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms.egg-info/SOURCES.txt
--rw-rw-rw-  2.0 fat        7 b- defN 23-Jun-30 06:47 pyhrms-0.5.9/pyhrms.egg-info/top_level.txt
-15 files, 290457 bytes uncompressed, 71924 bytes compressed:  75.2%
+Zip file size: 73821 bytes, number of entries: 15
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-03 06:42 pyhrms-0.6.0/
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms/
+drwxrwxrwx  2.0 fat        0 b- stor 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/
+-rw-rw-rw-  2.0 fat     1088 b- defN 21-Nov-11 20:05 pyhrms-0.6.0/License.txt
+-rw-rw-rw-  2.0 fat    23460 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/PKG-INFO
+-rw-rw-rw-  2.0 fat    22548 b- defN 23-Jul-03 06:41 pyhrms-0.6.0/README.rst
+-rw-rw-rw-  2.0 fat       42 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/setup.cfg
+-rw-rw-rw-  2.0 fat      899 b- defN 23-Jul-03 06:41 pyhrms-0.6.0/setup.py
+-rw-rw-rw-  2.0 fat   217077 b- defN 23-Jul-03 06:38 pyhrms-0.6.0/pyhrms/pyhrms.py
+-rw-rw-rw-  2.0 fat     1311 b- defN 23-Jul-03 06:38 pyhrms-0.6.0/pyhrms/__init__.py
+-rw-rw-rw-  2.0 fat        1 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/dependency_links.txt
+-rw-rw-rw-  2.0 fat    23460 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/PKG-INFO
+-rw-rw-rw-  2.0 fat      157 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/requires.txt
+-rw-rw-rw-  2.0 fat      216 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/SOURCES.txt
+-rw-rw-rw-  2.0 fat        7 b- defN 23-Jul-03 06:42 pyhrms-0.6.0/pyhrms.egg-info/top_level.txt
+15 files, 290266 bytes uncompressed, 71775 bytes compressed:  75.3%
```

## zipnote {}

```diff
@@ -1,46 +1,46 @@
-Filename: pyhrms-0.5.9/
+Filename: pyhrms-0.6.0/
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms/
+Filename: pyhrms-0.6.0/pyhrms/
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms.egg-info/
+Filename: pyhrms-0.6.0/pyhrms.egg-info/
 Comment: 
 
-Filename: pyhrms-0.5.9/License.txt
+Filename: pyhrms-0.6.0/License.txt
 Comment: 
 
-Filename: pyhrms-0.5.9/PKG-INFO
+Filename: pyhrms-0.6.0/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.5.9/README.rst
+Filename: pyhrms-0.6.0/README.rst
 Comment: 
 
-Filename: pyhrms-0.5.9/setup.cfg
+Filename: pyhrms-0.6.0/setup.cfg
 Comment: 
 
-Filename: pyhrms-0.5.9/setup.py
+Filename: pyhrms-0.6.0/setup.py
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms/pyhrms.py
+Filename: pyhrms-0.6.0/pyhrms/pyhrms.py
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms/__init__.py
+Filename: pyhrms-0.6.0/pyhrms/__init__.py
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms.egg-info/dependency_links.txt
+Filename: pyhrms-0.6.0/pyhrms.egg-info/dependency_links.txt
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms.egg-info/PKG-INFO
+Filename: pyhrms-0.6.0/pyhrms.egg-info/PKG-INFO
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms.egg-info/requires.txt
+Filename: pyhrms-0.6.0/pyhrms.egg-info/requires.txt
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms.egg-info/SOURCES.txt
+Filename: pyhrms-0.6.0/pyhrms.egg-info/SOURCES.txt
 Comment: 
 
-Filename: pyhrms-0.5.9/pyhrms.egg-info/top_level.txt
+Filename: pyhrms-0.6.0/pyhrms.egg-info/top_level.txt
 Comment: 
 
 Zip file comment:
```

## Comparing `pyhrms-0.5.9/License.txt` & `pyhrms-0.6.0/License.txt`

 * *Files identical despite different names*

## Comparing `pyhrms-0.5.9/PKG-INFO` & `pyhrms-0.6.0/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.5.9
+Version: 0.6.0
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,15 +22,15 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-June.30.2023: pyhrms 0.5.9 new features:
+July.3.2023: pyhrms 0.6.0 new features:
 
     * Fixed bugs for processing centroid data.
 
 
 
 
 pyhrms can be installed and import as following:
```

## Comparing `pyhrms-0.5.9/README.rst` & `pyhrms-0.6.0/README.rst`

 * *Files 2% similar despite different names*

```diff
@@ -7,15 +7,15 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-June.30.2023: pyhrms 0.5.9 new features:
+July.3.2023: pyhrms 0.6.0 new features:
 
     * Fixed bugs for processing centroid data.
 
 
 
 
 pyhrms can be installed and import as following:
```

## Comparing `pyhrms-0.5.9/setup.py` & `pyhrms-0.6.0/setup.py`

 * *Files 12% similar despite different names*

```diff
@@ -2,15 +2,15 @@
 
 def readme_file():
     with open('README.rst') as rf:
         return rf.read()
 
 setuptools.setup(
     name = 'pyhrms',
-    version = '0.5.9',
+    version = '0.6.0',
     author = 'Wang Rui',
     author_email = 'wtrt7009@gmail.com',
     url = 'https://github.com/WangRui5/PyHRMS.git',
     description = 'A powerful GC/LC-HRMS data analysis tool',
     long_description = readme_file(),
     packages = setuptools.find_packages(),
     install_requires = ['numpy>=1.19.2','pandas>=1.3.3'
```

## Comparing `pyhrms-0.5.9/pyhrms/pyhrms.py` & `pyhrms-0.6.0/pyhrms/pyhrms.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,18 +33,16 @@
           'Oiso': 17.999159, 'F': 18.998403, 'K': 38.963708, 'P': 30.973763, 'Cl': 34.968853,
           'Cliso': 36.965903, 'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770,
           'Si': 27.976928, 'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
           'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486
           })
 
 
-
 def one_step_process(path, company, profile=True, p_value=1, ms2_analysis=True, fold_change=0,
-                  area_threshold=200, filter_type=3, split_n = 20,sat_intensity = False, long_rt_split_n = 1):
-    
+                     area_threshold=200, filter_type=3, split_n=20, sat_intensity=False, long_rt_split_n=1):
     """
     This function using one processor to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - p_value: The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
@@ -60,22 +58,22 @@
 
     """
     print('                                                                            ')
     print('============================================================================')
     print('First process started...')
     print('============================================================================')
     print('                                                                            ')
-    
+
     files_mzml = glob(os.path.join(path, '*.mzML'))
     files_mzml_DDA = [file for file in files_mzml if 'DDA' in os.path.basename(file)]
     files_mzml = [file for file in files_mzml if 'DDA' not in os.path.basename(file)]
     for file in files_mzml:
-        first_process(file, company = company, profile=profile, ms2_analysis = ms2_analysis, 
-                      split_n = split_n, sat_intensity = sat_intensity,long_rt_split_n = long_rt_split_n)
-    
+        first_process(file, company=company, profile=profile, ms2_analysis=ms2_analysis,
+                      split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
+
     # 检查是否有遗漏的
     files_excel_temp = glob(os.path.join(path, '*.xlsx'))
     files_excel_names = [os.path.basename(i)[:-5] for i in files_excel_temp]
     path_omitted = []
     if len(files_mzml) > len(files_excel_names):
         # 检查是哪个文件漏掉了
         for path1 in files_mzml:
@@ -83,33 +81,31 @@
                 pass
             else:
                 path_omitted.append(path1)
     if len(path_omitted) == 0:
         pass
     else:
         for file in path_omitted:
-            first_process(file, company = company, profile=profile, ms2_analysis = ms2_analysis, 
-                      split_n = split_n, sat_intensity = sat_intensity, long_rt_split_n = long_rt_split_n)
+            first_process(file, company=company, profile=profile, ms2_analysis=ms2_analysis,
+                          split_n=split_n, sat_intensity=sat_intensity, long_rt_split_n=long_rt_split_n)
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
-    
-    
+
     # 第二个过程
     print('                                                                            ')
     print('============================================================================')
     print('Second process started...')
     print('============================================================================')
     print('                                                                            ')
     for file in files_mzml:
-        second_process(file, ref_all, company, profile = profile,long_rt_split_n = long_rt_split_n)
-    
-    
+        second_process(file, ref_all, company, profile=profile, long_rt_split_n=long_rt_split_n)
+
     # 第三个过程, 做fold change filter
     print('                                                                            ')
     print('============================================================================')
     print('Third process started...')
     print('============================================================================')
     print('                                                                            ')
     if filter_type == 1:
@@ -118,41 +114,39 @@
         fold_change_filter2(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
     elif filter_type == 3:
         fold_change_filter3(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
 
     # 如果有DDA，将DDA数据加入到excel里
     files_excel = glob(os.path.join(path, '*.xlsx'))
     unique_cmps = [file for file in files_excel if 'unique_cmps' in os.path.basename(file)]
-             
+
     for file in files_mzml_DDA:
         df2 = gen_DDA_ms2_df(file, company, profile=profile, opt=False)
         name = os.path.basename(file).replace('-DDA', '').replace('_DDA', '').replace('.mzML', '')  # 获得DDA文件的特征名称
         for file_excel in unique_cmps:
             if name in os.path.basename(file_excel):
                 df1 = pd.read_excel(file_excel)
                 for i in range(len(df1)):
                     rt, mz = df1.loc[i, ['rt', 'mz']]
                     df_frag = df2[(df2['precursor'] >= mz - 0.015) & (df2['precursor'] <= mz + 0.015)
                                   & (df2['rt'] >= rt - 0.1) & (df2['rt'] <= rt + 0.1)]
                     if len(df_frag) == 0:
                         df1.loc[i, 'frag_DDA'] = str([])
                     else:
-                        s_ms2_info = df_frag.iloc[np.argmin(abs(df_frag['rt'].values-rt))]
-                        ms2_rt = df_frag['rt'].values[np.argmin(abs(df_frag['rt'].values-rt))]
-                        s_ms2 = pd.Series(data = s_ms2_info['intensity'], index = s_ms2_info['frag'],name = ms2_rt)
+                        s_ms2_info = df_frag.iloc[np.argmin(abs(df_frag['rt'].values - rt))]
+                        ms2_rt = df_frag['rt'].values[np.argmin(abs(df_frag['rt'].values - rt))]
+                        s_ms2 = pd.Series(data=s_ms2_info['intensity'], index=s_ms2_info['frag'], name=ms2_rt)
                         df1.loc[i, 'frag_DDA'] = str(list(s_ms2.index))
-                        df1.loc[i,'MS2_spec_DDA'] = str(s_ms2.astype(int))
-                df1.to_excel(file_excel) 
+                        df1.loc[i, 'MS2_spec_DDA'] = str(s_ms2.astype(int))
+                df1.to_excel(file_excel)
 
 
-
-                
 def ultimate_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500,
-                       SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
-                       mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False,long_rt_split_n = 1,rt_overlap = 1):
+                          SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
+                          mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False, long_rt_split_n=1, rt_overlap=1):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
     Args:
         ms1 (scan list): generated from sep_scans(file.mzML).
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
@@ -166,90 +160,94 @@
         sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
         long_rt_split_n: The number of pieces to split the ms1.
         rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
     Returns:
         pandas.DataFrame: A dataframe with information on the peaks including retention time, m/z value,
         intensity, and area.
     """
-    
+
     if long_rt_split_n == 1:
-        peak_all = split_peak_picking(ms1, profile=profile, split_n=split_n, threshold=threshold, i_threshold=i_threshold,
-                       SN_threshold=SN_threshold, noise_threshold=noise_threshold, rt_error_alignment=rt_error_alignment,
-                       mz_error_alignment=mz_error_alignment, mz_overlap=mz_overlap, sat_intensity=sat_intensity)
+        peak_all = split_peak_picking(ms1, profile=profile, split_n=split_n, threshold=threshold,
+                                      i_threshold=i_threshold,
+                                      SN_threshold=SN_threshold, noise_threshold=noise_threshold,
+                                      rt_error_alignment=rt_error_alignment,
+                                      mz_error_alignment=mz_error_alignment, mz_overlap=mz_overlap,
+                                      sat_intensity=sat_intensity)
     else:
         # Calculate the length of each part
         total_spectra = len(ms1)
         part_length = total_spectra // long_rt_split_n
-        overlap_spectra = int(rt_overlap/ (ms1[1].scan_time[0] - ms1[0].scan_time[0]))  # calculate the number of spectra in 1 minute of retention time
+        overlap_spectra = int(rt_overlap / (ms1[1].scan_time[0] - ms1[0].scan_time[
+            0]))  # calculate the number of spectra in 1 minute of retention time
 
         # Split the list into parts
         parts = []
         for i in range(long_rt_split_n):
             start_index = i * part_length - overlap_spectra
             start_index = max(start_index, 0)  # set start index to 0 if it is less than 0
-            end_index = (i+1) * part_length + overlap_spectra
+            end_index = (i + 1) * part_length + overlap_spectra
             part = ms1[start_index:end_index]
             parts.append(part)
 
         # Add any remaining spectra to the last part
         if end_index < total_spectra:
             last_part = ms1[end_index:]
             parts[-1] += last_part
 
-
         # just get the split time point
-        parts1 = [ms1[i*part_length:(i+1)*part_length] for i in range(long_rt_split_n)]
+        parts1 = [ms1[i * part_length:(i + 1) * part_length] for i in range(long_rt_split_n)]
         ranges = []
         for i, part in enumerate(parts1):
             rt_start = part[0].scan_time[0]
             rt_end = part[-1].scan_time[0]
-            range1 = [rt_start,rt_end]
+            range1 = [rt_start, rt_end]
             ranges.append(range1)
-            
+
         # to make sure there is no gap between each list.
         my_list = ranges
-        ranges = [[my_list[i][0], my_list[i+1][0]] for i in range(len(my_list) - 1)] + [my_list[-1]]
-    
+        ranges = [[my_list[i][0], my_list[i + 1][0]] for i in range(len(my_list) - 1)] + [my_list[-1]]
+
         # start to do peak picking for each part
         peak_list_all = []
-        for n,part in enumerate(parts):
+        for n, part in enumerate(parts):
             peak_all = split_peak_picking(part, profile=profile, i_threshold=i_threshold,
-                                              SN_threshold=SN_threshold, split_n=split_n, sat_intensity=sat_intensity)
-            peak_all = peak_all[(peak_all['rt']>ranges[n][0])&(peak_all['rt']<=ranges[n][1])]
+                                          SN_threshold=SN_threshold, split_n=split_n, sat_intensity=sat_intensity)
+            peak_all = peak_all[(peak_all['rt'] > ranges[n][0]) & (peak_all['rt'] <= ranges[n][1])]
             peak_list_all.append(peak_all)
 
-        peak_all = pd.concat(peak_list_all).reset_index(drop = True)
+        peak_all = pd.concat(peak_list_all).reset_index(drop=True)
     return peak_all
 
 
 def first_process(file, company, profile=True, i_threshold=200, SN_threshold=3,
-                  ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n = 1):
+                  ms2_analysis=True, frag_rt_error=0.02, split_n=20, sat_intensity=False, long_rt_split_n=1):
     """
     Processes HRMS data by performing peak picking and generating a result file.
 
     Args:
         file (str): Path to the input file to be processed.
         company (str): The manufacturer of the instrument used to generate the data (e.g., 'Waters', 'Agilent', etc.).
         profile (bool): A flag indicating whether or not to perform profiling of chromatographic peaks.
         SN_threshold (int): The signal-to-noise threshold for peak detection.
         frag_rt_error (float): The retention time error to use for fragment MS2 analysis.
         i_threshold (int): The intensity threshold for peak detection.
         ms2_analysis (bool): A flag indicating whether or not to perform MS2 analysis on fragment peaks.
         split_n (int): The number of pieces to split the large dataframe.
         sat_intensity: The saturation intensity refers to the point where the intensity of an m/z value becomes so high that it may no longer be accurate. In such cases, the retention time can be adjusted to bring the intensity below the saturation intensity, thereby ensuring accurate measurement of the m/z value.
         long_rt_split_n: The number of pieces to split the ms1.
-        rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
-        
+
+
     Returns:
         None. Instead, the function exports an Excel file with the result information.
     """
     mz_round = 4
     ms1, ms2 = sep_scans(file, company)
     peak_all = ultimate_peak_picking(ms1, profile=profile, split_n=split_n, i_threshold=i_threshold,
-                       SN_threshold=SN_threshold,  sat_intensity=sat_intensity, long_rt_split_n = long_rt_split_n)
+                                     SN_threshold=SN_threshold, sat_intensity=sat_intensity,
+                                     long_rt_split_n=long_rt_split_n)
 
     # 是否分析ms2
     if len(ms2) == 0:
         pass
     else:
         if ms2_analysis is True:
             basename_file = os.path.basename(file)
@@ -258,29 +256,30 @@
                     'qaqc' in basename_file.lower()):
                 pass
             else:
                 print('----------------------------')
                 print('Starting DIA ms2 analysis...')
                 print('----------------------------')
                 peak_all2 = ultimate_peak_picking(ms2, profile=profile, split_n=split_n, i_threshold=i_threshold,
-                       SN_threshold=SN_threshold,  sat_intensity=sat_intensity, long_rt_split_n = long_rt_split_n)
+                                                  SN_threshold=SN_threshold, sat_intensity=sat_intensity,
+                                                  long_rt_split_n=long_rt_split_n)
 
                 frag_all = []
                 spec_all = []
-                for i in tqdm(range(len(peak_all)),desc = 'Assign DIA MS2 spectrum'):
+                for i in tqdm(range(len(peak_all)), desc='Assign DIA MS2 spectrum'):
                     rt = peak_all.loc[i, 'rt']
                     df_DIA = peak_all2[(peak_all2['rt'] > rt - frag_rt_error)
-                                              & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
+                                       & (peak_all2['rt'] < rt + frag_rt_error)].sort_values(
                         by='intensity', ascending=False)
                     # append fragments
                     frag = str(list(df_DIA['mz'].values))
                     frag_all.append(frag)
                     # append ms2 spectra
-                    
-                    s = pd.Series(data = df_DIA['intensity'].values,index = df_DIA['mz'])
+
+                    s = pd.Series(data=df_DIA['intensity'].values, index=df_DIA['mz'])
                     # Convert the series to a DataFrame
                     df = pd.DataFrame(s).reset_index()
                     df.columns = ['m/z', 'intensity']
 
                     # Sort the dataframe by 'm/z'
                     df = df.sort_values(by='m/z')
 
@@ -295,20 +294,20 @@
 
                     # Convert the DataFrame back to a Series
                     result = pd.Series(df['intensity'].values, index=df['m/z']).astype(int)
 
                     # Remove the name of the index
                     result.index.name = None
 
-                    result = result.sort_values(ascending = False).iloc[:20]
+                    result = result.sort_values(ascending=False).iloc[:20]
 
                     spec_all.append(str(result))
-    
+
                 peak_all.loc[:, 'frag_DIA'] = frag_all
-                peak_all.loc[:,'ms2_spec_DIA'] = spec_all
+                peak_all.loc[:, 'ms2_spec_DIA'] = spec_all
 
 
         else:
             pass
     file_name = os.path.basename(file)
     peak_selected = identify_isotopes(peak_all)
     peak_selected = remove_unnamed_columns(peak_selected)
@@ -316,17 +315,16 @@
     print('                                                              ')
     print(f'-------------------------------------------------------------')
     print(f'Result generated! File name:{file_name}')
     print('--------------------------------------------------------------')
     print('                                                              ')
 
 
-
 def multi_process(path, company, profile=True, processors=1, p_value=1, ms2_analysis=True, fold_change=0,
-                  area_threshold=200, filter_type=3, split_n=20, sat_intensity=False,long_rt_split_n = 1):
+                  area_threshold=200, filter_type=3, split_n=20, sat_intensity=False, long_rt_split_n=1):
     """
     This function is to process mzML data and perform a comparison between the sample set and the control set. The resulting data will be used to generate an Excel file that summarizes the differences between the two sets.
     Args:
        - path: The file path for the mzML files that will be processed. For example, '../Users/Desktop/my_HRMS_files'.
        - company: The type of mass spectrometer used to acquire the data. Valid options are 'Waters', 'Thermo', 'Sciex', and 'Agilent'.
        - profile: A Boolean value that indicates whether the data is in profile or centroid mode. True for profile mode, False for centroid mode.
        - processors: This setting determines the number of processors that will be used for data processing in parallel running. If the memory usage exceeds 90%, please note that some Excel files may not be generated.
@@ -349,15 +347,15 @@
     i_threshold = 200
     SN_threshold = 3,
     frag_rt_error = 0.02
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
         pool.apply_async(first_process, args=(file, company, profile, i_threshold, SN_threshold,
-                                              ms2_analysis, frag_rt_error, split_n, sat_intensity,long_rt_split_n))
+                                              ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n))
 
     print('                                                                            ')
     print('============================================================================')
     print('First process started...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
@@ -378,28 +376,28 @@
         pass
     else:
         pool = Pool(processes=processors)
         for file in path_omitted:
             print('Omitted files')
             print(file)
             pool.apply_async(first_process, args=(file, company, profile, i_threshold, SN_threshold,
-                                                  ms2_analysis, frag_rt_error, split_n, sat_intensity,long_rt_split_n))
+                                                  ms2_analysis, frag_rt_error, split_n, sat_intensity, long_rt_split_n))
         pool.close()
         pool.join()
 
     # 中间过程
     files_excel = glob(os.path.join(path, '*.xlsx'))
     peak_alignment(files_excel)
     ref_all = pd.read_excel(os.path.join(path, 'peak_ref.xlsx'), index_col='Unnamed: 0')
 
     # 第二个过程
     pool = Pool(processes=processors)
     for file in files_mzml:
         print(file)
-        pool.apply_async(second_process, args=(file, ref_all, company, profile,long_rt_split_n))
+        pool.apply_async(second_process, args=(file, ref_all, company, profile, long_rt_split_n))
     print('                                                                            ')
     print('============================================================================')
     print('Second process started...')
     print('============================================================================')
     print('                                                                            ')
     pool.close()
     pool.join()
@@ -416,34 +414,34 @@
         fold_change_filter2(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
     elif filter_type == 3:
         fold_change_filter3(path, fold_change=fold_change, p_value=p_value, area_threshold=area_threshold)
 
     # 如果有DDA，将DDA数据加入到excel里
     files_excel = glob(os.path.join(path, '*.xlsx'))
     unique_cmps = [file for file in files_excel if 'unique_cmps' in os.path.basename(file)]
-             
+
     for file in files_mzml_DDA:
         df2 = gen_DDA_ms2_df(file, company, profile=profile, opt=False)
         name = os.path.basename(file).replace('-DDA', '').replace('_DDA', '').replace('.mzML', '')  # 获得DDA文件的特征名称
         for file_excel in unique_cmps:
             if name in os.path.basename(file_excel):
                 df1 = pd.read_excel(file_excel)
                 for i in range(len(df1)):
                     rt, mz = df1.loc[i, ['rt', 'mz']]
                     df_frag = df2[(df2['precursor'] >= mz - 0.015) & (df2['precursor'] <= mz + 0.015)
                                   & (df2['rt'] >= rt - 0.1) & (df2['rt'] <= rt + 0.1)]
                     if len(df_frag) == 0:
                         df1.loc[i, 'frag_DDA'] = str([])
                     else:
-                        s_ms2_info = df_frag.iloc[np.argmin(abs(df_frag['rt'].values-rt))]
-                        ms2_rt = df_frag['rt'].values[np.argmin(abs(df_frag['rt'].values-rt))]
-                        s_ms2 = pd.Series(data = s_ms2_info['intensity'], index = s_ms2_info['frag'],name = ms2_rt)
+                        s_ms2_info = df_frag.iloc[np.argmin(abs(df_frag['rt'].values - rt))]
+                        ms2_rt = df_frag['rt'].values[np.argmin(abs(df_frag['rt'].values - rt))]
+                        s_ms2 = pd.Series(data=s_ms2_info['intensity'], index=s_ms2_info['frag'], name=ms2_rt)
                         df1.loc[i, 'frag_DDA'] = str(list(s_ms2.index))
-                        df1.loc[i,'MS2_spec_DDA'] = str(s_ms2.astype(int))
-                df1.to_excel(file_excel) 
+                        df1.loc[i, 'MS2_spec_DDA'] = str(s_ms2.astype(int))
+                df1.to_excel(file_excel)
 
 
 def sep_scans(path, company):
     """
     Separates scans for MS1 and MS2 in mzML files using pymzml package.
     Args:
        path (str): The path of the mzML file.
@@ -929,14 +927,15 @@
         tuple: A tuple containing the interpolated x and y values.
     """
     f = interp1d(x, y)
     x_new = np.linspace(x.min(), x.max(), num=num_points, endpoint=True)
     y_new = f(x_new)
     return x_new, y_new
 
+
 def split_peak_picking(ms1, profile=True, split_n=20, threshold=15, i_threshold=500,
                        SN_threshold=3, noise_threshold=0, rt_error_alignment=0.05,
                        mz_error_alignment=0.015, mz_overlap=1, sat_intensity=False):
     """
     Find peaks in the orginal ms1 list, analyze isotope and adduct information, and return a dataframe with
     information on the peaks including retention time, m/z value, intensity, and area.
 
@@ -1070,15 +1069,17 @@
         mz_opt = [mz_opt[i] if abs(mzs[i] - mz_opt[i]) < 0.02 else mzs[i] for i in range(len(mzs))]  # 去掉偏差大的矫正结果
 
         peak_all.loc[:, ['mz', 'mz_opt', 'resolution']] = np.array([mz_obs, mz_opt, resolution.astype(int)]).T
     else:
         spec1 = [raw_info_centroid[i] for i in rt_keys]  # 获得ms的spec
         target_spec = [spec1[i][(spec1[i].index > mzs[i] - 0.015) & (spec1[i].index < mzs[i] + 0.015)] for i in
                        range(len(spec1))]
-        mzs_obs = [target_spec[i].index.values[[np.argmax(target_spec[i].values)]][0] if len(target_spec[i])!=0 else mzs[i] for i in range(len(target_spec))]
+        mzs_obs = [
+            target_spec[i].index.values[[np.argmax(target_spec[i].values)]][0] if len(target_spec[i]) != 0 else mzs[i]
+            for i in range(len(target_spec))]
         peak_all['mz'] = mzs_obs
     t4 = time.time()
     t_ = round(t4 - t3, 1)
     print(f'\r *** Checking/Optimizing mass finished: {t_} s')
 
     # 如果担心饱和质量不准，使用sat_intensity 更新质量
     if (sat_intensity is False) | (sat_intensity is None):
@@ -1096,48 +1097,49 @@
                             new_spec1 = target_spec1(s1, mz,
                                                      0.2)  # cut the spectrum in a certain range. 1 m/z in this case.
                             peak_index, *_ = scipy.signal.find_peaks(new_spec1.values)
                             if max(new_spec1.values[peak_index]) < sat_intensity:
                                 insat_mz_obs, error1, insat_mz_opt, error2, resolution = evaluate_ms(new_spec1, mz)
                                 peak_all.loc[j, 'mz'] = insat_mz_obs
                                 peak_all.loc[j, 'mz_opt'] = insat_mz_opt
-                                peak_all.loc[j,'resolution'] = resolution
+                                peak_all.loc[j, 'resolution'] = resolution
                                 break
         else:
             for j in tqdm(range(len(peak_all)), desc='Optimize m/z based on sat_intensity'):
                 mz = peak_all.loc[j, 'mz']
                 rt = peak_all.loc[j, 'rt']
                 intensity = peak_all.loc[j, 'intensity']
                 if intensity > sat_intensity:
                     for k, v in raw_info_centroid.items():
                         if k > rt:  # find the time
                             s1 = raw_info_centroid[k]
-                            new_spec1 = target_spec1(s1, mz,0.2)  # cut the spectrum in a certain range. 1 m/z in this case.
+                            new_spec1 = target_spec1(s1, mz,
+                                                     0.2)  # cut the spectrum in a certain range. 1 m/z in this case.
                             max_intensity = max(new_spec1)
                             update_mz = new_spec1.idxmax()
                             if max_intensity < sat_intensity:
-                                peak_all.loc[j,'mz'] = update_mz
+                                peak_all.loc[j, 'mz'] = update_mz
                                 break
     return peak_all
 
+
 def remove_unnamed_columns(df):
     """
     Remove any columns in the input DataFrame that are named 'Unnamed:*'.
 
     Args:
         df (pandas.DataFrame): The input DataFrame to process.
 
     Returns:
         pandas.DataFrame: A new DataFrame with the 'Unnamed:*' columns removed.
     """
     unnamed_columns = [col for col in df.columns if col.startswith('Unnamed:')]
     return df.drop(columns=unnamed_columns).reset_index(drop=True)
 
 
-
 def identify_isotopes(cmp, iso_error=0.005):
     """
     Identify isotopes and adducts in the unique compounds dataframe based on their mass-to-charge ratio (m/z) and retention time (rt).
 
     Args:
         cmp: pandas DataFrame of unique compounds
         iso_error: maximum allowable mass error for identifying isotopes and adducts
@@ -1362,99 +1364,100 @@
 
     # Combine reference pairs
     final_reference_pairs = np.vstack([reference_pairs, omitted_reference_pairs])
 
     return final_reference_pairs
 
 
-def second_process(file, ref_all, company, profile=True,long_rt_split_n = 1):
+def second_process(file, ref_all, company, profile=True, long_rt_split_n=1):
     """
     This function will use the reference rt&mz pair, and obtain the peak area at specific rt & mz
     Args:
         profile: True or False
         file: single file to process
         ref_all: all reference peaks
         company: e.g., 'Waters', 'Agilent',etc,
     returns:
         export to files
- 
+
     """
     ms_round = 4
     ms1, ms2 = sep_scans(file, company)
 
     name1 = os.path.basename(file).split('.')[0]
     final_result = ultimate_checking_area(ref_all, ms1, name1, profile=profile,
-                                          rt_overlap=1, long_rt_split_n = long_rt_split_n)
+                                          rt_overlap=1, long_rt_split_n=long_rt_split_n)
     final_result.to_excel(file.replace('.mzML', '_final_area.xlsx'))
 
     print('                                                              ')
     print('--------------------------------------------------------------')
     print(f'Final area files have been created: {os.path.basename(file)}')
     print('--------------------------------------------------------------')
     print('                                                              ')
 
 
-    
-def ultimate_checking_area(ref_all, ms1, name1, profile = True, 
-                           split_n = 20, rt_overlap=1, long_rt_split_n = 4):
+def ultimate_checking_area(ref_all, ms1, name1, profile=True,
+                           split_n=20, rt_overlap=1, long_rt_split_n=4):
     """
     Based on peak reference, intergrate peak are for each reference m/z and retention time pair.
-    
+
     Args:
         ref_all: reference m/z and retention time pair
         ms1: generated from sep_scans(file.mzML).
         name1: file name.
         profile: A boolean indicating whether the data is in profile mode (True) or centroid mode (False)
         split_n: The number of pieces to split the large dataframe.
         long_rt_split_n: The number of pieces to split the ms1.
         rt_overlap: The rt overlap (min) between adjacent sections of data when splitting it.
     return:
         The final areas for reference m/z and retention time pair.
     """
-    
+
     if long_rt_split_n == 1:
-        final_area = peak_checking_area_split(ref_all, ms1, name1, profile=True, split_n=split_n, noise_threshold=0)
-       
+        final_area = peak_checking_area_split(ref_all, ms1, name1, profile=profile, split_n=split_n, noise_threshold=0)
+
     else:
         # Calculate the length of each part
         total_spectra = len(ms1)
         part_length = total_spectra // long_rt_split_n
-        overlap_spectra = int(rt_overlap/ (ms1[1].scan_time[0] - ms1[0].scan_time[0]))  # calculate the number of spectra in 1 minute of retention time
+        overlap_spectra = int(rt_overlap / (ms1[1].scan_time[0] - ms1[0].scan_time[
+            0]))  # calculate the number of spectra in 1 minute of retention time
 
         # Split the list into parts
         parts = []
         for i in range(long_rt_split_n):
             start_index = i * part_length - overlap_spectra
             start_index = max(start_index, 0)  # set start index to 0 if it is less than 0
-            end_index = (i+1) * part_length + overlap_spectra
+            end_index = (i + 1) * part_length + overlap_spectra
             part = ms1[start_index:end_index]
             parts.append(part)
 
         # Add any remaining spectra to the last part
         if end_index < total_spectra:
             last_part = ms1[end_index:]
             parts[-1] += last_part
 
-        parts1 = [ms1[i*part_length:(i+1)*part_length] for i in range(long_rt_split_n)]
-        mz_list = [round(part[0].scan_time[0],3) for part in parts1]
+        parts1 = [ms1[i * part_length:(i + 1) * part_length] for i in range(long_rt_split_n)]
+        mz_list = [round(part[0].scan_time[0], 3) for part in parts1]
         mz_list.append(parts1[-1][-1].scan_time[0])
-        ranges = [[mz_list[i], mz_list[i+1]] for i in range(len(mz_list)-1)]
+        ranges = [[mz_list[i], mz_list[i + 1]] for i in range(len(mz_list) - 1)]
         # start to split ref_all
         ref_all = remove_unnamed_columns(ref_all)
-        ref_all_parts = [ref_all[(ref_all['rt']>=ranges1[0])&(ref_all['rt']<ranges1[1])] for ranges1 in ranges]
-        
+        ref_all_parts = [ref_all[(ref_all['rt'] >= ranges1[0]) & (ref_all['rt'] < ranges1[1])] for ranges1 in ranges]
+
         # start to collect each peak_all
         peak_area_all = []
         for i in range(len(parts)):
-            each_peak_area = peak_checking_area_split(ref_all_parts[i],parts[i], '', profile = profile,split_n=split_n)
+            each_peak_area = peak_checking_area_split(ref_all_parts[i], parts[i], '', profile=profile, split_n=split_n)
             peak_area_all.append(each_peak_area)
         final_area = pd.concat(peak_area_all)
         final_area.columns = [name1]
-    return final_area    
-    
+    return final_area
+
+
 def peak_checking_area(ref_all, df1, name):
     """
     Obtain the area for each rt&mz pair in df1
     :param ref_all:  peak_reference
     :param df1: dataframe df1
     :param name: name
     :return: new_dataframe
@@ -1543,15 +1546,15 @@
     area_all = []
     for i in tqdm(range(split_n), desc='Integrating peak area:'):
         peak_ref1 = all_peak_ref[i]
         df1 = pd.concat(all_data[i], axis=1)
         if len(df1) == 0:
             pass
         else:
-            df1 = df1.fillna(0)
+            df1 = df1.fillna(0).sort_index()
             df_area = peak_checking_area(peak_ref1, df1, 'split')
             area_all.append(df_area)
 
     # 合成所有的area
     final_df = pd.concat(area_all)
     final_df.columns = [name1]
     return final_df
@@ -1632,15 +1635,14 @@
 def fold_change_filter2(path, fold_change=0, p_value=1, area_threshold=500):
     """
     To filter for both p-value and fold change (treating all controls as a single group).
 
     Args:
         path:The file path for the mzML files that will be processed.
         fold_change:The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
-        p_values:The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
         area_threshold:The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
 
     Returns:
         None. The output file will have the suffix '_unique_cmps.xlsx'.
     """
 
     def get_p_value(mean1, std1, nobs1, mean2, std2, nobs2):
@@ -1709,15 +1711,15 @@
 def fold_change_filter3(path, fold_change=0, p_value=1, area_threshold=500):
     """
     To filter for both p-value and fold change (treating each set of solvent_blank, field blank, and lab blank as separate groups).
 
     Args:
         path:The file path for the mzML files that will be processed.
         fold_change:The fold change threshold for peak area comparison. Peaks with a fold change below this threshold will be excluded from analysis.
-        p_values:The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
+        p_value:The maximum p-value threshold for peak identification. Peaks with a p-value above this threshold will be excluded from analysis.
         area_threshold:The minimum peak area threshold. Peaks with an area below this threshold will be excluded from analysis.
 
     Returns:
         None. The output file will have the suffix '_unique_cmps.xlsx'.
     """
 
     def get_p_value(mean1, std1, nobs1, mean2, std2, nobs2):
@@ -1934,23 +1936,23 @@
 
         # Get the m/z and intensity values from the scan
         mz = scan.mz
         intensity = scan.i
 
         # If data is in profile mode, convert it to centroid mode
         if profile is True:
-            spec = pd.Series(data=intensity, index=mz,dtype = 'float64')
+            spec = pd.Series(data=intensity, index=mz, dtype='float64')
             new_spec = ms_to_centroid(spec)
             mz = new_spec.index.values
             intensity = new_spec.values
         else:
             pass
 
         # Filter out low intensity peaks and get the top 20 peaks
-        s = pd.Series(data=intensity, index=mz,dtype='float64').sort_values(ascending=False).iloc[:20]
+        s = pd.Series(data=intensity, index=mz, dtype='float64').sort_values(ascending=False).iloc[:20]
         s = s[s > i_threshold]
 
         # Get the fragment m/z values and their intensities
         frag = list(s.index.values.round(4))
         frags.append(frag)
         intensities.append(list(s.values))
 
@@ -1964,15 +1966,15 @@
         if opt == False:
             pass
         else:
             mz_opt_all = []
             for i in tqdm(range(len(DDA_df)), desc='Optimizing mass'):
                 frag = DDA_df.loc[i].frag
                 x = DDA_df.loc[i].scan_index
-                s1 = pd.Series(ms2[x].i, ms2[x].mz.round(4),dtype = 'float64')
+                s1 = pd.Series(ms2[x].i, ms2[x].mz.round(4), dtype='float64')
                 mz_opt = [evaluate_ms(target_spec(s1, mz), mz)[2] for mz in frag]
                 mz_opt_all.append(mz_opt)
 
             DDA_df['frag_opt'] = mz_opt_all
     return DDA_df
 
 
@@ -3400,15 +3402,15 @@
         name_s = pd.concat(name_data, axis=1).mean(axis=1)
         final_result.append(name_s)
     data = pd.concat(final_result, axis=1).astype(int)
     data.columns = names
     return data
 
 
-def omics_index_dict(path, all_new_index, names,rt_threshold = 0.1, mz_threshold = 0.015):
+def omics_index_dict(path, all_new_index, names, rt_threshold=0.1, mz_threshold=0.015):
     """
     Given a list of new_index values, this function checks if each value corresponds to a pair of
     retention time (rt) and mass-to-charge ratio (mz) values that are present in a set of Excel files
     containing omics data. For each sample set, the function returns a dictionary mapping sample set
     names to lists of new_index values that correspond to valid rt/mz pairs in the omics data files.
 
     Args:
@@ -3440,16 +3442,16 @@
             df = pd.read_excel(file)
             df_all.append(df)
         df_all_df = pd.concat(df_all, axis=0).reset_index(drop=True)
         index_check = []
         for i in tqdm(range(len(pair_df)), desc='Checking compounds for each new_index:'):
             rt, mz = pair_df.iloc[i, [0, 1]]
             new_index = pair_df.iloc[i].name
-            check_result = df_all_df[(df_all_df.rt > rt - rt_threshold) & (df_all_df.rt < rt +rt_threshold) &
-                                     (df_all_df.mz < mz +  mz_threshold) & (df_all_df.mz > mz -  mz_threshold)]
+            check_result = df_all_df[(df_all_df.rt > rt - rt_threshold) & (df_all_df.rt < rt + rt_threshold) &
+                                     (df_all_df.mz < mz + mz_threshold) & (df_all_df.mz > mz - mz_threshold)]
             if len(check_result) > 0:
                 index_check.append(new_index)
         final_dict[name] = index_check
     return final_dict
 
 
 def omics_filter(data, index_dict, mz_range=[100, 800], rt_range=[1, 18], area_threshold=5000):
@@ -3739,44 +3741,45 @@
 
 
 """
 ========================================================================================================
 4. FT-ICRMS data processing
 ========================================================================================================
 """
+
+
 def FT_ICRMS_process(path, formula='C50H60O50N1S1', mz_range=None, peak_threshold=6, error=1, iso_error=0.0003,
-             iso_fold_change=2, mode='neg'):
-    
+                     iso_fold_change=2, mode='neg'):
     """
     Process FT_ICRMS data and find possible formula.
-    
+
     Args:
-        file_path (str): File path for profile raw data; supported formats: .xy, .csv, .xlsx
+        path (str): File path for profile raw data; supported formats: .xy, .csv, .xlsx
         formula (str): Formula range for prediction (default: 'C50H60O50N1S1')
         mz_range (list): m/z range for prediction (default: None, which sets it to [200, 800])
         peak_threshold (int): Similar to signal to noise (default: 6)
         error (int): m/z error for formula prediction, unit: part per million (ppm) (default: 1)
         iso_error (float): m/z error for isotope assignment, unit: Dalton (default: 0.0003)
         iso_fold_change (int): The peak intensity/isotope intensity (default: 2)
         mode (str): 'pos' for positive, 'neg' for negative (default: 'neg')
 
     Returns:
         DataFrame: A dataframe with formula prediction
     """
-    
+
     if mz_range is None:
         mz_range = [200, 800]
     atom_mass_table1 = pd.Series(
         data={'C': 12.000000, 'Ciso': 13.003355, 'N': 14.003074, 'Niso': 15.000109, 'O': 15.994915, 'H': 1.007825,
               'Oiso': 17.999159, 'F': 18.998403, 'K': 38.963708, 'P': 30.973763, 'Cl': 34.968853,
               'S': 31.972072, 'Siso': 33.967868, 'Br': 78.918336, 'Na': 22.989770, 'Si': 27.976928,
               'Fe': 55.934939, 'Se': 79.916521, 'As': 74.921596, 'I': 126.904477, 'D': 2.014102,
               'Co': 58.933198, 'Au': 196.966560, 'B': 11.009305, 'e': 0.0005486
               })
-    
+
     def formula_sep(formula):
         """
         Transform formula to atoms list and atoms number list.
         :param formula: e.g., 'C13H13N3'
         """
         a = re.findall('[A-Z][a-z]*|\\d+', formula)
         b = {}
@@ -3792,15 +3795,14 @@
         atoms = list(c.index)
         atom_n1 = list(c.values)
         atom_n = []
         for num in atom_n1:
             atom_n.append([0, num])
         return atoms, atom_n
 
-
     # 查找同位素的函数
     def find_isotopes(data1, atom_mass_table2, error1=0.0003, iso_fold_change1=5):
 
         Ciso = atom_mass_table2['Ciso'] - atom_mass_table2['C']
         Niso = atom_mass_table2['Niso'] - atom_mass_table2['N']
         Oiso = atom_mass_table2['Oiso'] - atom_mass_table2['O']
         Siso = atom_mass_table2['Siso'] - atom_mass_table2['S']
@@ -4018,18 +4020,14 @@
     AI_denominator = final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - final_result['N']
     AI_numerator = 1 + final_result['C'] - 0.5 * final_result['O'] - final_result['S'] - 0.5 * (final_result['H'] - x)
     AI = AI_numerator / (AI_denominator.sort_values() + 1 * 1e-6)
     final_result['AI'] = AI
     return final_result
 
 
-
-
-
-
 """
 ========================================================================================================
 5. other functions
 ========================================================================================================
 """
 
 
@@ -4338,16 +4336,16 @@
     :param mode: external or internal
     :return: result dataframe
     How to do it?
     1. concat all "final area" files by using concat_alignment, include samples and standard curve;
     2. save this data as "all_area_df"
     3. modify the "quan_info.xlsx" file, to include the file names and sample types
     4. use function calibration.
-    
-    
+
+
     """
     # 测试使用，2如果可以用就删除
     print('-----------------------')
     print('Reading files...')
     print('-----------------------')
     files_excel = glob(os.path.join(path, '*.xlsx'))  # 拿到所有excel文件
     area_file = [file for file in files_excel if 'all_area_df' in file][0]  # 拿到所有final_area
@@ -4424,16 +4422,16 @@
                 std_conc = std_df[std_df['file_name'] == std_df.file_name.values[j]][std_cmp_name].values[0]
                 istd_conc = std_df.ISTD_fold.iloc[j] * istd_conc_raw
                 RF = (area_sample / area_istd) * (istd_conc / std_conc)
                 raw_data.append([area_sample, area_istd, std_conc, istd_conc])
                 RFs.append(round(RF, 2))
             RF_mean = round(np.mean(RFs), 2)
             RF_error = round(np.std(RFs) / np.mean(RFs) * 100, 1)
-            cmp_info.loc[i, ['RF_mean', 'RF_std','RFs_raw']] = RF_mean, RF_error, str(RFs)
-                
+            cmp_info.loc[i, ['RF_mean', 'RF_std', 'RFs_raw']] = RF_mean, RF_error, str(RFs)
+
         # 获得sample文件中sample的area和istd的area
         sample_df = file_info[file_info['sample_type'] == 'Sample']  # 样品信息
         sample_area_df = area_df.loc[
             std_indice, sample_df['file_name'].apply(lambda x: x.split('_unique_cmp')[0])]  # 样品峰面积
         istd_area_df = area_df.loc[
             istd_indice, sample_df['file_name'].apply(lambda x: x.split('_unique_cmp')[0])]  # 样品峰面积
 
@@ -4473,15 +4471,15 @@
 
     # 将unique_cmp没有筛查出来的数据移除
     unique_files = [file for file in files_excel if 'unique_cmp' in file]
     unique_file_names = [os.path.basename(i).split('_unique_cmps')[0] for i in unique_files]
     path_s1 = pd.Series(data=unique_files, index=unique_file_names)
     final_result.index = final_result['new_index']
     std_index = final_result['new_index'].values
-    for name in tqdm(final_result.columns, desc = 'Remove nondetectable cmps'):
+    for name in tqdm(final_result.columns, desc='Remove nondetectable cmps'):
         if name in path_s1.index.values:
             df_unique = pd.read_excel(path_s1.loc[name])
             unique_info = pd.DataFrame(np.array([df_unique.rt.values, df_unique.mz.values]).T, columns=['rt', 'mz'])
             set_0_index = []
             for index in std_index:
                 rt, mz = index.split('_')
                 rt = eval(rt)
@@ -4494,36 +4492,18 @@
                     set_0_index.append(index)
                 else:
                     pass
             final_result.loc[set_0_index, name] = np.nan
     return final_result
 
 
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
 def peak_checking_plot(df1, mz, rt1, path=None):
     """
     Evaluating/visualizing the extracted mz
     :param path: whether export to path
-    :param Type: profile or centroid
     :param df1: LC-MS dataframe, generated by the function gen_df()
     :param mz: Targeted mass for extraction
     :param rt1: expected rt for peaks
     :return:
     """
     plt.rcParams['font.sans-serif'] = 'Times New Roman'  # 设置全局字体
     fig = plt.figure(figsize=(12, 4))
@@ -4598,15 +4578,14 @@
                     final_result2.loc[i, k] = eval(v)
     return final_result2
 
 
 def update_category(result, category_updates, good_category):
     """
     This function can transform old categories to new categories.
-    :param level1: final summarized result with level1 or level2
     :param category_updates: category updates information
     :param good_category: This category are doubtless，most of them are standards in our lab
     :return: new summarized results with new category
     """
     updates = category_updates[~category_updates['new category'].isna()].reset_index(drop=True)
     # 先做替换
     for i in range(len(result)):
@@ -4636,8 +4615,8 @@
             result.loc[index, 'new_category'] = str(new_category)
             result.loc[index, 'usage'] = str(usage)
     return result
 
 
 if __name__ == '__main__':
     pass  # %config InlineBackendlineBackend.figure_format ='retina'
-#  plt.rcParams['font.sans-serif'] = 'Times New Roman'  # 设置全局字体
+#  plt.rcParams['font.sans-serif'] = 'Times New Roman'  # 设置全局字体
```

## Comparing `pyhrms-0.5.9/pyhrms/__init__.py` & `pyhrms-0.6.0/pyhrms/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from . import pyhrms
 
-__version__ = "0.5.9"
+__version__ = "0.6.0"
 
 __all__ = [
     "sep_scans",
     "gen_df",
     "peak_picking",
     "split_peak_picking",
     "remove_unnamed_columns",
```

## Comparing `pyhrms-0.5.9/pyhrms.egg-info/PKG-INFO` & `pyhrms-0.6.0/pyhrms.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyhrms
-Version: 0.5.9
+Version: 0.6.0
 Summary: A powerful GC/LC-HRMS data analysis tool
 Home-page: https://github.com/WangRui5/PyHRMS.git
 Author: Wang Rui
 Author-email: wtrt7009@gmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
@@ -22,15 +22,15 @@
 
 Contributer: Rui Wang
 ======================
 First release date: Nov.15.2021
 
 Update
 ======
-June.30.2023: pyhrms 0.5.9 new features:
+July.3.2023: pyhrms 0.6.0 new features:
 
     * Fixed bugs for processing centroid data.
 
 
 
 
 pyhrms can be installed and import as following:
```

