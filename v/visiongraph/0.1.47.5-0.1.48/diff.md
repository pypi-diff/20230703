# Comparing `tmp/visiongraph-0.1.47.5-py3-none-any.whl.zip` & `tmp/visiongraph-0.1.48-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,267 +1,268 @@
-Zip file size: 244987 bytes, number of entries: 265
--rw-r--r--  2.0 unx     1758 b- defN 23-Jun-29 16:48 visiongraph/AsyncGraphNode.py
--rw-r--r--  2.0 unx     2806 b- defN 23-Jun-29 16:48 visiongraph/BaseGraph.py
--rw-r--r--  2.0 unx      678 b- defN 23-Jun-29 16:48 visiongraph/GraphNode.py
--rw-r--r--  2.0 unx      276 b- defN 23-Jun-29 16:48 visiongraph/Processable.py
--rw-r--r--  2.0 unx     2094 b- defN 23-Jun-29 16:48 visiongraph/VisionGraph.py
--rw-r--r--  2.0 unx     1898 b- defN 23-Jun-29 16:48 visiongraph/VisionGraphBuilder.py
--rw-r--r--  2.0 unx    23273 b- defN 23-Jun-29 16:49 visiongraph/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/cache/__init__.py
--rw-r--r--  2.0 unx      368 b- defN 23-Jun-29 16:48 visiongraph/data/Asset.py
--rw-r--r--  2.0 unx      376 b- defN 23-Jun-29 16:48 visiongraph/data/LocalAsset.py
--rw-r--r--  2.0 unx     1099 b- defN 23-Jun-29 16:48 visiongraph/data/RepositoryAsset.py
--rw-r--r--  2.0 unx      324 b- defN 23-Jun-29 16:48 visiongraph/data/__init__.py
--rw-r--r--  2.0 unx     2568 b- defN 23-Jun-29 16:48 visiongraph/data/labels/COCO.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/data/labels/__init__.py
--rw-r--r--  2.0 unx      172 b- defN 23-Jun-29 16:48 visiongraph/dsp/BaseFilterNumpy.py
--rw-r--r--  2.0 unx     2421 b- defN 23-Jun-29 16:48 visiongraph/dsp/LandmarkSmoothFilter.py
--rw-r--r--  2.0 unx     1487 b- defN 23-Jun-29 16:48 visiongraph/dsp/OneEuroFilter.py
--rw-r--r--  2.0 unx     1651 b- defN 23-Jun-29 16:48 visiongraph/dsp/OneEuroFilterNumba.py
--rw-r--r--  2.0 unx     2642 b- defN 23-Jun-29 16:48 visiongraph/dsp/OneEuroFilterNumpy.py
--rw-r--r--  2.0 unx     1037 b- defN 23-Jun-29 16:48 visiongraph/dsp/VectorNumpySmoothFilter.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/dsp/__init__.py
--rw-r--r--  2.0 unx      619 b- defN 23-Jun-29 16:48 visiongraph/estimator/BaseClassifier.py
--rw-r--r--  2.0 unx      397 b- defN 23-Jun-29 16:48 visiongraph/estimator/BaseEstimator.py
--rw-r--r--  2.0 unx     4921 b- defN 23-Jun-29 16:48 visiongraph/estimator/BaseVisionEngine.py
--rw-r--r--  2.0 unx     1100 b- defN 23-Jun-29 16:48 visiongraph/estimator/ChainEstimator.py
--rw-r--r--  2.0 unx      403 b- defN 23-Jun-29 16:48 visiongraph/estimator/ScoreThresholdEstimator.py
--rw-r--r--  2.0 unx      868 b- defN 23-Jun-29 16:48 visiongraph/estimator/VisionClassifier.py
--rw-r--r--  2.0 unx      410 b- defN 23-Jun-29 16:48 visiongraph/estimator/VisionEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/__init__.py
--rw-r--r--  2.0 unx     2448 b- defN 23-Jun-29 16:48 visiongraph/estimator/calculator/UndistortionCalculator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/calculator/__init__.py
--rw-r--r--  2.0 unx     1859 b- defN 23-Jun-29 16:48 visiongraph/estimator/engine/InferenceEngineFactory.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/engine/__init__.py
--rw-r--r--  2.0 unx      529 b- defN 23-Jun-29 16:48 visiongraph/estimator/inpaint/BaseInpainter.py
--rw-r--r--  2.0 unx     2161 b- defN 23-Jun-29 16:48 visiongraph/estimator/inpaint/GMCNNInpainter.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/inpaint/__init__.py
--rw-r--r--  2.0 unx     2660 b- defN 23-Jun-29 16:48 visiongraph/estimator/onnx/ONNXVisionEngine.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/onnx/__init__.py
--rw-r--r--  2.0 unx     2516 b- defN 23-Jun-29 16:48 visiongraph/estimator/openvino/OpenVinoEngine.py
--rw-r--r--  2.0 unx     2516 b- defN 23-Jun-29 16:48 visiongraph/estimator/openvino/OpenVinoObjectDetector.py
--rw-r--r--  2.0 unx     2967 b- defN 23-Jun-29 16:48 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py
--rw-r--r--  2.0 unx      726 b- defN 23-Jun-29 16:48 visiongraph/estimator/openvino/SyncInferencePipeline.py
--rw-r--r--  2.0 unx     1957 b- defN 23-Jun-29 16:48 visiongraph/estimator/openvino/VisionInferenceEngine.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/openvino/__init__.py
--rw-r--r--  2.0 unx     1750 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/CenterNetDetector.py
--rw-r--r--  2.0 unx     3270 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/CrowdHumanDetector.py
--rw-r--r--  2.0 unx     1781 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/DETRDetector.py
--rw-r--r--  2.0 unx      544 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py
--rw-r--r--  2.0 unx      523 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/LandmarkEstimator.py
--rw-r--r--  2.0 unx      524 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/ObjectDetector.py
--rw-r--r--  2.0 unx     1279 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/RoiEstimator.py
--rw-r--r--  2.0 unx     3147 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/SSDDetector.py
--rw-r--r--  2.0 unx     2082 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/SlidingWindowEstimator.py
--rw-r--r--  2.0 unx     1436 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/SpatialCascadeEstimator.py
--rw-r--r--  2.0 unx     3173 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/YOLODetector.py
--rw-r--r--  2.0 unx     2904 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/YOLOXE2EDetector.py
--rw-r--r--  2.0 unx     3594 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/YOLOv5Detector.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/__init__.py
--rw-r--r--  2.0 unx     3441 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py
--rw-r--r--  2.0 unx      850 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py
--rw-r--r--  2.0 unx     4276 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py
--rw-r--r--  2.0 unx     3145 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/camera/__init__.py
--rw-r--r--  2.0 unx     1127 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/AdasFaceDetector.py
--rw-r--r--  2.0 unx      512 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/FaceDetector.py
--rw-r--r--  2.0 unx     3805 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/OpenVinoFaceDetector.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/__init__.py
--rw-r--r--  2.0 unx     1781 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/emotion/AffectNetEmotionClassifier.py
--rw-r--r--  2.0 unx     2038 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/emotion/FERPlusEmotionClassifier.py
--rw-r--r--  2.0 unx      450 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/emotion/FaceEmotionEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/emotion/__init__.py
--rw-r--r--  2.0 unx      609 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/landmark/FaceLandmarkEstimator.py
--rw-r--r--  2.0 unx     2646 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/landmark/IrisDistanceCalculator.py
--rw-r--r--  2.0 unx     2204 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceDetector.py
--rw-r--r--  2.0 unx     2324 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceMeshEstimator.py
--rw-r--r--  2.0 unx     1832 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/landmark/RegressionLandmarkEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/landmark/__init__.py
--rw-r--r--  2.0 unx     1301 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/pose/AdasHeadPoseEstimator.py
--rw-r--r--  2.0 unx      315 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/pose/HeadPoseEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/pose/__init__.py
--rw-r--r--  2.0 unx     4265 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/recognition/FaceRecognitionEstimator.py
--rw-r--r--  2.0 unx     2842 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/recognition/FaceReidentificationEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/face/recognition/__init__.py
--rw-r--r--  2.0 unx      517 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/hand/HandDetector.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/hand/__init__.py
--rw-r--r--  2.0 unx      608 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py
--rw-r--r--  2.0 unx     2778 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py
--rw-r--r--  2.0 unx     2330 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/hand/landmark/__init__.py
--rw-r--r--  2.0 unx     2086 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/AEPoseEstimator.py
--rw-r--r--  2.0 unx     5139 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py
--rw-r--r--  2.0 unx     7134 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py
--rw-r--r--  2.0 unx     3890 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py
--rw-r--r--  2.0 unx     2855 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/LitePoseEstimator.py
--rw-r--r--  2.0 unx     3100 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py
--rw-r--r--  2.0 unx     6641 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py
--rw-r--r--  2.0 unx     9658 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py
--rw-r--r--  2.0 unx     4325 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py
--rw-r--r--  2.0 unx     1660 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/OpenPoseEstimator.py
--rw-r--r--  2.0 unx      518 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/PoseEstimator.py
--rw-r--r--  2.0 unx     2347 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py
--rw-r--r--  2.0 unx     5416 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/pose/__init__.py
--rw-r--r--  2.0 unx     7516 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py
--rw-r--r--  2.0 unx     1989 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py
--rw-r--r--  2.0 unx     3287 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/segmentation/YolactEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/spatial/segmentation/__init__.py
--rw-r--r--  2.0 unx     1714 b- defN 23-Jun-29 16:48 visiongraph/estimator/translation/DeblurGANv2.py
--rw-r--r--  2.0 unx      314 b- defN 23-Jun-29 16:48 visiongraph/estimator/translation/DepthEstimator.py
--rw-r--r--  2.0 unx     2898 b- defN 23-Jun-29 16:48 visiongraph/estimator/translation/MidasDepthEstimator.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/estimator/translation/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/external/__init__.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/external/intel/__init__.py
--rw-r--r--  2.0 unx     4337 b- defN 23-Jun-29 16:48 visiongraph/external/intel/performance_metrics.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/external/intel/adapters/__init__.py
--rw-r--r--  2.0 unx     5682 b- defN 23-Jun-29 16:48 visiongraph/external/intel/adapters/inference_adapter.py
--rw-r--r--  2.0 unx    15557 b- defN 23-Jun-29 16:48 visiongraph/external/intel/adapters/openvino_adapter.py
--rw-r--r--  2.0 unx     7413 b- defN 23-Jun-29 16:48 visiongraph/external/intel/adapters/ovms_adapter.py
--rw-r--r--  2.0 unx    10920 b- defN 23-Jun-29 16:48 visiongraph/external/intel/adapters/utils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/__init__.py
--rw-r--r--  2.0 unx     7582 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/centernet.py
--rw-r--r--  2.0 unx     6784 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/detection_model.py
--rw-r--r--  2.0 unx     3215 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/detr.py
--rw-r--r--  2.0 unx    16363 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/hpe_associative_embedding.py
--rw-r--r--  2.0 unx     8434 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/image_model.py
--rw-r--r--  2.0 unx    19269 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/model.py
--rw-r--r--  2.0 unx    19768 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/open_pose.py
--rw-r--r--  2.0 unx     5995 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/ssd.py
--rw-r--r--  2.0 unx     7510 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/types.py
--rw-r--r--  2.0 unx     7600 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/utils.py
--rw-r--r--  2.0 unx    24161 b- defN 23-Jun-29 16:48 visiongraph/external/intel/models/yolo.py
--rw-r--r--  2.0 unx      154 b- defN 23-Jun-29 16:48 visiongraph/external/intel/pipelines/__init__.py
--rw-r--r--  2.0 unx     5407 b- defN 23-Jun-29 16:48 visiongraph/external/intel/pipelines/async_pipeline.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/external/midas/__init__.py
--rw-r--r--  2.0 unx     7868 b- defN 23-Jun-29 16:48 visiongraph/external/midas/transforms.py
--rw-r--r--  2.0 unx      193 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/__init__.py
--rw-r--r--  2.0 unx     1816 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/core.py
--rw-r--r--  2.0 unx      333 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/detector.py
--rw-r--r--  2.0 unx     1147 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/metrics.py
--rw-r--r--  2.0 unx     5402 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/model.py
--rw-r--r--  2.0 unx     3576 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/testing.py
--rw-r--r--  2.0 unx     2647 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/testing_viz.py
--rw-r--r--  2.0 unx    16512 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/tracker.py
--rw-r--r--  2.0 unx      652 b- defN 23-Jun-29 16:48 visiongraph/external/motpy/utils.py
--rw-r--r--  2.0 unx     5312 b- defN 23-Jun-29 16:48 visiongraph/external/motrackers/Track.py
--rw-r--r--  2.0 unx     7486 b- defN 23-Jun-29 16:48 visiongraph/external/motrackers/Tracker.py
--rw-r--r--  2.0 unx      184 b- defN 23-Jun-29 16:48 visiongraph/external/motrackers/__init__.py
--rw-r--r--  2.0 unx       83 b- defN 23-Jun-29 16:48 visiongraph/external/motrackers/utils/__init__.py
--rw-r--r--  2.0 unx     8978 b- defN 23-Jun-29 16:48 visiongraph/external/motrackers/utils/misc.py
--rw-r--r--  2.0 unx    17182 b- defN 23-Jun-29 16:48 visiongraph/input/AzureKinectInput.py
--rw-r--r--  2.0 unx     4108 b- defN 23-Jun-29 16:48 visiongraph/input/BaseDepthCamera.py
--rw-r--r--  2.0 unx     1309 b- defN 23-Jun-29 16:48 visiongraph/input/BaseDepthInput.py
--rw-r--r--  2.0 unx     3760 b- defN 23-Jun-29 16:48 visiongraph/input/BaseInput.py
--rw-r--r--  2.0 unx     1721 b- defN 23-Jun-29 16:48 visiongraph/input/CamGearInput.py
--rw-r--r--  2.0 unx     1493 b- defN 23-Jun-29 16:48 visiongraph/input/ImageInput.py
--rw-r--r--  2.0 unx    19563 b- defN 23-Jun-29 16:48 visiongraph/input/RealSenseInput.py
--rw-r--r--  2.0 unx     5032 b- defN 23-Jun-29 16:48 visiongraph/input/VideoCaptureInput.py
--rw-r--r--  2.0 unx     1305 b- defN 23-Jun-29 16:48 visiongraph/input/__init__.py
--rw-r--r--  2.0 unx     1887 b- defN 23-Jun-29 16:48 visiongraph/model/CameraIntrinsics.py
--rw-r--r--  2.0 unx      101 b- defN 23-Jun-29 16:48 visiongraph/model/CameraStreamType.py
--rw-r--r--  2.0 unx      549 b- defN 23-Jun-29 16:48 visiongraph/model/DepthBuffer.py
--rw-r--r--  2.0 unx      753 b- defN 23-Jun-29 16:48 visiongraph/model/VisionEngineOutput.py
--rw-r--r--  2.0 unx      135 b- defN 23-Jun-29 16:48 visiongraph/model/_ImportStub.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/model/__init__.py
--rw-r--r--  2.0 unx     4229 b- defN 23-Jun-29 16:48 visiongraph/model/geometry/BoundingBox2D.py
--rw-r--r--  2.0 unx      815 b- defN 23-Jun-29 16:48 visiongraph/model/geometry/Size2D.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/model/geometry/__init__.py
--rw-r--r--  2.0 unx      585 b- defN 23-Jun-29 16:48 visiongraph/model/parameter/ArgumentConfigurable.py
--rw-r--r--  2.0 unx      186 b- defN 23-Jun-29 16:48 visiongraph/model/parameter/NamedParameter.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/model/parameter/__init__.py
--rw-r--r--  2.0 unx      399 b- defN 23-Jun-29 16:48 visiongraph/model/tracker/Trackable.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/model/tracker/__init__.py
--rw-r--r--  2.0 unx       78 b- defN 23-Jun-29 16:48 visiongraph/model/types/InputShapeOrder.py
--rw-r--r--  2.0 unx      440 b- defN 23-Jun-29 16:48 visiongraph/model/types/ModelPrecision.py
--rw-r--r--  2.0 unx      199 b- defN 23-Jun-29 16:48 visiongraph/model/types/RealSenseColorScheme.py
--rw-r--r--  2.0 unx      204 b- defN 23-Jun-29 16:48 visiongraph/model/types/RealSenseFilter.py
--rw-r--r--  2.0 unx     1180 b- defN 23-Jun-29 16:48 visiongraph/model/types/VideoCaptureBackend.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/model/types/__init__.py
--rw-r--r--  2.0 unx      905 b- defN 23-Jun-29 16:48 visiongraph/node/ApplyNode.py
--rw-r--r--  2.0 unx      753 b- defN 23-Jun-29 16:48 visiongraph/node/BreakpointNode.py
--rw-r--r--  2.0 unx      815 b- defN 23-Jun-29 16:48 visiongraph/node/CustomNode.py
--rw-r--r--  2.0 unx      903 b- defN 23-Jun-29 16:48 visiongraph/node/ExtractNode.py
--rw-r--r--  2.0 unx      525 b- defN 23-Jun-29 16:48 visiongraph/node/PassThroughNode.py
--rw-r--r--  2.0 unx      825 b- defN 23-Jun-29 16:48 visiongraph/node/SequenceNode.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/node/__init__.py
--rw-r--r--  2.0 unx     1471 b- defN 23-Jun-29 16:48 visiongraph/output/ImagePreview.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/output/__init__.py
--rw-r--r--  2.0 unx      935 b- defN 23-Jun-29 16:48 visiongraph/output/fbs/FrameBufferSharingServer.py
--rw-r--r--  2.0 unx     1390 b- defN 23-Jun-29 16:48 visiongraph/output/fbs/SpoutServer.py
--rw-r--r--  2.0 unx     3287 b- defN 23-Jun-29 16:48 visiongraph/output/fbs/SyphonServer.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/output/fbs/__init__.py
--rw-r--r--  2.0 unx      980 b- defN 23-Jun-29 16:48 visiongraph/recorder/AsyncFrameSetRecorder.py
--rw-r--r--  2.0 unx     1108 b- defN 23-Jun-29 16:48 visiongraph/recorder/BaseFrameRecorder.py
--rw-r--r--  2.0 unx     1143 b- defN 23-Jun-29 16:48 visiongraph/recorder/CV2VideoRecorder.py
--rw-r--r--  2.0 unx     1016 b- defN 23-Jun-29 16:48 visiongraph/recorder/FrameSetRecorder.py
--rw-r--r--  2.0 unx      905 b- defN 23-Jun-29 16:48 visiongraph/recorder/MoviePyVideoRecorder.py
--rw-r--r--  2.0 unx     1377 b- defN 23-Jun-29 16:48 visiongraph/recorder/VidGearVideoRecorder.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/recorder/__init__.py
--rw-r--r--  2.0 unx      564 b- defN 23-Jun-29 16:48 visiongraph/result/ArUcoCameraPose.py
--rw-r--r--  2.0 unx     1521 b- defN 23-Jun-29 16:48 visiongraph/result/ArUcoMarkerDetection.py
--rw-r--r--  2.0 unx      167 b- defN 23-Jun-29 16:48 visiongraph/result/BaseResult.py
--rw-r--r--  2.0 unx      525 b- defN 23-Jun-29 16:48 visiongraph/result/CameraPoseResult.py
--rw-r--r--  2.0 unx      378 b- defN 23-Jun-29 16:48 visiongraph/result/ClassificationResult.py
--rw-r--r--  2.0 unx     1709 b- defN 23-Jun-29 16:48 visiongraph/result/DepthMap.py
--rw-r--r--  2.0 unx      487 b- defN 23-Jun-29 16:48 visiongraph/result/EmbeddingResult.py
--rw-r--r--  2.0 unx      474 b- defN 23-Jun-29 16:48 visiongraph/result/HeadPoseResult.py
--rw-r--r--  2.0 unx      333 b- defN 23-Jun-29 16:48 visiongraph/result/ImageResult.py
--rw-r--r--  2.0 unx     1165 b- defN 23-Jun-29 16:48 visiongraph/result/ResultAnnotator.py
--rw-r--r--  2.0 unx      483 b- defN 23-Jun-29 16:48 visiongraph/result/ResultDict.py
--rw-r--r--  2.0 unx      466 b- defN 23-Jun-29 16:48 visiongraph/result/ResultList.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/result/__init__.py
--rw-r--r--  2.0 unx      779 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/CrowdHumanResult.py
--rw-r--r--  2.0 unx     1354 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/InstanceSegmentationResult.py
--rw-r--r--  2.0 unx     4199 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/LandmarkDetectionResult.py
--rw-r--r--  2.0 unx     3475 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/ObjectDetectionResult.py
--rw-r--r--  2.0 unx      878 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/SpatialCascadeResult.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/__init__.py
--rw-r--r--  2.0 unx      924 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/BlazeFace.py
--rw-r--r--  2.0 unx     2064 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/BlazeFaceMesh.py
--rw-r--r--  2.0 unx      702 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/EmotionClassificationResult.py
--rw-r--r--  2.0 unx      409 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/FaceDetectionResult.py
--rw-r--r--  2.0 unx      881 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/FaceLandmarkResult.py
--rw-r--r--  2.0 unx     1390 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/IrisDistanceResult.py
--rw-r--r--  2.0 unx      868 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/RegressionFace.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/face/__init__.py
--rw-r--r--  2.0 unx     2648 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/hand/BlazeHand.py
--rw-r--r--  2.0 unx      409 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/hand/HandDetectionResult.py
--rw-r--r--  2.0 unx     2590 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/hand/HandLandmarkResult.py
--rw-r--r--  2.0 unx       88 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/hand/Handedness.py
--rw-r--r--  2.0 unx      106 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/hand/OpenPoseHand.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/hand/__init__.py
--rw-r--r--  2.0 unx     3435 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/BlazePose.py
--rw-r--r--  2.0 unx     1266 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/BlazePoseSegmentation.py
--rw-r--r--  2.0 unx     2826 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/COCOOpenPose.py
--rw-r--r--  2.0 unx     2680 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/COCOPose.py
--rw-r--r--  2.0 unx     2337 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/EfficientPose.py
--rw-r--r--  2.0 unx     2738 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/MobileHumanPose.py
--rw-r--r--  2.0 unx     2823 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/PoseLandmarkResult.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/result/spatial/pose/__init__.py
--rw-r--r--  2.0 unx      492 b- defN 23-Jun-29 16:48 visiongraph/tracker/BaseObjectDetectionTracker.py
--rw-r--r--  2.0 unx     1908 b- defN 23-Jun-29 16:48 visiongraph/tracker/CentroidTracker.py
--rw-r--r--  2.0 unx     4906 b- defN 23-Jun-29 16:48 visiongraph/tracker/FlateTracker.py
--rw-r--r--  2.0 unx     2705 b- defN 23-Jun-29 16:48 visiongraph/tracker/MotpyTracker.py
--rw-r--r--  2.0 unx     3304 b- defN 23-Jun-29 16:48 visiongraph/tracker/ObjectAssignmentSolver.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/tracker/__init__.py
--rw-r--r--  2.0 unx     3105 b- defN 23-Jun-29 16:48 visiongraph/util/ArgUtils.py
--rw-r--r--  2.0 unx     1009 b- defN 23-Jun-29 16:48 visiongraph/util/CodeUtils.py
--rw-r--r--  2.0 unx      222 b- defN 23-Jun-29 16:48 visiongraph/util/CollectionUtils.py
--rw-r--r--  2.0 unx      314 b- defN 23-Jun-29 16:48 visiongraph/util/CommonArgs.py
--rw-r--r--  2.0 unx     3653 b- defN 23-Jun-29 16:48 visiongraph/util/DrawingUtils.py
--rw-r--r--  2.0 unx     3588 b- defN 23-Jun-29 16:48 visiongraph/util/ImageUtils.py
--rw-r--r--  2.0 unx     2608 b- defN 23-Jun-29 16:48 visiongraph/util/LinalgUtils.py
--rw-r--r--  2.0 unx      539 b- defN 23-Jun-29 16:48 visiongraph/util/LoggingUtils.py
--rw-r--r--  2.0 unx     1752 b- defN 23-Jun-29 16:48 visiongraph/util/MathUtils.py
--rw-r--r--  2.0 unx     2084 b- defN 23-Jun-29 16:48 visiongraph/util/NetworkUtils.py
--rw-r--r--  2.0 unx      195 b- defN 23-Jun-29 16:48 visiongraph/util/OSUtils.py
--rw-r--r--  2.0 unx      704 b- defN 23-Jun-29 16:48 visiongraph/util/OpenVinoUtils.py
--rw-r--r--  2.0 unx     1240 b- defN 23-Jun-29 16:48 visiongraph/util/ResultUtils.py
--rw-r--r--  2.0 unx     2102 b- defN 23-Jun-29 16:48 visiongraph/util/TimeUtils.py
--rw-r--r--  2.0 unx     3225 b- defN 23-Jun-29 16:48 visiongraph/util/VectorUtils.py
--rw-r--r--  2.0 unx        0 b- defN 23-Jun-29 16:48 visiongraph/util/__init__.py
--rw-r--r--  2.0 unx    11432 b- defN 23-Jun-29 16:49 visiongraph-0.1.47.5.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Jun-29 16:49 visiongraph-0.1.47.5.dist-info/WHEEL
--rw-r--r--  2.0 unx       20 b- defN 23-Jun-29 16:49 visiongraph-0.1.47.5.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       12 b- defN 23-Jun-29 16:49 visiongraph-0.1.47.5.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    26541 b- defN 23-Jun-29 16:49 visiongraph-0.1.47.5.dist-info/RECORD
-265 files, 672259 bytes uncompressed, 201449 bytes compressed:  70.0%
+Zip file size: 240554 bytes, number of entries: 266
+-rw-r--r--  2.0 unx     1758 b- defN 23-Jul-03 19:58 visiongraph/AsyncGraphNode.py
+-rw-r--r--  2.0 unx     2806 b- defN 23-Jul-03 19:58 visiongraph/BaseGraph.py
+-rw-r--r--  2.0 unx      678 b- defN 23-Jul-03 19:58 visiongraph/GraphNode.py
+-rw-r--r--  2.0 unx      276 b- defN 23-Jul-03 19:58 visiongraph/Processable.py
+-rw-r--r--  2.0 unx     2094 b- defN 23-Jul-03 19:58 visiongraph/VisionGraph.py
+-rw-r--r--  2.0 unx     1898 b- defN 23-Jul-03 19:58 visiongraph/VisionGraphBuilder.py
+-rw-r--r--  2.0 unx    23273 b- defN 23-Jul-03 19:58 visiongraph/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/cache/__init__.py
+-rw-r--r--  2.0 unx      368 b- defN 23-Jul-03 19:58 visiongraph/data/Asset.py
+-rw-r--r--  2.0 unx      376 b- defN 23-Jul-03 19:58 visiongraph/data/LocalAsset.py
+-rw-r--r--  2.0 unx     1099 b- defN 23-Jul-03 19:58 visiongraph/data/RepositoryAsset.py
+-rw-r--r--  2.0 unx      324 b- defN 23-Jul-03 19:58 visiongraph/data/__init__.py
+-rw-r--r--  2.0 unx     2568 b- defN 23-Jul-03 19:58 visiongraph/data/labels/COCO.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/data/labels/__init__.py
+-rw-r--r--  2.0 unx      172 b- defN 23-Jul-03 19:58 visiongraph/dsp/BaseFilterNumpy.py
+-rw-r--r--  2.0 unx     2421 b- defN 23-Jul-03 19:58 visiongraph/dsp/LandmarkSmoothFilter.py
+-rw-r--r--  2.0 unx     1487 b- defN 23-Jul-03 19:58 visiongraph/dsp/OneEuroFilter.py
+-rw-r--r--  2.0 unx     1651 b- defN 23-Jul-03 19:58 visiongraph/dsp/OneEuroFilterNumba.py
+-rw-r--r--  2.0 unx     2642 b- defN 23-Jul-03 19:58 visiongraph/dsp/OneEuroFilterNumpy.py
+-rw-r--r--  2.0 unx     1037 b- defN 23-Jul-03 19:58 visiongraph/dsp/VectorNumpySmoothFilter.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/dsp/__init__.py
+-rw-r--r--  2.0 unx      619 b- defN 23-Jul-03 19:58 visiongraph/estimator/BaseClassifier.py
+-rw-r--r--  2.0 unx      397 b- defN 23-Jul-03 19:58 visiongraph/estimator/BaseEstimator.py
+-rw-r--r--  2.0 unx     4921 b- defN 23-Jul-03 19:58 visiongraph/estimator/BaseVisionEngine.py
+-rw-r--r--  2.0 unx     1100 b- defN 23-Jul-03 19:58 visiongraph/estimator/ChainEstimator.py
+-rw-r--r--  2.0 unx      403 b- defN 23-Jul-03 19:58 visiongraph/estimator/ScoreThresholdEstimator.py
+-rw-r--r--  2.0 unx      868 b- defN 23-Jul-03 19:58 visiongraph/estimator/VisionClassifier.py
+-rw-r--r--  2.0 unx      410 b- defN 23-Jul-03 19:58 visiongraph/estimator/VisionEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/__init__.py
+-rw-r--r--  2.0 unx     2448 b- defN 23-Jul-03 19:58 visiongraph/estimator/calculator/UndistortionCalculator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/calculator/__init__.py
+-rw-r--r--  2.0 unx     1859 b- defN 23-Jul-03 19:58 visiongraph/estimator/engine/InferenceEngineFactory.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/engine/__init__.py
+-rw-r--r--  2.0 unx      529 b- defN 23-Jul-03 19:58 visiongraph/estimator/inpaint/BaseInpainter.py
+-rw-r--r--  2.0 unx     2161 b- defN 23-Jul-03 19:58 visiongraph/estimator/inpaint/GMCNNInpainter.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/inpaint/__init__.py
+-rw-r--r--  2.0 unx     2660 b- defN 23-Jul-03 19:58 visiongraph/estimator/onnx/ONNXVisionEngine.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/onnx/__init__.py
+-rw-r--r--  2.0 unx     2516 b- defN 23-Jul-03 19:58 visiongraph/estimator/openvino/OpenVinoEngine.py
+-rw-r--r--  2.0 unx     2516 b- defN 23-Jul-03 19:58 visiongraph/estimator/openvino/OpenVinoObjectDetector.py
+-rw-r--r--  2.0 unx     2967 b- defN 23-Jul-03 19:58 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py
+-rw-r--r--  2.0 unx      726 b- defN 23-Jul-03 19:58 visiongraph/estimator/openvino/SyncInferencePipeline.py
+-rw-r--r--  2.0 unx     2438 b- defN 23-Jul-03 19:58 visiongraph/estimator/openvino/VisionInferenceEngine.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/openvino/__init__.py
+-rw-r--r--  2.0 unx     1877 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/CenterNetDetector.py
+-rw-r--r--  2.0 unx     3270 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/CrowdHumanDetector.py
+-rw-r--r--  2.0 unx     1908 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/DETRDetector.py
+-rw-r--r--  2.0 unx      544 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py
+-rw-r--r--  2.0 unx      523 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/LandmarkEstimator.py
+-rw-r--r--  2.0 unx      524 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/ObjectDetector.py
+-rw-r--r--  2.0 unx     1279 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/RoiEstimator.py
+-rw-r--r--  2.0 unx     3251 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/SSDDetector.py
+-rw-r--r--  2.0 unx     2082 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/SlidingWindowEstimator.py
+-rw-r--r--  2.0 unx     1436 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/SpatialCascadeEstimator.py
+-rw-r--r--  2.0 unx     3300 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/YOLODetector.py
+-rw-r--r--  2.0 unx     2904 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/YOLOXE2EDetector.py
+-rw-r--r--  2.0 unx     3594 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/YOLOv5Detector.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/__init__.py
+-rw-r--r--  2.0 unx     3441 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py
+-rw-r--r--  2.0 unx      850 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py
+-rw-r--r--  2.0 unx     4276 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py
+-rw-r--r--  2.0 unx     3145 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/camera/__init__.py
+-rw-r--r--  2.0 unx     1127 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/AdasFaceDetector.py
+-rw-r--r--  2.0 unx      512 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/FaceDetector.py
+-rw-r--r--  2.0 unx     3805 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/OpenVinoFaceDetector.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/__init__.py
+-rw-r--r--  2.0 unx     1781 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/emotion/AffectNetEmotionClassifier.py
+-rw-r--r--  2.0 unx     2038 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/emotion/FERPlusEmotionClassifier.py
+-rw-r--r--  2.0 unx      450 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/emotion/FaceEmotionEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/emotion/__init__.py
+-rw-r--r--  2.0 unx      609 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/landmark/FaceLandmarkEstimator.py
+-rw-r--r--  2.0 unx     2646 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/landmark/IrisDistanceCalculator.py
+-rw-r--r--  2.0 unx     2204 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceDetector.py
+-rw-r--r--  2.0 unx     2324 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/landmark/MediaPipeFaceMeshEstimator.py
+-rw-r--r--  2.0 unx     1832 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/landmark/RegressionLandmarkEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/landmark/__init__.py
+-rw-r--r--  2.0 unx     1301 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/pose/AdasHeadPoseEstimator.py
+-rw-r--r--  2.0 unx      315 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/pose/HeadPoseEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/pose/__init__.py
+-rw-r--r--  2.0 unx     4265 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/recognition/FaceRecognitionEstimator.py
+-rw-r--r--  2.0 unx     2842 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/recognition/FaceReidentificationEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/face/recognition/__init__.py
+-rw-r--r--  2.0 unx      517 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/hand/HandDetector.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/hand/__init__.py
+-rw-r--r--  2.0 unx      608 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py
+-rw-r--r--  2.0 unx     2778 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py
+-rw-r--r--  2.0 unx     2330 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/hand/landmark/__init__.py
+-rw-r--r--  2.0 unx     2219 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/AEPoseEstimator.py
+-rw-r--r--  2.0 unx     5139 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py
+-rw-r--r--  2.0 unx     7134 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py
+-rw-r--r--  2.0 unx     3890 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py
+-rw-r--r--  2.0 unx     2855 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/LitePoseEstimator.py
+-rw-r--r--  2.0 unx     3100 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py
+-rw-r--r--  2.0 unx     6641 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py
+-rw-r--r--  2.0 unx     9658 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py
+-rw-r--r--  2.0 unx     4325 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py
+-rw-r--r--  2.0 unx     1876 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/OpenPoseEstimator.py
+-rw-r--r--  2.0 unx      518 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/PoseEstimator.py
+-rw-r--r--  2.0 unx     2347 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py
+-rw-r--r--  2.0 unx     5416 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/pose/__init__.py
+-rw-r--r--  2.0 unx     7516 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py
+-rw-r--r--  2.0 unx     1989 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py
+-rw-r--r--  2.0 unx     3287 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/segmentation/YolactEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/spatial/segmentation/__init__.py
+-rw-r--r--  2.0 unx     1714 b- defN 23-Jul-03 19:58 visiongraph/estimator/translation/DeblurGANv2.py
+-rw-r--r--  2.0 unx      314 b- defN 23-Jul-03 19:58 visiongraph/estimator/translation/DepthEstimator.py
+-rw-r--r--  2.0 unx     2898 b- defN 23-Jul-03 19:58 visiongraph/estimator/translation/MidasDepthEstimator.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/estimator/translation/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/external/__init__.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/external/intel/__init__.py
+-rw-r--r--  2.0 unx     4342 b- defN 23-Jul-03 19:58 visiongraph/external/intel/performance_metrics.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/external/intel/adapters/__init__.py
+-rw-r--r--  2.0 unx     5682 b- defN 23-Jul-03 19:58 visiongraph/external/intel/adapters/inference_adapter.py
+-rw-r--r--  2.0 unx     5151 b- defN 23-Jul-03 19:58 visiongraph/external/intel/adapters/model_adapter.py
+-rw-r--r--  2.0 unx     8056 b- defN 23-Jul-03 19:58 visiongraph/external/intel/adapters/openvino_adapter.py
+-rw-r--r--  2.0 unx     6585 b- defN 23-Jul-03 19:58 visiongraph/external/intel/adapters/ovms_adapter.py
+-rw-r--r--  2.0 unx     2794 b- defN 23-Jul-03 19:58 visiongraph/external/intel/adapters/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/__init__.py
+-rw-r--r--  2.0 unx     7442 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/centernet.py
+-rw-r--r--  2.0 unx     5700 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/detection_model.py
+-rw-r--r--  2.0 unx     2992 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/detr.py
+-rw-r--r--  2.0 unx    14421 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/hpe_associative_embedding.py
+-rw-r--r--  2.0 unx     7247 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/image_model.py
+-rw-r--r--  2.0 unx    12558 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/model.py
+-rw-r--r--  2.0 unx    17789 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/open_pose.py
+-rw-r--r--  2.0 unx     5874 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/ssd.py
+-rw-r--r--  2.0 unx     5805 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/types.py
+-rw-r--r--  2.0 unx     7260 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/utils.py
+-rw-r--r--  2.0 unx    21407 b- defN 23-Jul-03 19:58 visiongraph/external/intel/models/yolo.py
+-rw-r--r--  2.0 unx      154 b- defN 23-Jul-03 19:58 visiongraph/external/intel/pipelines/__init__.py
+-rw-r--r--  2.0 unx     5407 b- defN 23-Jul-03 19:58 visiongraph/external/intel/pipelines/async_pipeline.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/external/midas/__init__.py
+-rw-r--r--  2.0 unx     7868 b- defN 23-Jul-03 19:58 visiongraph/external/midas/transforms.py
+-rw-r--r--  2.0 unx      193 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/__init__.py
+-rw-r--r--  2.0 unx     1816 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/core.py
+-rw-r--r--  2.0 unx      333 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/detector.py
+-rw-r--r--  2.0 unx     1147 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/metrics.py
+-rw-r--r--  2.0 unx     5402 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/model.py
+-rw-r--r--  2.0 unx     3576 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/testing.py
+-rw-r--r--  2.0 unx     2647 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/testing_viz.py
+-rw-r--r--  2.0 unx    16512 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/tracker.py
+-rw-r--r--  2.0 unx      652 b- defN 23-Jul-03 19:58 visiongraph/external/motpy/utils.py
+-rw-r--r--  2.0 unx     5312 b- defN 23-Jul-03 19:58 visiongraph/external/motrackers/Track.py
+-rw-r--r--  2.0 unx     7486 b- defN 23-Jul-03 19:58 visiongraph/external/motrackers/Tracker.py
+-rw-r--r--  2.0 unx      184 b- defN 23-Jul-03 19:58 visiongraph/external/motrackers/__init__.py
+-rw-r--r--  2.0 unx       83 b- defN 23-Jul-03 19:58 visiongraph/external/motrackers/utils/__init__.py
+-rw-r--r--  2.0 unx     8978 b- defN 23-Jul-03 19:58 visiongraph/external/motrackers/utils/misc.py
+-rw-r--r--  2.0 unx    17182 b- defN 23-Jul-03 19:58 visiongraph/input/AzureKinectInput.py
+-rw-r--r--  2.0 unx     4108 b- defN 23-Jul-03 19:58 visiongraph/input/BaseDepthCamera.py
+-rw-r--r--  2.0 unx     1309 b- defN 23-Jul-03 19:58 visiongraph/input/BaseDepthInput.py
+-rw-r--r--  2.0 unx     3760 b- defN 23-Jul-03 19:58 visiongraph/input/BaseInput.py
+-rw-r--r--  2.0 unx     1721 b- defN 23-Jul-03 19:58 visiongraph/input/CamGearInput.py
+-rw-r--r--  2.0 unx     1493 b- defN 23-Jul-03 19:58 visiongraph/input/ImageInput.py
+-rw-r--r--  2.0 unx    19563 b- defN 23-Jul-03 19:58 visiongraph/input/RealSenseInput.py
+-rw-r--r--  2.0 unx     5032 b- defN 23-Jul-03 19:58 visiongraph/input/VideoCaptureInput.py
+-rw-r--r--  2.0 unx     1305 b- defN 23-Jul-03 19:58 visiongraph/input/__init__.py
+-rw-r--r--  2.0 unx     1887 b- defN 23-Jul-03 19:58 visiongraph/model/CameraIntrinsics.py
+-rw-r--r--  2.0 unx      101 b- defN 23-Jul-03 19:58 visiongraph/model/CameraStreamType.py
+-rw-r--r--  2.0 unx      549 b- defN 23-Jul-03 19:58 visiongraph/model/DepthBuffer.py
+-rw-r--r--  2.0 unx      753 b- defN 23-Jul-03 19:58 visiongraph/model/VisionEngineOutput.py
+-rw-r--r--  2.0 unx      135 b- defN 23-Jul-03 19:58 visiongraph/model/_ImportStub.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/model/__init__.py
+-rw-r--r--  2.0 unx     4229 b- defN 23-Jul-03 19:58 visiongraph/model/geometry/BoundingBox2D.py
+-rw-r--r--  2.0 unx      815 b- defN 23-Jul-03 19:58 visiongraph/model/geometry/Size2D.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/model/geometry/__init__.py
+-rw-r--r--  2.0 unx      585 b- defN 23-Jul-03 19:58 visiongraph/model/parameter/ArgumentConfigurable.py
+-rw-r--r--  2.0 unx      186 b- defN 23-Jul-03 19:58 visiongraph/model/parameter/NamedParameter.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/model/parameter/__init__.py
+-rw-r--r--  2.0 unx      399 b- defN 23-Jul-03 19:58 visiongraph/model/tracker/Trackable.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/model/tracker/__init__.py
+-rw-r--r--  2.0 unx       78 b- defN 23-Jul-03 19:58 visiongraph/model/types/InputShapeOrder.py
+-rw-r--r--  2.0 unx      440 b- defN 23-Jul-03 19:58 visiongraph/model/types/ModelPrecision.py
+-rw-r--r--  2.0 unx      199 b- defN 23-Jul-03 19:58 visiongraph/model/types/RealSenseColorScheme.py
+-rw-r--r--  2.0 unx      204 b- defN 23-Jul-03 19:58 visiongraph/model/types/RealSenseFilter.py
+-rw-r--r--  2.0 unx     1180 b- defN 23-Jul-03 19:58 visiongraph/model/types/VideoCaptureBackend.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/model/types/__init__.py
+-rw-r--r--  2.0 unx      905 b- defN 23-Jul-03 19:58 visiongraph/node/ApplyNode.py
+-rw-r--r--  2.0 unx      753 b- defN 23-Jul-03 19:58 visiongraph/node/BreakpointNode.py
+-rw-r--r--  2.0 unx      815 b- defN 23-Jul-03 19:58 visiongraph/node/CustomNode.py
+-rw-r--r--  2.0 unx      903 b- defN 23-Jul-03 19:58 visiongraph/node/ExtractNode.py
+-rw-r--r--  2.0 unx      525 b- defN 23-Jul-03 19:58 visiongraph/node/PassThroughNode.py
+-rw-r--r--  2.0 unx      825 b- defN 23-Jul-03 19:58 visiongraph/node/SequenceNode.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/node/__init__.py
+-rw-r--r--  2.0 unx     1471 b- defN 23-Jul-03 19:58 visiongraph/output/ImagePreview.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/output/__init__.py
+-rw-r--r--  2.0 unx      935 b- defN 23-Jul-03 19:58 visiongraph/output/fbs/FrameBufferSharingServer.py
+-rw-r--r--  2.0 unx     1390 b- defN 23-Jul-03 19:58 visiongraph/output/fbs/SpoutServer.py
+-rw-r--r--  2.0 unx     3287 b- defN 23-Jul-03 19:58 visiongraph/output/fbs/SyphonServer.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/output/fbs/__init__.py
+-rw-r--r--  2.0 unx      980 b- defN 23-Jul-03 19:58 visiongraph/recorder/AsyncFrameSetRecorder.py
+-rw-r--r--  2.0 unx     1108 b- defN 23-Jul-03 19:58 visiongraph/recorder/BaseFrameRecorder.py
+-rw-r--r--  2.0 unx     1143 b- defN 23-Jul-03 19:58 visiongraph/recorder/CV2VideoRecorder.py
+-rw-r--r--  2.0 unx     1016 b- defN 23-Jul-03 19:58 visiongraph/recorder/FrameSetRecorder.py
+-rw-r--r--  2.0 unx      905 b- defN 23-Jul-03 19:58 visiongraph/recorder/MoviePyVideoRecorder.py
+-rw-r--r--  2.0 unx     1377 b- defN 23-Jul-03 19:58 visiongraph/recorder/VidGearVideoRecorder.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/recorder/__init__.py
+-rw-r--r--  2.0 unx      564 b- defN 23-Jul-03 19:58 visiongraph/result/ArUcoCameraPose.py
+-rw-r--r--  2.0 unx     1521 b- defN 23-Jul-03 19:58 visiongraph/result/ArUcoMarkerDetection.py
+-rw-r--r--  2.0 unx      167 b- defN 23-Jul-03 19:58 visiongraph/result/BaseResult.py
+-rw-r--r--  2.0 unx      525 b- defN 23-Jul-03 19:58 visiongraph/result/CameraPoseResult.py
+-rw-r--r--  2.0 unx      378 b- defN 23-Jul-03 19:58 visiongraph/result/ClassificationResult.py
+-rw-r--r--  2.0 unx     1709 b- defN 23-Jul-03 19:58 visiongraph/result/DepthMap.py
+-rw-r--r--  2.0 unx      487 b- defN 23-Jul-03 19:58 visiongraph/result/EmbeddingResult.py
+-rw-r--r--  2.0 unx      474 b- defN 23-Jul-03 19:58 visiongraph/result/HeadPoseResult.py
+-rw-r--r--  2.0 unx      333 b- defN 23-Jul-03 19:58 visiongraph/result/ImageResult.py
+-rw-r--r--  2.0 unx     1165 b- defN 23-Jul-03 19:58 visiongraph/result/ResultAnnotator.py
+-rw-r--r--  2.0 unx      483 b- defN 23-Jul-03 19:58 visiongraph/result/ResultDict.py
+-rw-r--r--  2.0 unx      466 b- defN 23-Jul-03 19:58 visiongraph/result/ResultList.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/result/__init__.py
+-rw-r--r--  2.0 unx      779 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/CrowdHumanResult.py
+-rw-r--r--  2.0 unx     1354 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/InstanceSegmentationResult.py
+-rw-r--r--  2.0 unx     4199 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/LandmarkDetectionResult.py
+-rw-r--r--  2.0 unx     3475 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/ObjectDetectionResult.py
+-rw-r--r--  2.0 unx      878 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/SpatialCascadeResult.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/__init__.py
+-rw-r--r--  2.0 unx      924 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/BlazeFace.py
+-rw-r--r--  2.0 unx     2064 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/BlazeFaceMesh.py
+-rw-r--r--  2.0 unx      702 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/EmotionClassificationResult.py
+-rw-r--r--  2.0 unx      409 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/FaceDetectionResult.py
+-rw-r--r--  2.0 unx      881 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/FaceLandmarkResult.py
+-rw-r--r--  2.0 unx     1390 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/IrisDistanceResult.py
+-rw-r--r--  2.0 unx      868 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/RegressionFace.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/face/__init__.py
+-rw-r--r--  2.0 unx     2648 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/hand/BlazeHand.py
+-rw-r--r--  2.0 unx      409 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/hand/HandDetectionResult.py
+-rw-r--r--  2.0 unx     2590 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/hand/HandLandmarkResult.py
+-rw-r--r--  2.0 unx       88 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/hand/Handedness.py
+-rw-r--r--  2.0 unx      106 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/hand/OpenPoseHand.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/hand/__init__.py
+-rw-r--r--  2.0 unx     3435 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/BlazePose.py
+-rw-r--r--  2.0 unx     1266 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/BlazePoseSegmentation.py
+-rw-r--r--  2.0 unx     2826 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/COCOOpenPose.py
+-rw-r--r--  2.0 unx     2680 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/COCOPose.py
+-rw-r--r--  2.0 unx     2337 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/EfficientPose.py
+-rw-r--r--  2.0 unx     2738 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/MobileHumanPose.py
+-rw-r--r--  2.0 unx     2823 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/PoseLandmarkResult.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/result/spatial/pose/__init__.py
+-rw-r--r--  2.0 unx      492 b- defN 23-Jul-03 19:58 visiongraph/tracker/BaseObjectDetectionTracker.py
+-rw-r--r--  2.0 unx     1908 b- defN 23-Jul-03 19:58 visiongraph/tracker/CentroidTracker.py
+-rw-r--r--  2.0 unx     4906 b- defN 23-Jul-03 19:58 visiongraph/tracker/FlateTracker.py
+-rw-r--r--  2.0 unx     2705 b- defN 23-Jul-03 19:58 visiongraph/tracker/MotpyTracker.py
+-rw-r--r--  2.0 unx     3304 b- defN 23-Jul-03 19:58 visiongraph/tracker/ObjectAssignmentSolver.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/tracker/__init__.py
+-rw-r--r--  2.0 unx     3105 b- defN 23-Jul-03 19:58 visiongraph/util/ArgUtils.py
+-rw-r--r--  2.0 unx     1009 b- defN 23-Jul-03 19:58 visiongraph/util/CodeUtils.py
+-rw-r--r--  2.0 unx      222 b- defN 23-Jul-03 19:58 visiongraph/util/CollectionUtils.py
+-rw-r--r--  2.0 unx      314 b- defN 23-Jul-03 19:58 visiongraph/util/CommonArgs.py
+-rw-r--r--  2.0 unx     3653 b- defN 23-Jul-03 19:58 visiongraph/util/DrawingUtils.py
+-rw-r--r--  2.0 unx     3588 b- defN 23-Jul-03 19:58 visiongraph/util/ImageUtils.py
+-rw-r--r--  2.0 unx     2608 b- defN 23-Jul-03 19:58 visiongraph/util/LinalgUtils.py
+-rw-r--r--  2.0 unx      539 b- defN 23-Jul-03 19:58 visiongraph/util/LoggingUtils.py
+-rw-r--r--  2.0 unx     1752 b- defN 23-Jul-03 19:58 visiongraph/util/MathUtils.py
+-rw-r--r--  2.0 unx     2649 b- defN 23-Jul-03 19:58 visiongraph/util/NetworkUtils.py
+-rw-r--r--  2.0 unx      195 b- defN 23-Jul-03 19:58 visiongraph/util/OSUtils.py
+-rw-r--r--  2.0 unx      704 b- defN 23-Jul-03 19:58 visiongraph/util/OpenVinoUtils.py
+-rw-r--r--  2.0 unx     1240 b- defN 23-Jul-03 19:58 visiongraph/util/ResultUtils.py
+-rw-r--r--  2.0 unx     2102 b- defN 23-Jul-03 19:58 visiongraph/util/TimeUtils.py
+-rw-r--r--  2.0 unx     3225 b- defN 23-Jul-03 19:58 visiongraph/util/VectorUtils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Jul-03 19:58 visiongraph/util/__init__.py
+-rw-r--r--  2.0 unx    11430 b- defN 23-Jul-03 19:58 visiongraph-0.1.48.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-Jul-03 19:58 visiongraph-0.1.48.dist-info/WHEEL
+-rw-r--r--  2.0 unx       20 b- defN 23-Jul-03 19:58 visiongraph-0.1.48.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       12 b- defN 23-Jul-03 19:58 visiongraph-0.1.48.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    26638 b- defN 23-Jul-03 19:58 visiongraph-0.1.48.dist-info/RECORD
+266 files, 644749 bytes uncompressed, 196856 bytes compressed:  69.5%
```

## zipnote {}

```diff
@@ -339,14 +339,17 @@
 
 Filename: visiongraph/external/intel/adapters/__init__.py
 Comment: 
 
 Filename: visiongraph/external/intel/adapters/inference_adapter.py
 Comment: 
 
+Filename: visiongraph/external/intel/adapters/model_adapter.py
+Comment: 
+
 Filename: visiongraph/external/intel/adapters/openvino_adapter.py
 Comment: 
 
 Filename: visiongraph/external/intel/adapters/ovms_adapter.py
 Comment: 
 
 Filename: visiongraph/external/intel/adapters/utils.py
@@ -774,23 +777,23 @@
 
 Filename: visiongraph/util/VectorUtils.py
 Comment: 
 
 Filename: visiongraph/util/__init__.py
 Comment: 
 
-Filename: visiongraph-0.1.47.5.dist-info/METADATA
+Filename: visiongraph-0.1.48.dist-info/METADATA
 Comment: 
 
-Filename: visiongraph-0.1.47.5.dist-info/WHEEL
+Filename: visiongraph-0.1.48.dist-info/WHEEL
 Comment: 
 
-Filename: visiongraph-0.1.47.5.dist-info/entry_points.txt
+Filename: visiongraph-0.1.48.dist-info/entry_points.txt
 Comment: 
 
-Filename: visiongraph-0.1.47.5.dist-info/top_level.txt
+Filename: visiongraph-0.1.48.dist-info/top_level.txt
 Comment: 
 
-Filename: visiongraph-0.1.47.5.dist-info/RECORD
+Filename: visiongraph-0.1.48.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## visiongraph/estimator/openvino/VisionInferenceEngine.py

```diff
@@ -1,19 +1,20 @@
+import logging
 from typing import Dict, Optional, Any, Sequence, Union
 
 import numpy as np
 from openvino.inference_engine import IECore, IENetwork, ExecutableNetwork
 
 from visiongraph.data.Asset import Asset
 from visiongraph.estimator.BaseVisionEngine import BaseVisionEngine
 from visiongraph.model.VisionEngineOutput import VisionEngineOutput
 
 
 class VisionInferenceEngine(BaseVisionEngine):
-    def __init__(self, model: Asset, weights: Asset, flip_channels: bool = True,
+    def __init__(self, model: Asset, weights: Optional[Asset] = None, flip_channels: bool = True,
                  scale: Optional[Union[float, Sequence[float]]] = None,
                  mean: Optional[Union[float, Sequence[float]]] = None,
                  padding: bool = False, device: str = "AUTO"):
         super().__init__(flip_channels, scale, mean, padding)
 
         self.device = device
 
@@ -23,20 +24,31 @@
         self.ie: Optional[IECore] = None
         self.net: Optional[IENetwork] = None
         self.infer_network: Optional[ExecutableNetwork] = None
 
     def setup(self):
         # setup inference engine
         self.ie = IECore()
-        self.net = self.ie.read_network(model=self.model.path, weights=self.weights.path)
+
+        if self.weights is None:
+            self.net = self.ie.read_network(model=self.model.path)
+        else:
+            self.net = self.ie.read_network(model=self.model.path, weights=self.weights.path)
 
         self.input_names = list(self.net.input_info.keys())
         self.output_names = list(self.net.outputs.keys())
 
-        self.infer_network = self.ie.load_network(network=self.net, device_name=self.device)
+        try:
+            self.infer_network = self.ie.load_network(network=self.net, device_name=self.device)
+        except RuntimeError as ex:
+            logging.warning(f"Could not load network: {ex}")
+
+            if self.device != "CPU":
+                logging.warning(f"Trying to load network with CPU device directly")
+                self.infer_network = self.ie.load_network(network=self.net, device_name="CPU")
 
     def _inference(self, image: np.ndarray, inputs: Optional[Dict[str, Any]] = None) -> VisionEngineOutput:
         return VisionEngineOutput(self.infer_network.infer(inputs=inputs))
 
     def get_input_shape(self, input_name: str) -> Sequence[int]:
         if input_name in self.dynamic_input_shapes:
             return self.dynamic_input_shapes[input_name]
```

## visiongraph/estimator/spatial/CenterNetDetector.py

```diff
@@ -1,10 +1,12 @@
 from enum import Enum
 from typing import List
 
+import openvino.runtime
+
 from visiongraph.data.Asset import Asset
 from visiongraph.data.RepositoryAsset import RepositoryAsset
 from visiongraph.data.labels.COCO import COCO_80_LABELS
 from visiongraph.estimator.openvino.OpenVinoObjectDetector import OpenVinoObjectDetector
 from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter, create_core
 from visiongraph.external.intel.models.detection_model import DetectionModel
 from visiongraph.external.intel.models.centernet import CenterNet
@@ -27,13 +29,15 @@
             'reverse_input_channels': True,
             'path_to_labels': None,
             'confidence_threshold': self.min_score,
             'input_size': None,  # The CTPN specific
             'num_classes': None,  # The NanoDet and NanoDetPlus specific
         }
 
-        return CenterNet.create_model(self.model.path, CenterNet.__model__, config, device=self.device)
+        core = openvino.runtime.Core()
+        adapter = OpenvinoAdapter(core, self.model.path, device=self.device)
+        return CenterNet.create_model(CenterNet.__model__, adapter, config, preload=True)
 
     @staticmethod
     def create(config: CenterNetConfig = CenterNetConfig.CenterNet_FP32) -> "CenterNetDetector":
         model, weights, labels = config.value
         return CenterNetDetector(model, weights, labels)
```

## visiongraph/estimator/spatial/DETRDetector.py

```diff
@@ -1,10 +1,12 @@
 from enum import Enum
 from typing import List
 
+import openvino.runtime
+
 from visiongraph.data.Asset import Asset
 from visiongraph.data.RepositoryAsset import RepositoryAsset
 from visiongraph.data.labels.COCO import COCO_80_LABELS
 from visiongraph.estimator.openvino.OpenVinoObjectDetector import OpenVinoObjectDetector
 from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter, create_core
 from visiongraph.external.intel.models.detection_model import DetectionModel
 from visiongraph.external.intel.models.detr import DETR
@@ -27,15 +29,17 @@
             'reverse_input_channels': True,
             'path_to_labels': None,
             'confidence_threshold': self.min_score,
             'input_size': None,  # The CTPN specific
             'num_classes': None,  # The NanoDet and NanoDetPlus specific
         }
 
-        return DETR.create_model(self.model.path, DETR.__model__, config, device=self.device)
+        core = openvino.runtime.Core()
+        adapter = OpenvinoAdapter(core, self.model.path, device=self.device)
+        return DETR.create_model(DETR.__model__, adapter, config, preload=True)
 
     @staticmethod
     def create(config: DETRConfig = DETRConfig.DETR_Resnet50_FP32) -> "DETRDetector":
         model, weights, labels = config.value
         return DETRDetector(model, weights, labels)
 
     def _get_label(self, index: int):
```

## visiongraph/estimator/spatial/SSDDetector.py

```diff
@@ -1,15 +1,17 @@
 from enum import Enum
-from typing import List, Optional, Tuple
+from typing import List, Tuple
+
+import openvino.runtime
 
 from visiongraph.data.Asset import Asset
 from visiongraph.data.RepositoryAsset import RepositoryAsset
 from visiongraph.data.labels.COCO import COCO_90_LABELS
 from visiongraph.estimator.openvino.OpenVinoObjectDetector import OpenVinoObjectDetector
-from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter, create_core
+from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter
 from visiongraph.external.intel.models.detection_model import DetectionModel
 from visiongraph.external.intel.models.ssd import SSD
 
 _PERSON_LABELS = ["person"]
 
 
 def _person_net(name: str) -> Tuple:
@@ -52,15 +54,17 @@
             'reverse_input_channels': True,
             'path_to_labels': None,
             'confidence_threshold': self.min_score,
             'input_size': None,  # The CTPN specific
             'num_classes': None,  # The NanoDet and NanoDetPlus specific
         }
 
-        return SSD.create_model(self.model.path, SSD.__model__, config, device=self.device)
+        core = openvino.runtime.Core()
+        adapter = OpenvinoAdapter(core, self.model.path, device=self.device)
+        return SSD.create_model(SSD.__model__, adapter, config, preload=True)
 
     @staticmethod
     def create(config: SSDConfig = SSDConfig.SSDLiteMobileNetV2_FP32) -> "SSDDetector":
         model, weights, labels = config.value
         return SSDDetector(model, weights, labels)
 
     def _get_label(self, index: int):
```

## visiongraph/estimator/spatial/YOLODetector.py

```diff
@@ -1,10 +1,12 @@
 from enum import Enum
 from typing import List
 
+import openvino.runtime
+
 from visiongraph.data.Asset import Asset
 from visiongraph.data.RepositoryAsset import RepositoryAsset
 from visiongraph.data.labels.COCO import COCO_80_LABELS
 from visiongraph.estimator.openvino.OpenVinoObjectDetector import OpenVinoObjectDetector
 from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter, create_core
 from visiongraph.external.intel.models.detection_model import DetectionModel
 from visiongraph.external.intel.models.yolo import YOLO, YoloV4, YOLOX, YOLOF
@@ -51,13 +53,15 @@
             'reverse_input_channels': True,
             'path_to_labels': None,
             'confidence_threshold': self.min_score,
             'input_size': None,  # The CTPN specific
             'num_classes': None,  # The NanoDet and NanoDetPlus specific
         }
 
-        return DetectionModel.create_model(self.model.path, self.architecture.value, config, device=self.device)
+        core = openvino.runtime.Core()
+        adapter = OpenvinoAdapter(core, self.model.path, device=self.device)
+        return DetectionModel.create_model(self.architecture.value, adapter, config, preload=True)
 
     @staticmethod
     def create(config: YOLOConfig = YOLOConfig.YOLOv4_Tiny_FP16) -> "YOLODetector":
         model, weights, labels, architecture = config.value
         return YOLODetector(model, weights, labels, architecture=architecture)
```

## visiongraph/estimator/spatial/pose/AEPoseEstimator.py

```diff
@@ -1,10 +1,12 @@
 from enum import Enum
 from typing import Optional
 
+import openvino.runtime
+
 from visiongraph.data.Asset import Asset
 from visiongraph.data.RepositoryAsset import RepositoryAsset
 from visiongraph.estimator.openvino.OpenVinoPoseEstimator import OpenVinoPoseEstimator
 from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter, create_core
 from visiongraph.external.intel.models.hpe_associative_embedding import HpeAssociativeEmbedding
 from visiongraph.external.intel.models.model import Model
 
@@ -29,13 +31,15 @@
             'target_size': self.target_size,
             'aspect_ratio': self.aspect_ratio,
             'confidence_threshold': self.min_score,
             'padding_mode': 'center',
             'delta': 0.5
         }
 
-        return HpeAssociativeEmbedding.create_model(self.model.path, "HPE-assosiative-embedding", config, device=self.device)
+        core = openvino.runtime.Core()
+        adapter = OpenvinoAdapter(core, self.model.path, device=self.device)
+        return HpeAssociativeEmbedding.create_model(HpeAssociativeEmbedding.__model__, adapter, config, preload=True)
 
     @staticmethod
     def create(config: AEPoseConfig = AEPoseConfig.EfficientHRNet_288_FP16) -> "AEPoseEstimator":
         model, weights = config.value
         return AEPoseEstimator(model, weights)
```

## visiongraph/estimator/spatial/pose/OpenPoseEstimator.py

```diff
@@ -1,13 +1,16 @@
 from enum import Enum
 from typing import Optional
 
+import openvino.runtime
+
 from visiongraph.data.Asset import Asset
 from visiongraph.data.RepositoryAsset import RepositoryAsset
 from visiongraph.estimator.openvino.OpenVinoPoseEstimator import OpenVinoPoseEstimator
+from visiongraph.external.intel.adapters.openvino_adapter import OpenvinoAdapter
 from visiongraph.external.intel.models.model import Model
 from visiongraph.external.intel.models.open_pose import OpenPose
 
 
 class OpenPoseConfig(Enum):
     LightWeightOpenPose_INT8 = (*RepositoryAsset.openVino("human-pose-estimation-0001-int8"),)
     LightWeightOpenPose_FP16 = (*RepositoryAsset.openVino("human-pose-estimation-0001-fp16"),)
@@ -25,13 +28,15 @@
             'target_size': self.target_size,
             'aspect_ratio': self.aspect_ratio,
             'confidence_threshold': self.min_score,
             'padding_mode': None,
             'delta': None
         }
 
-        return OpenPose.create_model(self.model.path, "OpenPose", config, device=self.device)
+        core = openvino.runtime.Core()
+        adapter = OpenvinoAdapter(core, self.model.path, device=self.device)
+        return OpenPose.create_model(OpenPose.__model__, adapter, config, preload=True)
 
     @staticmethod
     def create(config: OpenPoseConfig = OpenPoseConfig.LightWeightOpenPose_FP16) -> "OpenPoseEstimator":
         model, weights = config.value
         return OpenPoseEstimator(model, weights)
```

## visiongraph/external/intel/performance_metrics.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2020 Intel Corporation
+ Copyright (C) 2020-2023 Intel Corporation
 
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
 
       http://www.apache.org/licenses/LICENSE-2.0
```

## visiongraph/external/intel/adapters/openvino_adapter.py

```diff
@@ -12,300 +12,131 @@
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
 import logging as log
 from pathlib import Path
-from typing import Dict, Set, Tuple
 
 try:
-    import openvino.runtime as ov
-    from openvino.preprocess import ColorFormat, PrePostProcessor
-    from openvino.runtime import (
-        AsyncInferQueue,
-        Core,
-        Dimension,
-        PartialShape,
-        Type,
-        get_version,
-        layout_helpers,
-    )
-
+    from openvino.runtime import AsyncInferQueue, Core, PartialShape, layout_helpers, get_version, Dimension
     openvino_absent = False
 except ImportError:
     openvino_absent = True
 
-from .inference_adapter import InferenceAdapter, Metadata
-from .utils import (
-    Layout,
-    crop_resize,
-    resize_image,
-    resize_image_letterbox,
-    resize_image_with_aspect,
-)
+from .model_adapter import ModelAdapter, Metadata
+from .utils import Layout
+from ..pipelines import parse_devices
 
 
 def create_core():
     if openvino_absent:
-        raise ImportError("The OpenVINO package is not installed")
+        raise ImportError('The OpenVINO package is not installed')
 
-    log.info("OpenVINO Runtime")
-    log.info("\tbuild: {}".format(get_version()))
+    log.info('OpenVINO Runtime')
+    log.info('\tbuild: {}'.format(get_version()))
     return Core()
 
 
-def parse_devices(device_string):
-    colon_position = device_string.find(":")
-    if colon_position != -1:
-        device_type = device_string[:colon_position]
-        if device_type == "HETERO" or device_type == "MULTI":
-            comma_separated_devices = device_string[colon_position + 1 :]
-            devices = comma_separated_devices.split(",")
-            for device in devices:
-                parenthesis_position = device.find(":")
-                if parenthesis_position != -1:
-                    device = device[:parenthesis_position]
-            return devices
-    return (device_string,)
-
-
-def parse_value_per_device(devices: Set[str], values_string: str) -> Dict[str, int]:
-    """Format: <device1>:<value1>,<device2>:<value2> or just <value>"""
-    values_string_upper = values_string.upper()
-    result = {}
-    device_value_strings = values_string_upper.split(",")
-    for device_value_string in device_value_strings:
-        device_value_list = device_value_string.split(":")
-        if len(device_value_list) == 2:
-            if device_value_list[0] in devices:
-                result[device_value_list[0]] = int(device_value_list[1])
-        elif len(device_value_list) == 1 and device_value_list[0] != "":
-            for device in devices:
-                result[device] = int(device_value_list[0])
-        elif device_value_list[0] != "":
-            raise RuntimeError(f"Unknown string format: {values_string}")
-    return result
-
-
-def get_user_config(
-    flags_d: str, flags_nstreams: str, flags_nthreads: int
-) -> Dict[str, str]:
-    config = {}
-
-    devices = set(parse_devices(flags_d))
-
-    device_nstreams = parse_value_per_device(devices, flags_nstreams)
-    for device in devices:
-        if device == "CPU":  # CPU supports a few special performance-oriented keys
-            # limit threading for CPU portion of inference
-            if flags_nthreads:
-                config["CPU_THREADS_NUM"] = str(flags_nthreads)
-
-            config["CPU_BIND_THREAD"] = "NO"
-
-            # for CPU execution, more throughput-oriented execution via streams
-            config["CPU_THROUGHPUT_STREAMS"] = (
-                str(device_nstreams[device])
-                if device in device_nstreams
-                else "CPU_THROUGHPUT_AUTO"
-            )
-        elif device == "GPU":
-            config["GPU_THROUGHPUT_STREAMS"] = (
-                str(device_nstreams[device])
-                if device in device_nstreams
-                else "GPU_THROUGHPUT_AUTO"
-            )
-            if "MULTI" in flags_d and "CPU" in devices:
-                # multi-device execution with the CPU + GPU performs best with GPU throttling hint,
-                # which releases another CPU thread (that is otherwise used by the GPU driver for active polling)
-                config["GPU_PLUGIN_THROTTLE"] = "1"
-    return config
-
-
-class OpenvinoAdapter(InferenceAdapter):
-    """
+class OpenvinoAdapter(ModelAdapter):
+    '''
     Works with OpenVINO model
-    """
+    '''
 
-    def __init__(
-        self,
-        core,
-        model,
-        weights_path="",
-        model_parameters={},
-        device="CPU",
-        plugin_config=None,
-        max_num_requests=0,
-        precision="FP16",
-        download_dir=None,
-        cache_dir=None,
-    ):
-        """
-        precision, download_dir and cache_dir are ignored if model is a path to a file
-        """
+    def __init__(self, core, model_path, weights_path=None, model_parameters = {}, device='CPU', plugin_config=None, max_num_requests=0):
         self.core = core
-        self.model_path = model
+        self.model_path = model_path
         self.device = device
         self.plugin_config = plugin_config
         self.max_num_requests = max_num_requests
         self.model_parameters = model_parameters
-        self.model_parameters["input_layouts"] = Layout.parse_layouts(
-            self.model_parameters.get("input_layouts", None)
-        )
-
-        if isinstance(self.model_path, (str, Path)):
-            if Path(self.model_path).suffix == ".onnx" and weights_path:
-                log.warning(
-                    'For model in ONNX format should set only "model_path" parameter.'
-                    'The "weights_path" will be omitted'
-                )
-
-        self.model_from_buffer = isinstance(self.model_path, bytes) and isinstance(
-            weights_path, bytes
-        )
-        model_from_file = Path(self.model_path).is_file()
-        if model_from_file or self.model_from_buffer:
-            log.info(
-                "Reading model {}".format(
-                    "from buffer" if self.model_from_buffer else self.model_path
-                )
-            )
-            self.model = core.read_model(self.model_path, weights_path)
-            return
-        if isinstance(model, str):
-            from openvino.model_zoo.models import OMZModel, list_models
-
-            if model in list_models():
-                omz_model = OMZModel.download(
-                    model,
-                    precision=precision,
-                    download_dir=download_dir,
-                    cache_dir=cache_dir,
-                )
-                self.model_path = omz_model.model_path
-                log.info(f"Reading model {self.model_path}")
-                self.model = core.read_model(self.model_path)
-                return
-        raise RuntimeError("Model must be bytes, a file or existing OMZ model name")
+        self.model_parameters['input_layouts'] = Layout.parse_layouts(self.model_parameters.get('input_layouts', None))
+
+        if isinstance(model_path, (str, Path)):
+            if Path(model_path).suffix == ".onnx" and weights_path:
+                log.warning('For model in ONNX format should set only "model_path" parameter.'
+                            'The "weights_path" will be omitted')
+
+        self.model_from_buffer = isinstance(model_path, bytes) and isinstance(weights_path, bytes)
+        log.info('Reading model {}'.format('from buffer' if self.model_from_buffer else model_path))
+        weights = weights_path if self.model_from_buffer else ''
+        self.model = core.read_model(model_path, weights)
 
     def load_model(self):
-        self.compiled_model = self.core.compile_model(
-            self.model, self.device, self.plugin_config
-        )
+        self.compiled_model = self.core.compile_model(self.model, self.device, self.plugin_config)
         self.async_queue = AsyncInferQueue(self.compiled_model, self.max_num_requests)
         if self.max_num_requests == 0:
             # +1 to use it as a buffer of the pipeline
-            self.async_queue = AsyncInferQueue(
-                self.compiled_model, len(self.async_queue) + 1
-            )
-
-        log.info(
-            "The model {} is loaded to {}".format(
-                "from buffer" if self.model_from_buffer else self.model_path,
-                self.device,
-            )
-        )
+            self.async_queue = AsyncInferQueue(self.compiled_model, len(self.async_queue) + 1)
+
+        log.info('The model {} is loaded to {}'.format("from buffer" if self.model_from_buffer else self.model_path, self.device))
         self.log_runtime_settings()
 
     def log_runtime_settings(self):
         devices = set(parse_devices(self.device))
-        if "AUTO" not in devices:
+        if 'AUTO' not in devices:
             for device in devices:
                 try:
-                    nstreams = self.compiled_model.get_property(
-                        device + "_THROUGHPUT_STREAMS"
-                    )
-                    log.info("\tDevice: {}".format(device))
-                    log.info("\t\tNumber of streams: {}".format(nstreams))
-                    if device == "CPU":
-                        nthreads = self.compiled_model.get_property("CPU_THREADS_NUM")
-                        log.info(
-                            "\t\tNumber of threads: {}".format(
-                                nthreads if int(nthreads) else "AUTO"
-                            )
-                        )
+                    nstreams = self.compiled_model.get_property(device + '_THROUGHPUT_STREAMS')
+                    log.info('\tDevice: {}'.format(device))
+                    log.info('\t\tNumber of streams: {}'.format(nstreams))
+                    if device == 'CPU':
+                        nthreads = self.compiled_model.get_property('CPU_THREADS_NUM')
+                        log.info('\t\tNumber of threads: {}'.format(nthreads if int(nthreads) else 'AUTO'))
                 except RuntimeError:
                     pass
-        log.info("\tNumber of model infer requests: {}".format(len(self.async_queue)))
+        log.info('\tNumber of model infer requests: {}'.format(len(self.async_queue)))
 
     def get_input_layers(self):
         inputs = {}
         for input in self.model.inputs:
             input_shape = get_input_shape(input)
             input_layout = self.get_layout_for_input(input, input_shape)
-            inputs[input.get_any_name()] = Metadata(
-                input.get_names(),
-                input_shape,
-                input_layout,
-                input.get_element_type().get_type_name(),
-            )
+            inputs[input.get_any_name()] = Metadata(input.get_names(), input_shape, input_layout, input.get_element_type().get_type_name())
         inputs = self._get_meta_from_ngraph(inputs)
         return inputs
 
     def get_layout_for_input(self, input, shape=None) -> str:
-        input_layout = ""
-        if self.model_parameters["input_layouts"]:
-            input_layout = Layout.from_user_layouts(
-                input.get_names(), self.model_parameters["input_layouts"]
-            )
+        input_layout = ''
+        if self.model_parameters['input_layouts']:
+            input_layout = Layout.from_user_layouts(input.get_names(), self.model_parameters['input_layouts'])
         if not input_layout:
             if not layout_helpers.get_layout(input).empty:
                 input_layout = Layout.from_openvino(input)
             else:
-                input_layout = Layout.from_shape(
-                    shape if shape is not None else input.shape
-                )
+                input_layout = Layout.from_shape(shape if shape is not None else input.shape)
         return input_layout
 
     def get_output_layers(self):
         outputs = {}
         for output in self.model.outputs:
-            output_shape = (
-                output.partial_shape.get_min_shape()
-                if self.model.is_dynamic()
-                else output.shape
-            )
-            outputs[output.get_any_name()] = Metadata(
-                output.get_names(),
-                list(output_shape),
-                precision=output.get_element_type().get_type_name(),
-            )
+            output_shape = output.partial_shape.get_min_shape() if self.model.is_dynamic() else output.shape
+            outputs[output.get_any_name()] = Metadata(output.get_names(), list(output_shape), precision=output.get_element_type().get_type_name())
         outputs = self._get_meta_from_ngraph(outputs)
         return outputs
 
     def reshape_model(self, new_shape):
-        new_shape = {
-            name: PartialShape(
-                [
-                    Dimension(dim)
-                    if not isinstance(dim, tuple)
-                    else Dimension(dim[0], dim[1])
-                    for dim in shape
-                ]
-            )
-            for name, shape in new_shape.items()
-        }
+        new_shape = {name: PartialShape(
+            [Dimension(dim) if not isinstance(dim, tuple) else Dimension(dim[0], dim[1])
+            for dim in shape]) for name, shape in new_shape.items()}
         self.model.reshape(new_shape)
 
     def get_raw_result(self, request):
         return {key: request.get_tensor(key).data for key in self.get_output_layers()}
 
     def copy_raw_result(self, request):
-        return {
-            key: request.get_tensor(key).data.copy() for key in self.get_output_layers()
-        }
+        return {key: request.get_tensor(key).data.copy() for key in self.get_output_layers()}
 
     def infer_sync(self, dict_data):
         self.infer_request = self.async_queue[self.async_queue.get_idle_request_id()]
         self.infer_request.infer(dict_data)
         return self.get_raw_result(self.infer_request)
 
     def infer_async(self, dict_data, callback_data) -> None:
-        self.async_queue.start_async(dict_data, callback_data)
+        self.async_queue.start_async(dict_data, (self.copy_raw_result, callback_data))
 
     def set_callback(self, callback_fn):
         self.async_queue.set_callback(callback_fn)
 
     def is_ready(self) -> bool:
         return self.async_queue.is_ready()
 
@@ -325,116 +156,32 @@
         return layers_info
 
     def operations_by_type(self, operation_type):
         layers_info = {}
         for node in self.model.get_ordered_ops():
             if node.get_type_name() == operation_type:
                 layer_name = node.get_friendly_name()
-                layers_info[layer_name] = Metadata(
-                    type=node.get_type_name(), meta=node.get_attributes()
-                )
+                layers_info[layer_name] = Metadata(type=node.get_type_name(), meta=node.get_attributes())
         return layers_info
 
-    def get_rt_info(self, path):
-        return self.model.get_rt_info(path)
-
-    def embed_preprocessing(
-        self,
-        layout="NCHW",
-        resize_mode: str = None,
-        interpolation_mode="LINEAR",
-        target_shape: Tuple[int] = None,
-        dtype=type(int),
-        brg2rgb=False,
-        mean=None,
-        scale=None,
-        input_idx=0,
-    ):
-        ppp = PrePostProcessor(self.model)
-
-        INTERPOLATION_MODE_MAP = {
-            "LINEAR": "linear",
-            "CUBIC": "cubic",
-            "NEAREST": "nearest",
-        }
-
-        RESIZE_MODE_MAP = {
-            "crop": crop_resize,
-            "standard": resize_image,
-            "fit_to_window": resize_image_with_aspect,
-            "fit_to_window_letterbox": resize_image_letterbox,
-        }
-
-        # Handle resize
-        # Change to dynamic shape to handle various image size
-        # TODO: check the number of input channels and rank of input shape
-        if resize_mode and target_shape:
-            if resize_mode in RESIZE_MODE_MAP:
-                input_shape = [1, -1, -1, 3]
-                ppp.input(input_idx).tensor().set_shape(input_shape)
-                ppp.input(input_idx).preprocess().custom(
-                    RESIZE_MODE_MAP[resize_mode](
-                        target_shape, INTERPOLATION_MODE_MAP[interpolation_mode]
-                    )
-                )
-
-            else:
-                raise ValueError(
-                    f"Upsupported resize type in model preprocessing: {resize_mode}"
-                )
-
-        # Change the input type to the 8-bit image
-        if dtype == type(int):
-            ppp.input(input_idx).tensor().set_element_type(Type.u8)
-
-        ppp.input(input_idx).tensor().set_layout(ov.Layout("NHWC")).set_color_format(
-            ColorFormat.BGR
-        )
-
-        # Handle layout
-        ppp.input(input_idx).model().set_layout(ov.Layout(layout))
-
-        # Handle color format
-        if brg2rgb:
-            ppp.input(input_idx).preprocess().convert_color(ColorFormat.RGB)
-
-        ppp.input(input_idx).preprocess().convert_element_type(Type.f32)
-
-        if mean:
-            ppp.input(input_idx).preprocess().mean(mean)
-        if scale:
-            ppp.input(input_idx).preprocess().scale(scale)
-
-        self.model = ppp.build()
-        self.load_model()
-
-    def get_model(self):
-        """Returns the openvino.runtime.Model object
-
-        Returns:
-            openvino.runtime.Model object
-        """
-        return self.model
-
 
 def get_input_shape(input_tensor):
     def string_to_tuple(string, casting_type=int):
-        processed = string.replace(" ", "").replace("(", "").replace(")", "").split(",")
+        processed = string.replace(' ', '').replace('(', '').replace(')', '').split(',')
         processed = filter(lambda x: x, processed)
         return tuple(map(casting_type, processed)) if casting_type else tuple(processed)
-
     if not input_tensor.partial_shape.is_dynamic:
         return list(input_tensor.shape)
     ps = str(input_tensor.partial_shape)
-    if ps[0] == "[" and ps[-1] == "]":
+    if ps[0] == '[' and ps[-1] == ']':
         ps = ps[1:-1]
-    preprocessed = ps.replace("{", "(").replace("}", ")").replace("?", "-1")
-    preprocessed = preprocessed.replace("(", "").replace(")", "")
-    if ".." in preprocessed:
+    preprocessed = ps.replace('{', '(').replace('}', ')').replace('?', '-1')
+    preprocessed = preprocessed.replace('(', '').replace(')', '')
+    if '..' in preprocessed:
         shape_list = []
-        for dim in preprocessed.split(","):
-            if ".." in dim:
-                shape_list.append(string_to_tuple(dim.replace("..", ",")))
+        for dim in preprocessed.split(','):
+            if '..' in dim:
+                shape_list.append(string_to_tuple(dim.replace('..', ',')))
             else:
                 shape_list.append(int(dim))
         return shape_list
-    return string_to_tuple(preprocessed)
+    return string_to_tuple(preprocessed)
```

## visiongraph/external/intel/adapters/ovms_adapter.py

```diff
@@ -10,84 +10,78 @@
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
-from typing import Tuple
-
 try:
     import ovmsclient
-
     ovmsclient_absent = False
 except ImportError:
     ovmsclient_absent = True
 
-import logging as log
 import re
-
 import numpy as np
-
-from .inference_adapter import InferenceAdapter, Metadata
+import logging as log
+from .model_adapter import ModelAdapter, Metadata
 from .utils import Layout
 
 
-class OVMSAdapter(InferenceAdapter):
-    """
+class OVMSAdapter(ModelAdapter):
+    '''
     Class that allows working with models served by the OpenVINO Model Server
-    """
+    '''
 
     tf2ov_precision = {
         "DT_INT64": "I64",
         "DT_UINT64": "U64",
         "DT_FLOAT": "FP32",
         "DT_UINT32": "U32",
         "DT_INT32": "I32",
-        "DT_HALF": "FP16",
+        "DT_HALF" : "FP16",
         "DT_INT16": "I16",
-        "DT_INT8": "I8",
+        "DT_INT8" : "I8",
         "DT_UINT8": "U8",
     }
 
     tf2np_precision = {
         "DT_INT64": np.int64,
         "DT_UINT64": np.uint64,
         "DT_FLOAT": np.float32,
         "DT_UINT32": np.uint32,
         "DT_INT32": np.int32,
-        "DT_HALF": np.float16,
+        "DT_HALF" : np.float16,
         "DT_INT16": np.int16,
-        "DT_INT8": np.int8,
+        "DT_INT8" : np.int8,
         "DT_UINT8": np.uint8,
     }
 
     @classmethod
     def parse_model_arg(cls, target_model):
         if not isinstance(target_model, str):
             raise TypeError("--model option should be str")
         # Expecting format: <address>:<port>/models/<model_name>[:<model_version>]
-        pattern = re.compile(r"(\w+\.*\-*)*\w+:\d+\/models\/[a-zA-Z0-9_-]+(\:\d+)*")
+        pattern = re.compile(r"(\w+\.*\-*)*\w+:\d+\/models\/\w+(\:\d+)*")
         if not pattern.fullmatch(target_model):
             raise ValueError("invalid --model option format")
         service_url, _, model = target_model.split("/")
         model_spec = model.split(":")
         if len(model_spec) == 1:
             # model version not specified - use latest
             return service_url, model_spec[0], 0
         elif len(model_spec) == 2:
             return service_url, model_spec[0], int(model_spec[1])
         else:
             raise ValueError("invalid --model option format")
 
+
     def _is_model_available(self):
         try:
-            model_status = self.client.get_model_status(
-                self.model_name, self.model_version
-            )
+            model_status = self.client.get_model_status(self.model_name, self.model_version)
         except ovmsclient.ModelNotFoundError:
             return False
         target_version = max(model_status.keys())
         version_status = model_status[target_version]
         if version_status["state"] == "AVAILABLE" and version_status["error_code"] == 0:
             return True
         return False
@@ -95,96 +89,71 @@
     def _prepare_inputs(self, dict_data):
         inputs = {}
         for input_name, input_data in dict_data.items():
             if input_name not in self.metadata["inputs"].keys():
                 raise ValueError("Input data does not match model inputs")
             input_info = self.metadata["inputs"][input_name]
             model_precision = self.tf2np_precision[input_info["dtype"]]
-            if (
-                isinstance(input_data, np.ndarray)
-                and input_data.dtype != model_precision
-            ):
+            if isinstance(input_data, np.ndarray) and input_data.dtype != model_precision:
                 input_data = input_data.astype(model_precision)
             elif isinstance(input_data, list):
                 input_data = np.array(input_data, dtype=model_precision)
             inputs[input_name] = input_data
         return inputs
 
     def __init__(self, target_model):
         if ovmsclient_absent:
             raise ImportError("The ovmsclient package is not installed")
 
-        log.info("Connecting to remote model: {}".format(target_model))
-        service_url, model_name, model_version = OVMSAdapter.parse_model_arg(
-            target_model
-        )
+        log.info('Connecting to remote model: {}'.format(target_model))
+        service_url, model_name, model_version = OVMSAdapter.parse_model_arg(target_model)
         self.model_name = model_name
         self.model_version = model_version
         self.client = ovmsclient.make_grpc_client(url=service_url)
         # Ensure the model is available
         if not self._is_model_available():
-            model_version_str = (
-                "latest" if self.model_version == 0 else str(self.model_version)
-            )
-            raise RuntimeError(
-                "Requested model: {}, version: {}, has not been found or is not "
-                "in available state".format(self.model_name, model_version_str)
-            )
-
-        self.preprocessing_embedded = False
-
-        self.metadata = self.client.get_model_metadata(
-            model_name=self.model_name, model_version=self.model_version
-        )
+            model_version_str = "latest" if self.model_version == 0 else str(self.model_version)
+            raise RuntimeError("Requested model: {}, version: {}, has not been found or is not "
+                "in available state".format(self.model_name, model_version_str))
+
+        self.metadata = self.client.get_model_metadata(model_name=self.model_name,
+                                                       model_version=self.model_version)
 
     def load_model(self):
         pass
 
     def get_input_layers(self):
         inputs = {}
         for name, meta in self.metadata["inputs"].items():
             input_layout = Layout.from_shape(meta["shape"])
-            inputs[name] = Metadata(
-                set(name),
-                meta["shape"],
-                input_layout,
-                self.tf2ov_precision.get(meta["dtype"], meta["dtype"]),
-            )
+            inputs[name] = Metadata(set(name), meta["shape"], input_layout, self.tf2ov_precision.get(meta["dtype"], meta["dtype"]))
         return inputs
 
     def get_output_layers(self):
         outputs = {}
         for name, meta in self.metadata["outputs"].items():
-            outputs[name] = Metadata(
-                names=set(name),
-                shape=meta["shape"],
-                precision=self.tf2ov_precision.get(meta["dtype"], meta["dtype"]),
-            )
+            outputs[name] = Metadata(names=set(name), shape=meta["shape"], precision=self.tf2ov_precision.get(meta["dtype"], meta["dtype"]))
         return outputs
 
     def reshape_model(self, new_shape):
         pass
 
     def infer_sync(self, dict_data):
         inputs = self._prepare_inputs(dict_data)
-        raw_result = self.client.predict(
-            inputs, model_name=self.model_name, model_version=self.model_version
-        )
+        raw_result = self.client.predict(inputs, model_name=self.model_name, model_version=self.model_version)
         # For models with single output ovmsclient returns ndarray with results,
         # so the dict must be created to correctly implement interface.
         if isinstance(raw_result, np.ndarray):
             output_name = list(self.metadata["outputs"].keys())[0]
             return {output_name: raw_result}
         return raw_result
 
     def infer_async(self, dict_data, callback_data):
         inputs = self._prepare_inputs(dict_data)
-        raw_result = self.client.predict(
-            inputs, model_name=self.model_name, model_version=self.model_version
-        )
+        raw_result = self.client.predict(inputs, model_name=self.model_name, model_version=self.model_version)
         # For models with single output ovmsclient returns ndarray with results,
         # so the dict must be created to correctly implement interface.
         if isinstance(raw_result, np.ndarray):
             output_name = list(self.metadata["outputs"].keys())[0]
             raw_result = {output_name: raw_result}
         self.callback_fn(raw_result, (lambda x: x, callback_data))
 
@@ -195,24 +164,7 @@
         return True
 
     def await_all(self):
         pass
 
     def await_any(self):
         pass
-
-    def get_rt_info(self, path):
-        raise NotImplementedError("OVMSAdapter does not support RT info getting")
-
-    def embed_preprocessing(
-        self,
-        layout="NCHW",
-        resize_mode: str = None,
-        interpolation_mode="LINEAR",
-        target_shape: Tuple[int] = None,
-        dtype=type(int),
-        brg2rgb=False,
-        mean=None,
-        scale=None,
-        input_idx=0,
-    ):
-        pass
```

## visiongraph/external/intel/adapters/utils.py

```diff
@@ -10,318 +10,69 @@
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
-from functools import partial
 from typing import Optional
-
-import numpy as np
-import openvino.runtime as ov
-from openvino.runtime import Output, Type, layout_helpers
-from openvino.runtime import opset10 as opset
-from openvino.runtime.utils.decorators import custom_preprocess_function
+from openvino.runtime import layout_helpers
 
 
 class Layout:
-    def __init__(self, layout="") -> None:
+    def __init__(self, layout = '') -> None:
         self.layout = layout
 
     @staticmethod
     def from_shape(shape):
-        """
+        '''
         Create Layout from given shape
-        """
+        '''
         if len(shape) == 2:
-            return "NC"
+            return 'NC'
         if len(shape) == 3:
-            return "CHW" if shape[0] in range(1, 5) else "HWC"
+            return 'CHW' if shape[0] in range(1, 5) else 'HWC'
         if len(shape) == 4:
-            return "NCHW" if shape[1] in range(1, 5) else "NHWC"
+            return 'NCHW' if shape[1] in range(1, 5) else 'NHWC'
 
-        raise RuntimeError(
-            "Get layout from shape method doesn't support {}D shape".format(len(shape))
-        )
+        raise RuntimeError("Get layout from shape method doesn't support {}D shape".format(len(shape)))
 
     @staticmethod
     def from_openvino(input):
-        """
+        '''
         Create Layout from openvino input
-        """
-        return layout_helpers.get_layout(input).to_string().strip("[]").replace(",", "")
+        '''
+        return layout_helpers.get_layout(input).to_string().strip('[]').replace(',', '')
 
     @staticmethod
     def from_user_layouts(input_names: set, user_layouts: dict):
-        """
+        '''
         Create Layout for input based on user info
-        """
+        '''
         for input_name in input_names:
             if input_name in user_layouts:
                 return user_layouts[input_name]
-        return user_layouts.get("", "")
+        return user_layouts.get('', '')
 
     @staticmethod
     def parse_layouts(layout_string: str) -> Optional[dict]:
-        """
+        '''
         Parse layout parameter in format "input0:NCHW,input1:NC" or "NCHW" (applied to all inputs)
-        """
+        '''
         if not layout_string:
             return None
-        search_string = (
-            layout_string if layout_string.rfind(":") != -1 else ":" + layout_string
-        )
-        colon_pos = search_string.rfind(":")
+        search_string = layout_string if layout_string.rfind(':') != -1 else ":" + layout_string
+        colon_pos = search_string.rfind(':')
         user_layouts = {}
-        while colon_pos != -1:
-            start_pos = search_string.rfind(",")
-            input_name = search_string[start_pos + 1 : colon_pos]
-            input_layout = search_string[colon_pos + 1 :]
+        while (colon_pos != -1):
+            start_pos = search_string.rfind(',')
+            input_name = search_string[start_pos + 1:colon_pos]
+            input_layout = search_string[colon_pos + 1:]
             user_layouts[input_name] = input_layout
-            search_string = search_string[: start_pos + 1]
-            if search_string == "" or search_string[-1] != ",":
+            search_string = search_string[:start_pos + 1]
+            if search_string == "" or search_string[-1] != ',':
                 break
             search_string = search_string[:-1]
-            colon_pos = search_string.rfind(":")
+            colon_pos = search_string.rfind(':')
         if search_string != "":
             raise ValueError("Can't parse input layout string: " + layout_string)
         return user_layouts
-
-
-def resize_image_letterbox_graph(input: Output, size, interpolation="linear"):
-    w, h = size
-    h_axis = 1
-    w_axis = 2
-    image_shape = opset.shape_of(input, name="shape")
-    iw = opset.convert(
-        opset.gather(image_shape, opset.constant(w_axis), axis=0),
-        destination_type="f32",
-    )
-    ih = opset.convert(
-        opset.gather(image_shape, opset.constant(h_axis), axis=0),
-        destination_type="f32",
-    )
-    w_ratio = opset.divide(opset.constant(w, dtype=Type.f32), iw)
-    h_ratio = opset.divide(opset.constant(h, dtype=Type.f32), ih)
-    scale = opset.minimum(w_ratio, h_ratio)
-    nw = opset.convert(opset.multiply(iw, scale), destination_type="i32")
-    nh = opset.convert(opset.multiply(ih, scale), destination_type="i32")
-    new_size = opset.concat([opset.unsqueeze(nh, 0), opset.unsqueeze(nw, 0)], axis=-1)
-    image = opset.interpolate(
-        input,
-        new_size,
-        scales=np.array([1.0, 1.0], dtype=np.float32),
-        axes=[h_axis, w_axis],
-        mode=interpolation,
-        shape_calculation_mode="sizes",
-    )
-    dx = opset.divide(
-        opset.subtract(opset.constant(w, dtype=np.int32), nw),
-        opset.constant(2, dtype=np.int32),
-    )
-    dy = opset.divide(
-        opset.subtract(opset.constant(h, dtype=np.int32), nh),
-        opset.constant(2, dtype=np.int32),
-    )
-    dx_border = opset.subtract(
-        opset.subtract(opset.constant(w, dtype=np.int32), nw), dx
-    )
-    dy_border = opset.subtract(
-        opset.subtract(opset.constant(h, dtype=np.int32), nh), dy
-    )
-    pads_begin = opset.concat(
-        [
-            opset.constant([0], dtype=np.int32),
-            opset.unsqueeze(dy, 0),
-            opset.unsqueeze(dx, 0),
-            opset.constant([0], dtype=np.int32),
-        ],
-        axis=0,
-    )
-    pads_end = opset.concat(
-        [
-            opset.constant([0], dtype=np.int32),
-            opset.unsqueeze(dy_border, 0),
-            opset.unsqueeze(dx_border, 0),
-            opset.constant([0], dtype=np.int32),
-        ],
-        axis=0,
-    )
-    resized_image = opset.pad(image, pads_begin, pads_end, "constant")
-    return resized_image
-
-
-def crop_resize_graph(input: Output, size):
-    h_axis = 1
-    w_axis = 2
-    desired_aspect_ratio = size[1] / size[0]  # width / height
-
-    image_shape = opset.shape_of(input, name="shape")
-    iw = opset.convert(
-        opset.gather(image_shape, opset.constant(w_axis), axis=0),
-        destination_type="i32",
-    )
-    ih = opset.convert(
-        opset.gather(image_shape, opset.constant(h_axis), axis=0),
-        destination_type="i32",
-    )
-
-    if desired_aspect_ratio == 1:
-        # then_body
-        image_t = opset.parameter([-1, -1, -1, 3], np.uint8, "image")
-        iw_t = opset.parameter([], np.int32, "iw")
-        ih_t = opset.parameter([], np.int32, "ih")
-        then_offset = opset.unsqueeze(
-            opset.divide(opset.subtract(ih_t, iw_t), opset.constant(2, dtype=np.int32)),
-            0,
-        )
-        then_stop = opset.add(then_offset, iw_t)
-        then_cropped_frame = opset.slice(
-            image_t, start=then_offset, stop=then_stop, step=[1], axes=[h_axis]
-        )
-        then_body_res_1 = opset.result(then_cropped_frame)
-        then_body = ov.Model(
-            [then_body_res_1], [image_t, iw_t, ih_t], "then_body_function"
-        )
-
-        # else_body
-        image_e = opset.parameter([-1, -1, -1, 3], np.uint8, "image")
-        iw_e = opset.parameter([], np.int32, "iw")
-        ih_e = opset.parameter([], np.int32, "ih")
-        else_offset = opset.unsqueeze(
-            opset.divide(opset.subtract(iw_e, ih_e), opset.constant(2, dtype=np.int32)),
-            0,
-        )
-        else_stop = opset.add(else_offset, ih_e)
-        else_cropped_frame = opset.slice(
-            image_e, start=else_offset, stop=else_stop, step=[1], axes=[w_axis]
-        )
-        else_body_res_1 = opset.result(else_cropped_frame)
-        else_body = ov.Model(
-            [else_body_res_1], [image_e, iw_e, ih_e], "else_body_function"
-        )
-
-        # if
-        condition = opset.greater(ih, iw)
-        if_node = opset.if_op(condition.output(0))
-        if_node.set_then_body(then_body)
-        if_node.set_else_body(else_body)
-        if_node.set_input(input, image_t, image_e)
-        if_node.set_input(iw.output(0), iw_t, iw_e)
-        if_node.set_input(ih.output(0), ih_t, ih_e)
-        cropped_frame = if_node.set_output(then_body_res_1, else_body_res_1)
-
-    elif desired_aspect_ratio < 1:
-        new_width = opset.floor(
-            opset.multiply(
-                opset.convert(ih, destination_type="f32"), desired_aspect_ratio
-            )
-        )
-        offset = opset.unsqueeze(
-            opset.divide(
-                opset.subtract(iw, new_width), opset.constant(2, dtype=np.int32)
-            ),
-            0,
-        )
-        stop = opset.add(offset, new_width)
-        cropped_frame = opset.slice(
-            input, start=offset, stop=stop, step=[1], axes=[w_axis]
-        )
-    elif desired_aspect_ratio > 1:
-        new_hight = opset.floor(
-            opset.multiply(
-                opset.convert(iw, destination_type="f32"), desired_aspect_ratio
-            )
-        )
-        offset = opset.unsqueeze(
-            opset.divide(
-                opset.subtract(ih, new_hight), opset.constant(2, dtype=np.int32)
-            ),
-            0,
-        )
-        stop = opset.add(offset, new_hight)
-        cropped_frame = opset.slice(
-            input, start=offset, stop=stop, step=[1], axes=[h_axis]
-        )
-
-    target_size = list(size)
-    target_size.reverse()
-    resized_image = opset.interpolate(
-        cropped_frame,
-        target_size,
-        scales=np.array([1.0, 1.0], dtype=np.float32),
-        axes=[h_axis, w_axis],
-        mode="linear",
-        shape_calculation_mode="sizes",
-    )
-    return resized_image
-
-
-def resize_image_graph(
-    input: Output, size, keep_aspect_ratio=False, interpolation="linear"
-):
-    h_axis = 1
-    w_axis = 2
-    w, h = size
-
-    target_size = list(size)
-    target_size.reverse()
-
-    if not keep_aspect_ratio:
-        resized_image = opset.interpolate(
-            input,
-            target_size,
-            scales=np.array([1.0, 1.0], dtype=np.float32),
-            axes=[h_axis, w_axis],
-            mode="linear",
-            shape_calculation_mode="sizes",
-        )
-    else:
-        image_shape = opset.shape_of(input, name="shape")
-        iw = opset.convert(
-            opset.gather(image_shape, opset.constant(w_axis), axis=0),
-            destination_type="f32",
-        )
-        ih = opset.convert(
-            opset.gather(image_shape, opset.constant(h_axis), axis=0),
-            destination_type="f32",
-        )
-        w_ratio = opset.divide(np.float32(w), iw)
-        h_ratio = opset.divide(np.float32(h), ih)
-        scale = opset.minimum(w_ratio, h_ratio)
-        resized_image = opset.interpolate(
-            input,
-            target_size,
-            scales=scale,
-            axes=[h_axis, w_axis],
-            mode="linear",
-            shape_calculation_mode="sizes",
-        )
-    return resized_image
-
-
-def resize_image(size, interpolation="linear"):
-    return custom_preprocess_function(
-        partial(resize_image_graph, size=size, interpolation=interpolation)
-    )
-
-
-def resize_image_with_aspect(size, interpolation="linear"):
-    return custom_preprocess_function(
-        partial(
-            resize_image_graph,
-            size=size,
-            keep_aspect_ratio=True,
-            interpolation=interpolation,
-        )
-    )
-
-
-def crop_resize(size, interpolation="linear"):
-    return custom_preprocess_function(partial(crop_resize_graph, size=size))
-
-
-def resize_image_letterbox(size, interpolation="linear"):
-    return custom_preprocess_function(
-        partial(resize_image_letterbox_graph, size=size, interpolation=interpolation)
-    )
```

## visiongraph/external/intel/models/centernet.py

```diff
@@ -19,32 +19,32 @@
 from numpy.lib.stride_tricks import as_strided
 
 from .detection_model import DetectionModel
 from .utils import Detection, clip_detections
 
 
 class CenterNet(DetectionModel):
-    __model__ = "centernet"
+    __model__ = 'centernet'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
         self._check_io_number(1, 3)
         self._output_layer_names = sorted(self.outputs)
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters["resize_type"].update_default_value("standard")
+        parameters['resize_type'].update_default_value('standard')
         return parameters
 
     def postprocess(self, outputs, meta):
         heat = outputs[self._output_layer_names[0]][0]
         reg = outputs[self._output_layer_names[1]][0]
         wh = outputs[self._output_layer_names[2]][0]
-        heat = np.exp(heat) / (1 + np.exp(heat))
+        heat = np.exp(heat)/(1 + np.exp(heat))
         height, width = heat.shape[1:3]
         num_predictions = 100
 
         heat = self._nms(heat)
         scores, inds, clses, ys, xs = self._topk(heat, K=num_predictions)
         reg = self._tranpose_and_gather_feat(reg, inds)
 
@@ -52,36 +52,30 @@
         xs = xs.reshape((num_predictions, 1)) + reg[:, 0:1]
         ys = ys.reshape((num_predictions, 1)) + reg[:, 1:2]
 
         wh = self._tranpose_and_gather_feat(wh, inds)
         wh = wh.reshape((num_predictions, 2))
         clses = clses.reshape((num_predictions, 1))
         scores = scores.reshape((num_predictions, 1))
-        bboxes = np.concatenate(
-            (
-                xs - wh[..., 0:1] / 2,
-                ys - wh[..., 1:2] / 2,
-                xs + wh[..., 0:1] / 2,
-                ys + wh[..., 1:2] / 2,
-            ),
-            axis=1,
-        )
+        bboxes = np.concatenate((xs - wh[..., 0:1] / 2,
+                                 ys - wh[..., 1:2] / 2,
+                                 xs + wh[..., 0:1] / 2,
+                                 ys + wh[..., 1:2] / 2), axis=1)
         detections = np.concatenate((bboxes, scores, clses), axis=1)
         mask = detections[..., 4] >= self.confidence_threshold
         filtered_detections = detections[mask]
-        scale = max(meta["original_shape"])
-        center = np.array(meta["original_shape"][:2]) / 2.0
-        dets = self._transform(
-            filtered_detections, np.flip(center, 0), scale, height, width
-        )
+        scale = max(meta['original_shape'])
+        center = np.array(meta['original_shape'][:2])/2.0
+        dets = self._transform(filtered_detections, np.flip(center, 0), scale, height, width)
         dets = [Detection(x[0], x[1], x[2], x[3], score=x[4], id=x[5]) for x in dets]
-        return clip_detections(dets, meta["original_shape"])
+        return clip_detections(dets, meta['original_shape'])
 
     @staticmethod
     def get_affine_transform(center, scale, rot, output_size, inv=False):
+
         def get_dir(src_point, rot_rad):
             sn, cs = np.sin(rot_rad), np.cos(rot_rad)
             src_result = [0, 0]
             src_result[0] = src_point[0] * cs - src_point[1] * sn
             src_result[1] = src_point[0] * sn + src_point[1] * cs
             return src_result
 
@@ -142,64 +136,53 @@
         topk_xs = (topk_inds % width).astype(np.int32).astype(float)
 
         topk_scores = topk_scores.reshape((-1))
         topk_ind = np.argpartition(topk_scores, -K)[-K:]
         topk_score = topk_scores[topk_ind]
         topk_clses = topk_ind / K
         topk_inds = CenterNet._gather_feat(
-            topk_inds.reshape((-1, 1)), topk_ind
-        ).reshape((K))
-        topk_ys = CenterNet._gather_feat(topk_ys.reshape((-1, 1)), topk_ind).reshape(
-            (K)
-        )
-        topk_xs = CenterNet._gather_feat(topk_xs.reshape((-1, 1)), topk_ind).reshape(
-            (K)
-        )
+            topk_inds.reshape((-1, 1)), topk_ind).reshape((K))
+        topk_ys = CenterNet._gather_feat(topk_ys.reshape((-1, 1)), topk_ind).reshape((K))
+        topk_xs = CenterNet._gather_feat(topk_xs.reshape((-1, 1)), topk_ind).reshape((K))
 
         return topk_score, topk_inds, topk_clses, topk_ys, topk_xs
 
     @staticmethod
     def _nms(heat, kernel=3):
         def max_pool2d(A, kernel_size, padding=1, stride=1):
-            A = np.pad(A, padding, mode="constant")
-            output_shape = (
-                (A.shape[0] - kernel_size) // stride + 1,
-                (A.shape[1] - kernel_size) // stride + 1,
-            )
+            A = np.pad(A, padding, mode='constant')
+            output_shape = ((A.shape[0] - kernel_size)//stride + 1,
+                            (A.shape[1] - kernel_size)//stride + 1)
             kernel_size = (kernel_size, kernel_size)
-            A_w = as_strided(
-                A,
-                shape=output_shape + kernel_size,
-                strides=(stride * A.strides[0], stride * A.strides[1]) + A.strides,
-            )
+            A_w = as_strided(A, shape=output_shape + kernel_size,
+                             strides=(stride*A.strides[0],
+                                      stride*A.strides[1]) + A.strides)
             A_w = A_w.reshape(-1, *kernel_size)
 
             return A_w.max(axis=(1, 2)).reshape(output_shape)
 
         pad = (kernel - 1) // 2
 
         hmax = np.array([max_pool2d(channel, kernel, pad) for channel in heat])
-        keep = hmax == heat
+        keep = (hmax == heat)
         return heat * keep
 
     @staticmethod
     def _transform_preds(coords, center, scale, output_size):
         def affine_transform(pt, t):
-            new_pt = np.array([pt[0], pt[1], 1.0], dtype=np.float32).T
+            new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32).T
             new_pt = np.dot(t, new_pt)
             return new_pt[:2]
 
         target_coords = np.zeros(coords.shape)
         trans = CenterNet.get_affine_transform(center, scale, 0, output_size, inv=True)
         for p in range(coords.shape[0]):
             target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)
         return target_coords
 
     @staticmethod
     def _transform(dets, center, scale, height, width):
         dets[:, :2] = CenterNet._transform_preds(
-            dets[:, 0:2], center, scale, (width, height)
-        )
+            dets[:, 0:2], center, scale, (width, height))
         dets[:, 2:4] = CenterNet._transform_preds(
-            dets[:, 2:4], center, scale, (width, height)
-        )
-        return dets
+            dets[:, 2:4], center, scale, (width, height))
+        return dets
```

## visiongraph/external/intel/models/detection_model.py

```diff
@@ -9,165 +9,119 @@
 
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
-from .image_model import ImageModel
 from .types import ListValue, NumericalValue, StringValue
-from .utils import clip_detections, load_labels
+from .image_model import ImageModel
+from .utils import load_labels, clip_detections
 
 
 class DetectionModel(ImageModel):
-    """An abstract wrapper for object detection model
+    '''An abstract wrapper for object detection model
 
     The DetectionModel must have a single image input.
     It inherits `preprocess` from `ImageModel` wrapper. Also, it defines `_resize_detections` method,
     which should be used in `postprocess`, to clip bounding boxes and resize ones to original image shape.
 
     The `postprocess` method must be implemented in a specific inherited wrapper.
-    """
+    '''
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        """Detection Model constructor
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        '''Detection Model constructor
 
         It extends the `ImageModel` construtor.
 
         Args:
-            inference_adapter (InferenceAdapter): allows working with the specified executor
+            model_adapter (ModelAdapter): allows working with the specified executor
             configuration (dict, optional): it contains values for parameters accepted by specific
               wrapper (`confidence_threshold`, `labels` etc.) which are set as data attributes
             preload (bool, optional): a flag whether the model is loaded to device while
               initialization. If `preload=False`, the model must be loaded via `load` method before inference
 
         Raises:
             WrapperError: if the model has more than 1 image inputs
-        """
+        '''
 
-        super().__init__(inference_adapter, configuration, preload)
+        super().__init__(model_adapter, configuration, preload)
 
         if not self.image_blob_name:
-            self.raise_error(
-                "The Wrapper supports only one image input, but {} found".format(
-                    len(self.image_blob_names)
-                )
-            )
+            self.raise_error("The Wrapper supports only one image input, but {} found".format(
+                len(self.image_blob_names)))
 
         if self.path_to_labels:
             self.labels = load_labels(self.path_to_labels)
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "confidence_threshold": NumericalValue(
-                    default_value=0.5,
-                    description="Probability threshold value for bounding box filtering",
-                ),
-                "labels": ListValue(description="List of class labels"),
-                "path_to_labels": StringValue(
-                    description="Path to file with labels. Overrides the labels, if they sets via 'labels' parameter"
-                ),
-            }
-        )
+        parameters.update({
+            'confidence_threshold': NumericalValue(default_value=0.5, description="Threshold value for detection box confidence"),
+            'labels': ListValue(description="List of class labels"),
+            'path_to_labels': StringValue(
+                description="Path to file with labels. Overrides the labels, if they sets via 'labels' parameter"
+            )
+        })
 
         return parameters
 
     def _resize_detections(self, detections, meta):
-        """Resizes detection bounding boxes according to initial image shape.
+        '''Resizes detection bounding boxes according to initial image shape.
 
         It implements image resizing depending on the set `resize_type`(see `ImageModel` for details).
         Next, it applies bounding boxes clipping.
 
         Args:
             detections (List[Detection]): list of detections with coordinates in normalized form
             meta (dict): the input metadata obtained from `preprocess` method
 
         Returns:
             - list of detections with resized and clipped coordinates fit to initial image
 
         Raises:
             WrapperError: If the model uses custom resize or `resize_type` is not set
-        """
-        resized_shape = meta["resized_shape"]
-        original_shape = meta["original_shape"]
-
-        if self.resize_type == "fit_to_window_letterbox":
-            detections = resize_detections_letterbox(
-                detections, original_shape[1::-1], resized_shape[1::-1]
-            )
-        elif self.resize_type == "fit_to_window":
-            detections = resize_detections_with_aspect_ratio(
-                detections,
-                original_shape[1::-1],
-                resized_shape[1::-1],
-                (self.w, self.h),
-            )
-        elif self.resize_type == "standard":
+        '''
+        resized_shape = meta['resized_shape']
+        original_shape = meta['original_shape']
+
+        if self.resize_type == 'fit_to_window_letterbox':
+            detections = resize_detections_letterbox(detections, original_shape[1::-1], resized_shape[1::-1])
+        elif self.resize_type == 'fit_to_window':
+            detections = resize_detections_with_aspect_ratio(detections, original_shape[1::-1], resized_shape[1::-1], (self.w, self.h))
+        elif self.resize_type == 'standard':
             detections = resize_detections(detections, original_shape[1::-1])
         else:
-            self.raise_error("Unknown resize type {}".format(self.resize_type))
+            self.raise_error('Unknown resize type {}'.format(self.resize_type))
         return clip_detections(detections, original_shape)
 
-    def _add_label_names(self, detections):
-        """Adds labels names to detections if they are available
-
-        Args:
-            detections (List[Detection]): list of detections with coordinates in normalized form
-
-        Returns:
-            - list of detections with label strings
-        """
-        if self.labels is None:
-            for detection in detections:
-                detection.str_label = f"#{detection.id}"
-        else:
-            for detection in detections:
-                detection.str_label = self.labels[detection.id]
-        return detections
-
 
 def resize_detections(detections, original_image_size):
     for detection in detections:
         detection.xmin *= original_image_size[0]
         detection.xmax *= original_image_size[0]
         detection.ymin *= original_image_size[1]
         detection.ymax *= original_image_size[1]
     return detections
 
-
-def resize_detections_with_aspect_ratio(
-    detections, original_image_size, resized_image_size, model_input_size
-):
+def resize_detections_with_aspect_ratio(detections, original_image_size, resized_image_size, model_input_size):
     scale_x = model_input_size[0] / resized_image_size[0] * original_image_size[0]
     scale_y = model_input_size[1] / resized_image_size[1] * original_image_size[1]
     for detection in detections:
         detection.xmin *= scale_x
         detection.xmax *= scale_x
         detection.ymin *= scale_y
         detection.ymax *= scale_y
     return detections
 
-
 def resize_detections_letterbox(detections, original_image_size, resized_image_size):
-    inverted_scale = max(
-        original_image_size[0] / resized_image_size[0],
-        original_image_size[1] / resized_image_size[1],
-    )
-    pad_left = (resized_image_size[0] - original_image_size[0] / inverted_scale) // 2
-    pad_top = (resized_image_size[1] - original_image_size[1] / inverted_scale) // 2
+    scales = [x / y for x, y in zip(resized_image_size, original_image_size)]
+    scale = min(scales)
+    scales = (scale / scales[0], scale / scales[1])
+    offset = [0.5 * (1 - x) for x in scales]
     for detection in detections:
-        detection.xmin = (
-            detection.xmin * resized_image_size[0] - pad_left
-        ) * inverted_scale
-        detection.ymin = (
-            detection.ymin * resized_image_size[1] - pad_top
-        ) * inverted_scale
-        detection.xmax = (
-            detection.xmax * resized_image_size[0] - pad_left
-        ) * inverted_scale
-        detection.ymax = (
-            detection.ymax * resized_image_size[1] - pad_top
-        ) * inverted_scale
-    return detections
+        detection.xmin = ((detection.xmin - offset[0]) / scales[0]) * original_image_size[0]
+        detection.xmax = ((detection.xmax - offset[0]) / scales[0]) * original_image_size[0]
+        detection.ymin = ((detection.ymin - offset[1]) / scales[1]) * original_image_size[1]
+        detection.ymax = ((detection.ymax - offset[1]) / scales[1]) * original_image_size[1]
+    return detections
```

## visiongraph/external/intel/models/detr.py

```diff
@@ -16,50 +16,41 @@
 import numpy as np
 
 from .detection_model import DetectionModel
 from .utils import Detection, softmax
 
 
 class DETR(DetectionModel):
-    __model__ = "DETR"
+    __model__ = 'DETR'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
         self._check_io_number(1, 2)
         self.bboxes_blob_name, self.scores_blob_name = self._get_outputs()
 
     def _get_outputs(self):
-        (bboxes_blob_name, bboxes_layer), (
-            scores_blob_name,
-            scores_layer,
-        ) = self.outputs.items()
+        (bboxes_blob_name, bboxes_layer), (scores_blob_name, scores_layer) = self.outputs.items()
 
         if bboxes_layer.shape[1] != scores_layer.shape[1]:
-            self.raise_error(
-                "Expected the same second dimension for boxes and scores, but got {} and {}".format(
-                    bboxes_layer.shape, scores_layer.shape
-                )
-            )
+            self.raise_error("Expected the same second dimension for boxes and scores, but got {} and {}".format(
+                bboxes_layer.shape, scores_layer.shape))
 
         if bboxes_layer.shape[2] == 4:
             return bboxes_blob_name, scores_blob_name
         elif scores_layer.shape[2] == 4:
             return scores_blob_name, bboxes_blob_name
         else:
-            self.raise_error(
-                "Expected shape [:,:,4] for bboxes output, but got {} and {}".format(
-                    bboxes_layer.shape, scores_layer.shape
-                )
-            )
+            self.raise_error("Expected shape [:,:,4] for bboxes output, but got {} and {}".format(
+                bboxes_layer.shape, scores_layer.shape))
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters["resize_type"].update_default_value("standard")
-        parameters["confidence_threshold"].update_default_value(0.5)
+        parameters['resize_type'].update_default_value('standard')
+        parameters['confidence_threshold'].update_default_value(0.5)
         return parameters
 
     def postprocess(self, outputs, meta):
         detections = self._parse_outputs(outputs)
         detections = self._resize_detections(detections, meta)
         return detections
 
@@ -71,25 +62,17 @@
 
         scores = np.array([softmax(logit) for logit in scores])
         labels = np.argmax(scores[:, :-1], axis=-1)
         det_scores = np.max(scores[:, :-1], axis=-1)
 
         keep = det_scores > self.confidence_threshold
 
-        detections = [
-            Detection(*det)
-            for det in zip(
-                x_mins[keep],
-                y_mins[keep],
-                x_maxs[keep],
-                y_maxs[keep],
-                det_scores[keep],
-                labels[keep],
-            )
-        ]
+        detections = [Detection(*det) for det in zip(x_mins[keep], y_mins[keep], x_maxs[keep], y_maxs[keep],
+                                                     det_scores[keep], labels[keep])]
         return detections
 
     @staticmethod
     def box_cxcywh_to_xyxy(box):
         x_c, y_c, w, h = box.T
-        b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]
-        return b
+        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),
+             (x_c + 0.5 * w), (y_c + 0.5 * h)]
+        return b
```

## visiongraph/external/intel/models/hpe_associative_embedding.py

```diff
@@ -13,64 +13,47 @@
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
 import numpy as np
 from scipy.optimize import linear_sum_assignment
 
+
+
 from .image_model import ImageModel
 from .types import NumericalValue, StringValue
 from .utils import resize_image
 
 
 class HpeAssociativeEmbedding(ImageModel):
-    __model__ = "HPE-assosiative-embedding"
+    __model__ = 'HPE-assosiative-embedding'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload=False)
-        self.heatmaps_blob_name = find_layer_by_name("heatmaps", self.outputs)
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload=False)
+        self.heatmaps_blob_name = find_layer_by_name('heatmaps', self.outputs)
         try:
-            self.nms_heatmaps_blob_name = find_layer_by_name(
-                "nms_heatmaps", self.outputs
-            )
+            self.nms_heatmaps_blob_name = find_layer_by_name('nms_heatmaps', self.outputs)
         except ValueError:
             self.nms_heatmaps_blob_name = self.heatmaps_blob_name
-        self.embeddings_blob_name = find_layer_by_name("embeddings", self.outputs)
+        self.embeddings_blob_name = find_layer_by_name('embeddings', self.outputs)
         self.output_scale = self.w / self.outputs[self.heatmaps_blob_name].shape[-1]
 
         if self.target_size is None:
             self.target_size = min(self.h, self.w)
         self.index_of_max_dimension = 0
         if self.aspect_ratio >= 1.0:  # img width >= height
-            input_height, input_width = self.target_size, round(
-                self.target_size * self.aspect_ratio
-            )
+            input_height, input_width = self.target_size, round(self.target_size * self.aspect_ratio)
             self.index_of_max_dimension = 1
         else:
-            input_height, input_width = (
-                round(self.target_size / self.aspect_ratio),
-                self.target_size,
-            )
-        self.h = (
-            (input_height + self.size_divisor - 1)
-            // self.size_divisor
-            * self.size_divisor
-        )
-        self.w = (
-            (input_width + self.size_divisor - 1)
-            // self.size_divisor
-            * self.size_divisor
-        )
+            input_height, input_width = round(self.target_size / self.aspect_ratio), self.target_size
+        self.h = (input_height + self.size_divisor - 1) // self.size_divisor * self.size_divisor
+        self.w = (input_width + self.size_divisor - 1) // self.size_divisor * self.size_divisor
         default_input_shape = self.inputs[self.image_blob_name].shape
         input_shape = {self.image_blob_name: [self.n, self.c, self.h, self.w]}
-        self.logger.debug(
-            "\tReshape model from {} to {}".format(
-                default_input_shape, input_shape[self.image_blob_name]
-            )
-        )
+        self.logger.debug('\tReshape model from {} to {}'.format(default_input_shape, input_shape[self.image_blob_name]))
         super().reshape(input_shape)
 
         if preload:
             self.load()
 
         self.decoder = AssociativeEmbeddingDecoder(
             num_joints=self.outputs[self.heatmaps_blob_name].shape[1],
@@ -79,102 +62,69 @@
             delta=self.delta,
             max_num_people=30,
             detection_threshold=0.1,
             tag_threshold=1,
             pose_threshold=self.confidence_threshold,
             use_detection_val=True,
             ignore_too_much=False,
-            dist_reweight=True,
-        )
+            dist_reweight=True)
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "target_size": NumericalValue(
-                    value_type=int,
-                    min=1,
-                    description="Image resolution which is going to be processed. Reshapes network to match a given size",
-                ),
-                "aspect_ratio": NumericalValue(
-                    description="Image aspect ratio which is going to be processed. Reshapes network to match a given size"
-                ),
-                "confidence_threshold": NumericalValue(
-                    description="Pose confidence threshold"
-                ),
-                "delta": NumericalValue(default_value=0.0),
-                "size_divisor": NumericalValue(
-                    default_value=32,
-                    value_type=int,
-                    description="Width and height of the rehaped model will be a multiple of this value",
-                ),
-                "padding_mode": StringValue(
-                    default_value="right_bottom", choices=("center", "right_bottom")
-                ),
-            }
-        )
+        parameters.update({
+            'target_size': NumericalValue(value_type=int, min=1),
+            'aspect_ratio': NumericalValue(),
+            'confidence_threshold': NumericalValue(),
+            'delta': NumericalValue(default_value=0.0),
+            'size_divisor': NumericalValue(default_value=32, value_type=int),
+            'padding_mode': StringValue(default_value='right_bottom', choices=('center', 'right_bottom')),
+        })
         return parameters
 
     def preprocess(self, inputs):
         img = resize_image(inputs, (self.w, self.h), keep_aspect_ratio=True)
         h, w = img.shape[:2]
-        if not (
-            self.h - self.size_divisor < h <= self.h
-            and self.w - self.size_divisor < w <= self.w
-        ):
-            self.logger.warning(
-                "\tChosen model aspect ratio doesn't match image aspect ratio"
-            )
-        resize_img_scale = np.array(
-            (inputs.shape[1] / w, inputs.shape[0] / h), np.float32
-        )
-
-        if self.padding_mode == "center":
-            pad = (
-                (self.h - h + 1) // 2,
-                (self.h - h) // 2,
-                (self.w - w + 1) // 2,
-                (self.w - w) // 2,
-            )
+        if not (self.h - self.size_divisor < h <= self.h and self.w - self.size_divisor < w <= self.w):
+            self.logger.warning("\tChosen model aspect ratio doesn't match image aspect ratio")
+        resize_img_scale = np.array((inputs.shape[1] / w, inputs.shape[0] / h), np.float32)
+
+        if self.padding_mode == 'center':
+            pad = ((self.h - h + 1) // 2, (self.h - h) // 2, (self.w - w + 1) // 2, (self.w - w) // 2)
         else:
             pad = (0, self.h - h, 0, self.w - w)
-        img = np.pad(
-            img, (pad[:2], pad[2:], (0, 0)), mode="constant", constant_values=0
-        )
+        img = np.pad(img, (pad[:2], pad[2:], (0, 0)), mode='constant', constant_values=0)
         img = img.transpose((2, 0, 1))  # Change data layout from HWC to CHW
         img = img[None]
-        meta = {"original_size": inputs.shape[:2], "resize_img_scale": resize_img_scale}
+        meta = {
+            'original_size': inputs.shape[:2],
+            'resize_img_scale': resize_img_scale
+        }
         return {self.image_blob_name: img}, meta
 
     def postprocess(self, outputs, meta):
         heatmaps = outputs[self.heatmaps_blob_name]
         nms_heatmaps = outputs[self.nms_heatmaps_blob_name]
         aembds = outputs[self.embeddings_blob_name]
         poses, scores = self.decoder(heatmaps, aembds, nms_heatmaps=nms_heatmaps)
         # Rescale poses to the original image.
-        if self.padding_mode == "center":
-            scale = meta["resize_img_scale"][self.index_of_max_dimension]
+        if self.padding_mode == 'center':
+            scale = meta['resize_img_scale'][self.index_of_max_dimension]
             poses[:, :, :2] *= scale * self.output_scale
-            shift = (
-                meta["original_size"][self.index_of_max_dimension]
-                - max(self.h, self.w) * scale
-            ) / 2
+            shift = (meta['original_size'][self.index_of_max_dimension] - max(self.h, self.w) * scale) / 2
             poses[:, :, 1 - self.index_of_max_dimension] += shift
         else:
-            poses[:, :, :2] *= meta["resize_img_scale"] * self.output_scale
+            poses[:, :, :2] *= meta['resize_img_scale'] * self.output_scale
         return poses, scores
 
 
 def find_layer_by_name(name, layers):
     suitable_layers = []
     for layer, metadata in layers.items():
-        count_names = len(
-            [layer_name for layer_name in metadata.names if layer_name.startswith(name)]
-        )
+        count_names = len([layer_name for layer_name in metadata.names if layer_name.startswith(name)])
         if count_names > 0:
             suitable_layers.append(layer)
     if not suitable_layers:
         raise ValueError('Suitable layer for "{}" output is not found'.format(name))
 
     if len(suitable_layers) > 1:
         raise ValueError('More than 1 layer matched to "{}" output'.format(name))
@@ -210,57 +160,28 @@
     def center(self):
         if self.valid_points_num > 0:
             return self.c
         return None
 
 
 class AssociativeEmbeddingDecoder:
-    def __init__(
-        self,
-        num_joints,
-        max_num_people,
-        detection_threshold,
-        use_detection_val,
-        ignore_too_much,
-        tag_threshold,
-        pose_threshold,
-        adjust=True,
-        refine=True,
-        delta=0.0,
-        joints_order=None,
-        dist_reweight=True,
-    ):
+    def __init__(self, num_joints, max_num_people, detection_threshold, use_detection_val,
+                 ignore_too_much, tag_threshold, pose_threshold,
+                 adjust=True, refine=True, delta=0.0, joints_order=None,
+                 dist_reweight=True):
         self.num_joints = num_joints
         self.max_num_people = max_num_people
         self.detection_threshold = detection_threshold
         self.tag_threshold = tag_threshold
         self.pose_threshold = pose_threshold
         self.use_detection_val = use_detection_val
         self.ignore_too_much = ignore_too_much
 
         if self.num_joints == 17 and joints_order is None:
-            self.joint_order = (
-                0,
-                1,
-                2,
-                3,
-                4,
-                5,
-                6,
-                11,
-                12,
-                7,
-                8,
-                9,
-                10,
-                13,
-                14,
-                15,
-                16,
-            )
+            self.joint_order = (0, 1, 2, 3, 4, 5, 6, 11, 12, 7, 8, 9, 10, 13, 14, 15, 16)
         else:
             self.joint_order = list(np.arange(self.num_joints))
 
         self.do_adjust = adjust
         self.do_refine = refine
         self.dist_reweight = dist_reweight
         self.delta = delta
@@ -286,83 +207,63 @@
             if len(poses) == 0:
                 for tag, joint in zip(tags, joints):
                     pose = Pose(self.num_joints, embd_size)
                     pose.add(idx, joint, tag)
                     poses.append(pose)
                 continue
 
-            if joints.shape[0] == 0 or (
-                self.ignore_too_much and len(poses) == self.max_num_people
-            ):
+            if joints.shape[0] == 0 or (self.ignore_too_much and len(poses) == self.max_num_people):
                 continue
 
             poses_tags = np.stack([p.tag for p in poses], axis=0)
             diff = tags[:, None] - poses_tags[None, :]
             diff_normed = np.linalg.norm(diff, ord=2, axis=2)
             diff_saved = np.copy(diff_normed)
 
             if self.dist_reweight:
                 # Reweight cost matrix to prefer nearby points among all that are close enough in a tag space.
                 centers = np.stack([p.center for p in poses], axis=0)[None]
-                dists = np.linalg.norm(
-                    joints[:, :2][:, None, :] - centers, ord=2, axis=2
-                )
+                dists = np.linalg.norm(joints[:, :2][:, None, :] - centers, ord=2, axis=2)
                 close_tags_masks = diff_normed < self.tag_threshold
                 min_dists = np.min(dists, axis=0, keepdims=True)
                 dists /= min_dists + 1e-10
                 diff_normed[close_tags_masks] *= dists[close_tags_masks]
 
             if self.use_detection_val:
                 diff_normed = np.round(diff_normed) * 100 - joints[:, 2:3]
             num_added = diff.shape[0]
             num_grouped = diff.shape[1]
             if num_added > num_grouped:
-                diff_normed = np.pad(
-                    diff_normed,
-                    ((0, 0), (0, num_added - num_grouped)),
-                    mode="constant",
-                    constant_values=1e10,
-                )
+                diff_normed = np.pad(diff_normed, ((0, 0), (0, num_added - num_grouped)),
+                                     mode='constant', constant_values=1e10)
 
             pairs = self._max_match(diff_normed)
             for row, col in pairs:
-                if (
-                    row < num_added
-                    and col < num_grouped
-                    and diff_saved[row][col] < self.tag_threshold
-                ):
+                if row < num_added and col < num_grouped and diff_saved[row][col] < self.tag_threshold:
                     poses[col].add(idx, joints[row], tags[row])
                 else:
                     pose = Pose(self.num_joints, embd_size)
                     pose.add(idx, joints[row], tags[row])
                     poses.append(pose)
 
-        ans = np.asarray([p.pose for p in poses], dtype=np.float32).reshape(
-            -1, self.num_joints, 2 + 1 + embd_size
-        )
-        tags = np.asarray([p.tag for p in poses], dtype=np.float32).reshape(
-            -1, embd_size
-        )
+        ans = np.asarray([p.pose for p in poses], dtype=np.float32).reshape(-1, self.num_joints, 2 + 1 + embd_size)
+        tags = np.asarray([p.tag for p in poses], dtype=np.float32).reshape(-1, embd_size)
         return ans, tags
 
     def top_k(self, heatmaps, tags):
         N, K, H, W = heatmaps.shape
         heatmaps = heatmaps.reshape(N, K, -1)
-        ind = heatmaps.argpartition(-self.max_num_people, axis=2)[
-            :, :, -self.max_num_people :
-        ]
+        ind = heatmaps.argpartition(-self.max_num_people, axis=2)[:, :, -self.max_num_people:]
         val_k = np.take_along_axis(heatmaps, ind, axis=2)
         subind = np.argsort(-val_k, axis=2)
         ind = np.take_along_axis(ind, subind, axis=2)
         val_k = np.take_along_axis(val_k, subind, axis=2)
 
         tags = tags.reshape(N, K, W * H, -1)
-        tag_k = [
-            np.take_along_axis(tags[..., i], ind, axis=2) for i in range(tags.shape[3])
-        ]
+        tag_k = [np.take_along_axis(tags[..., i], ind, axis=2) for i in range(tags.shape[3])]
         tag_k = np.stack(tag_k, axis=3)
 
         x = ind % W
         y = ind // W
         loc_k = np.stack((x, y), axis=3)
         return tag_k, loc_k, val_k
 
@@ -372,21 +273,19 @@
         for batch_idx, people in enumerate(ans):
             for person in people:
                 for k, joint in enumerate(person):
                     heatmap = heatmaps[batch_idx, k]
                     px = int(joint[0])
                     py = int(joint[1])
                     if 1 < px < W - 1 and 1 < py < H - 1:
-                        diff = np.array(
-                            [
-                                heatmap[py, px + 1] - heatmap[py, px - 1],
-                                heatmap[py + 1, px] - heatmap[py - 1, px],
-                            ]
-                        )
-                        joint[:2] += np.sign(diff) * 0.25
+                        diff = np.array([
+                            heatmap[py, px + 1] - heatmap[py, px - 1],
+                            heatmap[py + 1, px] - heatmap[py - 1, px]
+                        ])
+                        joint[:2] += np.sign(diff) * .25
         return ans
 
     @staticmethod
     def refine(heatmap, tag, keypoints, pose_tag=None):
         K, H, W = heatmap.shape
         if len(tag.shape) == 3:
             tag = tag[..., None]
@@ -411,29 +310,25 @@
             idx = diff.argmin()
             y, x = np.divmod(idx, _heatmap.shape[-1])
             # Corresponding keypoint detection score.
             val = _heatmap[y, x]
             if val > 0:
                 keypoints[i, :3] = x, y, val
                 if 1 < x < W - 1 and 1 < y < H - 1:
-                    diff = np.array(
-                        [
-                            _heatmap[y, x + 1] - _heatmap[y, x - 1],
-                            _heatmap[y + 1, x] - _heatmap[y - 1, x],
-                        ]
-                    )
-                    keypoints[i, :2] += np.sign(diff) * 0.25
+                    diff = np.array([
+                        _heatmap[y, x + 1] - _heatmap[y, x - 1],
+                        _heatmap[y + 1, x] - _heatmap[y - 1, x]
+                    ])
+                    keypoints[i, :2] += np.sign(diff) * .25
 
         return keypoints
 
     def __call__(self, heatmaps, tags, nms_heatmaps):
         tag_k, loc_k, val_k = self.top_k(nms_heatmaps, tags)
-        ans = tuple(
-            map(self._match_by_tag, zip(tag_k, loc_k, val_k))
-        )  # Call _match_by_tag() for each element in batch
+        ans = tuple(map(self._match_by_tag, zip(tag_k, loc_k, val_k)))  # Call _match_by_tag() for each element in batch
         ans, ans_tags = map(list, zip(*ans))
 
         np.abs(heatmaps, out=heatmaps)
 
         if self.do_adjust:
             ans = self.adjust(ans, heatmaps)
 
@@ -451,8 +346,8 @@
 
         if self.do_refine:
             heatmap_numpy = heatmaps[0]
             tag_numpy = tags[0]
             for i, pose in enumerate(ans):
                 ans[i] = self.refine(heatmap_numpy, tag_numpy, pose, ans_tags[0][i])
 
-        return ans, scores
+        return ans, scores
```

## visiongraph/external/intel/models/image_model.py

```diff
@@ -10,23 +10,21 @@
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
-import numpy as np
-
 from .model import Model
 from .types import BooleanValue, ListValue, StringValue
-from .utils import RESIZE_TYPES, InputTransform, pad_image
+from .utils import RESIZE_TYPES, pad_image, InputTransform
 
 
 class ImageModel(Model):
-    """An abstract wrapper for an image-based model
+    '''An abstract wrapper for an image-based model
 
     The ImageModel has 1 or more inputs with images - 4D tensors with NHWC or NCHW layout.
     It may support additional inputs - 2D tensors.
 
     The ImageModel implements basic preprocessing for an image provided as model input.
     See `preprocess` description.
 
@@ -36,114 +34,87 @@
         image_blob_names (List[str]): names of all image-like inputs (4D tensors)
         image_info_blob_names (List[str]): names of all secondary inputs (2D tensors)
         image_blob_name (str): name of the first image input
         nchw_layout (bool): a flag whether the model input layer has NCHW layout
         resize_type (str): the type for image resizing (see `RESIZE_TYPE` for info)
         resize (function): resizing function corresponding to the `resize_type`
         input_transform (InputTransform): instance of the `InputTransform` for image normalization
-    """
+    '''
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        """Image model constructor
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        '''Image model constructor
 
         It extends the `Model` constructor.
 
         Args:
-            inference_adapter (InferenceAdapter): allows working with the specified executor
+            model_adapter (ModelAdapter): allows working with the specified executor
             configuration (dict, optional): it contains values for parameters accepted by specific
               wrapper (`confidence_threshold`, `labels` etc.) which are set as data attributes
             preload (bool, optional): a flag whether the model is loaded to device while
               initialization. If `preload=False`, the model must be loaded via `load` method before inference
 
         Raises:
             WrapperError: if the wrapper failed to define appropriate inputs for images
-        """
-        super().__init__(inference_adapter, configuration, preload)
+        '''
+        super().__init__(model_adapter, configuration, preload)
         self.image_blob_names, self.image_info_blob_names = self._get_inputs()
         self.image_blob_name = self.image_blob_names[0]
 
-        self.nchw_layout = self.inputs[self.image_blob_name].layout == "NCHW"
+        self.nchw_layout = self.inputs[self.image_blob_name].layout == 'NCHW'
         if self.nchw_layout:
             self.n, self.c, self.h, self.w = self.inputs[self.image_blob_name].shape
         else:
             self.n, self.h, self.w, self.c = self.inputs[self.image_blob_name].shape
         self.resize = RESIZE_TYPES[self.resize_type]
-        self.input_transform = InputTransform(
-            self.reverse_input_channels, self.mean_values, self.scale_values
-        )
-
-        if self.embed_preprocessing:
-            layout = self.inputs[self.image_blob_name].layout
-            inference_adapter.embed_preprocessing(
-                layout=layout,
-                resize_mode=self.resize_type,
-                interpolation_mode="LINEAR",
-                target_shape=(self.w, self.h),
-                brg2rgb=self.reverse_input_channels,
-                mean=self.mean_values,
-                scale=self.scale_values,
-            )
+        self.input_transform = InputTransform(self.reverse_input_channels, self.mean_values, self.scale_values)
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "mean_values": ListValue(
-                    default_value=[],
-                    description="Normalization values, which will be subtracted from image channels for image-input layer during preprocessing",
-                ),
-                "scale_values": ListValue(
-                    default_value=[],
-                    description="Normalization values, which will divide the image channels for image-input layer",
-                ),
-                "reverse_input_channels": BooleanValue(
-                    default_value=False, description="Reverse the input channel order"
-                ),
-                "resize_type": StringValue(
-                    default_value="standard",
-                    choices=tuple(RESIZE_TYPES.keys()),
-                    description="Type of input image resizing",
-                ),
-                "embed_preprocessing": BooleanValue(
-                    default_value=False,
-                    description="Whether to embed preprocessing into the model",
-                ),
-            }
-        )
+        parameters.update({
+            'mean_values': ListValue(
+                default_value=None,
+                description='Normalization values, which will be subtracted from image channels for image-input layer during preprocessing'
+            ),
+            'scale_values': ListValue(
+                default_value=None,
+                description='Normalization values, which will divide the image channels for image-input layer'
+            ),
+            'reverse_input_channels': BooleanValue(default_value=False, description='Reverse the channel order'),
+            'resize_type': StringValue(
+                default_value='standard', choices=tuple(RESIZE_TYPES.keys()),
+                description="Type of input image resizing"
+            ),
+        })
         return parameters
 
     def _get_inputs(self):
-        """Defines the model inputs for images and additional info.
+        '''Defines the model inputs for images and additional info.
 
         Raises:
             WrapperError: if the wrapper failed to define appropriate inputs for images
 
         Returns:
             - list of inputs names for images
             - list of inputs names for additional info
-        """
+        '''
         image_blob_names, image_info_blob_names = [], []
         for name, metadata in self.inputs.items():
             if len(metadata.shape) == 4:
                 image_blob_names.append(name)
             elif len(metadata.shape) == 2:
                 image_info_blob_names.append(name)
             else:
-                self.raise_error(
-                    "Failed to identify the input for ImageModel: only 2D and 4D input layer supported"
-                )
+                self.raise_error('Failed to identify the input for ImageModel: only 2D and 4D input layer supported')
         if not image_blob_names:
-            self.raise_error(
-                "Failed to identify the input for the image: no 4D input layer found"
-            )
+            self.raise_error('Failed to identify the input for the image: no 4D input layer found')
         return image_blob_names, image_info_blob_names
 
     def preprocess(self, inputs):
-        """Data preprocess method
+        '''Data preprocess method
 
         It performs basic preprocessing of a single image:
             - Resizes the image to fit the model input size via the defined resize type
             - Normalizes the image: subtracts means, divides by scales, switch channels BGR-RGB
             - Changes the image layout according to the model input layout
 
         Also, it keeps the size of original image and resized one as `original_shape` and `resized_shape`
@@ -158,43 +129,35 @@
 
         Returns:
             - the preprocessed image in the following format:
                 {
                     'input_layer_name': preprocessed_image
                 }
             - the input metadata, which might be used in `postprocess` method
-        """
+        '''
         image = inputs
-        dict_inputs = {}
-        meta = {"original_shape": image.shape}
-
-        if self.embed_preprocessing:
-            meta.update({"resized_shape": (self.w, self.h, self.c)})
-
-            dict_inputs = {self.image_blob_name: np.expand_dims(image, axis=0)}
-        else:
-            resized_image = self.resize(image, (self.w, self.h))
-            meta.update({"resized_shape": resized_image.shape})
-            if self.resize_type == "fit_to_window":
-                resized_image = pad_image(resized_image, (self.w, self.h))
-                meta.update({"padded_shape": resized_image.shape})
-            resized_image = self.input_transform(resized_image)
-            resized_image = self._change_layout(resized_image)
-            dict_inputs = {self.image_blob_name: resized_image}
-
+        meta = {'original_shape': image.shape}
+        resized_image = self.resize(image, (self.w, self.h))
+        meta.update({'resized_shape': resized_image.shape})
+        if self.resize_type == 'fit_to_window':
+            resized_image = pad_image(resized_image, (self.w, self.h))
+            meta.update({'padded_shape': resized_image.shape})
+        resized_image = self.input_transform(resized_image)
+        resized_image = self._change_layout(resized_image)
+        dict_inputs = {self.image_blob_name: resized_image}
         return dict_inputs, meta
 
     def _change_layout(self, image):
-        """Changes the input image layout to fit the layout of the model input layer.
+        '''Changes the input image layout to fit the layout of the model input layer.
 
         Args:
             inputs (ndarray): a single image as 3D array in HWC layout
 
         Returns:
             - the image with layout aligned with the model layout
-        """
+        '''
         if self.nchw_layout:
             image = image.transpose((2, 0, 1))  # HWC->CHW
             image = image.reshape((1, self.c, self.h, self.w))
         else:
             image = image.reshape((1, self.h, self.w, self.c))
-        return image
+        return image
```

## visiongraph/external/intel/models/model.py

```diff
@@ -11,37 +11,27 @@
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
 import logging as log
-import re
-
-from ..adapters.inference_adapter import InferenceAdapter
-from ..adapters.openvino_adapter import (
-    OpenvinoAdapter,
-    create_core,
-    get_user_config,
-)
-from ..adapters.ovms_adapter import OVMSAdapter
 
 
 class WrapperError(RuntimeError):
-    """Special class for errors occurred in Model API wrappers"""
-
+    '''Special class for errors occurred in Model API wrappers'''
     def __init__(self, wrapper_name, message):
         super().__init__(f"{wrapper_name}: {message}")
 
 
 class Model:
-    """An abstract model wrapper
+    '''An abstract model wrapper
 
     The abstract model wrapper is free from any executor dependencies.
-    It sets the `InferenceAdapter` instance with the provided model
+    It sets the `ModelAdapter` instance with the provided model
     and defines model inputs/outputs.
 
     Next, it loads the provided configuration variables and sets it as wrapper attributes.
     The keys of the configuration dictionary should be presented in the `parameters` method.
 
     Also, it decorates the following adapter interface:
         - Loading the model to the device
@@ -49,182 +39,98 @@
         - Synchronous model inference
         - Asynchronous model inference
 
     The `preprocess` and `postprocess` methods must be implemented in a specific inherited wrapper.
 
     Attributes:
         logger (Logger): instance of the Logger
-        inference_adapter (InferenceAdapter): allows working with the specified executor
+        model_adapter (ModelAdapter): allows working with the specified executor
         inputs (dict): keeps the model inputs names and `Metadata` structure for each one
         outputs (dict): keeps the model outputs names and `Metadata` structure for each one
         model_loaded (bool): a flag whether the model is loaded to device
-    """
+    '''
 
-    __model__ = None  # Abstract wrapper has no name
+    __model__ = None # Abstract wrapper has no name
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        """Model constructor
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        '''Model constructor
 
         Args:
-            inference_adapter (InferenceAdapter): allows working with the specified executor
+            model_adapter (ModelAdapter): allows working with the specified executor
             configuration (dict, optional): it contains values for parameters accepted by specific
               wrapper (`confidence_threshold`, `labels` etc.) which are set as data attributes
             preload (bool, optional): a flag whether the model is loaded to device while
               initialization. If `preload=False`, the model must be loaded via `load` method before inference
 
         Raises:
             WrapperError: if the wrapper configuration is incorrect
-        """
+        '''
         self.logger = log.getLogger()
-        self.inference_adapter = inference_adapter
-        self.inputs = self.inference_adapter.get_input_layers()
-        self.outputs = self.inference_adapter.get_output_layers()
+        self.model_adapter = model_adapter
+        self.inputs = self.model_adapter.get_input_layers()
+        self.outputs = self.model_adapter.get_output_layers()
         for name, parameter in self.parameters().items():
             self.__setattr__(name, parameter.default_value)
         self._load_config(configuration)
         self.model_loaded = False
         if preload:
             self.load()
 
-    def get_model(self):
-        """Returns the ov.Model object stored in the InferenceAdapter.
-
-        Note: valid only for local inference
-
-        Returns:
-            ov.Model object
-        Raises:
-            RuntimeError: in case of remote inference (serving)
-        """
-        if isinstance(self.inference_adapter, OpenvinoAdapter):
-            return self.inference_adapter.get_model()
-
-        raise RuntimeError("get_model() is not supported for remote inference")
-
     @classmethod
-    def _get_model_class(cls, name):
-        subclasses = [
-            subclass for subclass in cls.get_subclasses() if subclass.__model__
-        ]
+    def get_model(cls, name):
+        subclasses = [subclass for subclass in cls.get_subclasses() if subclass.__model__]
         if cls.__model__:
             subclasses.append(cls)
         for subclass in subclasses:
             if name.lower() == subclass.__model__.lower():
                 return subclass
-        cls.raise_error(
-            'There is no model with name "{}" in list: {}'.format(
-                name, ", ".join([subclass.__model__ for subclass in subclasses])
-            )
-        )
+        cls.raise_error('There is no model with name "{}" in list: {}'.format(
+            name, ', '.join([subclass.__model__ for subclass in subclasses])))
 
     @classmethod
-    def create_model(
-        cls,
-        model,
-        model_type=None,
-        configuration={},
-        preload=True,
-        core=None,
-        weights_path="",
-        adaptor_parameters={},
-        device="AUTO",
-        nstreams="1",
-        nthreads=None,
-        max_num_requests=0,
-        precision="FP16",
-        download_dir=None,
-        cache_dir=None,
-    ):
-        """
-        Create an instance of the Model API model
-
-        Args:
-            model (str): model name from OpenVINO Model Zoo, path to model, OVMS URL
-            configuration (:obj:`dict`, optional): dictionary of model config with model properties, for example confidence_threshold, labels
-            model_type (:obj:`str`, optional): name of model wrapper to create (e.g. "ssd")
-            preload (:obj:`bool`, optional): whether to call load_model(). Can be set to false to reshape model before loading
-            core (optional): openvino.runtime.Core instance, passed to OpenvinoAdapter
-            weights_path (:obj:`str`, optional): path to .bin file with model weights
-            adaptor_parameters (:obj:`dict`, optional): parameters of ModelAdaptor
-            device (:obj:`str`, optional): name of OpenVINO device (e.g. "CPU, GPU")
-            nstreams (:obj:`int`, optional): number of inference streams
-            nthreads (:obj:`int`, optional): number of threads to use for inference on CPU
-            max_num_requests (:obj:`int`, optional): number of infer requests for asynchronous inference
-            precision (:obj:`str`, optional): inference precision (e.g. "FP16")
-            download_dir (:obj:`str`, optional): directory where to store downloaded models
-            cache_dir (:obj:`str`, optional): directory where to store compiled models to reduce the load time before the inference
-
-        Returns:
-            Model objcet
-        """
-        if isinstance(model, InferenceAdapter):
-            inference_adapter = model
-        elif isinstance(model, str) and re.compile(
-            r"(\w+\.*\-*)*\w+:\d+\/models\/[a-zA-Z0-9_-]+(\:\d+)*"
-        ).fullmatch(model):
-            inference_adapter = OVMSAdapter(model)
-        else:
-            if core is None:
-                core = create_core()
-                plugin_config = get_user_config(device, nstreams, nthreads)
-            inference_adapter = OpenvinoAdapter(
-                core=core,
-                model=model,
-                weights_path=weights_path,
-                model_parameters=adaptor_parameters,
-                device=device,
-                plugin_config=plugin_config,
-                max_num_requests=max_num_requests,
-                precision=precision,
-                download_dir=download_dir,
-                cache_dir=cache_dir,
-            )
-        if model_type is None:
-            model_type = inference_adapter.get_rt_info(["model_info", "model_type"])
-        Model = cls._get_model_class(model_type)
-        return Model(inference_adapter, configuration, preload)
+    def create_model(cls, name, model_adapter, configuration=None, preload=False):
+        Model = cls.get_model(name)
+        return Model(model_adapter, configuration, preload)
 
     @classmethod
     def get_subclasses(cls):
         all_subclasses = []
         for subclass in cls.__subclasses__():
             all_subclasses.append(subclass)
             all_subclasses.extend(subclass.get_subclasses())
         return all_subclasses
 
     @classmethod
     def available_wrappers(cls):
         available_classes = [cls] if cls.__model__ else []
         available_classes.extend(cls.get_subclasses())
-        return [
-            subclass.__model__ for subclass in available_classes if subclass.__model__
-        ]
+        return [subclass.__model__ for subclass in available_classes if subclass.__model__]
 
     @classmethod
     def parameters(cls):
-        """Defines the description and type of configurable data parameters for the wrapper.
+        '''Defines the description and type of configurable data parameters for the wrapper.
 
         See `types.py` to find available types of the data parameter. For each parameter
         the type, default value and description must be provided.
 
         The example of possible data parameter:
             'confidence_threshold': NumericalValue(
                 default_value=0.5, description="Threshold value for detection box confidence"
             )
 
         The method must be implemented in each specific inherited wrapper.
 
         Returns:
             - the dictionary with defined wrapper data parameters
-        """
+        '''
         parameters = {}
         return parameters
 
     def _load_config(self, config):
-        """Reads the configuration and creates data attributes
+        '''Reads the configuration and creates data attributes
            by setting the wrapper parameters with values from configuration.
 
         Args:
             config (dict): the dictionary with keys to be set as data attributes
               and its values. The example of the config is the following:
               {
                   'confidence_threshold': 0.5,
@@ -234,255 +140,164 @@
         Note:
             The config keys should be provided in `parameters` method for each wrapper,
             then the default value of the parameter will be updated. If some key presented
             in the config is not introduced in `parameters`, it will be omitted.
 
          Raises:
             WrapperError: if the configuration is incorrect
-        """
+        '''
+        if config is None: return
         parameters = self.parameters()
-        for name, param in parameters.items():
-            try:
-                str_val = self.inference_adapter.get_rt_info(["model_info", name])
-                value = param.from_str(str_val)
-                self.__setattr__(name, value)
-            except (
-                RuntimeError
-            ) as error:  # inference_adapter is not openvino adapter or IR doesn't contain requested rt_info
-                if (
-                    str(error)
-                    != "Cannot get runtime attribute. Path to runtime attribute is incorrect."
-                    and str(error) != "OVMSAdapter does not support RT info getting"
-                ):
-                    raise
-
         for name, value in config.items():
-            if value is None:
-                continue
             if name in parameters:
                 errors = parameters[name].validate(value)
                 if errors:
                     self.logger.error(f'Error with "{name}" parameter:')
                     for error in errors:
                         self.logger.error(f"\t{error}")
-                    self.raise_error("Incorrect user configuration")
+                    self.raise_error('Incorrect user configuration')
                 value = parameters[name].get_value(value)
                 self.__setattr__(name, value)
             else:
-                self.logger.warning(
-                    f'The parameter "{name}" not found in {self.__model__} wrapper, will be omitted'
-                )
+                self.logger.warning(f'The parameter "{name}" not found in {self.__model__} wrapper, will be omitted')
 
-    @classmethod
-    def raise_error(cls, message):
-        """Raises the WrapperError.
+    def raise_error(self, message):
+        '''Raises the WrapperError.
 
         Args:
             message (str): error message to be shown in the following format:
               "WrapperName: message"
-        """
-        raise WrapperError(cls.__model__, message)
+        '''
+        raise WrapperError(self.__model__, message)
 
     def preprocess(self, inputs):
-        """Interface for preprocess method.
+        '''Interface for preprocess method.
 
         Args:
             inputs: raw input data, the data type is defined by wrapper
 
         Returns:
             - the preprocessed data which is submitted to the model for inference
                 and has the following format:
                 {
                     'input_layer_name_1': data_1,
                     'input_layer_name_2': data_2,
                     ...
                 }
             - the input metadata, which might be used in `postprocess` method
-        """
+        '''
         raise NotImplementedError
 
     def postprocess(self, outputs, meta):
-        """Interface for postprocess method.
+        '''Interface for postprocess method.
 
         Args:
             outputs (dict): model raw output in the following format:
                 {
                     'output_layer_name_1': raw_result_1,
                     'output_layer_name_2': raw_result_2,
                     ...
                 }
             meta (dict): the input metadata obtained from `preprocess` method
 
         Returns:
             - postprocessed data in the format defined by wrapper
-        """
+        '''
         raise NotImplementedError
 
     def _check_io_number(self, number_of_inputs, number_of_outputs):
-        """Checks whether the number of model inputs/outputs is supported.
+        '''Checks whether the number of model inputs/outputs is supported.
 
         Args:
             number_of_inputs (int, Tuple(int)): number of inputs supported by wrapper.
               Use -1 to omit the check
             number_of_outputs (int, Tuple(int)): number of outputs supported by wrapper.
               Use -1 to omit the check
 
         Raises:
             WrapperError: if the model has unsupported number of inputs/outputs
-        """
+        '''
         if not isinstance(number_of_inputs, tuple):
             if len(self.inputs) != number_of_inputs and number_of_inputs != -1:
-                self.raise_error(
-                    "Expected {} input blob{}, but {} found: {}".format(
-                        number_of_inputs,
-                        "s" if number_of_inputs != 1 else "",
-                        len(self.inputs),
-                        ", ".join(self.inputs),
-                    )
-                )
+                self.raise_error("Expected {} input blob{}, but {} found: {}".format(
+                    number_of_inputs, 's' if number_of_inputs !=1 else '',
+                    len(self.inputs), ', '.join(self.inputs)
+                ))
         else:
             if not len(self.inputs) in number_of_inputs:
-                self.raise_error(
-                    "Expected {} or {} input blobs, but {} found: {}".format(
-                        ", ".join(str(n) for n in number_of_inputs[:-1]),
-                        int(number_of_inputs[-1]),
-                        len(self.inputs),
-                        ", ".join(self.inputs),
-                    )
-                )
+                self.raise_error("Expected {} or {} input blobs, but {} found: {}".format(
+                    ', '.join(str(n) for n in number_of_inputs[:-1]), int(number_of_inputs[-1]),
+                    len(self.inputs), ', '.join(self.inputs)
+                ))
 
         if not isinstance(number_of_outputs, tuple):
             if len(self.outputs) != number_of_outputs and number_of_outputs != -1:
-                self.raise_error(
-                    "Expected {} output blob{}, but {} found: {}".format(
-                        number_of_outputs,
-                        "s" if number_of_outputs != 1 else "",
-                        len(self.outputs),
-                        ", ".join(self.outputs),
-                    )
-                )
+                self.raise_error("Expected {} output blob{}, but {} found: {}".format(
+                    number_of_outputs, 's' if number_of_outputs !=1 else '',
+                    len(self.outputs), ', '.join(self.outputs)
+                ))
         else:
             if not len(self.outputs) in number_of_outputs:
-                self.raise_error(
-                    "Expected {} or {} output blobs, but {} found: {}".format(
-                        ", ".join(str(n) for n in number_of_outputs[:-1]),
-                        int(number_of_outputs[-1]),
-                        len(self.outputs),
-                        ", ".join(self.outputs),
-                    )
-                )
+                self.raise_error("Expected {} or {} output blobs, but {} found: {}".format(
+                    ', '.join(str(n) for n in number_of_outputs[:-1]), int(number_of_outputs[-1]),
+                    len(self.outputs), ', '.join(self.outputs)
+                ))
 
     def __call__(self, inputs):
-        """
+        '''
         Applies preprocessing, synchronous inference, postprocessing routines while one call.
 
         Args:
             inputs: raw input data, the data type is defined by wrapper
 
         Returns:
             - postprocessed data in the format defined by wrapper
-        """
+            - the input metadata obtained from `preprocess` method
+        '''
         dict_data, input_meta = self.preprocess(inputs)
         raw_result = self.infer_sync(dict_data)
-        return self.postprocess(raw_result, input_meta)
+        return self.postprocess(raw_result, input_meta), input_meta
 
     def load(self, force=False):
         if not self.model_loaded or force:
             self.model_loaded = True
-            self.inference_adapter.load_model()
+            self.model_adapter.load_model()
 
     def reshape(self, new_shape):
         if self.model_loaded:
-            self.logger.warning(
-                f"{self.__model__}: the model already loaded to device, ",
-                "should be reloaded after reshaping.",
-            )
+            self.logger.warning(f'{self.__model__}: the model already loaded to device, ',
+                                'should be reloaded after reshaping.')
             self.model_loaded = False
-        self.inference_adapter.reshape_model(new_shape)
-        self.inputs = self.inference_adapter.get_input_layers()
-        self.outputs = self.inference_adapter.get_output_layers()
+        self.model_adapter.reshape_model(new_shape)
+        self.inputs = self.model_adapter.get_input_layers()
+        self.outputs = self.model_adapter.get_output_layers()
 
     def infer_sync(self, dict_data):
         if not self.model_loaded:
-            self.raise_error(
-                "The model is not loaded to the device. Please, create the wrapper "
-                "with preload=True option or call load() method before infer_sync()"
-            )
-        return self.inference_adapter.infer_sync(dict_data)
+            self.raise_error("The model is not loaded to the device. Please, create the wrapper "
+                "with preload=True option or call load() method before infer_sync()")
+        return self.model_adapter.infer_sync(dict_data)
 
-    def infer_async_raw(self, dict_data, callback_data):
+    def infer_async(self, dict_data, callback_data):
         if not self.model_loaded:
-            self.raise_error(
-                "The model is not loaded to the device. Please, create the wrapper "
-                "with preload=True option or call load() method before infer_async()"
-            )
-        self.inference_adapter.infer_async(dict_data, callback_data)
-
-    def infer_async(self, input_data, user_data):
-        if not self.model_loaded:
-            self.raise_error(
-                "The model is not loaded to the device. Please, create the wrapper "
-                "with preload=True option or call load() method before infer_async()"
-            )
-        dict_data, meta = self.preprocess(input_data)
-        self.inference_adapter.infer_async(
-            dict_data,
-            (
-                meta,
-                self.inference_adapter.get_raw_result,
-                self.postprocess,
-                self.callback_fn,
-                user_data,
-            ),
-        )
-
-    @staticmethod
-    def process_callback(request, callback_data):
-        meta, get_result_fn, postprocess_fn, callback_fn, user_data = callback_data
-        raw_result = get_result_fn(request)
-        result = postprocess_fn(raw_result, meta)
-        callback_fn(result, user_data)
-
-    def set_callback(self, callback_fn):
-        self.callback_fn = callback_fn
-        self.inference_adapter.set_callback(Model.process_callback)
+            self.raise_error("The model is not loaded to the device. Please, create the wrapper "
+                "with preload=True option or call load() method before infer_async()")
+        self.model_adapter.infer_async(dict_data, callback_data)
 
     def is_ready(self):
-        return self.inference_adapter.is_ready()
+        return self.model_adapter.is_ready()
 
     def await_all(self):
-        self.inference_adapter.await_all()
+        self.model_adapter.await_all()
 
     def await_any(self):
-        self.inference_adapter.await_any()
+        self.model_adapter.await_any()
 
     def log_layers_info(self):
-        """Prints the shape, precision and layout for all model inputs/outputs."""
+        '''Prints the shape, precision and layout for all model inputs/outputs.
+        '''
         for name, metadata in self.inputs.items():
-            self.logger.info(
-                "\tInput layer: {}, shape: {}, precision: {}, layout: {}".format(
-                    name, metadata.shape, metadata.precision, metadata.layout
-                )
-            )
+            self.logger.info('\tInput layer: {}, shape: {}, precision: {}, layout: {}'.format(
+                name, metadata.shape, metadata.precision, metadata.layout))
         for name, metadata in self.outputs.items():
-            self.logger.info(
-                "\tOutput layer: {}, shape: {}, precision: {}, layout: {}".format(
-                    name, metadata.shape, metadata.precision, metadata.layout
-                )
-            )
-
-    def get_model(self):
-        model = self.inference_adapter.get_model()
-        model.set_rt_info(self.__model__, ["model_info", "model_type"])
-        for name in self.parameters():
-            if [] == getattr(self, name):
-                # ov cant serialize empty list. Replace it with ""
-                # TODO: remove when Anastasia Kuporosova fixes that
-                model.set_rt_info("", ["model_info", name])
-            else:
-                model.set_rt_info(getattr(self, name), ["model_info", name])
-        return model
-
-    def save(self, xml_path, bin_path="", version="UNSPECIFIED"):
-        import openvino.runtime as ov
-
-        ov.serialize(self.get_model(), xml_path, bin_path, version)
+            self.logger.info('\tOutput layer: {}, shape: {}, precision: {}, layout: {}'.format(
+                name, metadata.shape, metadata.precision, metadata.layout))
```

## visiongraph/external/intel/models/open_pose.py

```diff
@@ -12,147 +12,95 @@
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
 import cv2
 import numpy as np
-
 try:
     from numpy.core.umath import clip
 except ImportError:
     from numpy import clip
-
 import openvino.runtime.opset8 as opset8
 
 from .image_model import ImageModel
 from .types import NumericalValue
 
 
 class OpenPose(ImageModel):
-    __model__ = "OpenPose"
+    __model__ = 'OpenPose'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload=False)
-        self.pooled_heatmaps_blob_name = "pooled_heatmaps"
-        self.heatmaps_blob_name = "heatmaps"
-        self.pafs_blob_name = "pafs"
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload=False)
+        self.pooled_heatmaps_blob_name = 'pooled_heatmaps'
+        self.heatmaps_blob_name = 'heatmaps'
+        self.pafs_blob_name = 'pafs'
 
-        function = self.inference_adapter.model
+        function = self.model_adapter.model
         paf = function.get_output_op(0)
         paf_shape = paf.output(0).get_shape()
         heatmap = function.get_output_op(1)
 
         heatmap_shape = heatmap.output(0).get_shape()
         if len(paf_shape) != 4 and len(heatmap_shape) != 4:
-            self.raise_error("OpenPose outputs must be 4-dimensional")
+            self.raise_error('OpenPose outputs must be 4-dimensional')
         if paf_shape[2] != heatmap_shape[2] and paf_shape[3] != heatmap_shape[3]:
-            self.raise_error("Last two dimensions of OpenPose outputs must match")
+            self.raise_error('Last two dimensions of OpenPose outputs must match')
         if paf_shape[1] * 2 == heatmap_shape[1]:
             paf, heatmap = heatmap, paf
         elif paf_shape[1] != heatmap_shape[1] * 2:
-            self.raise_error(
-                "Size of second dimension of OpenPose of one output must be two times larger then size "
-                "of second dimension of another output"
-            )
+            self.raise_error('Size of second dimension of OpenPose of one output must be two times larger then size '
+                             'of second dimension of another output')
 
         paf = paf.inputs()[0].get_source_output().get_node()
         paf.get_output_tensor(0).set_names({self.pafs_blob_name})
         heatmap = heatmap.inputs()[0].get_source_output().get_node()
 
         heatmap.get_output_tensor(0).set_names({self.heatmaps_blob_name})
 
         # Add keypoints NMS to the network.
         # Heuristic NMS kernel size adjustment depending on the feature maps upsampling ratio.
         p = int(np.round(6 / 7 * self.upsample_ratio))
         k = 2 * p + 1
-        pooled_heatmap = opset8.max_pool(
-            heatmap,
-            kernel_shape=(k, k),
-            dilations=(1, 1),
-            pads_begin=(p, p),
-            pads_end=(p, p),
-            strides=(1, 1),
-            name=self.pooled_heatmaps_blob_name,
-        )
-        pooled_heatmap.output(0).get_tensor().set_names(
-            {self.pooled_heatmaps_blob_name}
-        )
-        self.inference_adapter.model.add_outputs([pooled_heatmap.output(0)])
-
-        self.inputs = self.inference_adapter.get_input_layers()
-        self.outputs = self.inference_adapter.get_output_layers()
-
-        self.output_scale = (
-                self.inputs[self.image_blob_name].shape[-2]
-                / self.outputs[self.heatmaps_blob_name].shape[-2]
-        )
+        pooled_heatmap = opset8.max_pool(heatmap, kernel_shape=(k, k), dilations=(1, 1), pads_begin=(p, p), pads_end=(p, p),
+                                     strides=(1, 1), name=self.pooled_heatmaps_blob_name)
+        pooled_heatmap.output(0).get_tensor().set_names({self.pooled_heatmaps_blob_name})
+        self.model_adapter.model.add_outputs([pooled_heatmap.output(0)])
+
+        self.inputs = self.model_adapter.get_input_layers()
+        self.outputs = self.model_adapter.get_output_layers()
+
+        self.output_scale = self.inputs[self.image_blob_name].shape[-2] / self.outputs[self.heatmaps_blob_name].shape[-2]
 
         if self.target_size is None:
             self.target_size = self.inputs[self.image_blob_name].shape[-2]
-        self.h = (
-                (self.target_size + self.size_divisor - 1)
-                // self.size_divisor
-                * self.size_divisor
-        )
+        self.h = (self.target_size + self.size_divisor - 1) // self.size_divisor * self.size_divisor
         input_width = round(self.target_size * self.aspect_ratio)
-        self.w = (
-                (input_width + self.size_divisor - 1)
-                // self.size_divisor
-                * self.size_divisor
-        )
+        self.w = (input_width + self.size_divisor - 1) // self.size_divisor * self.size_divisor
         default_input_shape = self.inputs[self.image_blob_name].shape
-        input_shape = {
-            self.image_blob_name: (default_input_shape[:-2] + [self.h, self.w])
-        }
-        self.logger.debug(
-            "\tReshape model from {} to {}".format(
-                default_input_shape, input_shape[self.image_blob_name]
-            )
-        )
+        input_shape = {self.image_blob_name: (default_input_shape[:-2] + [self.h, self.w])}
+        self.logger.debug('\tReshape model from {} to {}'.format(default_input_shape, input_shape[self.image_blob_name]))
         super().reshape(input_shape)
 
         if preload:
             self.load()
 
-        num_joints = (
-                self.outputs[self.heatmaps_blob_name].shape[1] - 1
-        )  # The last channel is for background
-        self.decoder = OpenPoseDecoder(
-            num_joints, score_threshold=self.confidence_threshold
-        )
+        num_joints = self.outputs[self.heatmaps_blob_name].shape[1] - 1  # The last channel is for background
+        self.decoder = OpenPoseDecoder(num_joints, score_threshold=self.confidence_threshold)
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "target_size": NumericalValue(
-                    value_type=int,
-                    min=1,
-                    description="Image resolution which is going to be processed. Reshapes network to match a given size",
-                ),
-                "aspect_ratio": NumericalValue(
-                    description="Image aspect ratio which is going to be processed. Reshapes network to match a given size"
-                ),
-                "confidence_threshold": NumericalValue(
-                    description="pose confidence threshold"
-                ),
-                "upsample_ratio": NumericalValue(
-                    default_value=1,
-                    value_type=int,
-                    description="Upsample ratio of a model backbone",
-                ),
-                "size_divisor": NumericalValue(
-                    default_value=8,
-                    value_type=int,
-                    description="Width and height of the rehaped model will be a multiple of this value",
-                ),
-            }
-        )
+        parameters.update({
+            'target_size': NumericalValue(value_type=int, min=1),
+            'aspect_ratio': NumericalValue(),
+            'confidence_threshold': NumericalValue(),
+            'upsample_ratio': NumericalValue(default_value=1, value_type=int),
+            'size_divisor': NumericalValue(default_value=8, value_type=int),
+        })
         return parameters
 
     @staticmethod
     def heatmap_nms(heatmaps, pooled_heatmaps):
         return heatmaps * (heatmaps == pooled_heatmaps)
 
     @staticmethod
@@ -163,134 +111,81 @@
 
     def preprocess(self, inputs):
         img = self._resize_image(inputs, self.h)
         h, w = img.shape[:2]
         if self.w < w:
             self.raise_error("The image aspect ratio doesn't fit current model shape")
         if not (self.w - self.size_divisor < w <= self.w):
-            self.logger.warning(
-                "\tChosen model aspect ratio doesn't match image aspect ratio"
-            )
-        resize_img_scale = np.array(
-            (inputs.shape[1] / w, inputs.shape[0] / h), np.float32
-        )
-
-        img = np.pad(
-            img, ((0, 0), (0, self.w - w), (0, 0)), mode="constant", constant_values=0
-        )
+            self.logger.warning("\tChosen model aspect ratio doesn't match image aspect ratio")
+        resize_img_scale = np.array((inputs.shape[1] / w, inputs.shape[0] / h), np.float32)
+
+        img = np.pad(img, ((0, 0), (0, self.w - w), (0, 0)),
+                     mode='constant', constant_values=0)
         img = img.transpose((2, 0, 1))  # Change data layout from HWC to CHW
         img = img[None]
-        meta = {"resize_img_scale": resize_img_scale}
+        meta = {'resize_img_scale': resize_img_scale}
         return {self.image_blob_name: img}, meta
 
     def postprocess(self, outputs, meta):
         heatmaps = outputs[self.heatmaps_blob_name]
         pafs = outputs[self.pafs_blob_name]
         pooled_heatmaps = outputs[self.pooled_heatmaps_blob_name]
         nms_heatmaps = self.heatmap_nms(heatmaps, pooled_heatmaps)
         poses, scores = self.decoder(heatmaps, nms_heatmaps, pafs)
         # Rescale poses to the original image.
-        poses[:, :, :2] *= meta["resize_img_scale"] * self.output_scale
+        poses[:, :, :2] *= meta['resize_img_scale'] * self.output_scale
         return poses, scores
 
 
 class OpenPoseDecoder:
-    BODY_PARTS_KPT_IDS = (
-        (1, 2),
-        (1, 5),
-        (2, 3),
-        (3, 4),
-        (5, 6),
-        (6, 7),
-        (1, 8),
-        (8, 9),
-        (9, 10),
-        (1, 11),
-        (11, 12),
-        (12, 13),
-        (1, 0),
-        (0, 14),
-        (14, 16),
-        (0, 15),
-        (15, 17),
-        (2, 16),
-        (5, 17),
-    )
-    BODY_PARTS_PAF_IDS = (
-        12,
-        20,
-        14,
-        16,
-        22,
-        24,
-        0,
-        2,
-        4,
-        6,
-        8,
-        10,
-        28,
-        30,
-        34,
-        32,
-        36,
-        18,
-        26,
-    )
-
-    def __init__(
-            self,
-            num_joints=18,
-            skeleton=BODY_PARTS_KPT_IDS,
-            paf_indices=BODY_PARTS_PAF_IDS,
-            max_points=100,
-            score_threshold=0.1,
-            min_paf_alignment_score=0.05,
-            delta=0.5,
-    ):
+
+    BODY_PARTS_KPT_IDS = ((1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),
+                          (11, 12), (12, 13), (1, 0), (0, 14), (14, 16), (0, 15), (15, 17), (2, 16), (5, 17))
+    BODY_PARTS_PAF_IDS = (12, 20, 14, 16, 22, 24, 0, 2, 4, 6, 8, 10, 28, 30, 34, 32, 36, 18, 26)
+
+    def __init__(self, num_joints=18, skeleton=BODY_PARTS_KPT_IDS, paf_indices=BODY_PARTS_PAF_IDS,
+                 max_points=100, score_threshold=0.1, min_paf_alignment_score=0.05, delta=0.5):
         self.num_joints = num_joints
         self.skeleton = skeleton
         self.paf_indices = paf_indices
         self.max_points = max_points
         self.score_threshold = score_threshold
         self.min_paf_alignment_score = min_paf_alignment_score
         self.delta = delta
 
         self.points_per_limb = 10
         self.grid = np.arange(self.points_per_limb, dtype=np.float32).reshape(1, -1, 1)
 
     def __call__(self, heatmaps, nms_heatmaps, pafs):
         batch_size, _, h, w = heatmaps.shape
-        assert batch_size == 1, "Batch size of 1 only supported"
+        assert batch_size == 1, 'Batch size of 1 only supported'
 
         keypoints = self.extract_points(heatmaps, nms_heatmaps)
         pafs = np.transpose(pafs, (0, 2, 3, 1))
 
         if self.delta > 0:
             for kpts in keypoints:
                 kpts[:, :2] += self.delta
                 clip(kpts[:, 0], 0, w - 1, out=kpts[:, 0])
                 clip(kpts[:, 1], 0, h - 1, out=kpts[:, 1])
 
-        pose_entries, keypoints = self.group_keypoints(
-            keypoints, pafs, pose_entry_size=self.num_joints + 2
-        )
+        pose_entries, keypoints = self.group_keypoints(keypoints, pafs, pose_entry_size=self.num_joints + 2)
         poses, scores = self.convert_to_coco_format(pose_entries, keypoints)
         if len(poses) > 0:
             poses = np.asarray(poses, dtype=np.float32)
             poses = poses.reshape((poses.shape[0], -1, 3))
         else:
             poses = np.empty((0, 17, 3), dtype=np.float32)
             scores = np.empty(0, dtype=np.float32)
 
         return poses, scores
 
     def extract_points(self, heatmaps, nms_heatmaps):
         batch_size, channels_num, h, w = heatmaps.shape
-        assert batch_size == 1, "Batch size of 1 only supported"
+        assert batch_size == 1, 'Batch size of 1 only supported'
         assert channels_num >= self.num_joints
 
         xs, ys, scores = self.top_k(nms_heatmaps)
         masks = scores > self.score_threshold
         all_keypoints = []
         keypoint_id = 0
         for k in range(self.num_joints):
@@ -329,17 +224,15 @@
         scores = np.take_along_axis(scores, subind, axis=2)
         y, x = np.divmod(ind, W)
         return x, y, scores
 
     @staticmethod
     def refine(heatmap, x, y):
         h, w = heatmap.shape[-2:]
-        valid = np.logical_and(
-            np.logical_and(x > 0, x < w - 1), np.logical_and(y > 0, y < h - 1)
-        )
+        valid = np.logical_and(np.logical_and(x > 0, x < w - 1), np.logical_and(y > 0, y < h - 1))
         xx = x[valid]
         yy = y[valid]
         dx = np.sign(heatmap[yy, xx + 1] - heatmap[yy, xx - 1], dtype=np.float32) * 0.25
         dy = np.sign(heatmap[yy + 1, xx] - heatmap[yy - 1, xx], dtype=np.float32) * 0.25
         x = x.astype(np.float32)
         y = y.astype(np.float32)
         x[valid] += dx
@@ -348,40 +241,30 @@
 
     @staticmethod
     def is_disjoint(pose_a, pose_b):
         pose_a = pose_a[:-2]
         pose_b = pose_b[:-2]
         return np.all(np.logical_or.reduce((pose_a == pose_b, pose_a < 0, pose_b < 0)))
 
-    def update_poses(
-            self,
-            kpt_a_id,
-            kpt_b_id,
-            all_keypoints,
-            connections,
-            pose_entries,
-            pose_entry_size,
-    ):
+    def update_poses(self, kpt_a_id, kpt_b_id, all_keypoints, connections, pose_entries, pose_entry_size):
         for connection in connections:
             pose_a_idx = -1
             pose_b_idx = -1
             for j, pose in enumerate(pose_entries):
                 if pose[kpt_a_id] == connection[0]:
                     pose_a_idx = j
                 if pose[kpt_b_id] == connection[1]:
                     pose_b_idx = j
             if pose_a_idx < 0 and pose_b_idx < 0:
                 # Create new pose entry.
                 pose_entry = np.full(pose_entry_size, -1, dtype=np.float32)
                 pose_entry[kpt_a_id] = connection[0]
                 pose_entry[kpt_b_id] = connection[1]
                 pose_entry[-1] = 2
-                pose_entry[-2] = (
-                        np.sum(all_keypoints[connection[0:2], 2]) + connection[2]
-                )
+                pose_entry[-2] = np.sum(all_keypoints[connection[0:2], 2]) + connection[2]
                 pose_entries.append(pose_entry)
             elif pose_a_idx >= 0 and pose_b_idx >= 0 and pose_a_idx != pose_b_idx:
                 # Merge two poses are disjoint merge them, otherwise ignore connection.
                 pose_a = pose_entries[pose_a_idx]
                 pose_b = pose_entries[pose_b_idx]
                 if self.is_disjoint(pose_a, pose_b):
                     pose_a += pose_b
@@ -443,70 +326,52 @@
             # Get vectors between all pairs of keypoints, i.e. candidate limb vectors.
             a = kpts_a[:, :2]
             a = np.broadcast_to(a[None], (m, n, 2))
             b = kpts_b[:, :2]
             vec_raw = (b[:, None, :] - a).reshape(-1, 1, 2)
 
             # Sample points along every candidate limb vector.
-            steps = 1 / (self.points_per_limb - 1) * vec_raw
+            steps = (1 / (self.points_per_limb - 1) * vec_raw)
             points = steps * self.grid + a.reshape(-1, 1, 2)
             points = points.round().astype(dtype=np.int32)
             x = points[..., 0].ravel()
             y = points[..., 1].ravel()
 
             # Compute affinity score between candidate limb vectors and part affinity field.
-            part_pafs = pafs[0, :, :, paf_channel: paf_channel + 2]
+            part_pafs = pafs[0, :, :, paf_channel:paf_channel + 2]
             field = part_pafs[y, x].reshape(-1, self.points_per_limb, 2)
             vec_norm = np.linalg.norm(vec_raw, ord=2, axis=-1, keepdims=True)
             vec = vec_raw / (vec_norm + 1e-6)
             affinity_scores = (field * vec).sum(-1).reshape(-1, self.points_per_limb)
             valid_affinity_scores = affinity_scores > self.min_paf_alignment_score
             valid_num = valid_affinity_scores.sum(1)
-            affinity_scores = (affinity_scores * valid_affinity_scores).sum(1) / (
-                    valid_num + 1e-6
-            )
+            affinity_scores = (affinity_scores * valid_affinity_scores).sum(1) / (valid_num + 1e-6)
             success_ratio = valid_num / self.points_per_limb
 
             # Get a list of limbs according to the obtained affinity score.
-            valid_limbs = np.where(
-                np.logical_and(affinity_scores > 0, success_ratio > 0.8)
-            )[0]
+            valid_limbs = np.where(np.logical_and(affinity_scores > 0, success_ratio > 0.8))[0]
             if len(valid_limbs) == 0:
                 continue
             b_idx, a_idx = np.divmod(valid_limbs, n)
             affinity_scores = affinity_scores[valid_limbs]
 
             # Suppress incompatible connections.
-            a_idx, b_idx, affinity_scores = self.connections_nms(
-                a_idx, b_idx, affinity_scores
-            )
-            connections = list(
-                zip(
-                    kpts_a[a_idx, 3].astype(np.int32),
-                    kpts_b[b_idx, 3].astype(np.int32),
-                    affinity_scores,
-                )
-            )
+            a_idx, b_idx, affinity_scores = self.connections_nms(a_idx, b_idx, affinity_scores)
+            connections = list(zip(kpts_a[a_idx, 3].astype(np.int32),
+                                   kpts_b[b_idx, 3].astype(np.int32),
+                                   affinity_scores))
             if len(connections) == 0:
                 continue
 
             # Update poses with new connections.
-            pose_entries = self.update_poses(
-                kpt_a_id,
-                kpt_b_id,
-                all_keypoints,
-                connections,
-                pose_entries,
-                pose_entry_size,
-            )
+            pose_entries = self.update_poses(kpt_a_id, kpt_b_id, all_keypoints,
+                                             connections, pose_entries, pose_entry_size)
 
         # Remove poses with not enough points.
-        pose_entries = np.asarray(pose_entries, dtype=np.float32).reshape(
-            -1, pose_entry_size
-        )
+        pose_entries = np.asarray(pose_entries, dtype=np.float32).reshape(-1, pose_entry_size)
         pose_entries = pose_entries[pose_entries[:, -1] >= 3]
         return pose_entries, all_keypoints
 
     @staticmethod
     def convert_to_coco_format(pose_entries, all_keypoints):
         num_joints = 17
         coco_keypoints = []
@@ -524,8 +389,8 @@
                 if keypoint_id != -1:
                     cx, cy, score = all_keypoints[int(keypoint_id), 0:3]
                 keypoints[target_id * 3 + 0] = cx
                 keypoints[target_id * 3 + 1] = cy
                 keypoints[target_id * 3 + 2] = score
             coco_keypoints.append(keypoints)
             scores.append(person_score * max(0, (pose[-1] - 1)))  # -1 for 'neck'
-        return np.asarray(coco_keypoints), np.asarray(scores)
+        return np.asarray(coco_keypoints), np.asarray(scores)
```

## visiongraph/external/intel/models/ssd.py

```diff
@@ -16,65 +16,63 @@
 import numpy as np
 
 from .detection_model import DetectionModel
 from .utils import Detection
 
 
 class SSD(DetectionModel):
-    __model__ = "SSD"
+    __model__ = 'SSD'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
-        self.image_info_blob_name = (
-            self.image_info_blob_names[0]
-            if len(self.image_info_blob_names) == 1
-            else None
-        )
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
+        self.image_info_blob_name = self.image_info_blob_names[0] if len(self.image_info_blob_names) == 1 else None
         self.output_parser = self._get_output_parser(self.image_blob_name)
 
+    @classmethod
+    def parameters(cls):
+        parameters = super().parameters()
+        parameters['resize_type'].update_default_value('standard')
+        parameters['confidence_threshold'].update_default_value(0.5)
+        return parameters
+
     def preprocess(self, inputs):
         dict_inputs, meta = super().preprocess(inputs)
         if self.image_info_blob_name:
             dict_inputs[self.image_info_blob_name] = np.array([[self.h, self.w, 1]])
         return dict_inputs, meta
 
     def postprocess(self, outputs, meta):
-        detections = self._parse_outputs(outputs)
+        detections = self._parse_outputs(outputs, meta)
         detections = self._resize_detections(detections, meta)
-        detections = self._add_label_names(detections)
         return detections
 
-    def _get_output_parser(
-        self, image_blob_name, bboxes="bboxes", labels="labels", scores="scores"
-    ):
+    def _get_output_parser(self, image_blob_name, bboxes='bboxes', labels='labels', scores='scores'):
         try:
             parser = SingleOutputParser(self.outputs)
-            self.logger.debug("\tUsing SSD model with single output parser")
+            self.logger.debug('\tUsing SSD model with single output parser')
             return parser
         except ValueError:
             pass
 
         try:
             parser = MultipleOutputParser(self.outputs, bboxes, scores, labels)
-            self.logger.debug("\tUsing SSD model with multiple output parser")
+            self.logger.debug('\tUsing SSD model with multiple output parser')
             return parser
         except ValueError:
             pass
 
         try:
-            parser = BoxesLabelsParser(
-                self.outputs, self.inputs[image_blob_name].shape[2:][::-1]
-            )
+            parser = BoxesLabelsParser(self.outputs, self.inputs[image_blob_name].shape[2:][::-1])
             self.logger.debug('\tUsing SSD model with "boxes-labels" output parser')
             return parser
         except ValueError:
             pass
-        self.raise_error("Unsupported model outputs")
+        self.raise_error('Unsupported model outputs')
 
-    def _parse_outputs(self, outputs):
+    def _parse_outputs(self, outputs, meta):
         detections = self.output_parser(outputs)
 
         detections = [d for d in detections if d.score > self.confidence_threshold]
 
         return detections
 
 
@@ -88,87 +86,65 @@
 
     return suitable_layers[0]
 
 
 class SingleOutputParser:
     def __init__(self, all_outputs):
         if len(all_outputs) != 1:
-            raise ValueError("Network must have only one output.")
+            raise ValueError('Network must have only one output.')
         self.output_name, output_data = next(iter(all_outputs.items()))
         last_dim = output_data.shape[-1]
         if last_dim != 7:
-            raise ValueError(
-                "The last dimension of the output blob must be equal to 7, "
-                "got {} instead.".format(last_dim)
-            )
+            raise ValueError('The last dimension of the output blob must be equal to 7, '
+                             'got {} instead.'.format(last_dim))
 
     def __call__(self, outputs):
-        return [
-            Detection(xmin, ymin, xmax, ymax, score, label)
-            for _, label, score, xmin, ymin, xmax, ymax in outputs[self.output_name][0][
-                0
-            ]
-        ]
+        return [Detection(xmin, ymin, xmax, ymax, score, label)
+                for _, label, score, xmin, ymin, xmax, ymax in outputs[self.output_name][0][0]]
 
 
 class MultipleOutputParser:
-    def __init__(
-        self,
-        layers,
-        bboxes_layer="bboxes",
-        scores_layer="scores",
-        labels_layer="labels",
-    ):
+    def __init__(self, layers, bboxes_layer='bboxes', scores_layer='scores', labels_layer='labels'):
         self.labels_layer = find_layer_by_name(labels_layer, layers)
         self.scores_layer = find_layer_by_name(scores_layer, layers)
         self.bboxes_layer = find_layer_by_name(bboxes_layer, layers)
 
     def __call__(self, outputs):
         bboxes = outputs[self.bboxes_layer][0]
         scores = outputs[self.scores_layer][0]
         labels = outputs[self.labels_layer][0]
-        return [
-            Detection(*bbox, score, label)
-            for label, score, bbox in zip(labels, scores, bboxes)
-        ]
+        return [Detection(*bbox, score, label) for label, score, bbox in zip(labels, scores, bboxes)]
 
 
 class BoxesLabelsParser:
-    def __init__(self, layers, input_size, labels_layer="labels", default_label=0):
+    def __init__(self, layers, input_size, labels_layer='labels', default_label=0):
         try:
             self.labels_layer = find_layer_by_name(labels_layer, layers)
         except ValueError:
             self.labels_layer = None
             self.default_label = default_label
 
         self.bboxes_layer = self.find_layer_bboxes_output(layers)
         self.input_size = input_size
 
     @staticmethod
     def find_layer_bboxes_output(layers):
-        filter_outputs = [
-            name
-            for name, data in layers.items()
-            if len(data.shape) == 2 and data.shape[-1] == 5
-        ]
+        filter_outputs = [name for name, data in layers.items() if len(data.shape) == 2 and data.shape[-1] == 5]
         if not filter_outputs:
-            raise ValueError("Suitable output with bounding boxes is not found")
+            raise ValueError('Suitable output with bounding boxes is not found')
         if len(filter_outputs) > 1:
-            raise ValueError("More than 1 candidate for output with bounding boxes.")
+            raise ValueError('More than 1 candidate for output with bounding boxes.')
         return filter_outputs[0]
 
     def __call__(self, outputs):
         bboxes = outputs[self.bboxes_layer]
         scores = bboxes[:, 4]
         bboxes = bboxes[:, :4]
         bboxes[:, 0::2] /= self.input_size[0]
         bboxes[:, 1::2] /= self.input_size[1]
         if self.labels_layer:
             labels = outputs[self.labels_layer]
         else:
             labels = np.full(len(bboxes), self.default_label, dtype=bboxes.dtype)
 
-        detections = [
-            Detection(*bbox, score, label)
-            for label, score, bbox in zip(labels, scores, bboxes)
-        ]
-        return detections
+        detections = [Detection(*bbox, score, label) for label, score, bbox in zip(labels, scores, bboxes)]
+        return detections
```

## visiongraph/external/intel/models/types.py

```diff
@@ -10,25 +10,22 @@
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
-
 class ConfigurableValueError(ValueError):
     def __init__(self, message, prefix=None):
-        self.message = f"{prefix}: {message}" if prefix else message
+        self.message = f'{prefix}: {message}' if prefix else message
         super().__init__(self.message)
 
 
 class BaseValue:
-    def __init__(
-        self, description="No description available", default_value=None
-    ) -> None:
+    def __init__(self, description="No description available", default_value=None) -> None:
         self.default_value = default_value
         self.description = description
 
     def update_default_value(self, default_value):
         self.default_value = default_value
 
     def validate(self, value):
@@ -46,95 +43,60 @@
         info = self.description
         if self.default_value:
             info += f"\nThe default value is '{self.default_value}'"
         return info
 
 
 class NumericalValue(BaseValue):
-    def __init__(
-        self, value_type=float, choices=(), min=None, max=None, **kwargs
-    ) -> None:
+    def __init__(self, value_type=float, choices=(), min=None, max=None, **kwargs) -> None:
         super().__init__(**kwargs)
         self.choices = choices
         self.min = min
         self.max = max
         self.value_type = value_type
 
-    def from_str(self, value):
-        return float(value)
-
     def validate(self, value):
         errors = super().validate(value)
         if not value:
             return errors
         if not isinstance(value, self.value_type):
-            errors.append(
-                ConfigurableValueError(
-                    f"Incorrect value type {type(value)}: should be {self.value_type}"
-                )
-            )
+            errors.append(ConfigurableValueError(f'Incorrect value type {type(value)}: should be {self.value_type}'))
             return errors
         if len(self.choices):
             if value not in self.choices:
-                errors.append(
-                    ConfigurableValueError(
-                        f"Incorrect value {value}: out of allowable list - {self.choices}"
-                    )
-                )
+                errors.append(ConfigurableValueError(f'Incorrect value {value}: out of allowable list - {self.choices}'))
         if self.min is not None and value < self.min:
-            errors.append(
-                ConfigurableValueError(
-                    f"Incorrect value {value}: less than minimum allowable {self.min}"
-                )
-            )
+            errors.append(ConfigurableValueError(f'Incorrect value {value}: less than minimum allowable {self.min}'))
         if self.max is not None and value > self.max:
-            errors.append(
-                ConfigurableValueError(
-                    f"Incorrect value {value}: bigger than maximum allowable {self.min}"
-                )
-            )
+            errors.append(ConfigurableValueError(f'Incorrect value {value}: bigger than maximum allowable {self.min}'))
         return errors
 
     def __str__(self) -> str:
         info = super().__str__()
         info += f"\nAppropriate type is {self.value_type}"
         if self.choices:
             info += f"\nAppropriate values are {self.choices}"
         return info
 
-
 class StringValue(BaseValue):
-    def __init__(
-        self, choices=(), description="No description available", default_value=""
-    ):
-        super().__init__(description, default_value)
+    def __init__(self, choices=(), **kwargs):
+        super().__init__(**kwargs)
         self.choices = choices
         for choice in self.choices:
             if not isinstance(choice, str):
-                raise ValueError("Incorrect option in choice list - {}.".format(choice))
-
-    def from_str(self, value):
-        return value
+                raise ValueError("Incorrect option in choice list - {}.". format(choice))
 
     def validate(self, value):
         errors = super().validate(value)
         if not value:
             return errors
         if not isinstance(value, str):
-            errors.append(
-                ConfigurableValueError(
-                    f'Incorrect value type {type(value)}: should be "str"'
-                )
-            )
-        if len(self.choices) > 0 and value not in self.choices:
-            errors.append(
-                ConfigurableValueError(
-                    f"Incorrect value {value}: out of allowable list - {self.choices}"
-                )
-            )
+            errors.append(ConfigurableValueError(f'Incorrect value type {type(value)}: should be "str"'))
+        if len(self.choices)>0 and value not in self.choices:
+            errors.append(ConfigurableValueError(f'Incorrect value {value}: out of allowable list - {self.choices}'))
         return errors
 
     def __str__(self) -> str:
         info = super().__str__()
         info += "\nAppropriate type is str"
         if self.choices:
             info += f"\nAppropriate values are {self.choices}"
@@ -142,93 +104,51 @@
         return info
 
 
 class BooleanValue(BaseValue):
     def __init__(self, **kwargs) -> None:
         super().__init__(**kwargs)
 
-    def from_str(self, value):
-        return "YES" == value
-
     def validate(self, value):
         errors = super().validate(value)
         if not value:
             return errors
         if not isinstance(value, bool):
-            errors.append(
-                ConfigurableValueError(
-                    f'Incorrect value type - {type(value)}: should be "bool"'
-                )
-            )
+            errors.append(ConfigurableValueError(f'Incorrect value type - {type(value)}: should be "bool"'))
         return errors
 
 
 class ListValue(BaseValue):
-    def __init__(
-        self, value_type=None, description="No description available", default_value=[]
-    ) -> None:
-        super().__init__(description, default_value)
+    def __init__(self, value_type=None, **kwargs) -> None:
+        super().__init__(**kwargs)
         self.value_type = value_type
 
-    def from_str(self, value):
-        try:
-            floats = [float(i) for i in value.split()]
-            ints = [int(i) for i in value.split()]
-            if ints == floats:
-                return ints
-            return floats
-        except ValueError:
-            return value.split()
-
     def validate(self, value):
         errors = super().validate(value)
         if not value:
             return errors
         if not isinstance(value, (tuple, list)):
-            errors.append(
-                ConfigurableValueError(
-                    f"Incorrect value type - {type(value)}: should be list or tuple"
-                )
-            )
+            errors.append(ConfigurableValueError(f'Incorrect value type - {type(value)}: should be list or tuple'))
         if self.value_type:
             if isinstance(self.value_type, BaseValue):
                 for i, element in enumerate(value):
                     temp_errors = self.value_type.validate(element)
                     if len(temp_errors) > 0:
-                        errors.extend(
-                            [
-                                ConfigurableValueError(
-                                    f"Incorrect #{i} element of the list"
-                                ),
-                                *temp_errors,
-                            ]
-                        )
+                        errors.extend([ConfigurableValueError(f'Incorrect #{i} element of the list'), *temp_errors])
             else:
                 for i, element in enumerate(value):
                     if not isinstance(element, self.value_type):
-                        errors.append(
-                            ConfigurableValueError(
-                                f"Incorrect #{i} element type - {type(element)}: should be {self.value_type}"
-                            )
-                        )
+                        errors.append(ConfigurableValueError(f'Incorrect #{i} element type - {type(element)}: should be {self.value_type}'))
         return errors
 
 
 class DictValue(BaseValue):
     def __init__(self, **kwargs) -> None:
         super().__init__(**kwargs)
 
-    def from_str(self, value):
-        # TODO
-        raise NotImplementedError
-
     def validate(self, value):
         errors = super().validate(value)
         if not value:
             return errors
         if not isinstance(value, dict):
-            errors.append(
-                ConfigurableValueError(
-                    f'Incorrect value type - {type(value)}: should be "dict"'
-                )
-            )
-        return errors
+            errors.append(ConfigurableValueError(f'Incorrect value type - {type(value)}: should be "dict"'))
+        return errors
```

## visiongraph/external/intel/models/utils.py

```diff
@@ -10,49 +10,44 @@
  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
-import math
-
 import cv2
 import numpy as np
+import math
 
 
 class Detection:
-    def __init__(self, xmin, ymin, xmax, ymax, score, id, str_label=None):
+    def __init__(self, xmin, ymin, xmax, ymax, score, id):
         self.xmin = xmin
         self.ymin = ymin
         self.xmax = xmax
         self.ymax = ymax
         self.score = score
-        self.id = int(id)
-        self.str_label = str_label
-
-    def get_coords(self):
-        return self.xmin, self.ymin, self.xmax, self.ymax
+        self.id = id
 
-    def __to_str(self):
-        return f"({self.xmin}, {self.ymin}, {self.xmax}, {self.ymax}, {self.score:.3f}, {self.id}, {self.str_label})"
+    def bottom_left_point(self):
+        return self.xmin, self.ymin
 
-    def __str__(self):
-        return self.__to_str()
+    def top_right_point(self):
+        return self.xmax, self.ymax
 
-    def __repr__(self):
-        return self.__to_str()
+    def get_coords(self):
+        return self.xmin, self.ymin, self.xmax, self.ymax
 
 
 def clip_detections(detections, size):
     for detection in detections:
-        detection.xmin = min(max(round(detection.xmin), 0), size[1])
-        detection.ymin = min(max(round(detection.ymin), 0), size[0])
-        detection.xmax = min(max(round(detection.xmax), 0), size[1])
-        detection.ymax = min(max(round(detection.ymax), 0), size[0])
+        detection.xmin = max(int(detection.xmin), 0)
+        detection.ymin = max(int(detection.ymin), 0)
+        detection.xmax = min(int(detection.xmax), size[1])
+        detection.ymax = min(int(detection.ymax), size[0])
     return detections
 
 
 class DetectionWithLandmarks(Detection):
     def __init__(self, xmin, ymin, xmax, ymax, score, id, landmarks_x, landmarks_y):
         super().__init__(xmin, ymin, xmax, ymax, score, id)
         self.landmarks = []
@@ -65,17 +60,16 @@
         self.output_resolution = output_resolution
         if self.output_resolution:
             self.new_resolution = self.compute_resolution(input_size)
 
     def compute_resolution(self, input_size):
         self.input_size = input_size
         size = self.input_size[::-1]
-        self.scale_factor = min(
-            self.output_resolution[0] / size[0], self.output_resolution[1] / size[1]
-        )
+        self.scale_factor = min(self.output_resolution[0] / size[0],
+                                self.output_resolution[1] / size[1])
         return self.scale(size)
 
     def resize(self, image):
         if not self.output_resolution:
             return image
         curr_size = image.shape[:2]
         if curr_size != self.input_size:
@@ -87,121 +81,104 @@
     def scale(self, inputs):
         if not self.output_resolution or self.scale_factor == 1:
             return inputs
         return (np.array(inputs) * self.scale_factor).astype(np.int32)
 
 
 class InputTransform:
-    def __init__(
-        self, reverse_input_channels=False, mean_values=None, scale_values=None
-    ):
+    def __init__(self, reverse_input_channels=False, mean_values=None, scale_values=None):
         self.reverse_input_channels = reverse_input_channels
         self.is_trivial = not (reverse_input_channels or mean_values or scale_values)
-        self.means = (
-            np.array(mean_values, dtype=np.float32)
-            if mean_values
-            else np.array([0.0, 0.0, 0.0])
-        )
-        self.std_scales = (
-            np.array(scale_values, dtype=np.float32)
-            if scale_values
-            else np.array([1.0, 1.0, 1.0])
-        )
+        self.means = np.array(mean_values, dtype=np.float32) if mean_values else np.array([0., 0., 0.])
+        self.std_scales = np.array(scale_values, dtype=np.float32) if scale_values else np.array([1., 1., 1.])
 
     def __call__(self, inputs):
         if self.is_trivial:
             return inputs
         if self.reverse_input_channels:
             inputs = cv2.cvtColor(inputs, cv2.COLOR_BGR2RGB)
         return (inputs - self.means) / self.std_scales
 
 
 def load_labels(label_file):
-    with open(label_file, "r") as f:
+    with open(label_file, 'r') as f:
         labels_map = [x.strip() for x in f]
     return labels_map
 
 
 def resize_image(image, size, keep_aspect_ratio=False, interpolation=cv2.INTER_LINEAR):
-    if keep_aspect_ratio:
+    if not keep_aspect_ratio:
+        resized_frame = cv2.resize(image, size, interpolation=interpolation)
+    else:
         h, w = image.shape[:2]
         scale = min(size[1] / h, size[0] / w)
-        return cv2.resize(image, None, fx=scale, fy=scale, interpolation=interpolation)
-    return cv2.resize(image, size, interpolation=interpolation)
+        resized_frame = cv2.resize(image, None, fx=scale, fy=scale, interpolation=interpolation)
+    return resized_frame
 
 
 def resize_image_with_aspect(image, size, interpolation=cv2.INTER_LINEAR):
-    return resize_image(
-        image, size, keep_aspect_ratio=True, interpolation=interpolation
-    )
+    return resize_image(image, size, keep_aspect_ratio=True, interpolation=interpolation)
 
 
 def pad_image(image, size):
     h, w = image.shape[:2]
     if h != size[1] or w != size[0]:
-        image = np.pad(
-            image,
-            ((0, size[1] - h), (0, size[0] - w), (0, 0)),
-            mode="constant",
-            constant_values=0,
-        )
+        image = np.pad(image, ((0, size[1] - h), (0, size[0] - w), (0, 0)),
+                               mode='constant', constant_values=0)
     return image
 
 
 def resize_image_letterbox(image, size, interpolation=cv2.INTER_LINEAR):
     ih, iw = image.shape[0:2]
     w, h = size
     scale = min(w / iw, h / ih)
     nw = int(iw * scale)
     nh = int(ih * scale)
     image = cv2.resize(image, (nw, nh), interpolation=interpolation)
     dx = (w - nw) // 2
     dy = (h - nh) // 2
-    return np.pad(
-        image,
-        ((dy, h - nh - dy), (dx, w - nw - dx), (0, 0)),
-        mode="constant",
-        constant_values=0,
-    )
+    resized_image = np.pad(image, ((dy, dy + (h - nh) % 2), (dx, dx + (w - nw) % 2), (0, 0)),
+                           mode='constant', constant_values=0)
+    return resized_image
 
 
 def crop_resize(image, size):
-    desired_aspect_ratio = size[1] / size[0]  # width / height
+    desired_aspect_ratio = size[1] / size[0] # width / height
     if desired_aspect_ratio == 1:
-        if image.shape[0] > image.shape[1]:
+        if (image.shape[0] > image.shape[1]):
             offset = (image.shape[0] - image.shape[1]) // 2
-            cropped_frame = image[offset : image.shape[1] + offset]
+            cropped_frame = image[offset:image.shape[1] + offset]
         else:
             offset = (image.shape[1] - image.shape[0]) // 2
-            cropped_frame = image[:, offset : image.shape[0] + offset]
+            cropped_frame = image[:, offset:image.shape[0] + offset]
     elif desired_aspect_ratio < 1:
         new_width = math.floor(image.shape[0] * desired_aspect_ratio)
         offset = (image.shape[1] - new_width) // 2
-        cropped_frame = image[:, offset : new_width + offset]
+        cropped_frame = image[:, offset:new_width + offset]
     elif desired_aspect_ratio > 1:
         new_height = math.floor(image.shape[1] / desired_aspect_ratio)
         offset = (image.shape[0] - new_height) // 2
-        cropped_frame = image[offset : new_height + offset]
+        cropped_frame = image[offset:new_height + offset]
 
     return cv2.resize(cropped_frame, size)
 
 
 RESIZE_TYPES = {
-    "crop": crop_resize,
-    "standard": resize_image,
-    "fit_to_window": resize_image_with_aspect,
-    "fit_to_window_letterbox": resize_image_letterbox,
+    'crop' : crop_resize,
+    'standard': resize_image,
+    'fit_to_window': resize_image_with_aspect,
+    'fit_to_window_letterbox': resize_image_letterbox,
 }
 
 
 INTERPOLATION_TYPES = {
-    "LINEAR": cv2.INTER_LINEAR,
-    "CUBIC": cv2.INTER_CUBIC,
-    "NEAREST": cv2.INTER_NEAREST,
-    "AREA": cv2.INTER_AREA,
+    'LINEAR': cv2.INTER_LINEAR,
+    'CUBIC': cv2.INTER_CUBIC,
+    'NEAREST': cv2.INTER_NEAREST,
+    'AREA': cv2.INTER_AREA,
 }
 
 
 def nms(x1, y1, x2, y2, scores, thresh, include_boundaries=False, keep_top_k=None):
     b = 1 if include_boundaries else 0
     areas = (x2 - x1 + b) * (y2 - y1 + b)
     order = scores.argsort()[::-1]
@@ -219,23 +196,18 @@
         xx2 = np.minimum(x2[i], x2[order[1:]])
         yy2 = np.minimum(y2[i], y2[order[1:]])
 
         w = np.maximum(0.0, xx2 - xx1 + b)
         h = np.maximum(0.0, yy2 - yy1 + b)
         intersection = w * h
 
-        union = areas[i] + areas[order[1:]] - intersection
-        overlap = np.divide(
-            intersection,
-            union,
-            out=np.zeros_like(intersection, dtype=float),
-            where=union != 0,
-        )
+        union = (areas[i] + areas[order[1:]] - intersection)
+        overlap = np.divide(intersection, union, out=np.zeros_like(intersection, dtype=float), where=union != 0)
 
         order = order[np.where(overlap <= thresh)[0] + 1]
 
     return keep
 
 
 def softmax(logits, axis=None, keepdims=False):
     exp = np.exp(logits - np.max(logits))
-    return exp / np.sum(exp, axis=axis, keepdims=keepdims)
+    return exp / np.sum(exp, axis=axis, keepdims=keepdims)
```

## visiongraph/external/intel/models/yolo.py

```diff
@@ -8,201 +8,130 @@
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
 """
 
 from collections import namedtuple
-
 import numpy as np
 
 from .detection_model import DetectionModel
 from .types import ListValue, NumericalValue
-from .utils import INTERPOLATION_TYPES, Detection, clip_detections, nms, resize_image
+from .utils import Detection, clip_detections, nms, resize_image, INTERPOLATION_TYPES
 
-DetectionBox = namedtuple("DetectionBox", ["x", "y", "w", "h"])
+DetectionBox = namedtuple('DetectionBox', ["x", "y", "w", "h"])
 
 ANCHORS = {
-    "YOLOV3": [
-        10.0,
-        13.0,
-        16.0,
-        30.0,
-        33.0,
-        23.0,
-        30.0,
-        61.0,
-        62.0,
-        45.0,
-        59.0,
-        119.0,
-        116.0,
-        90.0,
-        156.0,
-        198.0,
-        373.0,
-        326.0,
-    ],
-    "YOLOV4": [
-        12.0,
-        16.0,
-        19.0,
-        36.0,
-        40.0,
-        28.0,
-        36.0,
-        75.0,
-        76.0,
-        55.0,
-        72.0,
-        146.0,
-        142.0,
-        110.0,
-        192.0,
-        243.0,
-        459.0,
-        401.0,
-    ],
-    "YOLOV4-TINY": [
-        10.0,
-        14.0,
-        23.0,
-        27.0,
-        37.0,
-        58.0,
-        81.0,
-        82.0,
-        135.0,
-        169.0,
-        344.0,
-        319.0,
-    ],
-    "YOLOF": [
-        16.0,
-        16.0,
-        32.0,
-        32.0,
-        64.0,
-        64.0,
-        128.0,
-        128.0,
-        256.0,
-        256.0,
-        512.0,
-        512.0,
-    ],
+    'YOLOV3': [10.0, 13.0, 16.0, 30.0, 33.0, 23.0,
+               30.0, 61.0, 62.0, 45.0, 59.0, 119.0,
+               116.0, 90.0, 156.0, 198.0, 373.0, 326.0],
+    'YOLOV4': [12.0, 16.0, 19.0, 36.0, 40.0, 28.0,
+               36.0, 75.0, 76.0, 55.0, 72.0, 146.0,
+               142.0, 110.0, 192.0, 243.0, 459.0, 401.0],
+    'YOLOV4-TINY': [10.0, 14.0, 23.0, 27.0, 37.0, 58.0,
+                    81.0, 82.0, 135.0, 169.0, 344.0, 319.0],
+    'YOLOF' : [16.0, 16.0, 32.0, 32.0, 64.0, 64.0,
+               128.0, 128.0, 256.0, 256.0, 512.0, 512.0]
 }
 
-
 def permute_to_N_HWA_K(tensor, K, output_layout):
     """
     Transpose/reshape a tensor from (N, (A x K), H, W) to (N, (HxWxA), K)
     """
     assert tensor.ndim == 4, tensor.shape
-    if output_layout == "NHWC":
+    if output_layout == 'NHWC':
         tensor = tensor.transpose(0, 3, 1, 2)
     N, _, H, W = tensor.shape
     tensor = tensor.reshape(N, -1, K, H, W)
     tensor = tensor.transpose(0, 3, 4, 1, 2)
     tensor = tensor.reshape(N, -1, K)
     return tensor
 
-
 def sigmoid(x):
-    return 1.0 / (1.0 + np.exp(-x))
+    return 1. / (1. + np.exp(-x))
 
 
 class YOLO(DetectionModel):
-    __model__ = "YOLO"
+    __model__ = 'YOLO'
 
     class Params:
         # Magic numbers are copied from yolo samples
         def __init__(self, param, sides):
-            self.num = param.get("num", 3)
-            self.coords = param.get("coord", 4)
-            self.classes = param.get("classes", 80)
+            self.num = param.get('num', 3)
+            self.coords = param.get('coord', 4)
+            self.classes = param.get('classes', 80)
             self.bbox_size = self.coords + self.classes + 1
             self.sides = sides
-            self.anchors = param.get("anchors", ANCHORS["YOLOV3"])
+            self.anchors = param.get('anchors', ANCHORS['YOLOV3'])
 
             self.use_input_size = False
-            self.output_layout = "NCHW"
+            self.output_layout = 'NCHW'
 
-            mask = param.get("mask", None)
+            mask = param.get('mask', None)
             if mask:
                 self.num = len(mask)
 
                 masked_anchors = []
                 for idx in mask:
                     masked_anchors += [self.anchors[idx * 2], self.anchors[idx * 2 + 1]]
                 self.anchors = masked_anchors
 
                 self.use_input_size = True  # Weak way to determine but the only one.
 
-    def __init__(self, inference_adapter, configuration, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
-        self.is_tiny = (
-            len(self.outputs) == 2
-        )  # Weak way to distinguish between YOLOv4 and YOLOv4-tiny
+    def __init__(self, model_adapter, configuration, preload=False):
+        super().__init__(model_adapter, configuration, preload)
+        self.is_tiny = len(self.outputs) == 2  # Weak way to distinguish between YOLOv4 and YOLOv4-tiny
 
         self._check_io_number(1, -1)
 
         self.yolo_layer_params = self._get_output_info()
 
     def _get_output_info(self):
         output_info = {}
-        yolo_regions = self.inference_adapter.operations_by_type("RegionYolo")
+        yolo_regions = self.model_adapter.operations_by_type('RegionYolo')
         for name, info in self.outputs.items():
             shape = info.shape
             if len(shape) == 2:
                 # we use 32x32 cell as default, cause 1D tensor is V2 specific
                 cx = self.w // 32
                 cy = self.h // 32
 
-                bboxes = shape[1] // (cx * cy)
-                if self.w % 32 != 0 or self.h % 32 != 0 or shape[1] % (cx * cy) != 0:
-                    self.raise_error("The cannot reshape 2D output tensor into 4D")
+                bboxes = shape[1] // (cx*cy)
+                if self.w % 32 != 0 or self.h % 32 != 0 or shape[1] % (cx*cy) != 0:
+                    self.raise_error('The cannot reshape 2D output tensor into 4D')
                 shape = (shape[0], bboxes, cy, cx)
             meta = info.meta
-            if info.type != "RegionYolo" and yolo_regions:
+            if info.type != 'RegionYolo' and yolo_regions:
                 for region_name in yolo_regions:
                     if region_name in name:
                         meta = yolo_regions[region_name].meta
             params = self.Params(meta, shape[2:4])
             output_info[name] = (shape, params)
         return output_info
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "iou_threshold": NumericalValue(
-                    default_value=0.5,
-                    description="Threshold for non-maximum suppression (NMS) intersection over union (IOU) filtering",
-                ),
-            }
-        )
-        parameters["resize_type"].update_default_value("fit_to_window_letterbox")
-        parameters["confidence_threshold"].update_default_value(0.5)
+        parameters.update({
+            'iou_threshold': NumericalValue(default_value=0.5, description="Threshold for NMS filtering"),
+        })
+        parameters['resize_type'].update_default_value('fit_to_window_letterbox')
+        parameters['confidence_threshold'].update_default_value(0.5)
         return parameters
 
     def postprocess(self, outputs, meta):
         detections = self._parse_outputs(outputs, meta)
         detections = self._resize_detections(detections, meta)
-        detections = self._add_label_names(detections)
         return detections
 
     def _parse_yolo_region(self, predictions, input_size, params):
         # ------------------------------------------ Extracting layer parameters ---------------------------------------
         objects = []
         size_normalizer = input_size if params.use_input_size else params.sides
-        predictions = permute_to_N_HWA_K(
-            predictions, params.bbox_size, params.output_layout
-        )
+        predictions = permute_to_N_HWA_K(predictions, params.bbox_size, params.output_layout)
         # ------------------------------------------- Parsing YOLO Region output ---------------------------------------
         for prediction in predictions:
             # Getting probabilities from raw outputs
             class_probabilities = self._get_probabilities(prediction, params.classes)
 
             # filter out the proposals with low confidence score
             keep_idxs = np.nonzero(class_probabilities > self.confidence_threshold)[0]
@@ -211,36 +140,25 @@
             class_idx = keep_idxs % params.classes
 
             for ind, obj_ind in enumerate(obj_indx):
                 row, col, n = self._get_location(obj_ind, params.sides[1], params.num)
 
                 # Process raw value to get absolute coordinates of boxes
                 raw_box = self._get_raw_box(prediction, obj_ind)
-                predicted_box = self._get_absolute_det_box(
-                    raw_box,
-                    row,
-                    col,
-                    params.anchors[2 * n : 2 * n + 2],
-                    params.sides,
-                    size_normalizer,
-                )
+                predicted_box = self._get_absolute_det_box(raw_box, row, col, params.anchors[2 * n:2 * n + 2],
+                                                           params.sides, size_normalizer)
 
                 # Define class_label and cofidence
                 label = class_idx[ind]
                 confidence = class_probabilities[ind]
-                objects.append(
-                    Detection(
-                        predicted_box.x - predicted_box.w / 2,
-                        predicted_box.y - predicted_box.h / 2,
-                        predicted_box.x + predicted_box.w / 2,
-                        predicted_box.y + predicted_box.h / 2,
-                        confidence.item(),
-                        label.item(),
-                    )
-                )
+                objects.append(Detection(predicted_box.x - predicted_box.w / 2,
+                                         predicted_box.y - predicted_box.h / 2,
+                                         predicted_box.x + predicted_box.w / 2,
+                                         predicted_box.y + predicted_box.h / 2,
+                                         confidence.item(), label.item()))
 
         return objects
 
     @staticmethod
     def _get_probabilities(prediction, classes):
         object_probabilities = prediction[:, 4].flatten()
         class_probabilities = prediction[:, 5:].flatten()
@@ -255,33 +173,27 @@
         return row, col, n
 
     @staticmethod
     def _get_raw_box(prediction, obj_ind):
         return DetectionBox(*prediction[obj_ind, :4])
 
     @staticmethod
-    def _get_absolute_det_box(
-        box, row, col, anchors, coord_normalizer, size_normalizer
-    ):
+    def _get_absolute_det_box(box, row, col, anchors, coord_normalizer, size_normalizer):
         x = (col + box.x) / coord_normalizer[1]
         y = (row + box.y) / coord_normalizer[0]
         width = np.exp(box.w) * anchors[0] / size_normalizer[1]
         height = np.exp(box.h) * anchors[1] / size_normalizer[0]
 
         return DetectionBox(x, y, width, height)
 
     @staticmethod
     def _filter(detections, iou_threshold):
         def iou(box_1, box_2):
-            width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(
-                box_1.xmin, box_2.xmin
-            )
-            height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(
-                box_1.ymin, box_2.ymin
-            )
+            width_of_overlap_area = min(box_1.xmax, box_2.xmax) - max(box_1.xmin, box_2.xmin)
+            height_of_overlap_area = min(box_1.ymax, box_2.ymax) - max(box_1.ymin, box_2.ymin)
             if width_of_overlap_area < 0 or height_of_overlap_area < 0:
                 area_of_overlap = 0
             else:
                 area_of_overlap = width_of_overlap_area * height_of_overlap_area
             box_1_area = (box_1.ymax - box_1.ymin) * (box_1.xmax - box_1.xmin)
             box_2_area = (box_2.ymax - box_2.ymin) * (box_2.xmax - box_2.xmin)
             area_of_union = box_1_area + box_2_area - area_of_overlap
@@ -305,24 +217,22 @@
 
     def _parse_outputs(self, outputs, meta):
         detections = []
         for layer_name in self.yolo_layer_params.keys():
             out_blob = outputs[layer_name]
             layer_params = self.yolo_layer_params[layer_name]
             out_blob.shape = layer_params[0]
-            detections += self._parse_yolo_region(
-                out_blob, meta["resized_shape"], layer_params[1]
-            )
+            detections += self._parse_yolo_region(out_blob, meta['resized_shape'], layer_params[1])
 
         detections = self._filter(detections, self.iou_threshold)
         return detections
 
 
 class YoloV4(YOLO):
-    __model__ = "YOLOV4"
+    __model__ = 'YOLOV4'
 
     class Params:
         def __init__(self, classes, num, sides, anchors, mask, layout):
             self.num = num
             self.coords = 4
             self.classes = classes
             self.bbox_size = self.coords + self.classes + 1
@@ -330,64 +240,47 @@
             self.output_layout = layout
             masked_anchors = []
             for idx in mask:
                 masked_anchors += [anchors[idx * 2], anchors[idx * 2 + 1]]
             self.anchors = masked_anchors
             self.use_input_size = True
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
 
     def _get_output_info(self):
         if not self.anchors:
-            self.anchors = ANCHORS["YOLOV4-TINY"] if self.is_tiny else ANCHORS["YOLOV4"]
+            self.anchors = ANCHORS['YOLOV4-TINY'] if self.is_tiny else ANCHORS['YOLOV4']
         if not self.masks:
-            self.masks = (
-                [1, 2, 3, 3, 4, 5] if self.is_tiny else [0, 1, 2, 3, 4, 5, 6, 7, 8]
-            )
-
-        outputs = sorted(
-            self.outputs.items(), key=lambda x: x[1].shape[2], reverse=True
-        )
+            self.masks = [1, 2, 3, 3, 4, 5] if self.is_tiny else [0, 1, 2, 3, 4, 5, 6, 7, 8]
+
+        outputs = sorted(self.outputs.items(), key=lambda x: x[1].shape[2], reverse=True)
 
         output_info = {}
         num = 3
         for i, (name, layer) in enumerate(outputs):
             shape = layer.shape
             if shape[2] == shape[3]:
-                channels, sides, layout = shape[1], shape[2:4], "NCHW"
+                channels, sides, layout = shape[1], shape[2:4], 'NCHW'
             else:
-                channels, sides, layout = shape[3], shape[1:3], "NHWC"
+                channels, sides, layout = shape[3], shape[1:3], 'NHWC'
             classes = channels // num - 5
             if channels % num != 0:
-                self.raise_error(
-                    "The output blob {} has wrong 2nd dimension".format(name)
-                )
-            yolo_params = self.Params(
-                classes,
-                num,
-                sides,
-                self.anchors,
-                self.masks[i * num : (i + 1) * num],
-                layout,
-            )
+                self.raise_error("The output blob {} has wrong 2nd dimension".format(name))
+            yolo_params = self.Params(classes, num, sides, self.anchors, self.masks[i*num : (i+1)*num], layout)
             output_info[name] = (shape, yolo_params)
         return output_info
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "anchors": ListValue(description="List of custom anchor values"),
-                "masks": ListValue(
-                    description="List of mask, applied to anchors for each output layer"
-                ),
-            }
-        )
+        parameters.update({
+            'anchors': ListValue(description="List of custom anchor values"),
+            'masks': ListValue(description="List of mask, applied to anchors for each output layer"),
+        })
         return parameters
 
     @staticmethod
     def _get_probabilities(prediction, classes):
         object_probabilities = sigmoid(prediction[:, 4].flatten())
         class_probabilities = sigmoid(prediction[:, 5:].flatten())
         class_probabilities *= np.repeat(object_probabilities, classes)
@@ -398,152 +291,122 @@
         bbox = prediction[obj_ind, :4]
         x, y = sigmoid(bbox[:2])
         width, height = bbox[2:]
         return DetectionBox(x, y, width, height)
 
 
 class YOLOF(YOLO):
-    __model__ = "YOLOF"
+    __model__ = 'YOLOF'
 
     class Params:
         def __init__(self, classes, num, sides, anchors):
             self.num = num
             self.coords = 4
             self.classes = classes
             self.bbox_size = self.coords + self.classes
             self.sides = sides
             self.anchors = anchors
-            self.output_layout = "NCHW"
+            self.output_layout = 'NCHW'
             self.use_input_size = True
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
 
     def _get_output_info(self):
-        anchors = ANCHORS["YOLOF"]
+        anchors = ANCHORS['YOLOF']
 
         output_info = {}
         num = 6
         for name, layer in self.outputs.items():
             shape = layer.shape
             classes = shape[1] // num - 4
             yolo_params = self.Params(classes, num, shape[2:4], anchors)
             output_info[name] = (shape, yolo_params)
         return output_info
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters["resize_type"].update_default_value("standard")
+        parameters['resize_type'].update_default_value('standard')
         return parameters
 
     @staticmethod
     def _get_probabilities(prediction, classes):
         return sigmoid(prediction[:, 4:].flatten())
 
     @staticmethod
-    def _get_absolute_det_box(
-        box, row, col, anchors, coord_normalizer, size_normalizer
-    ):
+    def _get_absolute_det_box(box, row, col, anchors, coord_normalizer, size_normalizer):
         anchor_x = anchors[0] / size_normalizer[0]
         anchor_y = anchors[1] / size_normalizer[1]
         x = box.x * anchor_x + col / coord_normalizer[1]
         y = box.y * anchor_y + row / coord_normalizer[0]
         width = np.exp(box.w) * anchor_x
         height = np.exp(box.h) * anchor_y
 
         return DetectionBox(x, y, width, height)
 
 
 class YOLOX(DetectionModel):
-    __model__ = "YOLOX"
+    __model__ = 'YOLOX'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
         self._check_io_number(1, 1)
         self.output_blob_name = next(iter(self.outputs))
 
         self.expanded_strides = []
         self.grids = []
         self.set_strides_grids()
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters.update(
-            {
-                "iou_threshold": NumericalValue(
-                    default_value=0.65,
-                    description="Threshold for non-maximum suppression (NMS) intersection over union (IOU) filtering",
-                ),
-            }
-        )
-        parameters["confidence_threshold"].update_default_value(0.5)
+        parameters.update({
+            'iou_threshold': NumericalValue(default_value=0.65, description="Threshold for NMS filtering"),
+        })
+        parameters['confidence_threshold'].update_default_value(0.5)
         return parameters
 
     def preprocess(self, inputs):
         image = inputs
         resized_image = resize_image(image, (self.w, self.h), keep_aspect_ratio=True)
 
         padded_image = np.ones((self.h, self.w, 3), dtype=np.uint8) * 114
         padded_image[: resized_image.shape[0], : resized_image.shape[1]] = resized_image
 
-        meta = {
-            "original_shape": image.shape,
-            "scale": min(self.w / image.shape[1], self.h / image.shape[0]),
-        }
+        meta = {'original_shape': image.shape,
+                'scale': min(self.w / image.shape[1], self.h / image.shape[0])}
 
         preprocessed_image = self.input_transform(padded_image)
-        preprocessed_image = preprocessed_image.transpose(
-            (2, 0, 1)
-        )  # Change data layout from HWC to CHW
-        preprocessed_image = preprocessed_image.reshape(
-            (self.n, self.c, self.h, self.w)
-        )
+        preprocessed_image = preprocessed_image.transpose((2, 0, 1))  # Change data layout from HWC to CHW
+        preprocessed_image = preprocessed_image.reshape((self.n, self.c, self.h, self.w))
 
         dict_inputs = {self.image_blob_name: preprocessed_image}
         return dict_inputs, meta
 
     def postprocess(self, outputs, meta):
         output = outputs[self.output_blob_name][0]
 
         if np.size(self.expanded_strides) != 0 and np.size(self.grids) != 0:
             output[..., :2] = (output[..., :2] + self.grids) * self.expanded_strides
             output[..., 2:4] = np.exp(output[..., 2:4]) * self.expanded_strides
 
         valid_predictions = output[output[..., 4] > self.confidence_threshold]
         valid_predictions[:, 5:] *= valid_predictions[:, 4:5]
 
-        boxes = self.xywh2xyxy(valid_predictions[:, :4]) / meta["scale"]
+        boxes = self.xywh2xyxy(valid_predictions[:, :4]) / meta['scale']
         i, j = (valid_predictions[:, 5:] > self.confidence_threshold).nonzero()
         x_mins, y_mins, x_maxs, y_maxs = boxes[i].T
         scores = valid_predictions[i, j + 5]
 
-        keep_nms = nms(
-            x_mins,
-            y_mins,
-            x_maxs,
-            y_maxs,
-            scores,
-            self.iou_threshold,
-            include_boundaries=True,
-        )
-
-        detections = [
-            Detection(*det)
-            for det in zip(
-                x_mins[keep_nms],
-                y_mins[keep_nms],
-                x_maxs[keep_nms],
-                y_maxs[keep_nms],
-                scores[keep_nms],
-                j[keep_nms],
-            )
-        ]
-        return clip_detections(detections, meta["original_shape"])
+        keep_nms = nms(x_mins, y_mins, x_maxs, y_maxs, scores, self.iou_threshold, include_boundaries=True)
+
+        detections = [Detection(*det) for det in zip(x_mins[keep_nms], y_mins[keep_nms], x_maxs[keep_nms],
+                                                     y_maxs[keep_nms], scores[keep_nms], j[keep_nms])]
+        return clip_detections(detections, meta['original_shape'])
 
     def set_strides_grids(self):
         grids = []
         expanded_strides = []
 
         strides = [8, 16, 32]
 
@@ -567,143 +430,90 @@
         y[:, 1] = x[:, 1] - x[:, 3] / 2
         y[:, 2] = x[:, 0] + x[:, 2] / 2
         y[:, 3] = x[:, 1] + x[:, 3] / 2
         return y
 
 
 class YoloV3ONNX(DetectionModel):
-    __model__ = "YOLOv3-ONNX"
+    __model__ = 'YOLOv3-ONNX'
 
-    def __init__(self, inference_adapter, configuration=None, preload=False):
-        super().__init__(inference_adapter, configuration, preload)
-        self.image_info_blob_name = (
-            self.image_info_blob_names[0]
-            if len(self.image_info_blob_names) == 1
-            else None
-        )
+    def __init__(self, model_adapter, configuration=None, preload=False):
+        super().__init__(model_adapter, configuration, preload)
+        self.image_info_blob_name = self.image_info_blob_names[0] if len(self.image_info_blob_names) == 1 else None
         self._check_io_number(2, 3)
         self.classes = 80
-        (
-            self.bboxes_blob_name,
-            self.scores_blob_name,
-            self.indices_blob_name,
-        ) = self._get_outputs()
-
-        if self.embed_preprocessing:
-            layout = "NHWC" if self.nchw_layout else "NCHW"
-            inference_adapter.embed_preprocessing(
-                image_layout=layout,
-                resize_mode="standard",
-                interpolation_mode="CUBIC",
-                target_shape=(self.w, self.h),
-            )
+        self.bboxes_blob_name, self.scores_blob_name, self.indices_blob_name = self._get_outputs()
 
     def _get_outputs(self):
         bboxes_blob_name = None
         scores_blob_name = None
         indices_blob_name = None
         for name, layer in self.outputs.items():
             if layer.shape[-1] == 3:
                 indices_blob_name = name
             elif layer.shape[2] == 4:
                 bboxes_blob_name = name
             elif layer.shape[1] == self.classes:
                 scores_blob_name = name
             else:
-                self.raise_error(
-                    "Expected shapes [:,:,4], [:,{},:] and [:,3] for outputs, but got {}, {} and {}".format(
-                        self.classes,
-                        *[output.shape for output in self.outputs.values()]
-                    )
-                )
-        if (
-            self.outputs[bboxes_blob_name].shape[1]
-            != self.outputs[scores_blob_name].shape[2]
-        ):
-            self.raise_error(
-                "Expected the same dimension for boxes and scores, but got {} and {}".format(
-                    self.outputs[bboxes_blob_name].shape[1],
-                    self.outputs[scores_blob_name].shape[2],
-                )
-            )
+                self.raise_error("Expected shapes [:,:,4], [:,{},:] and [:,3] for outputs, but got {}, {} and {}"
+                    .format(self.classes, *[output.shape for output in self.outputs.values()]))
+        if self.outputs[bboxes_blob_name].shape[1] != self.outputs[scores_blob_name].shape[2]:
+            self.raise_error("Expected the same dimension for boxes and scores, but got {} and {}".format(
+                self.outputs[bboxes_blob_name].shape[1], self.outputs[scores_blob_name].shape[2]))
         return bboxes_blob_name, scores_blob_name, indices_blob_name
 
     @classmethod
     def parameters(cls):
         parameters = super().parameters()
-        parameters["resize_type"].update_default_value("fit_to_window_letterbox")
-        parameters["confidence_threshold"].update_default_value(0.5)
+        parameters['resize_type'].update_default_value('fit_to_window_letterbox')
+        parameters['confidence_threshold'].update_default_value(0.5)
         return parameters
 
     def preprocess(self, inputs):
         image = inputs
-        dict_inputs = {}
-        meta = {"original_shape": image.shape}
-
-        if self.embed_preprocessing:
-            meta.update({"resized_shape": (self.w, self.h)})
-
-            dict_inputs = {
-                self.image_blob_name: np.expand_dims(image, axis=0),
-                self.image_info_blob_name: np.array(
-                    [[image.shape[0], image.shape[1]]], dtype=np.float32
-                ),
-            }
-        else:
-            resized_image = self.resize(
-                image, (self.w, self.h), interpolation=INTERPOLATION_TYPES["CUBIC"]
-            )
-            meta.update({"resized_shape": resized_image.shape})
-            resized_image = self._change_layout(resized_image)
-            dict_inputs = {
-                self.image_blob_name: resized_image,
-                self.image_info_blob_name: np.array(
-                    [[image.shape[0], image.shape[1]]], dtype=np.float32
-                ),
-            }
-
+        meta = {'original_shape': image.shape}
+        resized_image = self.resize(image, (self.w, self.h), interpolation=INTERPOLATION_TYPES['CUBIC'])
+        meta.update({'resized_shape': resized_image.shape})
+        resized_image = self._change_layout(resized_image)
+        dict_inputs = {
+            self.image_blob_name: resized_image,
+            self.image_info_blob_name: np.array([[image.shape[0], image.shape[1]]], dtype=np.float32)
+        }
         return dict_inputs, meta
 
     def postprocess(self, outputs, meta):
         detections = self._parse_outputs(outputs)
-        detections = clip_detections(detections, meta["original_shape"])
+        detections = clip_detections(detections, meta['original_shape'])
         return detections
 
     def _parse_outputs(self, outputs):
         boxes = outputs[self.bboxes_blob_name][0]
         scores = outputs[self.scores_blob_name][0]
-        indices = (
-            outputs[self.indices_blob_name]
-            if len(outputs[self.indices_blob_name].shape) == 2
-            else outputs[self.indices_blob_name][0]
-        )
+        indices = outputs[self.indices_blob_name] if len(
+            outputs[self.indices_blob_name].shape) == 2 else outputs[self.indices_blob_name][0]
 
         out_boxes, out_scores, out_classes = [], [], []
         for idx_ in indices:
             if idx_[0] == -1:
                 break
             out_classes.append(idx_[1])
             out_scores.append(scores[tuple(idx_[1:])])
             out_boxes.append(boxes[idx_[2]])
         transposed_boxes = np.array(out_boxes).T if out_boxes else ([], [], [], [])
         mask = np.array(out_scores) > self.confidence_threshold
 
         if mask.size == 0:
             return []
 
-        out_classes, out_scores, transposed_boxes = (
-            np.array(out_classes)[mask],
-            np.array(out_scores)[mask],
-            transposed_boxes[:, mask],
-        )
+        out_classes, out_scores, transposed_boxes = (np.array(out_classes)[mask], np.array(out_scores)[mask],
+                                                     transposed_boxes[:, mask])
 
         x_mins = transposed_boxes[1]
         y_mins = transposed_boxes[0]
         x_maxs = transposed_boxes[3]
         y_maxs = transposed_boxes[2]
 
-        detections = [
-            Detection(*det)
-            for det in zip(x_mins, y_mins, x_maxs, y_maxs, out_scores, out_classes)
-        ]
+        detections = [Detection(*det) for det in zip(x_mins, y_mins, x_maxs,
+                                                     y_maxs, out_scores, out_classes)]
 
-        return detections
+        return detections
```

## visiongraph/external/intel/pipelines/async_pipeline.py

```diff
@@ -1,9 +1,9 @@
 """
- Copyright (C) 2020-2022 Intel Corporation
+ Copyright (C) 2020-2023 Intel Corporation
 
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at
 
       http://www.apache.org/licenses/LICENSE-2.0
```

## visiongraph/util/NetworkUtils.py

```diff
@@ -1,30 +1,41 @@
 import logging
 import os
+import shutil
 import sys
+from pathlib import Path
 from typing import Tuple
 
 import requests
 from tqdm import tqdm
 
 import visiongraph.cache
 
 PUBLIC_DATA_URL = "https://github.com/cansik/data-storage/releases/download/sarmotion/"
 
 
-def download_file(url: str, path: str, description: str = "download"):
+def download_file(url: str, path: str, description: str = "download", with_progress: bool = True):
+    os.makedirs(os.path.dirname(path), exist_ok=True)
+
+    if not with_progress:
+        with tqdm(total=1, desc=description) as pb:
+            response = requests.get(url, stream=True)
+
+            with open(path, "wb") as out_file:
+                shutil.copyfileobj(response.raw, out_file)
+            pb.update()
+        return
+
     head_request = requests.head(url)
 
     if "Content-Length" in head_request.headers:
         filesize = int(head_request.headers["Content-Length"])
     else:
         filesize = 0
 
-    os.makedirs(os.path.dirname(path), exist_ok=True)
-
     dl_path = path
     chunk_size = 1024
 
     with requests.get(url, stream=True) as r, open(dl_path, "wb") as f, tqdm(
             unit="B",
             unit_scale=True,
             unit_divisor=1024,
@@ -54,15 +65,19 @@
         return file_path
 
     temp_file = os.path.join(data_path, f"{file_name}.tmp")
 
     if os.path.exists(temp_file):
         os.remove(temp_file)
 
-    download_file(url, temp_file, f"Downloading {file_name}")
+    try:
+        download_file(url, temp_file, f"Downloading {file_name}")
+    except Exception as ex:
+        logging.warning(f"Retry download because {file_name} could not be download: {ex}")
+        download_file(url, temp_file, f"Downloading {file_name}", with_progress=False)
 
     # check if file has been downloaded correctly
     head = ""
     try:
         with open(temp_file, 'rb') as f:
             head = f.read(9).decode()
     except Exception as ex:
```

## Comparing `visiongraph-0.1.47.5.dist-info/METADATA` & `visiongraph-0.1.48.dist-info/METADATA`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: visiongraph
-Version: 0.1.47.5
+Version: 0.1.48
 Summary: Visiongraph is a high level computer vision framework.
 Home-page: https://github.com/cansik/visiongraph
 Author: Florian Bruggisser
 Author-email: github@broox.ch
 License: MIT License
 Platform: UNKNOWN
 Classifier: Development Status :: 3 - Alpha
@@ -28,15 +28,15 @@
 Requires-Dist: numpy (<=1.22.4)
 Requires-Dist: requests
 Requires-Dist: tqdm
 Requires-Dist: vector (~=1.0.0)
 Requires-Dist: scipy
 Requires-Dist: filterpy
 Provides-Extra: all
-Requires-Dist: openvino (~=2022.3.0) ; extra == 'all'
+Requires-Dist: openvino (~=2023.0.1) ; extra == 'all'
 Requires-Dist: mediapipe (~=0.10.0) ; extra == 'all'
 Requires-Dist: onnxruntime (~=1.15.0) ; extra == 'all'
 Requires-Dist: moviepy ; extra == 'all'
 Requires-Dist: vidgear[core] ; extra == 'all'
 Requires-Dist: numba ; extra == 'all'
 Requires-Dist: pyrealsense2 ; (platform_system != "Darwin") and extra == 'all'
 Requires-Dist: onnxruntime-gpu (~=1.15.0) ; (platform_system != "Darwin") and extra == 'all'
@@ -63,15 +63,15 @@
 Requires-Dist: numba ; extra == 'numba'
 Provides-Extra: onnx
 Requires-Dist: onnxruntime (~=1.15.0) ; extra == 'onnx'
 Provides-Extra: onnx-gpu
 Requires-Dist: onnxruntime-gpu (~=1.15.0) ; (platform_system != "Darwin") and extra == 'onnx-gpu'
 Requires-Dist: onnxruntime (~=1.15.0) ; (platform_system == "Darwin") and extra == 'onnx-gpu'
 Provides-Extra: openvino
-Requires-Dist: openvino (~=2022.3.0) ; extra == 'openvino'
+Requires-Dist: openvino (~=2023.0.1) ; extra == 'openvino'
 Provides-Extra: realsense
 Requires-Dist: pyrealsense2 ; (platform_system != "Darwin") and extra == 'realsense'
 Requires-Dist: pyrealsense2-macosx ; (platform_system == "Darwin") and extra == 'realsense'
 
 # ![image](https://user-images.githubusercontent.com/5220162/192808079-2043fb41-8637-4697-8286-985bc5340f37.png) Visiongraph [![PyPI](https://img.shields.io/pypi/v/visiongraph)](https://pypi.org/project/visiongraph/)
 Visiongraph is a high level computer vision framework that includes predefined modules to quickly create and run algorithms on images. It is based on opencv and includes other computer vision frameworks like [Intel openVINO](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) and [Google MediaPipe](https://google.github.io/mediapipe/).
```

## Comparing `visiongraph-0.1.47.5.dist-info/RECORD` & `visiongraph-0.1.48.dist-info/RECORD`

 * *Files 1% similar despite different names*

```diff
@@ -36,27 +36,27 @@
 visiongraph/estimator/inpaint/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/estimator/onnx/ONNXVisionEngine.py,sha256=sV1H67cOv-fiTsOBAq4c7wxHkQNN5RSbdtRwGZn-53g,2660
 visiongraph/estimator/onnx/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/estimator/openvino/OpenVinoEngine.py,sha256=_f_6jE9ONTJeZ4WiCQ1BfY_J2HDk84yt7w9tl8a1UAE,2516
 visiongraph/estimator/openvino/OpenVinoObjectDetector.py,sha256=vJryusJDc1YWMZdRdQnc1XLomowE1TcVF4mwODR02tU,2516
 visiongraph/estimator/openvino/OpenVinoPoseEstimator.py,sha256=c3geJ5Qm18uCRTp-_iBj62yfPXV5TP4HJ0aWRx9kZJ4,2967
 visiongraph/estimator/openvino/SyncInferencePipeline.py,sha256=A9Wpw7DkqXb584UhqJPBNLddi44fHMuNjXhI6RtD9ws,726
-visiongraph/estimator/openvino/VisionInferenceEngine.py,sha256=ZaQbBA2pfE2muxd5nVp-Yngz7-HLauBiMLe6fCOZCEE,1957
+visiongraph/estimator/openvino/VisionInferenceEngine.py,sha256=5LGBMP_LVmUSJFPcosod-joIm6NZiOUsxjLWb4jTPUk,2438
 visiongraph/estimator/openvino/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph/estimator/spatial/CenterNetDetector.py,sha256=v0RPUeSgWHl8n60xKl0HT13aZ25XEPRYJsLbjbgX2iU,1750
+visiongraph/estimator/spatial/CenterNetDetector.py,sha256=3EsFgVjTYHgm_ayN6KflKlPUodzhonLDcOkg8C1sBEg,1877
 visiongraph/estimator/spatial/CrowdHumanDetector.py,sha256=FA-rh-0mbcoqSNhs6SUwY9GFhhISclIP1QM2rnhIML4,3270
-visiongraph/estimator/spatial/DETRDetector.py,sha256=WdO5By3b4GO4jBsT4YdYjztAC6sEP-ntdeh55FeQLZc,1781
+visiongraph/estimator/spatial/DETRDetector.py,sha256=C9X02LjDmngU1k9fGY1EhoNJs5FoJZdEpvxtkNQ3yfI,1908
 visiongraph/estimator/spatial/InstanceSegmentationEstimator.py,sha256=FLwQBX8ZcB5-brYe-ye7XHIWmUaPaX0UOa1eCNikOD0,544
 visiongraph/estimator/spatial/LandmarkEstimator.py,sha256=H8bvBn3QWTsT3cqvCmyBFazT-yXhUaUTKPMUjbWGAns,523
 visiongraph/estimator/spatial/ObjectDetector.py,sha256=U5rEA-bGvLaHsV3zhMhAJumnPPiaTGUu0p6Rlk1nhcg,524
 visiongraph/estimator/spatial/RoiEstimator.py,sha256=cfZUne-PGV1PuFk7Hwy6Vqijvrlzqcpl9IryBfx0VaA,1279
-visiongraph/estimator/spatial/SSDDetector.py,sha256=NJHlb-3KMLaCS8l4TsfH4xk0gTjY5BnnEic8Mu0310s,3147
+visiongraph/estimator/spatial/SSDDetector.py,sha256=apiuTiQyIISyV4PIglZ-e5FQYhG-ezoY3RceOey92-o,3251
 visiongraph/estimator/spatial/SlidingWindowEstimator.py,sha256=x2KMrhsQesB8KVhDnx28399IgevXYQ_pGntREncmwJ4,2082
 visiongraph/estimator/spatial/SpatialCascadeEstimator.py,sha256=thm2fFIJ-SJZrDatQcgiT5DK35MNwA_7O0N1lWEO5jk,1436
-visiongraph/estimator/spatial/YOLODetector.py,sha256=g0XvES7Ol8SVHCerrKB64Hp3p_ldN0uVwzsrlafdJJw,3173
+visiongraph/estimator/spatial/YOLODetector.py,sha256=b36BNtcM6B-juVK2FwOIqqV2DaO8QW7l0Wf7C48xuQg,3300
 visiongraph/estimator/spatial/YOLOXE2EDetector.py,sha256=Hz6d6jyS9KZ0u-fSNFLAhF0yNhOx_67WvqLMToGeQVY,2904
 visiongraph/estimator/spatial/YOLOv5Detector.py,sha256=CDGhz50x23KGjbHTB29to0Wjoaj0E_3BmNNY01W2mC0,3594
 visiongraph/estimator/spatial/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/estimator/spatial/camera/ArUcoCameraPoseEstimator.py,sha256=GrahyzDfdN0gDaGlAIjmZ27ZF2G3_-TZZMWM_Xqt2YU,3441
 visiongraph/estimator/spatial/camera/BoardCameraCalibrator.py,sha256=qlLwrjPfPa2kB-y_0NMYEqpqCaCU4K3EUaNY1F6sK8E,850
 visiongraph/estimator/spatial/camera/ChArUcoCalibrator.py,sha256=f0aX1ONvblYcItbf-R8e_qQOyixvhgDB_DKBRIrZcXU,4276
 visiongraph/estimator/spatial/camera/ChessboardCalibrator.py,sha256=8WN2RC93HRtlTz5bY0iNItGpOSBUGX58SAU7O-9T5JU,3145
@@ -83,57 +83,58 @@
 visiongraph/estimator/spatial/face/recognition/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/estimator/spatial/hand/HandDetector.py,sha256=ugGPMuf-q4_odSRlysOuZpWp7G9gwCOfZ59oL18xy3Q,517
 visiongraph/estimator/spatial/hand/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/estimator/spatial/hand/landmark/HandLandmarkEstimator.py,sha256=P-6AOjF86e3Dyk-zWeWJkg_sap6HmWYuYJpGZqtVK-s,608
 visiongraph/estimator/spatial/hand/landmark/MediaPipeHandEstimator.py,sha256=ITpw82zjKLrgZck7iGS81AHoSxuqMr69FNchLu-p2io,2778
 visiongraph/estimator/spatial/hand/landmark/OpenPoseHandEstimator.py,sha256=lB8c8N6CNJ0HRhSmrhTUsNWJc2yTySsEwpN5uJ8rXWo,2330
 visiongraph/estimator/spatial/hand/landmark/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph/estimator/spatial/pose/AEPoseEstimator.py,sha256=rULNcWF_Beo3EJVZnlFCv0imiv6WM1M9yK7fEdWFYpY,2086
+visiongraph/estimator/spatial/pose/AEPoseEstimator.py,sha256=QuoJv0ODk95yUL0XvbaaxWHG5XdPx6FJv5FoNsqO7D0,2219
 visiongraph/estimator/spatial/pose/EfficientPoseEstimator.py,sha256=h86wNPcUWykjB6qVdZGqhCwYmSA1mdUHGHsqfvZGXHs,5139
 visiongraph/estimator/spatial/pose/KAPAOPoseEstimator.py,sha256=A2FaPU0wwhOOPYTrpHfESwJqKeK_bGEv-sRPggb4B_c,7134
 visiongraph/estimator/spatial/pose/LiteHRNetEstimator.py,sha256=iKR93_EEfwoJXmOt1PBq80pYFB8dXGpoGqvw9AVVaEs,3890
 visiongraph/estimator/spatial/pose/LitePoseEstimator.py,sha256=CuJ7HcLcHX1aznd6slA3PUZ7cOqyHVc8bV0TFsuK2oc,2855
 visiongraph/estimator/spatial/pose/MediaPipePoseEstimator.py,sha256=m69OREObNr8IAfLxm0xu5Un9ZpBG9QAtekIOvOJ7FWE,3100
 visiongraph/estimator/spatial/pose/MobileHumanPoseEstimator.py,sha256=NrCn6kfhgIoa0ACWPyjwDEmBIKVD5lU4LjgdFz_Yu_g,6641
 visiongraph/estimator/spatial/pose/MobileNetV2PoseEstimator.py,sha256=43afruD9d_zQ0ESAc1vKKojbDccmBsym8KD44l0ZiIk,9658
 visiongraph/estimator/spatial/pose/MoveNetPoseEstimator.py,sha256=SyypcLTvZuYHVRtVXF6aSz86n8iiGX8ZmuK5sYAggAw,4325
-visiongraph/estimator/spatial/pose/OpenPoseEstimator.py,sha256=tmds9aUZSMvAvUo197vh4L_oYnEMoCrXrNpMW_vyyvo,1660
+visiongraph/estimator/spatial/pose/OpenPoseEstimator.py,sha256=PGobij9rNafA0bVIfI0qfeER1QhcgYzXBPQrZGRm55o,1876
 visiongraph/estimator/spatial/pose/PoseEstimator.py,sha256=X1JvTnef7g9frs89SwjCE8PiJtFS6qXWUZda66zKcKc,518
 visiongraph/estimator/spatial/pose/TopDownPoseEstimator.py,sha256=8OcnaFirohSyCyYchoUNwVoPWCh-qjU5F-bp6SkvdIM,2347
 visiongraph/estimator/spatial/pose/__init__.py,sha256=J3zVaVQ4SZJZRMQfrZNIlIbC_5WMT2StI2XcpS4PrhE,5416
 visiongraph/estimator/spatial/segmentation/MaskRCNNEstimator.py,sha256=QUnUP9IIEbnantEcQPuGvQjle8NhSeTbXv5Ns-y3pOE,7516
 visiongraph/estimator/spatial/segmentation/MediaPipeSelfieSegmentation.py,sha256=y7oi5fZiEHRlbKFstap3ai5I0AYyXJrKMsSmWESMtRU,1989
 visiongraph/estimator/spatial/segmentation/YolactEstimator.py,sha256=tO6z6y-PvDW-8bJvmJdIyiQsHZbQhA0WDB50uXWVLRs,3287
 visiongraph/estimator/spatial/segmentation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/estimator/translation/DeblurGANv2.py,sha256=E1cooQe8gIvtLWdwyd492bNfnbp_naic4pC4JmyABvg,1714
 visiongraph/estimator/translation/DepthEstimator.py,sha256=7-3ajpANUrYmkWHxTE6VDphAfmtp12ty-UXxcoddXzI,314
 visiongraph/estimator/translation/MidasDepthEstimator.py,sha256=-OxQaZrS7qc9-2DQwna2LfRlkCHY5NYREDDBRGb8oBc,2898
 visiongraph/estimator/translation/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/external/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/external/intel/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph/external/intel/performance_metrics.py,sha256=WWU9IXR7COl790yl3qhUU0fLLPBov8mkFsVplVkxuhs,4337
+visiongraph/external/intel/performance_metrics.py,sha256=dr2UaWXFQpjASlmKz-c_EFL0SVltrsh6kmOZ0aCF62s,4342
 visiongraph/external/intel/adapters/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/external/intel/adapters/inference_adapter.py,sha256=XM5DKi9T5BHsUPUZtdFASCDDvEs8PjjzOm5VsY_ep6o,5682
-visiongraph/external/intel/adapters/openvino_adapter.py,sha256=2LMvnhJwSrsWfLni84Q97kKeknsuwkohVtPE6FMRJv0,15557
-visiongraph/external/intel/adapters/ovms_adapter.py,sha256=uIiFuteKImT1DOKzXuJMRCliPuzb4NgFHu0YPVIyPhE,7413
-visiongraph/external/intel/adapters/utils.py,sha256=aVJ_KGWiVsj4XBK_EZaQNwjeqfeK7WAdmmlvpnooq-w,10920
+visiongraph/external/intel/adapters/model_adapter.py,sha256=YRCgonYxmXDF_RJ5DgRITMFCbWMTzslcILjdwP1sY8Q,5151
+visiongraph/external/intel/adapters/openvino_adapter.py,sha256=YLrI94TE7XlwnMCnbAda37QdeOeC7i7qD8IzFC_LBHk,8056
+visiongraph/external/intel/adapters/ovms_adapter.py,sha256=wXWJeYKGpNt9KgNsa15BkJ9s3u-RQ6RGfibt5398EDc,6585
+visiongraph/external/intel/adapters/utils.py,sha256=YBHNa1wOtFqpt53QtztQRG_yaK45DSokmIdwvaxyPJ8,2794
 visiongraph/external/intel/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph/external/intel/models/centernet.py,sha256=Tv_TwdMmrsI_kA6olewQs6hFE-QhhVntIugztnDcFqg,7582
-visiongraph/external/intel/models/detection_model.py,sha256=gibIbBQtlel1fCOD0CKl0MsMOXk9oPE4UTlDKmDPlx0,6784
-visiongraph/external/intel/models/detr.py,sha256=GR6u9ZyzIbqQqNUD1x-xVumIhqIsZibXYAXayiTGSSc,3215
-visiongraph/external/intel/models/hpe_associative_embedding.py,sha256=6m9iNepZnLNoYSVfrEXqb-s5O5c8vU3FhMfuZtCg6dA,16363
-visiongraph/external/intel/models/image_model.py,sha256=uatKXGl0BX7THXvwldX5OBtQ3dpGHjBHG_uEcVpC_gc,8434
-visiongraph/external/intel/models/model.py,sha256=Z8VSCNpkClJcn07HQav31nxUD4e8Hh0Gb1oN40irC5k,19269
-visiongraph/external/intel/models/open_pose.py,sha256=Eutygt7wckFxRdEslzAm90sqnBRMrARLjE_b_eOLTAA,19768
-visiongraph/external/intel/models/ssd.py,sha256=ZZ8Ysk_t5CS1qDQquxz9wFNV0Gy0sHqZFP-wLDlMzPY,5995
-visiongraph/external/intel/models/types.py,sha256=9K_tEtT_WUyoDXxdI86EvJwIfmLq9CY-TzjiGoIuGsE,7510
-visiongraph/external/intel/models/utils.py,sha256=f-WruGM6OsXpc_FMSFPhnb1PgitJzWouU4hLyrbWMRs,7600
-visiongraph/external/intel/models/yolo.py,sha256=TmsoEN7bFIYjKkC8IQbGZ2qFs-h7MlUw9bYO5p71KoA,24161
+visiongraph/external/intel/models/centernet.py,sha256=48m3B-RGnpkR5KwhkAdD8IuAzyaxBlj6SIe4_g9RKmE,7442
+visiongraph/external/intel/models/detection_model.py,sha256=ZVlXKp8YwogVg6z2LU_HfyQxnsioiuZcHstix9R6fUU,5700
+visiongraph/external/intel/models/detr.py,sha256=8Mjhc_uBnQ-MQgI0E9tvTmnGkihe7uMZM-hTd39_jV8,2992
+visiongraph/external/intel/models/hpe_associative_embedding.py,sha256=KXMKdQ5T8cTSdv58cUBLNxQ16MO42iMbyJZ2GLTsUoM,14421
+visiongraph/external/intel/models/image_model.py,sha256=4XZk3RyvqxsXP69u7r08GcfIQPhgPPxvggtR48y_NaU,7247
+visiongraph/external/intel/models/model.py,sha256=2kCapKc8aCbB2l1uPFu6i318ZcIUpFN7dS-B8Pu3ldE,12558
+visiongraph/external/intel/models/open_pose.py,sha256=fPGPL0Mn_FsKIwwIo4GfW0DioDgQO7we_NIMskMSAcE,17789
+visiongraph/external/intel/models/ssd.py,sha256=1hIYDmbJjt3VEuTeKW86nUDH-gWv3T7sO05eCrzD4bQ,5874
+visiongraph/external/intel/models/types.py,sha256=r4lFvkkzwRsRg0h1mwxxv8I_sOHsFY9Sxm5OjaVik7g,5805
+visiongraph/external/intel/models/utils.py,sha256=mu0SO87-h3WGh4N8iTaKWvWewjMogC6kwPSD7vRFFKo,7260
+visiongraph/external/intel/models/yolo.py,sha256=16-6zQnS8fIHKwaM1MvO_g3D9vuBHAFKAiQKxoW2qUc,21407
 visiongraph/external/intel/pipelines/__init__.py,sha256=mtvCKMqg3vVds71znEIBZHW1V5ez0OYcZTBvB6GJIrc,154
-visiongraph/external/intel/pipelines/async_pipeline.py,sha256=IsEscJCvunkoFu2zzUviUpIXFmfOtVbN8isfFWxPJ_c,5407
+visiongraph/external/intel/pipelines/async_pipeline.py,sha256=mpVwo0Tz1sf2eOiFuxj3tww90nfKtdzBqQ71R9yzzXM,5407
 visiongraph/external/midas/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 visiongraph/external/midas/transforms.py,sha256=2INCPf0l6DkTVu50xib4O32BVZkntWGGlQiYYzlri1o,7868
 visiongraph/external/motpy/__init__.py,sha256=awcPN-cDSBjKClBfbgaTDrxBGeEH8Xfy0B3TdUHruv4,193
 visiongraph/external/motpy/core.py,sha256=zLPJplXdn5tbqDHGXKhmqpcsjfxvWTOaGWk1--NNn4I,1816
 visiongraph/external/motpy/detector.py,sha256=AtAMIB3RNqtvNoSDRzz8KnJeIwl8D4uKQTZbtGV-Smw,333
 visiongraph/external/motpy/metrics.py,sha256=wHSjfJpmn8XO450TQmmrkfXWS02zonGrR3arWF6hfbM,1147
 visiongraph/external/motpy/model.py,sha256=wkyRd0df-h3FIs_nU-uVfuoD_tKPsNt_mZX7-E1pMZw,5402
@@ -247,19 +248,19 @@
 visiongraph/util/CollectionUtils.py,sha256=IR178KrIULFRiSZzSMQkH78AaTvjhpaGFjJ57jZu0vw,222
 visiongraph/util/CommonArgs.py,sha256=sLhJv-vgMkaSCwbrfpDsZy8cAK7-kTAa5pnw_X_gqgU,314
 visiongraph/util/DrawingUtils.py,sha256=7_vksJauc7scpmFKHs8_S5xviGQoOd4n_p7sfix5G2U,3653
 visiongraph/util/ImageUtils.py,sha256=_eP8x8IrRiQv7IIOPsrBXFycGa8IgOPfKk_xsF9syi8,3588
 visiongraph/util/LinalgUtils.py,sha256=CXKcuq8WkLiARi9HU7gdfS9CdlWW7rlBKvnMl9gkFuc,2608
 visiongraph/util/LoggingUtils.py,sha256=kT0Z5N3PVpaD0kZmIUT3LIYGRz7xXfawXrwEjwITz2s,539
 visiongraph/util/MathUtils.py,sha256=K0O_ZLD1SJ9mq5q6cdtOxNXDrtjZ0MMnSZpqfB2wNxU,1752
-visiongraph/util/NetworkUtils.py,sha256=JIYgwgTfbPfHmMtLGEZmouWUgRDkafmLJ78NlMb8eIQ,2084
+visiongraph/util/NetworkUtils.py,sha256=yOtRuGaNQR5emjXdJVmvzGKlK17S3Nu8k5krnAEcxsk,2649
 visiongraph/util/OSUtils.py,sha256=B1wbK-iZMKCfVJf1j_0GGAror2VEDto1aikTBnhcEqw,195
 visiongraph/util/OpenVinoUtils.py,sha256=aQNS4nHxi16TZArVqQLZNZMEwOuJR-lUEGBwT8zr6Es,704
 visiongraph/util/ResultUtils.py,sha256=ow5p1FtfIIuXo7VJMBQz0cuaG0nfZr0NO9LnTJrWgH8,1240
 visiongraph/util/TimeUtils.py,sha256=IVCLc5PtSYhmGB8hp6-oZMceva8Y6uU0K_1BDTZXGmE,2102
 visiongraph/util/VectorUtils.py,sha256=OSjJ1LGBQiEJp-wEocJvLXM1CyS09LesNA-H86Irnjw,3225
 visiongraph/util/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-visiongraph-0.1.47.5.dist-info/METADATA,sha256=FPOqLVk8aQI6CNzUytv8oU3INdqFfBkp5_hn_Sq92ME,11432
-visiongraph-0.1.47.5.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
-visiongraph-0.1.47.5.dist-info/entry_points.txt,sha256=eqGnTHtEVMYwfXVk1Z9MhC8O2N8wuqAbr0lisLmrkxs,20
-visiongraph-0.1.47.5.dist-info/top_level.txt,sha256=rMp8bfRr_CcL2T8juTpUUszIVf1_BFmagl0lhq3L16o,12
-visiongraph-0.1.47.5.dist-info/RECORD,,
+visiongraph-0.1.48.dist-info/METADATA,sha256=S1UIcV9qo5UXgK6BKcdy5hHMvP6enfXjm0svWb4HEho,11430
+visiongraph-0.1.48.dist-info/WHEEL,sha256=pkctZYzUS4AYVn6dJ-7367OJZivF2e8RA9b_ZBjif18,92
+visiongraph-0.1.48.dist-info/entry_points.txt,sha256=eqGnTHtEVMYwfXVk1Z9MhC8O2N8wuqAbr0lisLmrkxs,20
+visiongraph-0.1.48.dist-info/top_level.txt,sha256=rMp8bfRr_CcL2T8juTpUUszIVf1_BFmagl0lhq3L16o,12
+visiongraph-0.1.48.dist-info/RECORD,,
```

